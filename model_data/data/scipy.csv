text
"scipy_basic_functions Basic functions basic.html  Interaction with NumPy  SciPy builds on NumPy, and for all basic array handling needs you can use NumPy functions: Rather than giving a detailed description of each of these functions (which is available in the NumPy Reference Guide or by using the help , info and source commands), this tutorial will discuss some of the more useful commands, which require a little introduction to use to their full potential. To use functions from some of the SciPy modules, you can do: The top level of scipy also contains functions from numpy and numpy.lib.scimath . However, it is better to use them directly from the numpy module instead. Index tricks  There are some class instances that make special use of the slicing functionality to provide efficient means for array construction. This part will discuss the operation of numpy.mgrid , numpy.ogrid , numpy.r_ , and numpy.c_ for quickly constructing arrays. For example, rather than writing something like the following with the r_ command one can enter this as which can ease typing and make for more readable code. Notice how objects are concatenated, and the slicing syntax is (ab)used to construct ranges. The other term that deserves a little explanation is the use of the complex number 10j as the step size in the slicing syntax. This non-standard use allows the number to be interpreted as the number of points to produce in the range rather than as a step size (note we would have used the long integer notation, 10L, but this notation may go away in Python as the integers become unified). This non-standard usage may be unsightly to some, but it gives the user the ability to quickly construct complicated vectors in a very readable fashion. When the number of points is specified in this way, the end- point is inclusive. The “r” stands for row concatenation because if the objects between commas are 2-D arrays, they are stacked by rows (and thus must have commensurate columns). There is an equivalent command c_ that stacks 2-D arrays by columns but works identically to r_ for 1-D arrays. Another very useful class instance which makes use of extended slicing notation is the function mgrid . In the simplest case, this function can be used to construct 1-D ranges as a convenient substitute for arange. It also allows the use of complex-numbers in the step-size to indicate the number of points to place between the (inclusive) end-points. The real purpose of this function however is to produce N, N-D arrays, which provide coordinate arrays for an N-D volume. The easiest way to understand this is with an example of its usage: Having meshed arrays like this is sometimes very useful. However, it is not always needed just to evaluate some N-D function over a grid due to the array-broadcasting rules of NumPy and SciPy. If this is the only purpose for generating a meshgrid, you should instead use the function ogrid which generates an “open” grid using newaxis judiciously to create N, N-D arrays where only one dimension in each array has length greater than 1. This will save memory and create the same result if the only purpose for the meshgrid is to generate sample points for evaluation of an N-D function. Shape manipulation  In this category of functions are routines for squeezing out length- one dimensions from N-D arrays, ensuring that an array is at least 1-, 2-, or 3-D, and stacking (concatenating) arrays by rows, columns, and “pages” (in the third dimension). Routines for splitting arrays (roughly the opposite of stacking arrays) are also available. Polynomials  There are two (interchangeable) ways to deal with 1-D polynomials in SciPy. The first is to use the poly1d class from NumPy. This class accepts coefficients or polynomial roots to initialize a polynomial. The polynomial object can then be manipulated in algebraic expressions, integrated, differentiated, and evaluated. It even prints like a polynomial: The other way to handle polynomials is as an array of coefficients with the first element of the array giving the coefficient of the highest power. There are explicit functions to add, subtract, multiply, divide, integrate, differentiate, and evaluate polynomials represented as sequences of coefficients. Vectorizing functions (vectorize)  One of the features that NumPy provides is a class vectorize to convert an ordinary Python function which accepts scalars and returns scalars into a “vectorized-function” with the same broadcasting rules as other NumPy functions (i.e., the Universal functions, or ufuncs). For example, suppose you have a Python function named defined as: which defines a function of two scalar variables and returns a scalar result. The class vectorize can be used to “vectorize” this function so that returns a function which takes array arguments and returns an array result: This particular function could have been written in vector form without the use of vectorize . However, functions that employ optimization or integration routines can likely only be vectorized using Type handling  Note the difference between numpy.iscomplex / numpy.isreal and numpy.iscomplexobj / numpy.isrealobj . The former command is array-based and returns byte arrays of ones and zeros providing the result of the element-wise test. The latter command is object-based and returns a scalar describing the result of the test on the entire object. Often it is required to get just the real and/or imaginary part of a complex number. While complex numbers and arrays have attributes that return those values, if one is not sure whether or not the object will be complex-valued, it is better to use the functional forms numpy.real and numpy.imag . These functions succeed for anything that can be turned into a NumPy array. Consider also the function numpy.real_if_close which transforms a complex-valued number with a tiny imaginary part into a real number. Occasionally the need to check whether or not a number is a scalar (Python (long)int, Python float, Python complex, or rank-0 array) occurs in coding. This functionality is provided in the convenient function numpy.isscalar which returns a 1 or a 0. Other useful functions  There are also several other useful functions which should be mentioned. For doing phase processing, the functions angle , and unwrap are useful. Also, the linspace and logspace functions return equally spaced samples in a linear or log scale. Finally, it’s useful to be aware of the indexing capabilities of NumPy. Mention should be made of the function select which extends the functionality of where to include multiple conditions and multiple choices. The calling convention is . numpy.select is a vectorized form of the multiple if-statement. It allows rapid construction of a function which returns an array of results based on a list of conditions. Each element of the return array is taken from the array in a corresponding to the first condition in that is true. For example: Some additional useful functions can also be found in the module scipy.special . For example the factorial and comb functions compute and using either exact integer arithmetic (thanks to Python’s Long integer object), or by using floating-point precision and the gamma function. Other useful functions can be found in scipy.misc . For example, two functions are provided that are useful for approximating derivatives of functions using discrete-differences. The function central_diff_weights returns weighting coefficients for an equally-spaced -point approximation to the derivative of order o . These weights must be multiplied by the function corresponding to these points and the results added to obtain the derivative approximation. This function is intended for use when only samples of the function are available. When the function is an object that can be handed to a routine and evaluated, the function derivative can be used to automatically evaluate the object at the correct points to obtain an N-point approximation to the o -th derivative at a given point. "
"scipy_compressed_sparse_graph_routines_scipy_sparse_csgraph Compressed Sparse Graph Routines (scipy.sparse.csgraph) csgraph.html  Example: Word Ladders  A Word Ladder is a word game invented by Lewis Carroll, in which players find paths between words by switching one letter at a time. For example, one can link “ape” and “man” in the following way: \[{\rm ape \to apt \to ait \to bit \to big \to bag \to mag \to man}\] Note that each step involves changing just one letter of the word. This is just one possible path from “ape” to “man”, but is it the shortest possible path? If we desire to find the shortest word-ladder path between two given words, the sparse graph submodule can help. First, we need a list of valid words. Many operating systems have such a list built in. For example, on linux, a word list can often be found at one of the following locations: Another easy source for words are the Scrabble word lists available at various sites around the internet (search with your favorite search engine). We’ll first create this list. The system word lists consist of a file with one word per line. The following should be modified to use the particular word list you have available: We want to look at words of length 3, so let’s select just those words of the correct length. We’ll also eliminate words which start with upper-case (proper nouns) or contain non-alphanumeric characters, like apostrophes and hyphens. Finally, we’ll make sure everything is lower-case for comparison later: Now we have a list of 586 valid three-letter words (the exact number may change depending on the particular list used). Each of these words will become a node in our graph, and we will create edges connecting the nodes associated with each pair of words which differs by only one letter. There are efficient ways to do this, and inefficient ways to do this. To do this as efficiently as possible, we’re going to use some sophisticated numpy array manipulation: We have an array where each entry is three unicode characters long. We’d like to find all pairs where exactly one character is different. We’ll start by converting each word to a 3-D vector: Now, we’ll use the Hamming distance between each point to determine which pairs of words are connected. The Hamming distance measures the fraction of entries between two vectors which differ: any two words with a Hamming distance equal to , where is the number of letters, are connected in the word ladder: When comparing the distances, we don’t use an equality because this can be unstable for floating point values. The inequality produces the desired result, as long as no two entries of the word list are identical. Now, that our graph is set up, we’ll use a shortest path search to find the path between any two words in the graph: We need to check that these match, because if the words are not in the list, that will not be the case. Now, all we need is to find the shortest path between these two indices in the graph. We’ll use Dijkstra’s algorithm , because it allows us to find the path for just one node: So we see that the shortest path between “ape” and “man” contains only five steps. We can use the predecessors returned by the algorithm to reconstruct this path: This is three fewer links than our initial example: the path from “ape” to “man” is only five steps. Using other tools in the module, we can answer other questions. For example, are there three-letter words which are not linked in a word ladder? This is a question of connected components in the graph: In this particular sample of three-letter words, there are 15 connected components: that is, 15 distinct sets of words with no paths between the sets. How many words are there in each of these sets? We can learn this from the list of components: There is one large connected set and 14 smaller ones. Let’s look at the words in the smaller ones: These are all the three-letter words which do not connect to others via a word ladder. We might also be curious about which words are maximally separated. Which two words take the most links to connect? We can determine this by computing the matrix of all shortest paths. Note that, by convention, the distance between two non-connected points is reported to be infinity, so we’ll need to remove these before finding the maximum: So, there is at least one pair of words which takes 13 steps to get from one to the other! Let’s determine which these are: We see that there are two pairs of words which are maximally separated from each other: ‘imp’ and ‘ump’ on the one hand, and ‘ohm’ and ‘ohs’ on the other. We can find the connecting list in the same way as above: This gives us the path we desired to see. Word ladders are just one potential application of scipy’s fast graph algorithms for sparse matrices. Graph theory makes appearances in many areas of mathematics, data analysis, and machine learning. The sparse graph tools are flexible enough to handle many of these situations. "
"scipy_file_io_scipy_io File IO (scipy.io) io.html  Arff files ( scipy.io.arff )  loadarff (f) Read an arff file. "
"scipy_file_io_scipy_io File IO (scipy.io) io.html  IDL files  readsav (file_name[, idict, python_dict, …]) Read an IDL .sav file. "
"scipy_file_io_scipy_io File IO (scipy.io) io.html  MATLAB files  loadmat (file_name[, mdict, appendmat]) Load MATLAB file. savemat (file_name, mdict[, appendmat, …]) Save a dictionary of names and arrays into a MATLAB-style .mat file. whosmat (file_name[, appendmat]) List variables inside a MATLAB file. The basic functions  We’ll start by importing scipy.io and calling it for convenience: If you are using IPython, try tab-completing on . Among the many options, you will find: These are the high-level functions you will most likely use when working with MATLAB files. You’ll also find: This is the package from which , , and are imported. Within , you will find the module This module contains the machinery that and use. From time to time you may find yourself re-using this machinery. How do I start?  You may have a file that you want to read into SciPy. Or, you want to pass some variables from SciPy / NumPy into MATLAB. To save us using a MATLAB license, let’s start in Octave . Octave has MATLAB-compatible save and load functions. Start Octave ( at the command line for me): Now, to Python: Now let’s try the other way round: Then back to Octave: If you want to inspect the contents of a MATLAB file without reading the data into memory, use the command: returns a list of tuples, one for each array (or other object) in the file. Each tuple contains the name, shape and data type of the array. MATLAB structs  MATLAB structs are a little bit like Python dicts, except the field names must be strings. Any MATLAB object can be a value of a field. As for all objects in MATLAB, structs are, in fact, arrays of structs, where a single struct is an array of shape (1, 1). We can load this in Python: In the SciPy versions from 0.12.0, MATLAB structs come back as NumPy structured arrays, with fields named for the struct fields. You can see the field names in the output above. Note also: and: So, in MATLAB, the struct array must be at least 2-D, and we replicate that when we read into SciPy. If you want all length 1 dimensions squeezed out, try this: Sometimes, it’s more convenient to load the MATLAB structs as Python objects rather than NumPy structured arrays - it can make the access syntax in Python a bit more similar to that in MATLAB. In order to do this, use the parameter setting to . works nicely with : Saving struct arrays can be done in various ways. One simple method is to use dicts: loaded as: You can also save structs back again to MATLAB (or Octave in our case) like this: MATLAB cell arrays  Cell arrays in MATLAB are rather like Python lists, in the sense that the elements in the arrays can contain any type of MATLAB object. In fact, they are most similar to NumPy object arrays, and that is how we load them into NumPy. Back to Python: Saving to a MATLAB cell array just involves making a NumPy object array: "
"scipy_file_io_scipy_io File IO (scipy.io) io.html  Matrix Market files  mminfo (source) Return size and storage parameters from Matrix Market file-like ‘source’. mmread (source) Reads the contents of a Matrix Market file-like ‘source’ into a matrix. mmwrite (target, a[, comment, field, …]) Writes the sparse or dense array a to Matrix Market file-like target . "
"scipy_file_io_scipy_io File IO (scipy.io) io.html  Netcdf  netcdf_file (filename[, mode, mmap, version, …]) A file object for NetCDF data. Allows reading of NetCDF files (version of pupynere package) "
"scipy_file_io_scipy_io File IO (scipy.io) io.html  Wav sound files ( scipy.io.wavfile )  read (filename[, mmap]) Open a WAV file write (filename, rate, data) Write a NumPy array as a WAV file. "
"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  Discrete Cosine Transforms  SciPy provides a DCT with the function dct and a corresponding IDCT with the function idct . There are 8 types of the DCT [WPC] , [Mak] ; however, only the first 3 types are implemented in scipy. “The” DCT generally refers to DCT type 2, and “the” Inverse DCT generally refers to DCT type 3. In addition, the DCT coefficients can be normalized differently (for most types, scipy provides and ). Two parameters of the dct/idct function calls allow setting the DCT type and coefficient normalization. For a single dimension array x, dct(x, norm’ortho’) is equal to MATLAB dct(x). Type I DCT  SciPy uses the following definition of the unnormalized DCT-I ( ): \[y[k]  x_0 + (-1)^k x_{N-1} + 2\sum_{n1}^{N-2} x[n] \cos\left(\frac{\pi nk}{N-1}\right), \qquad 0 \le k < N.\] Note that the DCT-I is only supported for input size > 1. Type II DCT  SciPy uses the following definition of the unnormalized DCT-II ( ): \[y[k]  2 \sum_{n0}^{N-1} x[n] \cos \left({\pi(2n+1)k \over 2N} \right) \qquad 0 \le k < N.\] In case of the normalized DCT ( ), the DCT coefficients are multiplied by a scaling factor f : \[\begin{split}f  \begin{cases} \sqrt{1/(4N)}, & \text{if $k  0$} \\ \sqrt{1/(2N)}, & \text{otherwise} \end{cases} \, .\end{split}\] In this case, the DCT “base functions” become orthonormal: \[\sum_{n0}^{N-1} \phi_k[n] \phi_l[n]  \delta_{lk}.\] Type III DCT  SciPy uses the following definition of the unnormalized DCT-III ( ): \[y[k]  x_0 + 2 \sum_{n1}^{N-1} x[n] \cos\left({\pi n(2k+1) \over 2N}\right) \qquad 0 \le k < N,\] or, for : \[y[k]  {x_0\over\sqrt{N}} + {2\over\sqrt{N}} \sum_{n1}^{N-1} x[n] \cos\left({\pi n(2k+1) \over 2N}\right) \qquad 0 \le k < N.\] DCT and IDCT  The (unnormalized) DCT-III is the inverse of the (unnormalized) DCT-II, up to a factor of 2N . The orthonormalized DCT-III is exactly the inverse of the orthonormalized DCT- II. The function idct performs the mappings between the DCT and IDCT types, as well as the correct normalization. The following example shows the relation between DCT and IDCT for different types and normalizations. The DCT-II and DCT-III are each other’s inverses, so for an orthonormal transform we return back to the original signal. Doing the same under default normalization, however, we pick up an extra scaling factor of since the forward transform is unnormalized. For this reason, we should use the function idct using the same type for both, giving a correctly normalized result. Analogous results can be seen for the DCT-I, which is its own inverse up to a factor of . And for the DCT-IV, which is also its own inverse up to a factor of . Example  The DCT exhibits the “energy compaction property”, meaning that for many signals only the first few DCT coefficients have significant magnitude. Zeroing out the other coefficients leads to a small reconstruction error, a fact which is exploited in lossy signal compression (e.g. JPEG compression). The example below shows a signal x and two reconstructions ( and ) from the signal’s DCT coefficients. The signal is reconstructed from the first 20 DCT coefficients, is reconstructed from the first 15 DCT coefficients. It can be seen that the relative error of using 20 coefficients is still very small (~0.1%), but provides a five-fold compression rate. "
"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  Discrete Sine Transforms  SciPy provides a DST [Mak] with the function dst and a corresponding IDST with the function idst . There are, theoretically, 8 types of the DST for different combinations of even/odd boundary conditions and boundary off sets [WPS] , only the first 3 types are implemented in scipy. Type I DST  DST-I assumes the input is odd around n-1 and nN. SciPy uses the following definition of the unnormalized DST-I ( ): \[y[k]  2\sum_{n0}^{N-1} x[n] \sin\left( \pi {(n+1) (k+1)}\over{N+1} \right), \qquad 0 \le k < N.\] Note also that the DST-I is only supported for input size > 1. The (unnormalized) DST-I is its own inverse, up to a factor of 2(N+1) . Type II DST  DST-II assumes the input is odd around n-1/2 and even around nN. SciPy uses the following definition of the unnormalized DST-II ( ): \[y[k]  2 \sum_{n0}^{N-1} x[n] \sin\left( {\pi (n+1/2)(k+1)} \over N \right), \qquad 0 \le k < N.\] Type III DST  DST-III assumes the input is odd around n-1 and even around nN-1. SciPy uses the following definition of the unnormalized DST-III ( ): \[y[k]  (-1)^k x[N-1] + 2 \sum_{n0}^{N-2} x[n] \sin \left( {\pi (n+1)(k+1/2)} \over N \right), \qquad 0 \le k < N.\] DST and IDST  The following example shows the relation between DST and IDST for different types and normalizations. The DST-II and DST-III are each other’s inverses, so for an orthonormal transform we return back to the original signal. Doing the same under default normalization, however, we pick up an extra scaling factor of since the forward transform is unnormalized. For this reason, we should use the function idst using the same type for both, giving a correctly normalized result. Analogous results can be seen for the DST-I, which is its own inverse up to a factor of . And for the DST-IV, which is also its own inverse up to a factor of . "
"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  Fast Fourier transforms  1-D discrete Fourier transforms  The FFT y[k] of length of the length- sequence x[n] is defined as \[y[k]  \sum_{n0}^{N-1} e^{-2 \pi j \frac{k n}{N} } x[n] \, ,\] and the inverse transform is defined as follows \[x[n]  \frac{1}{N} \sum_{k0}^{N-1} e^{2 \pi j \frac{k n}{N} } y[k] \, .\] These transforms can be calculated by means of fft and ifft , respectively, as shown in the following example. From the definition of the FFT it can be seen that \[y[0]  \sum_{n0}^{N-1} x[n] \, .\] In the example which corresponds to . For N even, the elements contain the positive-frequency terms, and the elements contain the negative-frequency terms, in order of decreasingly negative frequency. For N odd, the elements contain the positive-frequency terms, and the elements contain the negative-frequency terms, in order of decreasingly negative frequency. In case the sequence x is real-valued, the values of for positive frequencies is the conjugate of the values for negative frequencies (because the spectrum is symmetric). Typically, only the FFT corresponding to positive frequencies is plotted. The example plots the FFT of the sum of two sines. The FFT input signal is inherently truncated. This truncation can be modeled as multiplication of an infinite signal with a rectangular window function. In the spectral domain this multiplication becomes convolution of the signal spectrum with the window function spectrum, being of form . This convolution is the cause of an effect called spectral leakage (see [WPW] ). Windowing the signal with a dedicated window function helps mitigate spectral leakage. The example below uses a Blackman window from scipy.signal and shows the effect of windowing (the zero component of the FFT has been truncated for illustrative purposes). In case the sequence x is complex-valued, the spectrum is no longer symmetric. To simplify working with the FFT functions, scipy provides the following two helper functions. The function fftfreq returns the FFT sample frequency points. In a similar spirit, the function fftshift allows swapping the lower and upper halves of a vector, so that it becomes suitable for display. The example below plots the FFT of two complex exponentials; note the asymmetric spectrum. The function rfft calculates the FFT of a real sequence and outputs the complex FFT coefficients for only half of the frequency range. The remaining negative frequency components are implied by the Hermitian symmetry of the FFT for a real input ( ). In case of N being even: ; in case of N being odd . The terms shown explicitly as are restricted to be purely real since, by the hermitian property, they are their own complex conjugate. The corresponding function irfft calculates the IFFT of the FFT coefficients with this special ordering. Notice that the rfft of odd and even length signals are of the same shape. By default, irfft assumes the output signal should be of even length. And so, for odd signals, it will give the wrong result: To recover the original odd-length signal, we must pass the output shape by the n parameter. 2- and N-D discrete Fourier transforms  The functions fft2 and ifft2 provide 2-D FFT and IFFT, respectively. Similarly, fftn and ifftn provide N-D FFT, and IFFT, respectively. For real-input signals, similarly to rfft , we have the functions rfft2 and irfft2 for 2-D real transforms; rfftn and irfftn for N-D real transforms. The example below demonstrates a 2-D IFFT and plots the resulting (2-D) time-domain signals. "
"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  References  CT65 Cooley, James W., and John W. Tukey, 1965, “An algorithm for the machine calculation of complex Fourier series,” Math. Comput. 19: 297-301. NR07 Press, W., Teukolsky, S., Vetterline, W.T., and Flannery, B.P., 2007, Numerical Recipes: The Art of Scientific Computing , ch. 12-13. Cambridge Univ. Press, Cambridge, UK. Mak ( 1 , 2 ) J. Makhoul, 1980, ‘A Fast Cosine Transform in One and Two Dimensions’, IEEE Transactions on acoustics, speech and signal processing vol. 28(1), pp. 27-34, DOI:10.1109/TASSP.1980.1163351 WPW https://en.wikipedia.org/wiki/Window_function WPC https://en.wikipedia.org/wiki/Discrete_cosine_transform WPS https://en.wikipedia.org/wiki/Discrete_sine_transform "
"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Faster integration using low-level callback functions  A user desiring reduced integration times may pass a C function pointer through scipy.LowLevelCallable to quad , dblquad , tplquad or nquad and it will be integrated and return a result in Python. The performance increase here arises from two factors. The primary improvement is faster function evaluation, which is provided by compilation of the function itself. Additionally we have a speedup provided by the removal of function calls between C and Python in quad . This method may provide a speed improvements of ~2x for trivial functions such as sine but can produce a much more noticeable improvements (10x+) for more complex functions. This feature then, is geared towards a user with numerically intensive integrations willing to write a little C to reduce computation time significantly. The approach can be used, for example, via ctypes in a few simple steps: 1.) Write an integrand function in C with the function signature , where is an array containing the point the function f is evaluated at, and to arbitrary additional data you want to provide. 2.) Now compile this file to a shared/dynamic library (a quick search will help with this as it is OS-dependent). The user must link any math libraries, etc., used. On linux this looks like: The output library will be referred to as , but it may have a different file extension. A library has now been created that can be loaded into Python with ctypes . 3.) Load shared library into Python using ctypes and set and - this allows SciPy to interpret the function correctly: The last in the function is optional and can be omitted (both in the C function and ctypes argtypes) if not needed. Note that the coordinates are passed in as an array of doubles rather than a separate argument. 4.) Now integrate the library function as normally, here using nquad : The Python tuple is returned as expected in a reduced amount of time. All optional parameters can be used with this method including specifying singularities, infinite bounds, etc. "
"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Gaussian quadrature  A few functions are also provided in order to perform simple Gaussian quadrature over a fixed interval. The first is fixed_quad , which performs fixed-order Gaussian quadrature. The second function is quadrature , which performs Gaussian quadrature of multiple orders until the difference in the integral estimate is beneath some tolerance supplied by the user. These functions both use the module , which can calculate the roots and quadrature weights of a large variety of orthogonal polynomials (the polynomials themselves are available as special functions returning instances of the polynomial class — e.g., special.legendre ). "
"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  General integration ( quad )  The function quad is provided to integrate a function of one variable between two points. The points can be ( ) to indicate infinite limits. For example, suppose you wish to integrate a bessel function along the interval \[I\int_{0}^{4.5}J_{2.5}\left(x\right)\, dx.\] This could be computed using quad : The first argument to quad is a “callable” Python object (i.e., a function, method, or class instance). Notice the use of a lambda- function in this case as the argument. The next two arguments are the limits of integration. The return value is a tuple, with the first element holding the estimated value of the integral and the second element holding an upper bound on the error. Notice, that in this case, the true value of this integral is \[I\sqrt{\frac{2}{\pi}}\left(\frac{18}{27}\sqrt{2}\cos\left(4.5\right)-\frac{4}{27}\sqrt{2}\sin\left(4.5\right)+\sqrt{2\pi}\textrm{Si}\left(\frac{3}{\sqrt{\pi}}\right)\right),\] where \[\textrm{Si}\left(x\right)\int_{0}^{x}\sin\left(\frac{\pi}{2}t^{2}\right)\, dt.\] is the Fresnel sine integral. Note that the numerically-computed integral is within of the exact result — well below the reported error bound. If the function to integrate takes additional parameters, they can be provided in the args argument. Suppose that the following integral shall be calculated: \[I(a,b)\int_{0}^{1} ax^2+b \, dx.\] This integral can be evaluated by using the following code: Infinite inputs are also allowed in quad by using as one of the arguments. For example, suppose that a numerical value for the exponential integral: \[E_{n}\left(x\right)\int_{1}^{\infty}\frac{e^{-xt}}{t^{n}}\, dt.\] is desired (and the fact that this integral can be computed as is forgotten). The functionality of the function special.expn can be replicated by defining a new function based on the routine quad : The function which is integrated can even use the quad argument (though the error bound may underestimate the error due to possible numerical error in the integrand from the use of quad ). The integral in this case is \[I_{n}\int_{0}^{\infty}\int_{1}^{\infty}\frac{e^{-xt}}{t^{n}}\, dt\, dx\frac{1}{n}.\] This last example shows that multiple integration can be handled using repeated calls to quad . "
"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  General multiple integration ( dblquad , tplquad , nquad )  The mechanics for double and triple integration have been wrapped up into the functions dblquad and tplquad . These functions take the function to integrate and four, or six arguments, respectively. The limits of all inner integrals need to be defined as functions. An example of using double integration to compute several values of is shown below: As example for non-constant limits consider the integral \[I\int_{y0}^{1/2}\int_{x0}^{1-2y} x y \, dx\, dy\frac{1}{96}.\] This integral can be evaluated using the expression below (Note the use of the non-constant lambda functions for the upper limit of the inner integral): For n-fold integration, scipy provides the function nquad . The integration bounds are an iterable object: either a list of constant bounds, or a list of functions for the non-constant integration bounds. The order of integration (and therefore the bounds) is from the innermost integral to the outermost one. The integral from above \[I_{n}\int_{0}^{\infty}\int_{1}^{\infty}\frac{e^{-xt}}{t^{n}}\, dt\, dx\frac{1}{n}\] can be calculated as Note that the order of arguments for f must match the order of the integration bounds; i.e., the inner integral with respect to is on the interval and the outer integral with respect to is on the interval . Non-constant integration bounds can be treated in a similar manner; the example from above \[I\int_{y0}^{1/2}\int_{x0}^{1-2y} x y \, dx\, dy\frac{1}{96}.\] can be evaluated by means of which is the same result as before. "
"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Integrating using Samples  If the samples are equally-spaced and the number of samples available is for some integer , then Romberg romb integration can be used to obtain high-precision estimates of the integral using the available samples. Romberg integration uses the trapezoid rule at step-sizes related by a power of two and then performs Richardson extrapolation on these estimates to approximate the integral with a higher degree of accuracy. In case of arbitrary spaced samples, the two functions trapz and simps are available. They are using Newton-Coates formulas of order 1 and 2 respectively to perform integration. The trapezoidal rule approximates the function as a straight line between adjacent points, while Simpson’s rule approximates the function between three adjacent points as a parabola. For an odd number of samples that are equally spaced Simpson’s rule is exact if the function is a polynomial of order 3 or less. If the samples are not equally spaced, then the result is exact only if the function is a polynomial of order 2 or less. This corresponds exactly to \[\int_{1}^{4} x^2 \, dx  21,\] whereas integrating the second function does not correspond to \[\int_{1}^{4} x^3 \, dx  63.75\] because the order of the polynomial in f2 is larger than two. "
"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Ordinary differential equations ( solve_ivp )  Integrating a set of ordinary differential equations (ODEs) given initial conditions is another useful example. The function solve_ivp is available in SciPy for integrating a first-order vector differential equation: \[\frac{d\mathbf{y}}{dt}\mathbf{f}\left(\mathbf{y},t\right),\] given initial conditions , where is a length vector and is a mapping from to A higher-order ordinary differential equation can always be reduced to a differential equation of this type by introducing intermediate derivatives into the vector. For example, suppose it is desired to find the solution to the following second-order differential equation: \[\frac{d^{2}w}{dz^{2}}-zw(z)0\] with initial conditions and It is known that the solution to this differential equation with these boundary conditions is the Airy function \[w\textrm{Ai}\left(z\right),\] which gives a means to check the integrator using special.airy . First, convert this ODE into standard form by setting and . Thus, the differential equation becomes \[\begin{split}\frac{d\mathbf{y}}{dt}\left[\begin{array}{c} ty_{1}\\ y_{0}\end{array}\right]\left[\begin{array}{cc} 0 & t\\ 1 & 0\end{array}\right]\left[\begin{array}{c} y_{0}\\ y_{1}\end{array}\right]\left[\begin{array}{cc} 0 & t\\ 1 & 0\end{array}\right]\mathbf{y}.\end{split}\] In other words, \[\mathbf{f}\left(\mathbf{y},t\right)\mathbf{A}\left(t\right)\mathbf{y}.\] As an interesting reminder, if commutes with under matrix multiplication, then this linear differential equation has an exact solution using the matrix exponential: \[\mathbf{y}\left(t\right)\exp\left(\int_{0}^{t}\mathbf{A}\left(\tau\right)d\tau\right)\mathbf{y}\left(0\right),\] However, in this case, and its integral do not commute. This differential equation can be solved using the function solve_ivp . It requires the derivative, fprime , the time span [t_start, t_end] and the initial conditions vector, y0 , as input arguments and returns an object whose y field is an array with consecutive solution values as columns. The initial conditions are therefore given in the first output column. As it can be seen solve_ivp determines its time steps automatically if not specified otherwise. To compare the solution of solve_ivp with the airy function the time vector created by solve_ivp is passed to the airy function. The solution of solve_ivp with its standard parameters shows a big deviation to the airy function. To minimize this deviation, relative and absolute tolerances can be used. To specify user defined time points for the solution of solve_ivp , solve_ivp offers two possibilities that can also be used complementarily. By passing the t_eval option to the function call solve_ivp returns the solutions of these time points of t_eval in its output. If the jacobian matrix of function is known, it can be passed to the solve_ivp to achieve better results. Please be aware however that the default integration method RK45 does not support jacobian matrices and thereby another integration method has to be chosen. One of the integration methods that support a jacobian matrix is the for example the Radau method of following example. Solving a system with a banded Jacobian matrix  odeint can be told that the Jacobian is banded . For a large system of differential equations that are known to be stiff, this can improve performance significantly. As an example, we’ll solve the 1-D Gray-Scott partial differential equations using the method of lines [MOL] . The Gray-Scott equations for the functions and on the interval are \[\begin{split}\begin{split} \frac{\partial u}{\partial t}  D_u \frac{\partial^2 u}{\partial x^2} - uv^2 + f(1-u) \\ \frac{\partial v}{\partial t}  D_v \frac{\partial^2 v}{\partial x^2} + uv^2 - (f + k)v \\ \end{split}\end{split}\] where and are the diffusion coefficients of the components and , respectively, and and are constants. (For more information about the system, see http://groups.csail.mit.edu/mac/projects/amorphous/GrayScott/ ) We’ll assume Neumann (i.e., “no flux”) boundary conditions: \[\frac{\partial u}{\partial x}(0,t)  0, \quad \frac{\partial v}{\partial x}(0,t)  0, \quad \frac{\partial u}{\partial x}(L,t)  0, \quad \frac{\partial v}{\partial x}(L,t)  0\] To apply the method of lines, we discretize the variable by defining the uniformly spaced grid of points , with and . We define and , and replace the derivatives with finite differences. That is, \[\frac{\partial^2 u}{\partial x^2}(x_j, t) \rightarrow \frac{u_{j-1}(t) - 2 u_{j}(t) + u_{j+1}(t)}{(\Delta x)^2}\] We then have a system of ordinary differential equations: (1)  \[\begin{split} \begin{split} \frac{du_j}{dt}  \frac{D_u}{(\Delta x)^2} \left(u_{j-1} - 2 u_{j} + u_{j+1}\right) -u_jv_j^2 + f(1 - u_j) \\ \frac{dv_j}{dt}  \frac{D_v}{(\Delta x)^2} \left(v_{j-1} - 2 v_{j} + v_{j+1}\right) + u_jv_j^2 - (f + k)v_j \end{split}\end{split}\] For convenience, the arguments have been dropped. To enforce the boundary conditions, we introduce “ghost” points and , and define , ; and are defined analogously. Then (2)  \[\begin{split} \begin{split} \frac{du_0}{dt}  \frac{D_u}{(\Delta x)^2} \left(2u_{1} - 2 u_{0}\right) -u_0v_0^2 + f(1 - u_0) \\ \frac{dv_0}{dt}  \frac{D_v}{(\Delta x)^2} \left(2v_{1} - 2 v_{0}\right) + u_0v_0^2 - (f + k)v_0 \end{split}\end{split}\] and (3)  \[\begin{split} \begin{split} \frac{du_{N-1}}{dt}  \frac{D_u}{(\Delta x)^2} \left(2u_{N-2} - 2 u_{N-1}\right) -u_{N-1}v_{N-1}^2 + f(1 - u_{N-1}) \\ \frac{dv_{N-1}}{dt}  \frac{D_v}{(\Delta x)^2} \left(2v_{N-2} - 2 v_{N-1}\right) + u_{N-1}v_{N-1}^2 - (f + k)v_{N-1} \end{split}\end{split}\] Our complete system of ordinary differential equations is (1) for , along with (2) and (3) . We can now starting implementing this system in code. We must combine and into a single vector of length . The two obvious choices are and . Mathematically, it does not matter, but the choice affects how efficiently odeint can solve the system. The reason is in how the order affects the pattern of the nonzero elements of the Jacobian matrix. When the variables are ordered as , the pattern of nonzero elements of the Jacobian matrix is \[\begin{split}\begin{smallmatrix} * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\ * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 \\ 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 \\ 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 \\ 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 \\ 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 \\ 0 & 0 & 0 & 0 & 0 & * & * & 0 & 0 & 0 & 0 & 0 & 0 & * \\ * & 0 & 0 & 0 & 0 & 0 & 0 & * & * & 0 & 0 & 0 & 0 & 0 \\ 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 \\ 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 \\ 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 \\ 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 \\ 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * \\ 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & ) & * & * \\ \end{smallmatrix}\end{split}\] The Jacobian pattern with variables interleaved as is \[\begin{split}\begin{smallmatrix} * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * \\ \end{smallmatrix}\end{split}\] In both cases, there are just five nontrivial diagonals, but when the variables are interleaved, the bandwidth is much smaller. That is, the main diagonal and the two diagonals immediately above and the two immediately below the main diagonal are the nonzero diagonals. This is important, because the inputs and of odeint are the upper and lower bandwidths of the Jacobian matrix. When the variables are interleaved, and are 2. When the variables are stacked with following , the upper and lower bandwidths are . With that decision made, we can write the function that implements the system of differential equations. First, we define the functions for the source and reaction terms of the system: Next, we define the function that computes the right-hand side of the system of differential equations: We won’t implement a function to compute the Jacobian, but we will tell odeint that the Jacobian matrix is banded. This allows the underlying solver (LSODA) to avoid computing values that it knows are zero. For a large system, this improves the performance significantly, as demonstrated in the following ipython session. First, we define the required inputs: Time the computation without taking advantage of the banded structure of the Jacobian matrix: Now set and , so odeint knows that the Jacobian matrix is banded: That is quite a bit faster! Let’s ensure that they have computed the same result: References  WPR https://en.wikipedia.org/wiki/Romberg’s_method MOL https://en.wikipedia.org/wiki/Method_of_lines "
"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Romberg Integration  Romberg’s method [WPR] is another method for numerically evaluating an integral. See the help function for romberg for further details. "
"scipy_interpolation_scipy_interpolate Interpolation (scipy.interpolate) interpolate.html  1-D interpolation ( interp1d )  The interp1d class in scipy.interpolate is a convenient method to create a function based on fixed data points, which can be evaluated anywhere within the domain defined by the given data using linear interpolation. An instance of this class is created by passing the 1-D vectors comprising the data. The instance of this class defines a __call__ method and can therefore by treated like a function which interpolates between known data values to obtain unknown values (it also has a docstring for help). Behavior at the boundary can be specified at instantiation time. The following example demonstrates its use, for linear and cubic spline interpolation: Another set of interpolations in interp1d is nearest , previous , and next , where they return the nearest, previous, or next point along the x-axis. Nearest and next can be thought of as a special case of a causal interpolating filter. The following example demonstrates their use, using the same data as in the previous example: "
"scipy_interpolation_scipy_interpolate Interpolation (scipy.interpolate) interpolate.html  Multivariate data interpolation ( griddata )  Suppose you have multidimensional data, for instance, for an underlying function f(x, y) you only know the values at points (x[i], y[i]) that do not form a regular grid. Suppose we want to interpolate the 2-D function on a grid in [0, 1]x[0, 1] but we only know its values at 1000 data points: This can be done with griddata – below, we try out all of the interpolation methods: One can see that the exact result is reproduced by all of the methods to some degree, but for this smooth function the piecewise cubic interpolant gives the best results: "
"scipy_interpolation_scipy_interpolate Interpolation (scipy.interpolate) interpolate.html  Spline interpolation  Spline interpolation in 1-D: Procedural (interpolate.splXXX)  Spline interpolation requires two essential steps: (1) a spline representation of the curve is computed, and (2) the spline is evaluated at the desired points. In order to find the spline representation, there are two different ways to represent a curve and obtain (smoothing) spline coefficients: directly and parametrically. The direct method finds the spline representation of a curve in a 2-D plane using the function splrep . The first two arguments are the only ones required, and these provide the and components of the curve. The normal output is a 3-tuple, , containing the knot-points, , the coefficients and the order of the spline. The default spline order is cubic, but this can be changed with the input keyword, k. For curves in N-D space the function splprep allows defining the curve parametrically. For this function only 1 input argument is required. This input is a list of -arrays representing the curve in N-D space. The length of each array is the number of curve points, and each array provides one component of the N-D data point. The parameter variable is given with the keyword argument, u, , which defaults to an equally-spaced monotonic sequence between and . The default output consists of two objects: a 3-tuple, , containing the spline representation and the parameter variable The keyword argument, s , is used to specify the amount of smoothing to perform during the spline fit. The default value of is where is the number of data-points being fit. Therefore, if no smoothing is desired a value of should be passed to the routines. Once the spline representation of the data has been determined, functions are available for evaluating the spline ( splev ) and its derivatives ( splev , spalde ) at any point and the integral of the spline between any two points ( splint ). In addition, for cubic splines ( ) with 8 or more knots, the roots of the spline can be estimated ( sproot ). These functions are demonstrated in the example that follows. Cubic-spline Derivative of spline Integral of spline Roots of spline Notice that sproot failed to find an obvious solution at the edge of the approximation interval, . If we define the spline on a slightly larger interval, we recover both roots and : Parametric spline Spline interpolation in 1-d: Object-oriented ( UnivariateSpline )  The spline-fitting capabilities described above are also available via an objected-oriented interface. The 1-D splines are objects of the UnivariateSpline class, and are created with the and components of the curve provided as arguments to the constructor. The class defines __call__ , allowing the object to be called with the x-axis values, at which the spline should be evaluated, returning the interpolated y-values. This is shown in the example below for the subclass InterpolatedUnivariateSpline . The integral , derivatives , and roots methods are also available on UnivariateSpline objects, allowing definite integrals, derivatives, and roots to be computed for the spline. The UnivariateSpline class can also be used to smooth data by providing a non-zero value of the smoothing parameter s , with the same meaning as the s keyword of the splrep function described above. This results in a spline that has fewer knots than the number of data points, and hence is no longer strictly an interpolating spline, but rather a smoothing spline. If this is not desired, the InterpolatedUnivariateSpline class is available. It is a subclass of UnivariateSpline that always passes through all points (equivalent to forcing the smoothing parameter to 0). This class is demonstrated in the example below. The LSQUnivariateSpline class is the other subclass of UnivariateSpline . It allows the user to specify the number and location of internal knots explicitly with the parameter t . This allows for the creation of customized splines with non-linear spacing, to interpolate in some domains and smooth in others, or change the character of the spline. InterpolatedUnivariateSpline LSQUnivarateSpline with non-uniform knots 2-D spline representation: Procedural ( bisplrep )  For (smooth) spline-fitting to a 2-D surface, the function bisplrep is available. This function takes as required inputs the 1-D arrays x , y , and z , which represent points on the surface The default output is a list whose entries represent respectively, the components of the knot positions, the coefficients of the spline, and the order of the spline in each coordinate. It is convenient to hold this list in a single object, tck, so that it can be passed easily to the function bisplev . The keyword, s , can be used to change the amount of smoothing performed on the data while determining the appropriate spline. The default value is , where is the number of data points in the x, y, and z vectors. As a result, if no smoothing is desired, then should be passed to bisplrep . To evaluate the 2-D spline and its partial derivatives (up to the order of the spline), the function bisplev is required. This function takes as the first two arguments two 1-D arrays whose cross-product specifies the domain over which to evaluate the spline. The third argument is the tck list returned from bisplrep . If desired, the fourth and fifth arguments provide the orders of the partial derivative in the and direction, respectively. It is important to note that 2-D interpolation should not be used to find the spline representation of images. The algorithm used is not amenable to large numbers of input points. The signal-processing toolbox contains more appropriate algorithms for finding the spline representation of an image. The 2-D interpolation commands are intended for use when interpolating a 2-D function as shown in the example that follows. This example uses the mgrid command in NumPy which is useful for defining a “mesh-grid” in many dimensions. (See also the ogrid command if the full-mesh is not needed). The number of output arguments and the number of dimensions of each argument is determined by the number of indexing objects passed in mgrid . Define function over a sparse 20x20 grid Interpolate function over a new 70x70 grid 2-D spline representation: Object-oriented ( BivariateSpline )  The BivariateSpline class is the 2-D analog of the UnivariateSpline class. It and its subclasses implement the FITPACK functions described above in an object-oriented fashion, allowing objects to be instantiated that can be called to compute the spline value by passing in the two coordinates as the two arguments. "
"scipy_introduction Introduction general.html  Finding Documentation  SciPy and NumPy have documentation versions in both HTML and PDF format available at https://docs.scipy.org/ , that cover nearly all available functionality. However, this documentation is still work-in-progress and some parts may be incomplete or sparse. As we are a volunteer organization and depend on the community for growth, your participation - everything from providing feedback to improving the documentation and code - is welcome and actively encouraged. Python’s documentation strings are used in SciPy for on-line documentation. There are two methods for reading them and getting help. One is Python’s command help in the pydoc module. Entering this command with no arguments (i.e. ) launches an interactive help session that allows searching through the keywords and modules available to all of Python. Secondly, running the command help(obj) with an object as the argument displays that object’s calling signature, and documentation string. The pydoc method of is sophisticated but uses a pager to display the text. Sometimes this can interfere with the terminal within which you are running the interactive session. A numpy/scipy-specific help system is also available under the command . The signature and documentation string for the object passed to the command are printed to standard output (or to a writeable object passed as the third argument). The second keyword argument of defines the maximum width of the line for printing. If a module is passed as the argument to then a list of the functions and classes defined in that module is printed. For example: Another useful command is , which can be used to look at the namespace of a module or package. "
"scipy_introduction Introduction general.html  SciPy Organization  SciPy is organized into subpackages covering different scientific computing domains. These are summarized in the following table: Subpackage Description cluster Clustering algorithms constants Physical and mathematical constants fftpack Fast Fourier Transform routines integrate Integration and ordinary differential equation solvers interpolate Interpolation and smoothing splines io Input and Output linalg Linear algebra ndimage N-dimensional image processing odr Orthogonal distance regression optimize Optimization and root-finding routines signal Signal processing sparse Sparse matrices and associated routines spatial Spatial data structures and algorithms special Special functions stats Statistical distributions and functions SciPy sub-packages need to be imported separately, for example: Because of their ubiquitousness, some of the functions in these subpackages are also made available in the scipy namespace to ease their use in interactive sessions and programs. In addition, many basic array functions from numpy are also available at the top-level of the scipy package. Before looking at the sub-packages individually, we will first look at some of these common functions. "
"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Basic routines  Finding the inverse  The inverse of a matrix is the matrix , such that , where is the identity matrix consisting of ones down the main diagonal. Usually, is denoted . In SciPy, the matrix inverse of the NumPy array, A, is obtained using linalg.inv , or using if is a Matrix. For example, let \[\begin{split}\mathbf{A}  \left[\begin{array}{ccc} 1 & 3 & 5\\ 2 & 5 & 1\\ 2 & 3 & 8\end{array}\right],\end{split}\] then \[\begin{split}\mathbf{A^{-1}}  \frac{1}{25} \left[\begin{array}{ccc} -37 & 9 & 22 \\ 14 & 2 & -9 \\ 4 & -3 & 1 \end{array}\right]  % \left[\begin{array}{ccc} -1.48 & 0.36 & 0.88 \\ 0.56 & 0.08 & -0.36 \\ 0.16 & -0.12 & 0.04 \end{array}\right].\end{split}\] The following example demonstrates this computation in SciPy Solving a linear system  Solving linear systems of equations is straightforward using the scipy command linalg.solve . This command expects an input matrix and a right-hand side vector. The solution vector is then computed. An option for entering a symmetric matrix is offered, which can speed up the processing when applicable. As an example, suppose it is desired to solve the following simultaneous equations: \begin{eqnarray*} x + 3y + 5z &  & 10 \\ 2x + 5y + z &  & 8 \\ 2x + 3y + 8z &  & 3 \end{eqnarray*} We could find the solution vector using a matrix inverse: \[\begin{split}\left[\begin{array}{c} x\\ y\\ z\end{array}\right]\left[\begin{array}{ccc} 1 & 3 & 5\\ 2 & 5 & 1\\ 2 & 3 & 8\end{array}\right]^{-1}\left[\begin{array}{c} 10\\ 8\\ 3\end{array}\right]\frac{1}{25}\left[\begin{array}{c} -232\\ 129\\ 19\end{array}\right]\left[\begin{array}{c} -9.28\\ 5.16\\ 0.76\end{array}\right].\end{split}\] However, it is better to use the linalg.solve command, which can be faster and more numerically stable. In this case, it, however, gives the same answer as shown in the following example: Finding the determinant  The determinant of a square matrix is often denoted and is a quantity often used in linear algebra. Suppose are the elements of the matrix and let be the determinant of the matrix left by removing the row and column from . Then, for any row \[\left|\mathbf{A}\right|\sum_{j}\left(-1\right)^{i+j}a_{ij}M_{ij}.\] This is a recursive way to define the determinant, where the base case is defined by accepting that the determinant of a matrix is the only matrix element. In SciPy the determinant can be calculated with linalg.det . For example, the determinant of \[\begin{split}\mathbf{A}\left[\begin{array}{ccc} 1 & 3 & 5\\ 2 & 5 & 1\\ 2 & 3 & 8\end{array}\right]\end{split}\] is \begin{eqnarray*} \left|\mathbf{A}\right| &  & 1\left|\begin{array}{cc} 5 & 1\\ 3 & 8\end{array}\right|-3\left|\begin{array}{cc} 2 & 1\\ 2 & 8\end{array}\right|+5\left|\begin{array}{cc} 2 & 5\\ 2 & 3\end{array}\right|\\ &  & 1\left(5\cdot8-3\cdot1\right)-3\left(2\cdot8-2\cdot1\right)+5\left(2\cdot3-2\cdot5\right)-25.\end{eqnarray*}. In SciPy, this is computed as shown in this example: Computing norms  Matrix and vector norms can also be computed with SciPy. A wide range of norm definitions are available using different parameters to the order argument of linalg.norm . This function takes a rank-1 (vectors) or a rank-2 (matrices) array and an optional order argument (default is 2). Based on these inputs, a vector or matrix norm of the requested order is computed. For vector x , the order parameter can be any real number including or . The computed norm is \[\begin{split}\left\Vert \mathbf{x}\right\Vert \left\{ \begin{array}{cc} \max\left|x_{i}\right| & \textrm{ord}\textrm{inf}\\ \min\left|x_{i}\right| & \textrm{ord}-\textrm{inf}\\ \left(\sum_{i}\left|x_{i}\right|^{\textrm{ord}}\right)^{1/\textrm{ord}} & \left|\textrm{ord}\right|<\infty.\end{array}\right.\end{split}\] For matrix , the only valid values for norm are inf, and ‘fro’ (or ‘f’) Thus, \[\begin{split}\left\Vert \mathbf{A}\right\Vert \left\{ \begin{array}{cc} \max_{i}\sum_{j}\left|a_{ij}\right| & \textrm{ord}\textrm{inf}\\ \min_{i}\sum_{j}\left|a_{ij}\right| & \textrm{ord}-\textrm{inf}\\ \max_{j}\sum_{i}\left|a_{ij}\right| & \textrm{ord}1\\ \min_{j}\sum_{i}\left|a_{ij}\right| & \textrm{ord}-1\\ \max\sigma_{i} & \textrm{ord}2\\ \min\sigma_{i} & \textrm{ord}-2\\ \sqrt{\textrm{trace}\left(\mathbf{A}^{H}\mathbf{A}\right)} & \textrm{ord}\textrm{'fro'}\end{array}\right.\end{split}\] where are the singular values of . Examples: Solving linear least-squares problems and pseudo-inverses  Linear least-squares problems occur in many branches of applied mathematics. In this problem, a set of linear scaling coefficients is sought that allows a model to fit the data. In particular, it is assumed that data is related to data through a set of coefficients and model functions via the model \[y_{i}\sum_{j}c_{j}f_{j}\left(\mathbf{x}_{i}\right)+\epsilon_{i},\] where represents uncertainty in the data. The strategy of least squares is to pick the coefficients to minimize \[J\left(\mathbf{c}\right)\sum_{i}\left|y_{i}-\sum_{j}c_{j}f_{j}\left(x_{i}\right)\right|^{2}.\] Theoretically, a global minimum will occur when \[\frac{\partial J}{\partial c_{n}^{*}}0\sum_{i}\left(y_{i}-\sum_{j}c_{j}f_{j}\left(x_{i}\right)\right)\left(-f_{n}^{*}\left(x_{i}\right)\right)\] or \begin{eqnarray*} \sum_{j}c_{j}\sum_{i}f_{j}\left(x_{i}\right)f_{n}^{*}\left(x_{i}\right) &  & \sum_{i}y_{i}f_{n}^{*}\left(x_{i}\right)\\ \mathbf{A}^{H}\mathbf{Ac} &  & \mathbf{A}^{H}\mathbf{y}\end{eqnarray*}, where \[\left\{ \mathbf{A}\right\} _{ij}f_{j}\left(x_{i}\right).\] When is invertible, then \[\mathbf{c}\left(\mathbf{A}^{H}\mathbf{A}\right)^{-1}\mathbf{A}^{H}\mathbf{y}\mathbf{A}^{\dagger}\mathbf{y},\] where is called the pseudo-inverse of Notice that using this definition of the model can be written \[\mathbf{y}\mathbf{Ac}+\boldsymbol{\epsilon}.\] The command linalg.lstsq will solve the linear least-squares problem for given and . In addition, linalg.pinv or linalg.pinv2 (uses a different method based on singular value decomposition) will find given The following example and figure demonstrate the use of linalg.lstsq and linalg.pinv for solving a data-fitting problem. The data shown below were generated using the model: \[y_{i}c_{1}e^{-x_{i}}+c_{2}x_{i},\] where for , , and Noise is added to and the coefficients and are estimated using linear least squares. Generalized inverse  The generalized inverse is calculated using the command linalg.pinv or linalg.pinv2 . These two commands differ in how they compute the generalized inverse. The first uses the linalg.lstsq algorithm, while the second uses singular value decomposition. Let be an matrix, then if , the generalized inverse is \[\mathbf{A}^{\dagger}\left(\mathbf{A}^{H}\mathbf{A}\right)^{-1}\mathbf{A}^{H},\] while if matrix, the generalized inverse is \[\mathbf{A}^{\#}\mathbf{A}^{H}\left(\mathbf{A}\mathbf{A}^{H}\right)^{-1}.\] In the case that , then \[\mathbf{A}^{\dagger}\mathbf{A}^{\#}\mathbf{A}^{-1},\] as long as is invertible. "
"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Decompositions  In many applications, it is useful to decompose a matrix using other representations. There are several decompositions supported by SciPy. Eigenvalues and eigenvectors  The eigenvalue-eigenvector problem is one of the most commonly employed linear algebra operations. In one popular form, the eigenvalue-eigenvector problem is to find for some square matrix scalars and corresponding vectors , such that \[\mathbf{Av}\lambda\mathbf{v}.\] For an matrix, there are (not necessarily distinct) eigenvalues — roots of the (characteristic) polynomial \[\left|\mathbf{A}-\lambda\mathbf{I}\right|0.\] The eigenvectors, , are also sometimes called right eigenvectors to distinguish them from another set of left eigenvectors that satisfy \[\mathbf{v}_{L}^{H}\mathbf{A}\lambda\mathbf{v}_{L}^{H}\] or \[\mathbf{A}^{H}\mathbf{v}_{L}\lambda^{*}\mathbf{v}_{L}.\] With its default optional arguments, the command linalg.eig returns and However, it can also return and just by itself ( linalg.eigvals returns just as well). In addition, linalg.eig can also solve the more general eigenvalue problem \begin{eqnarray*} \mathbf{Av} &  & \lambda\mathbf{Bv}\\ \mathbf{A}^{H}\mathbf{v}_{L} &  & \lambda^{*}\mathbf{B}^{H}\mathbf{v}_{L}\end{eqnarray*} for square matrices and The standard eigenvalue problem is an example of the general eigenvalue problem for When a generalized eigenvalue problem can be solved, it provides a decomposition of as \[\mathbf{A}\mathbf{BV}\boldsymbol{\Lambda}\mathbf{V}^{-1},\] where is the collection of eigenvectors into columns and is a diagonal matrix of eigenvalues. By definition, eigenvectors are only defined up to a constant scale factor. In SciPy, the scaling factor for the eigenvectors is chosen so that As an example, consider finding the eigenvalues and eigenvectors of the matrix \[\begin{split}\mathbf{A}\left[\begin{array}{ccc} 1 & 5 & 2\\ 2 & 4 & 1\\ 3 & 6 & 2\end{array}\right].\end{split}\] The characteristic polynomial is \begin{eqnarray*} \left|\mathbf{A}-\lambda\mathbf{I}\right| &  & \left(1-\lambda\right)\left[\left(4-\lambda\right)\left(2-\lambda\right)-6\right]-\\ & & 5\left[2\left(2-\lambda\right)-3\right]+2\left[12-3\left(4-\lambda\right)\right]\\ &  & -\lambda^{3}+7\lambda^{2}+8\lambda-3.\end{eqnarray*} The roots of this polynomial are the eigenvalues of : \begin{eqnarray*} \lambda_{1} &  & 7.9579\\ \lambda_{2} &  & -1.2577\\ \lambda_{3} &  & 0.2997.\end{eqnarray*} The eigenvectors corresponding to each eigenvalue can be found using the original equation. The eigenvectors associated with these eigenvalues can then be found. Singular value decomposition  Singular value decomposition (SVD) can be thought of as an extension of the eigenvalue problem to matrices that are not square. Let be an matrix with and arbitrary. The matrices and are square hermitian matrices 1 of size and , respectively. It is known that the eigenvalues of square hermitian matrices are real and non-negative. In addition, there are at most identical non-zero eigenvalues of and Define these positive eigenvalues as The square-root of these are called singular values of The eigenvectors of are collected by columns into an unitary 2 matrix , while the eigenvectors of are collected by columns in the unitary matrix , the singular values are collected in an zero matrix with main diagonal entries set to the singular values. Then \[\mathbf{AU}\boldsymbol{\Sigma}\mathbf{V}^{H}\] is the singular value decomposition of Every matrix has a singular value decomposition. Sometimes, the singular values are called the spectrum of The command linalg.svd will return , , and as an array of the singular values. To obtain the matrix , use linalg.diagsvd . The following example illustrates the use of linalg.svd : 1 A hermitian matrix satisfies 2 A unitary matrix satisfies so that LU decomposition  The LU decomposition finds a representation for the matrix as \[\mathbf{A}\mathbf{P}\,\mathbf{L}\,\mathbf{U},\] where is an permutation matrix (a permutation of the rows of the identity matrix), is in lower triangular or trapezoidal matrix ( ) with unit-diagonal, and is an upper triangular or trapezoidal matrix. The SciPy command for this decomposition is linalg.lu . Such a decomposition is often useful for solving many simultaneous equations where the left-hand side does not change but the right-hand side does. For example, suppose we are going to solve \[\mathbf{A}\mathbf{x}_{i}\mathbf{b}_{i}\] for many different . The LU decomposition allows this to be written as \[\mathbf{PLUx}_{i}\mathbf{b}_{i}.\] Because is lower-triangular, the equation can be solved for and, finally, very rapidly using forward- and back-substitution. An initial time spent factoring allows for very rapid solution of similar systems of equations in the future. If the intent for performing LU decomposition is for solving linear systems, then the command linalg.lu_factor should be used followed by repeated applications of the command linalg.lu_solve to solve the system for each new right-hand side. Cholesky decomposition  Cholesky decomposition is a special case of LU decomposition applicable to Hermitian positive definite matrices. When and for all , then decompositions of can be found so that \begin{eqnarray*} \mathbf{A} &  & \mathbf{U}^{H}\mathbf{U}\\ \mathbf{A} &  & \mathbf{L}\mathbf{L}^{H}\end{eqnarray*}, where is lower triangular and is upper triangular. Notice that The command linalg.cholesky computes the Cholesky factorization. For using the Cholesky factorization to solve systems of equations, there are also linalg.cho_factor and linalg.cho_solve routines that work similarly to their LU decomposition counterparts. QR decomposition  The QR decomposition (sometimes called a polar decomposition) works for any array and finds an unitary matrix and an upper-trapezoidal matrix , such that \[\mathbf{AQR}.\] Notice that if the SVD of is known, then the QR decomposition can be found. \[\mathbf{A}\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^{H}\mathbf{QR}\] implies that and Note, however, that in SciPy independent algorithms are used to find QR and SVD decompositions. The command for QR decomposition is linalg.qr . Schur decomposition  For a square matrix, , the Schur decomposition finds (not necessarily unique) matrices and , such that \[\mathbf{A}\mathbf{ZT}\mathbf{Z}^{H},\] where is a unitary matrix and is either upper triangular or quasi upper triangular, depending on whether or not a real Schur form or complex Schur form is requested. For a real Schur form both and are real-valued when is real-valued. When is a real-valued matrix, the real Schur form is only quasi upper triangular because blocks extrude from the main diagonal corresponding to any complex-valued eigenvalues. The command linalg.schur finds the Schur decomposition, while the command linalg.rsf2csf converts and from a real Schur form to a complex Schur form. The Schur form is especially useful in calculating functions of matrices. The following example illustrates the Schur decomposition: Interpolative decomposition  scipy.linalg.interpolative contains routines for computing the interpolative decomposition (ID) of a matrix. For a matrix of rank this is a factorization \[A \Pi  \begin{bmatrix} A \Pi_{1} & A \Pi_{2} \end{bmatrix}  A \Pi_{1} \begin{bmatrix} I & T \end{bmatrix},\] where is a permutation matrix with , i.e., . This can equivalently be written as , where and are the skeleton and interpolation matrices , respectively. See also scipy.linalg.interpolative — for more information. "
"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Matrix functions  Consider the function with Taylor series expansion \[f\left(x\right)\sum_{k0}^{\infty}\frac{f^{\left(k\right)}\left(0\right)}{k!}x^{k}.\] A matrix function can be defined using this Taylor series for the square matrix as \[f\left(\mathbf{A}\right)\sum_{k0}^{\infty}\frac{f^{\left(k\right)}\left(0\right)}{k!}\mathbf{A}^{k}.\] While this serves as a useful representation of a matrix function, it is rarely the best way to calculate a matrix function. Exponential and logarithm functions  The matrix exponential is one of the more common matrix functions. The preferred method for implementing the matrix exponential is to use scaling and a Padé approximation for . This algorithm is implemented as linalg.expm . The inverse of the matrix exponential is the matrix logarithm defined as the inverse of the matrix exponential: \[\mathbf{A}\equiv\exp\left(\log\left(\mathbf{A}\right)\right).\] The matrix logarithm can be obtained with linalg.logm . Trigonometric functions  The trigonometric functions, , , and , are implemented for matrices in linalg.sinm , linalg.cosm , and linalg.tanm , respectively. The matrix sine and cosine can be defined using Euler’s identity as \begin{eqnarray*} \sin\left(\mathbf{A}\right) &  & \frac{e^{j\mathbf{A}}-e^{-j\mathbf{A}}}{2j}\\ \cos\left(\mathbf{A}\right) &  & \frac{e^{j\mathbf{A}}+e^{-j\mathbf{A}}}{2}.\end{eqnarray*} The tangent is \[\tan\left(x\right)\frac{\sin\left(x\right)}{\cos\left(x\right)}\left[\cos\left(x\right)\right]^{-1}\sin\left(x\right)\] and so the matrix tangent is defined as \[\left[\cos\left(\mathbf{A}\right)\right]^{-1}\sin\left(\mathbf{A}\right).\] Hyperbolic trigonometric functions  The hyperbolic trigonometric functions, , , and , can also be defined for matrices using the familiar definitions: \begin{eqnarray*} \sinh\left(\mathbf{A}\right) &  & \frac{e^{\mathbf{A}}-e^{-\mathbf{A}}}{2}\\ \cosh\left(\mathbf{A}\right) &  & \frac{e^{\mathbf{A}}+e^{-\mathbf{A}}}{2}\\ \tanh\left(\mathbf{A}\right) &  & \left[\cosh\left(\mathbf{A}\right)\right]^{-1}\sinh\left(\mathbf{A}\right).\end{eqnarray*} These matrix functions can be found using linalg.sinhm , linalg.coshm , and linalg.tanhm . Arbitrary function  Finally, any arbitrary function that takes one complex number and returns a complex number can be called as a matrix function using the command linalg.funm . This command takes the matrix and an arbitrary Python function. It then implements an algorithm from Golub and Van Loan’s book “Matrix Computations” to compute the function applied to the matrix using a Schur decomposition. Note that the function needs to accept complex numbers as input in order to work with this algorithm. For example, the following code computes the zeroth-order Bessel function applied to a matrix. Note how, by virtue of how matrix analytic functions are defined, the Bessel function has acted on the matrix eigenvalues. "
"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  numpy.matrix vs 2-D numpy.ndarray  The classes that represent matrices, and basic operations, such as matrix multiplications and transpose are a part of . For convenience, we summarize the differences between numpy.matrix and numpy.ndarray here. is matrix class that has a more convenient interface than for matrix operations. This class supports, for example, MATLAB-like creation syntax via the semicolon, has matrix multiplication as default for the operator, and contains and members that serve as shortcuts for inverse and transpose: Despite its convenience, the use of the class is discouraged, since it adds nothing that cannot be accomplished with 2-D objects, and may lead to a confusion of which class is being used. For example, the above code can be rewritten as: operations can be applied equally to or to 2D objects. "
"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  scipy.linalg vs numpy.linalg  scipy.linalg contains all the functions in numpy.linalg . plus some other more advanced ones not contained in . Another advantage of using over is that it is always compiled with BLAS/LAPACK support, while for numpy this is optional. Therefore, the scipy version might be faster depending on how numpy was installed. Therefore, unless you don’t want to add as a dependency to your program, use instead of . "
"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Special matrices  SciPy and NumPy provide several functions for creating special matrices that are frequently used in engineering and science. Type Function Description block diagonal scipy.linalg.block_diag Create a block diagonal matrix from the provided arrays. circulant scipy.linalg.circulant Create a circulant matrix. companion scipy.linalg.companion Create a companion matrix. convolution scipy.linalg.convolution_matrix Create a convolution matrix. Discrete Fourier scipy.linalg.dft Create a discrete Fourier transform matrix. Fiedler scipy.linalg.fiedler Create a symmetric Fiedler matrix. Fiedler Companion scipy.linalg.fiedler_companion Create a Fiedler companion matrix. Hadamard scipy.linalg.hadamard Create an Hadamard matrix. Hankel scipy.linalg.hankel Create a Hankel matrix. Helmert scipy.linalg.helmert Create a Helmert matrix. Hilbert scipy.linalg.hilbert Create a Hilbert matrix. Inverse Hilbert scipy.linalg.invhilbert Create the inverse of a Hilbert matrix. Leslie scipy.linalg.leslie Create a Leslie matrix. Pascal scipy.linalg.pascal Create a Pascal matrix. Inverse Pascal scipy.linalg.invpascal Create the inverse of a Pascal matrix. Toeplitz scipy.linalg.toeplitz Create a Toeplitz matrix. Van der Monde numpy.vander Create a Van der Monde matrix. For examples of the use of these functions, see their respective docstrings. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Distance transforms  Distance transforms are used to calculate the minimum distance from each element of an object to the background. The following functions implement distance transforms for three different distance metrics: Euclidean, city block, and chessboard distances. The function distance_transform_cdt uses a chamfer type algorithm to calculate the distance transform of the input, by replacing each object element (defined by values larger than zero) with the shortest distance to the background (all non-object elements). The structure determines the type of chamfering that is done. If the structure is equal to ‘cityblock’, a structure is generated using generate_binary_structure with a squared distance equal to 1. If the structure is equal to ‘chessboard’, a structure is generated using generate_binary_structure with a squared distance equal to the rank of the array. These choices correspond to the common interpretations of the city block and the chessboard distance metrics in two dimensions. In addition to the distance transform, the feature transform can be calculated. In this case, the index of the closest background element is returned along the first axis of the result. The return_distances , and return_indices flags can be used to indicate if the distance transform, the feature transform, or both must be returned. The distances and indices arguments can be used to give optional output arrays that must be of the correct size and type (both ). The basics of the algorithm used to implement this function are described in 2 . The function distance_transform_edt calculates the exact Euclidean distance transform of the input, by replacing each object element (defined by values larger than zero) with the shortest Euclidean distance to the background (all non-object elements). In addition to the distance transform, the feature transform can be calculated. In this case, the index of the closest background element is returned along the first axis of the result. The return_distances and return_indices flags can be used to indicate if the distance transform, the feature transform, or both must be returned. Optionally, the sampling along each axis can be given by the sampling parameter, which should be a sequence of length equal to the input rank, or a single number in which the sampling is assumed to be equal along all axes. The distances and indices arguments can be used to give optional output arrays that must be of the correct size and type ( and ).The algorithm used to implement this function is described in 3 . The function distance_transform_bf uses a brute-force algorithm to calculate the distance transform of the input, by replacing each object element (defined by values larger than zero) with the shortest distance to the background (all non-object elements). The metric must be one of “euclidean”, “cityblock”, or “chessboard”. In addition to the distance transform, the feature transform can be calculated. In this case, the index of the closest background element is returned along the first axis of the result. The return_distances and return_indices flags can be used to indicate if the distance transform, the feature transform, or both must be returned. Optionally, the sampling along each axis can be given by the sampling parameter, which should be a sequence of length equal to the input rank, or a single number in which the sampling is assumed to be equal along all axes. This parameter is only used in the case of the Euclidean distance transform. The distances and indices arguments can be used to give optional output arrays that must be of the correct size and type ( and ). Note This function uses a slow brute-force algorithm, the function distance_transform_cdt can be used to more efficiently calculate city block and chessboard distance transforms. The function distance_transform_edt can be used to more efficiently calculate the exact Euclidean distance transform. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Extending scipy.ndimage in C  A few functions in scipy.ndimage take a callback argument. This can be either a python function or a scipy.LowLevelCallable containing a pointer to a C function. Using a C function will generally be more efficient, since it avoids the overhead of calling a python function on many elements of an array. To use a C function, you must write a C extension that contains the callback function and a Python function that returns a scipy.LowLevelCallable containing a pointer to the callback. An example of a function that supports callbacks is geometric_transform , which accepts a callback function that defines a mapping from all output coordinates to corresponding coordinates in the input array. Consider the following python example, which uses geometric_transform to implement a shift function. We can also implement the callback function with the following C code: More information on writing Python extension modules can be found here . If the C code is in the file , then it can be compiled with the following , and now running the script produces the same result as the original python script. In the C version, is the callback function and the parameters and play the same role as they do in the python version, while and provide the equivalents of and . The variable is passed through instead of . Finally, the C callback function returns an integer status, which is one upon success and zero otherwise. The function wraps the callback function in a PyCapsule . The main steps are: Initialize a PyCapsule . The first argument is a pointer to the callback function. The second argument is the function signature, which must match exactly the one expected by ndimage . Above, we used scipy.LowLevelCallable to specify that we generated with ctypes . A different approach would be to supply the data in the capsule context, that can be set by PyCapsule_SetContext and omit specifying in scipy.LowLevelCallable . However, in this approach we would need to deal with allocation/freeing of the data — freeing the data after the capsule has been destroyed can be done by specifying a non-NULL callback function in the third argument of PyCapsule_New . C callback functions for ndimage all follow this scheme. The next section lists the ndimage functions that accept a C callback function and gives the prototype of the function. See also The functions that support low-level callback arguments are: generic_filter , generic_filter1d , geometric_transform Below, we show alternative ways to write the code, using Numba , Cython , ctypes , or cffi instead of writing wrapper code in C. Numba Numba provides a way to write low-level functions easily in Python. We can write the above using Numba as: Cython Functionally the same code as above can be written in Cython with somewhat less boilerplate as follows: cffi With cffi , you can interface with a C function residing in a shared library (DLL). First, we need to write the shared library, which we do in C — this example is for Linux/OSX: The Python code calling the library is: You can find more information in the cffi documentation. ctypes With ctypes , the C code and the compilation of the so/DLL is as for cffi above. The Python code is different: You can find more information in the ctypes documentation. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Filter functions  The functions described in this section all perform some type of spatial filtering of the input array: the elements in the output are some function of the values in the neighborhood of the corresponding input element. We refer to this neighborhood of elements as the filter kernel, which is often rectangular in shape but may also have an arbitrary footprint. Many of the functions described below allow you to define the footprint of the kernel by passing a mask through the footprint parameter. For example, a cross-shaped kernel can be defined as follows: Usually, the origin of the kernel is at the center calculated by dividing the dimensions of the kernel shape by two. For instance, the origin of a 1-D kernel of length three is at the second element. Take, for example, the correlation of a 1-D array with a filter of length 3 consisting of ones: Sometimes, it is convenient to choose a different origin for the kernel. For this reason, most functions support the origin parameter, which gives the origin of the filter relative to its center. For example: The effect is a shift of the result towards the left. This feature will not be needed very often, but it may be useful, especially for filters that have an even size. A good example is the calculation of backward and forward differences: We could also have calculated the forward difference as follows: However, using the origin parameter instead of a larger kernel is more efficient. For multidimensional kernels, origin can be a number, in which case the origin is assumed to be equal along all axes, or a sequence giving the origin along each axis. Since the output elements are a function of elements in the neighborhood of the input elements, the borders of the array need to be dealt with appropriately by providing the values outside the borders. This is done by assuming that the arrays are extended beyond their boundaries according to certain boundary conditions. In the functions described below, the boundary conditions can be selected using the mode parameter, which must be a string with the name of the boundary condition. The following boundary conditions are currently supported: “nearest” use the value at the boundary [1 2 3]->[1 1 2 3 3] “wrap” periodically replicate the array [1 2 3]->[3 1 2 3 1] “reflect” reflect the array at the boundary [1 2 3]->[1 1 2 3 3] “constant” use a constant value, default is 0.0 [1 2 3]->[0 1 2 3 0] The “constant” mode is special since it needs an additional parameter to specify the constant value that should be used. Note The easiest way to implement such boundary conditions would be to copy the data to a larger array and extend the data at the borders according to the boundary conditions. For large arrays and large filter kernels, this would be very memory consuming, and the functions described below, therefore, use a different approach that does not require allocating large temporary buffers. Correlation and convolution  The correlate1d function calculates a 1-D correlation along the given axis. The lines of the array along the given axis are correlated with the given weights . The weights parameter must be a 1-D sequence of numbers. The function correlate implements multidimensional correlation of the input array with a given kernel. The convolve1d function calculates a 1-D convolution along the given axis. The lines of the array along the given axis are convoluted with the given weights . The weights parameter must be a 1-D sequence of numbers. The function convolve implements multidimensional convolution of the input array with a given kernel. Note A convolution is essentially a correlation after mirroring the kernel. As a result, the origin parameter behaves differently than in the case of a correlation: the results is shifted in the opposite direction. Smoothing filters  The gaussian_filter1d function implements a 1-D Gaussian filter. The standard deviation of the Gaussian filter is passed through the parameter sigma . Setting order  0 corresponds to convolution with a Gaussian kernel. An order of 1, 2, or 3 corresponds to convolution with the first, second, or third derivatives of a Gaussian. Higher-order derivatives are not implemented. The gaussian_filter function implements a multidimensional Gaussian filter. The standard deviations of the Gaussian filter along each axis are passed through the parameter sigma as a sequence or numbers. If sigma is not a sequence but a single number, the standard deviation of the filter is equal along all directions. The order of the filter can be specified separately for each axis. An order of 0 corresponds to convolution with a Gaussian kernel. An order of 1, 2, or 3 corresponds to convolution with the first, second, or third derivatives of a Gaussian. Higher-order derivatives are not implemented. The order parameter must be a number, to specify the same order for all axes, or a sequence of numbers to specify a different order for each axis. Note The multidimensional filter is implemented as a sequence of 1-D Gaussian filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a lower precision, the results may be imprecise because intermediate results may be stored with insufficient precision. This can be prevented by specifying a more precise output type. The uniform_filter1d function calculates a 1-D uniform filter of the given size along the given axis. The uniform_filter implements a multidimensional uniform filter. The sizes of the uniform filter are given for each axis as a sequence of integers by the size parameter. If size is not a sequence, but a single number, the sizes along all axes are assumed to be equal. Note The multidimensional filter is implemented as a sequence of 1-D uniform filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a lower precision, the results may be imprecise because intermediate results may be stored with insufficient precision. This can be prevented by specifying a more precise output type. Filters based on order statistics  The minimum_filter1d function calculates a 1-D minimum filter of the given size along the given axis. The maximum_filter1d function calculates a 1-D maximum filter of the given size along the given axis. The minimum_filter function calculates a multidimensional minimum filter. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The maximum_filter function calculates a multidimensional maximum filter. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The rank_filter function calculates a multidimensional rank filter. The rank may be less then zero, i.e., rank  -1 indicates the largest element. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The percentile_filter function calculates a multidimensional percentile filter. The percentile may be less then zero, i.e., percentile  -20 equals percentile  80. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The median_filter function calculates a multidimensional median filter. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint if provided, must be an array that defines the shape of the kernel by its non-zero elements. Derivatives  Derivative filters can be constructed in several ways. The function gaussian_filter1d , described in Smoothing filters , can be used to calculate derivatives along a given axis using the order parameter. Other derivative filters are the Prewitt and Sobel filters: The prewitt function calculates a derivative along the given axis. The sobel function calculates a derivative along the given axis. The Laplace filter is calculated by the sum of the second derivatives along all axes. Thus, different Laplace filters can be constructed using different second-derivative functions. Therefore, we provide a general function that takes a function argument to calculate the second derivative along a given direction. The function generic_laplace calculates a Laplace filter using the function passed through to calculate second derivatives. The function should have the following signature It should calculate the second derivative along the dimension axis . If output is not , it should use that for the output and return , otherwise it should return the result. mode , cval have the usual meaning. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and a dictionary of named arguments that are passed to at each call. For example To demonstrate the use of the extra_arguments argument, we could do or The following two functions are implemented using generic_laplace by providing appropriate functions for the second-derivative function: The function laplace calculates the Laplace using discrete differentiation for the second derivative (i.e., convolution with ). The function gaussian_laplace calculates the Laplace filter using gaussian_filter to calculate the second derivatives. The standard deviations of the Gaussian filter along each axis are passed through the parameter sigma as a sequence or numbers. If sigma is not a sequence but a single number, the standard deviation of the filter is equal along all directions. The gradient magnitude is defined as the square root of the sum of the squares of the gradients in all directions. Similar to the generic Laplace function, there is a generic_gradient_magnitude function that calculates the gradient magnitude of an array. The function generic_gradient_magnitude calculates a gradient magnitude using the function passed through to calculate first derivatives. The function should have the following signature It should calculate the derivative along the dimension axis . If output is not , it should use that for the output and return , otherwise it should return the result. mode , cval have the usual meaning. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and a dictionary of named arguments that are passed to derivative at each call. For example, the sobel function fits the required signature See the documentation of generic_laplace for examples of using the extra_arguments and extra_keywords arguments. The sobel and prewitt functions fit the required signature and can, therefore, be used directly with generic_gradient_magnitude . The function gaussian_gradient_magnitude calculates the gradient magnitude using gaussian_filter to calculate the first derivatives. The standard deviations of the Gaussian filter along each axis are passed through the parameter sigma as a sequence or numbers. If sigma is not a sequence but a single number, the standard deviation of the filter is equal along all directions. Generic filter functions  To implement filter functions, generic functions can be used that accept a callable object that implements the filtering operation. The iteration over the input and output arrays is handled by these generic functions, along with such details as the implementation of the boundary conditions. Only a callable object implementing a callback function that does the actual filtering work must be provided. The callback function can also be written in C and passed using a PyCapsule (see Extending scipy.ndimage in C for more information). The generic_filter1d function implements a generic 1-D filter function, where the actual filtering operation must be supplied as a python function (or other callable object). The generic_filter1d function iterates over the lines of an array and calls at each line. The arguments that are passed to are 1-D arrays of the type. The first contains the values of the current line. It is extended at the beginning and the end, according to the filter_size and origin arguments. The second array should be modified in-place to provide the output values of the line. For example, consider a correlation along one dimension: The same operation can be implemented using generic_filter1d , as follows: Here, the origin of the kernel was (by default) assumed to be in the middle of the filter of length 3. Therefore, each input line had been extended by one value at the beginning and at the end, before the function was called. Optionally, extra arguments can be defined and passed to the filter function. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and/or a dictionary of named arguments that are passed to derivative at each call. For example, we can pass the parameters of our filter as an argument or The generic_filter function implements a generic filter function, where the actual filtering operation must be supplied as a python function (or other callable object). The generic_filter function iterates over the array and calls at each element. The argument of is a 1-D array of the type that contains the values around the current element that are within the footprint of the filter. The function should return a single value that can be converted to a double precision number. For example, consider a correlation: The same operation can be implemented using generic_filter , as follows: Here, a kernel footprint was specified that contains only two elements. Therefore, the filter function receives a buffer of length equal to two, which was multiplied with the proper weights and the result summed. When calling generic_filter , either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. Optionally, extra arguments can be defined and passed to the filter function. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and/or a dictionary of named arguments that are passed to derivative at each call. For example, we can pass the parameters of our filter as an argument or These functions iterate over the lines or elements starting at the last axis, i.e., the last index changes the fastest. This order of iteration is guaranteed for the case that it is important to adapt the filter depending on spatial location. Here is an example of using a class that implements the filter and keeps track of the current coordinates while iterating. It performs the same filter operation as described above for generic_filter , but additionally prints the current coordinates: For the generic_filter1d function, the same approach works, except that this function does not iterate over the axis that is being filtered. The example for generic_filter1d then becomes this: Fourier domain filters  The functions described in this section perform filtering operations in the Fourier domain. Thus, the input array of such a function should be compatible with an inverse Fourier transform function, such as the functions from the numpy.fft module. We, therefore, have to deal with arrays that may be the result of a real or a complex Fourier transform. In the case of a real Fourier transform, only half of the of the symmetric complex transform is stored. Additionally, it needs to be known what the length of the axis was that was transformed by the real fft. The functions described here provide a parameter n that, in the case of a real transform, must be equal to the length of the real transform axis before transformation. If this parameter is less than zero, it is assumed that the input array was the result of a complex Fourier transform. The parameter axis can be used to indicate along which axis the real transform was executed. The fourier_shift function multiplies the input array with the multidimensional Fourier transform of a shift operation for the given shift. The shift parameter is a sequence of shifts for each dimension or a single value for all dimensions. The fourier_gaussian function multiplies the input array with the multidimensional Fourier transform of a Gaussian filter with given standard deviations sigma . The sigma parameter is a sequence of values for each dimension or a single value for all dimensions. The fourier_uniform function multiplies the input array with the multidimensional Fourier transform of a uniform filter with given sizes size . The size parameter is a sequence of values for each dimension or a single value for all dimensions. The fourier_ellipsoid function multiplies the input array with the multidimensional Fourier transform of an elliptically-shaped filter with given sizes size . The size parameter is a sequence of values for each dimension or a single value for all dimensions. This function is only implemented for dimensions 1, 2, and 3. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Interpolation functions  This section describes various interpolation functions that are based on B-spline theory. A good introduction to B-splines can be found in 1 . Spline pre-filters  Interpolation using splines of an order larger than 1 requires a pre-filtering step. The interpolation functions described in section Interpolation functions apply pre-filtering by calling spline_filter , but they can be instructed not to do this by setting the prefilter keyword equal to False. This is useful if more than one interpolation operation is done on the same array. In this case, it is more efficient to do the pre-filtering only once and use a pre-filtered array as the input of the interpolation functions. The following two functions implement the pre-filtering: The spline_filter1d function calculates a 1-D spline filter along the given axis. An output array can optionally be provided. The order of the spline must be larger than 1 and less than 6. The spline_filter function calculates a multidimensional spline filter. Note The multidimensional filter is implemented as a sequence of 1-D spline filters. The intermediate arrays are stored in the same data type as the output. Therefore, if an output with a limited precision is requested, the results may be imprecise because intermediate results may be stored with insufficient precision. This can be prevented by specifying a output type of high precision. Interpolation functions  The following functions all employ spline interpolation to effect some type of geometric transformation of the input array. This requires a mapping of the output coordinates to the input coordinates, and therefore, the possibility arises that input values outside the boundaries may be needed. This problem is solved in the same way as described in Filter functions for the multidimensional filter functions. Therefore, these functions all support a mode parameter that determines how the boundaries are handled, and a cval parameter that gives a constant value in case that the ‘constant’ mode is used. The geometric_transform function applies an arbitrary geometric transform to the input. The given mapping function is called at each point in the output to find the corresponding coordinates in the input. mapping must be a callable object that accepts a tuple of length equal to the output array rank and returns the corresponding input coordinates as a tuple of length equal to the input array rank. The output shape and output type can optionally be provided. If not given, they are equal to the input shape and type. For example: Optionally, extra arguments can be defined and passed to the filter function. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and/or a dictionary of named arguments that are passed to derivative at each call. For example, we can pass the shifts in our example as arguments or Note The mapping function can also be written in C and passed using a scipy.LowLevelCallable . See Extending scipy.ndimage in C for more information. The function map_coordinates applies an arbitrary coordinate transformation using the given array of coordinates. The shape of the output is derived from that of the coordinate array by dropping the first axis. The parameter coordinates is used to find for each point in the output the corresponding coordinates in the input. The values of coordinates along the first axis are the coordinates in the input array at which the output value is found. (See also the numarray coordinates function.) Since the coordinates may be non- integer coordinates, the value of the input at these coordinates is determined by spline interpolation of the requested order. Here is an example that interpolates a 2D array at and : The affine_transform function applies an affine transformation to the input array. The given transformation matrix and offset are used to find for each point in the output the corresponding coordinates in the input. The value of the input at the calculated coordinates is determined by spline interpolation of the requested order. The transformation matrix must be 2-D or can also be given as a 1-D sequence or array. In the latter case, it is assumed that the matrix is diagonal. A more efficient interpolation algorithm is then applied that exploits the separability of the problem. The output shape and output type can optionally be provided. If not given, they are equal to the input shape and type. The shift function returns a shifted version of the input, using spline interpolation of the requested order . The zoom function returns a rescaled version of the input, using spline interpolation of the requested order . The rotate function returns the input array rotated in the plane defined by the two axes given by the parameter axes , using spline interpolation of the requested order . The angle must be given in degrees. If reshape is true, then the size of the output array is adapted to contain the rotated input. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Introduction  Image processing and analysis are generally seen as operations on 2-D arrays of values. There are, however, a number of fields where images of higher dimensionality must be analyzed. Good examples of these are medical imaging and biological imaging. numpy is suited very well for this type of applications due to its inherent multidimensional nature. The scipy.ndimage packages provides a number of general image processing and analysis functions that are designed to operate with arrays of arbitrary dimensionality. The packages currently includes: functions for linear and non-linear filtering, binary morphology, B-spline interpolation, and object measurements. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Morphology  Binary morphology  The generate_binary_structure functions generates a binary structuring element for use in binary morphology operations. The rank of the structure must be provided. The size of the structure that is returned is equal to three in each direction. The value of each element is equal to one if the square of the Euclidean distance from the element to the center is less than or equal to connectivity . For instance, 2-D 4-connected and 8-connected structures are generated as follows: Most binary morphology functions can be expressed in terms of the basic operations erosion and dilation. The binary_erosion function implements binary erosion of arrays of arbitrary rank with the given structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The border_value parameter gives the value of the array outside boundaries. The erosion is repeated iterations times. If iterations is less than one, the erosion is repeated until the result does not change anymore. If a mask array is given, only those elements with a true value at the corresponding mask element are modified at each iteration. The binary_dilation function implements binary dilation of arrays of arbitrary rank with the given structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The border_value parameter gives the value of the array outside boundaries. The dilation is repeated iterations times. If iterations is less than one, the dilation is repeated until the result does not change anymore. If a mask array is given, only those elements with a true value at the corresponding mask element are modified at each iteration. Here is an example of using binary_dilation to find all elements that touch the border, by repeatedly dilating an empty array from the border using the data array as the mask: The binary_erosion and binary_dilation functions both have an iterations parameter, which allows the erosion or dilation to be repeated a number of times. Repeating an erosion or a dilation with a given structure n times is equivalent to an erosion or a dilation with a structure that is n-1 times dilated with itself. A function is provided that allows the calculation of a structure that is dilated a number of times with itself: The iterate_structure function returns a structure by dilation of the input structure iteration - 1 times with itself. For instance: Other morphology operations can be defined in terms of erosion and dilation. The following functions provide a few of these operations for convenience: The binary_opening function implements binary opening of arrays of arbitrary rank with the given structuring element. Binary opening is equivalent to a binary erosion followed by a binary dilation with the same structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The iterations parameter gives the number of erosions that is performed followed by the same number of dilations. The binary_closing function implements binary closing of arrays of arbitrary rank with the given structuring element. Binary closing is equivalent to a binary dilation followed by a binary erosion with the same structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The iterations parameter gives the number of dilations that is performed followed by the same number of erosions. The binary_fill_holes function is used to close holes in objects in a binary image, where the structure defines the connectivity of the holes. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The binary_hit_or_miss function implements a binary hit-or-miss transform of arrays of arbitrary rank with the given structuring elements. The hit-or-miss transform is calculated by erosion of the input with the first structure, erosion of the logical not of the input with the second structure, followed by the logical and of these two erosions. The origin parameters control the placement of the structuring elements, as described in Filter functions . If origin2 equals , it is set equal to the origin1 parameter. If the first structuring element is not provided, a structuring element with connectivity equal to one is generated using generate_binary_structure . If structure2 is not provided, it is set equal to the logical not of structure1 . Grey-scale morphology  Grey-scale morphology operations are the equivalents of binary morphology operations that operate on arrays with arbitrary values. Below, we describe the grey-scale equivalents of erosion, dilation, opening and closing. These operations are implemented in a similar fashion as the filters described in Filter functions , and we refer to this section for the description of filter kernels and footprints, and the handling of array borders. The grey-scale morphology operations optionally take a structure parameter that gives the values of the structuring element. If this parameter is not given, the structuring element is assumed to be flat with a value equal to zero. The shape of the structure can optionally be defined by the footprint parameter. If this parameter is not given, the structure is assumed to be rectangular, with sizes equal to the dimensions of the structure array, or by the size parameter if structure is not given. The size parameter is only used if both structure and footprint are not given, in which case the structuring element is assumed to be rectangular and flat with the dimensions given by size . The size parameter, if provided, must be a sequence of sizes or a single number in which case the size of the filter is assumed to be equal along each axis. The footprint parameter, if provided, must be an array that defines the shape of the kernel by its non-zero elements. Similarly to binary erosion and dilation, there are operations for grey-scale erosion and dilation: The grey_erosion function calculates a multidimensional grey-scale erosion. The grey_dilation function calculates a multidimensional grey-scale dilation. Grey-scale opening and closing operations can be defined similarly to their binary counterparts: The grey_opening function implements grey-scale opening of arrays of arbitrary rank. Grey-scale opening is equivalent to a grey-scale erosion followed by a grey-scale dilation. The grey_closing function implements grey-scale closing of arrays of arbitrary rank. Grey-scale opening is equivalent to a grey-scale dilation followed by a grey-scale erosion. The morphological_gradient function implements a grey-scale morphological gradient of arrays of arbitrary rank. The grey-scale morphological gradient is equal to the difference of a grey-scale dilation and a grey-scale erosion. The morphological_laplace function implements a grey-scale morphological laplace of arrays of arbitrary rank. The grey-scale morphological laplace is equal to the sum of a grey-scale dilation and a grey-scale erosion minus twice the input. The white_tophat function implements a white top-hat filter of arrays of arbitrary rank. The white top-hat is equal to the difference of the input and a grey-scale opening. The black_tophat function implements a black top-hat filter of arrays of arbitrary rank. The black top-hat is equal to the difference of a grey-scale closing and the input. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Object measurements  Given an array of labeled objects, the properties of the individual objects can be measured. The find_objects function can be used to generate a list of slices that for each object, give the smallest sub-array that fully contains the object: The find_objects function finds all objects in a labeled array and returns a list of slices that correspond to the smallest regions in the array that contains the object. For instance: The function find_objects returns slices for all objects, unless the max_label parameter is larger then zero, in which case only the first max_label objects are returned. If an index is missing in the label array, is return instead of a slice. For example: The list of slices generated by find_objects is useful to find the position and dimensions of the objects in the array, but can also be used to perform measurements on the individual objects. Say, we want to find the sum of the intensities of an object in image: Then we can calculate the sum of the elements in the second object: That is, however, not particularly efficient and may also be more complicated for other types of measurements. Therefore, a few measurements functions are defined that accept the array of object labels and the index of the object to be measured. For instance, calculating the sum of the intensities can be done by: For large arrays and small objects, it is more efficient to call the measurement functions after slicing the array: Alternatively, we can do the measurements for a number of labels with a single function call, returning a list of results. For instance, to measure the sum of the values of the background and the second object in our example, we give a list of labels: The measurement functions described below all support the index parameter to indicate which object(s) should be measured. The default value of index is . This indicates that all elements where the label is larger than zero should be treated as a single object and measured. Thus, in this case the labels array is treated as a mask defined by the elements that are larger than zero. If index is a number or a sequence of numbers it gives the labels of the objects that are measured. If index is a sequence, a list of the results is returned. Functions that return more than one result return their result as a tuple if index is a single number, or as a tuple of lists if index is a sequence. The sum function calculates the sum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The mean function calculates the mean of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The variance function calculates the variance of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The standard_deviation function calculates the standard deviation of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The minimum function calculates the minimum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The maximum function calculates the maximum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The minimum_position function calculates the position of the minimum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The maximum_position function calculates the position of the maximum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The extrema function calculates the minimum, the maximum, and their positions, of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The result is a tuple giving the minimum, the maximum, the position of the minimum, and the position of the maximum. The result is the same as a tuple formed by the results of the functions minimum , maximum , minimum_position , and maximum_position that are described above. The center_of_mass function calculates the center of mass of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The histogram function calculates a histogram of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. Histograms are defined by their minimum ( min ), maximum ( max ), and the number of bins ( bins ). They are returned as 1-D arrays of type . "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Properties shared by all functions  All functions share some common properties. Notably, all functions allow the specification of an output array with the output argument. With this argument, you can specify an array that will be changed in-place with the result with the operation. In this case, the result is not returned. Usually, using the output argument is more efficient, since an existing array is used to store the result. The type of arrays returned is dependent on the type of operation, but it is, in most cases, equal to the type of the input. If, however, the output argument is used, the type of the result is equal to the type of the specified output argument. If no output argument is given, it is still possible to specify what the result of the output should be. This is done by simply assigning the desired numpy type object to the output argument. For example: "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  References  1 M. Unser, “Splines: A Perfect Fit for Signal and Image Processing,” IEEE Signal Processing Magazine, vol. 16, no. 6, pp. 22-38, November 1999. 2 G. Borgefors, “Distance transformations in arbitrary dimensions.”, Computer Vision, Graphics, and Image Processing, 27:321-345, 1984. 3 C. R. Maurer, Jr., R. Qi, and V. Raghavan, “A linear time algorithm for computing exact euclidean distance transforms of binary images in arbitrary dimensions. IEEE Trans. PAMI 25, 265-270, 2003. 4 P. Felkel, R. Wegenkittl, and M. Bruckschwaiger, “Implementation and Complexity of the Watershed-from-Markers Algorithm Computed as a Minimal Cost Forest.”, Eurographics 2001, pp. C:26-35. "
"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Segmentation and labeling  Segmentation is the process of separating objects of interest from the background. The most simple approach is, probably, intensity thresholding, which is easily done with numpy functions: The result is a binary image, in which the individual objects still need to be identified and labeled. The function label generates an array where each object is assigned a unique number: The label function generates an array where the objects in the input are labeled with an integer index. It returns a tuple consisting of the array of object labels and the number of objects found, unless the output parameter is given, in which case only the number of objects is returned. The connectivity of the objects is defined by a structuring element. For instance, in 2D using a 4-connected structuring element gives: These two objects are not connected because there is no way in which we can place the structuring element, such that it overlaps with both objects. However, an 8-connected structuring element results in only a single object: If no structuring element is provided, one is generated by calling generate_binary_structure (see Binary morphology ) using a connectivity of one (which in 2D is the 4-connected structure of the first example). The input can be of any type, any value not equal to zero is taken to be part of an object. This is useful if you need to ‘re-label’ an array of object indices, for instance, after removing unwanted objects. Just apply the label function again to the index array. For instance: Note The structuring element used by label is assumed to be symmetric. There is a large number of other approaches for segmentation, for instance, from an estimation of the borders of the objects that can be obtained by derivative filters. One such approach is watershed segmentation. The function watershed_ift generates an array where each object is assigned a unique label, from an array that localizes the object borders, generated, for instance, by a gradient magnitude filter. It uses an array containing initial markers for the objects: The watershed_ift function applies a watershed from markers algorithm, using an Iterative Forest Transform, as described in 4 . The inputs of this function are the array to which the transform is applied, and an array of markers that designate the objects by a unique label, where any non-zero value is a marker. For instance: Here, two markers were used to designate an object ( marker  2) and the background ( marker  1). The order in which these are processed is arbitrary: moving the marker for the background to the lower-right corner of the array yields a different result: The result is that the object ( marker  2) is smaller because the second marker was processed earlier. This may not be the desired effect if the first marker was supposed to designate a background object. Therefore, watershed_ift treats markers with a negative value explicitly as background markers and processes them after the normal markers. For instance, replacing the first marker by a negative marker gives a result similar to the first example: The connectivity of the objects is defined by a structuring element. If no structuring element is provided, one is generated by calling generate_binary_structure (see Binary morphology ) using a connectivity of one (which in 2D is a 4-connected structure.) For example, using an 8-connected structure with the last example yields a different object: Note The implementation of watershed_ift limits the data types of the input to and . "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Constrained minimization of multivariate scalar functions ( minimize )  The minimize function provides algorithms for constrained minimization, namely , and . They require the constraints to be defined using slightly different structures. The method requires the constraints to be defined as a sequence of objects LinearConstraint and NonlinearConstraint . Methods and , on the other hand, require constraints to be defined as a sequence of dictionaries, with keys , and . As an example let us consider the constrained minimization of the Rosenbrock function: \begin{eqnarray*} \min_{x_0, x_1} & ~~100\left(x_{1}-x_{0}^{2}\right)^{2}+\left(1-x_{0}\right)^{2} &\\ \text{subject to: } & x_0 + 2 x_1 \leq 1 & \\ & x_0^2 + x_1 \leq 1 & \\ & x_0^2 - x_1 \leq 1 & \\ & 2 x_0 + x_1  1 & \\ & 0 \leq x_0 \leq 1 & \\ & -0.5 \leq x_1 \leq 2.0. & \end{eqnarray*} This optimization problem has the unique solution , for which only the first and fourth constraints are active. Trust-Region Constrained Algorithm ( )  The trust-region constrained method deals with constrained minimization problems of the form: \begin{eqnarray*} \min_x & f(x) & \\ \text{subject to: } & ~~~ c^l \leq c(x) \leq c^u, &\\ & x^l \leq x \leq x^u. & \end{eqnarray*} When the method reads the -th constraint as an equality constraint and deals with it accordingly. Besides that, one-sided constraint can be specified by setting the upper or lower bound to with the appropriate sign. The implementation is based on [EQSQP] for equality-constraint problems and on [TRIP] for problems with inequality constraints. Both are trust-region type algorithms suitable for large-scale problems. Defining Bounds Constraints:  The bound constraints and are defined using a Bounds object. Defining Linear Constraints:  The constraints and can be written in the linear constraint standard format: \begin{equation*} \begin{bmatrix}-\infty \\1\end{bmatrix} \leq \begin{bmatrix} 1& 2 \\ 2& 1\end{bmatrix} \begin{bmatrix} x_0 \\x_1\end{bmatrix} \leq \begin{bmatrix} 1 \\ 1\end{bmatrix},\end{equation*} and defined using a LinearConstraint object. Defining Nonlinear Constraints:  The nonlinear constraint: \begin{equation*} c(x)  \begin{bmatrix} x_0^2 + x_1 \\ x_0^2 - x_1\end{bmatrix} \leq \begin{bmatrix} 1 \\ 1\end{bmatrix}, \end{equation*} with Jacobian matrix: \begin{equation*} J(x)  \begin{bmatrix} 2x_0 & 1 \\ 2x_0 & -1\end{bmatrix},\end{equation*} and linear combination of the Hessians: \begin{equation*} H(x, v)  \sum_{i0}^1 v_i \nabla^2 c_i(x)  v_0\begin{bmatrix} 2 & 0 \\ 0 & 0\end{bmatrix} + v_1\begin{bmatrix} 2 & 0 \\ 0 & 0\end{bmatrix}, \end{equation*} is defined using a NonlinearConstraint object. Alternatively, it is also possible to define the Hessian as a sparse matrix, or as a LinearOperator object. When the evaluation of the Hessian is difficult to implement or computationally infeasible, one may use HessianUpdateStrategy . Currently available strategies are BFGS and SR1 . Alternatively, the Hessian may be approximated using finite differences. The Jacobian of the constraints can be approximated by finite differences as well. In this case, however, the Hessian cannot be computed with finite differences and needs to be provided by the user or defined using HessianUpdateStrategy . Solving the Optimization Problem:  The optimization problem is solved using: When needed, the objective function Hessian can be defined using a LinearOperator object, or a Hessian-vector product through the parameter . Alternatively, the first and second derivatives of the objective function can be approximated. For instance, the Hessian can be approximated with SR1 quasi-Newton approximation and the gradient with finite differences. TRIP Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999. An interior point algorithm for large-scale nonlinear programming. SIAM Journal on Optimization 9.4: 877-900. EQSQP Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the implementation of an algorithm for large-scale equality constrained optimization. SIAM Journal on Optimization 8.3: 682-706. Sequential Least SQuares Programming (SLSQP) Algorithm ( )  The SLSQP method deals with constrained minimization problems of the form: \begin{eqnarray*} \min_x & f(x) \\ \text{subject to: } & c_j(x)  0 , &j \in \mathcal{E}\\ & c_j(x) \geq 0 , &j \in \mathcal{I}\\ & \text{lb}_i \leq x_i \leq \text{ub}_i , &i  1,...,N. \end{eqnarray*} Where or are sets of indices containing equality and inequality constraints. Both linear and nonlinear constraints are defined as dictionaries with keys , and . And the optimization problem is solved with: Most of the options available for the method are not available for . "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Custom minimizers  Sometimes, it may be useful to use a custom method as a (multivariate or univariate) minimizer, for example, when using some library wrappers of minimize (e.g., basinhopping ). We can achieve that by, instead of passing a method name, passing a callable (either a function or an object implementing a __call__ method) as the method parameter. Let us consider an (admittedly rather virtual) need to use a trivial custom multivariate minimization method that will just search the neighborhood in each dimension independently with a fixed step size: This will work just as well in case of univariate optimization: "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Global optimization  Global optimization aims to find the global minimum of a function within given bounds, in the presence of potentially many local minima. Typically, global minimizers efficiently search the parameter space, while using a local minimizer (e.g., minimize ) under the hood. SciPy contains a number of good global optimizers. Here, we’ll use those on the same objective function, namely the (aptly named) function: This function looks like an egg carton: We now use the global optimizers to obtain the minimum and the function value at the minimum. We’ll store the results in a dictionary so we can compare different optimization results later. All optimizers return an , which in addition to the solution contains information on the number of function evaluations, whether the optimization was successful, and more. For brevity, we won’t show the full output of the other optimizers: shgo has a second method, which returns all local minima rather than only what it thinks is the global minimum: We’ll now plot all found minima on a heatmap of the function: "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Least-squares minimization ( least_squares )  SciPy is capable of solving robustified bound-constrained nonlinear least-squares problems: \begin{align} &\min_\mathbf{x} \frac{1}{2} \sum_{i  1}^m \rho\left(f_i(\mathbf{x})^2\right) \\ &\text{subject to }\mathbf{lb} \leq \mathbf{x} \leq \mathbf{ub} \end{align} Here are smooth functions from to , we refer to them as residuals. The purpose of a scalar-valued function is to reduce the influence of outlier residuals and contribute to robustness of the solution, we refer to it as a loss function. A linear loss function gives a standard least-squares problem. Additionally, constraints in a form of lower and upper bounds on some of are allowed. All methods specific to least-squares minimization utilize a matrix of partial derivatives called Jacobian and defined as . It is highly recommended to compute this matrix analytically and pass it to least_squares , otherwise, it will be estimated by finite differences, which takes a lot of additional time and can be very inaccurate in hard cases. Function least_squares can be used for fitting a function to empirical data . To do this, one should simply precompute residuals as , where are weights assigned to each observation. Example of solving a fitting problem  Here we consider an enzymatic reaction 1 . There are 11 residuals defined as \[f_i(x)  \frac{x_0 (u_i^2 + u_i x_1)}{u_i^2 + u_i x_2 + x_3} - y_i, \quad i  0, \ldots, 10,\] where are measurement values and are values of the independent variable. The unknown vector of parameters is . As was said previously, it is recommended to compute Jacobian matrix in a closed form: \begin{align} &J_{i0}  \frac{\partial f_i}{\partial x_0}  \frac{u_i^2 + u_i x_1}{u_i^2 + u_i x_2 + x_3} \\ &J_{i1}  \frac{\partial f_i}{\partial x_1}  \frac{u_i x_0}{u_i^2 + u_i x_2 + x_3} \\ &J_{i2}  \frac{\partial f_i}{\partial x_2}  -\frac{x_0 (u_i^2 + u_i x_1) u_i}{(u_i^2 + u_i x_2 + x_3)^2} \\ &J_{i3}  \frac{\partial f_i}{\partial x_3}  -\frac{x_0 (u_i^2 + u_i x_1)}{(u_i^2 + u_i x_2 + x_3)^2} \end{align} We are going to use the “hard” starting point defined in 2 . To find a physically meaningful solution, avoid potential division by zero and assure convergence to the global minimum we impose constraints . The code below implements least-squares estimation of and finally plots the original data and the fitted model function: 1 J. Kowalik and J. F. Morrison, “Analysis of kinetic data for allosteric enzyme reactions as a nonlinear regression problem”, Math. Biosci., vol. 2, pp. 57-66, 1968. 2 Averick et al., “The MINPACK-2 Test Problem Collection”. Further examples  Three interactive examples below illustrate usage of least_squares in greater detail. Large-scale bundle adjustment in scipy demonstrates large-scale capabilities of least_squares and how to efficiently compute finite difference approximation of sparse Jacobian. Robust nonlinear regression in scipy shows how to handle outliers with a robust loss function in a nonlinear regression. Solving a discrete boundary-value problem in scipy examines how to solve a large system of equations and use bounds to achieve desired properties of the solution. For the details about mathematical algorithms behind the implementation refer to documentation of least_squares . "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Linear programming ( linprog )  The function linprog can minimize a linear objective function subject to linear equality and inequality constraints. This kind of problem is well known as linear programming. Linear programming solves problems of the following form: \[\begin{split}\min_x \ & c^T x \\ \mbox{such that} \ & A_{ub} x \leq b_{ub},\\ & A_{eq} x  b_{eq},\\ & l \leq x \leq u ,\end{split}\] where is a vector of decision variables; , , , , and are vectors; and and are matrices. In this tutorial, we will try to solve a typical linear programming problem using linprog . Linear programming example  Consider the following simple linear programming problem: \[\begin{split}\max_{x_1, x_2, x_3, x_4} \ & 29x_1 + 45x_2 \\ \mbox{such that} \ & x_1 -x_2 -3x_3 \leq 5\\ & 2x_1 -3x_2 -7x_3 + 3x_4 \geq 10\\ & 2x_1 + 8x_2 + x_3  60\\ & 4x_1 + 4x_2 + x_4  60\\ & 0 \leq x_0\\ & 0 \leq x_1 \leq 5\\ & x_2 \leq 0.5\\ & -3 \leq x_3\\\end{split}\] We need some mathematical manipulations to convert the target problem to the form accepted by linprog . First of all, let’s consider the objective function. We want to maximize the objective function, but linprog can only accept a minimization problem. This is easily remedied by converting the maximize to minimizing . Also, are not shown in the objective function. That means the weights corresponding with are zero. So, the objective function can be converted to: \[\min_{x_1, x_2, x_3, x_4} \ -29x_1 -45x_2 + 0x_3 + 0x_4\] If we define the vector of decision variables , the objective weights vector of linprog in this problem should be \[c  [-29, -45, 0, 0]^T\] Next, let’s consider the two inequality constraints. The first one is a “less than” inequality, so it is already in the form accepted by linprog . The second one is a “greater than” inequality, so we need to multiply both sides by to convert it to a “less than” inequality. Explicitly showing zero coefficients, we have: \[\begin{split}x_1 -x_2 -3x_3 + 0x_4 &\leq 5\\ -2x_1 + 3x_2 + 7x_3 - 3x_4 &\leq -10\\\end{split}\] These equations can be converted to matrix form: \[\begin{split}A_{ub} x \leq b_{ub}\\\end{split}\] where \begin{equation*} A_{ub}  \begin{bmatrix} 1 & -1 & -3 & 0 \\ -2 & 3 & 7 & -3 \end{bmatrix} \end{equation*} \begin{equation*} b_{ub}  \begin{bmatrix} 5 \\ -10 \end{bmatrix} \end{equation*} Next, let’s consider the two equality constraints. Showing zero weights explicitly, these are: \[\begin{split}2x_1 + 8x_2 + 1x_3 + 0x_4 & 60\\ 4x_1 + 4x_2 + 0x_3 + 1x_4 & 60\\\end{split}\] These equations can be converted to matrix form: \[\begin{split}A_{eq} x  b_{eq}\\\end{split}\] where \begin{equation*} A_{eq}  \begin{bmatrix} 2 & 8 & 1 & 0 \\ 4 & 4 & 0 & 1 \end{bmatrix} \end{equation*} \begin{equation*} b_{eq}  \begin{bmatrix} 60 \\ 60 \end{bmatrix} \end{equation*} Lastly, let’s consider the separate inequality constraints on individual decision variables, which are known as “box constraints” or “simple bounds”. These constraints can be applied using the bounds argument of linprog . As noted in the linprog documentation, the default value of bounds is , meaning that the lower bound on each decision variable is 0, and the upper bound on each decision variable is infinity: all the decision variables are non-negative. Our bounds are different, so we will need to specify the lower and upper bound on each decision variable as a tuple and group these tuples into a list. Finally, we can solve the transformed problem using linprog . The result states that our problem is infeasible, meaning that there is no solution vector that satisfies all the constraints. That doesn’t necessarily mean we did anything wrong; some problems truly are infeasible. Suppose, however, that we were to decide that our bound constraint on was too tight and that it could be loosened to . After adjusting our code to reflect the change and executing it again: The result shows the optimization was successful. We can check the objective value ( ) is same as : We can also check that all constraints are satisfied within reasonable tolerances: If we need greater accuracy, typically at the expense of speed, we can solve using the method: References Some further reading and related software, such as Newton-Krylov [KK] , PETSc [PP] , and PyAMG [AMG] : KK D.A. Knoll and D.E. Keyes, “Jacobian-free Newton-Krylov methods”, J. Comp. Phys. 193, 357 (2004). doi:10.1016/j.jcp.2003.08.010 PP PETSc https://www.mcs.anl.gov/petsc/ and its Python bindings https://bitbucket.org/petsc/petsc4py/ AMG PyAMG (algebraic multigrid preconditioners/solvers) https://github.com/pyamg/pyamg/issues "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Root finding  Scalar functions  If one has a single-variable equation, there are multiple different root finding algorithms that can be tried. Most of these algorithms require the endpoints of an interval in which a root is expected (because the function changes signs). In general, brentq is the best choice, but the other methods may be useful in certain circumstances or for academic purposes. When a bracket is not available, but one or more derivatives are available, then newton (or , ) may be applicable. This is especially the case if the function is defined on a subset of the complex plane, and the bracketing methods cannot be used. Fixed-point solving  A problem closely related to finding the zeros of a function is the problem of finding a fixed point of a function. A fixed point of a function is the point at which evaluation of the function returns the point: Clearly, the fixed point of is the root of Equivalently, the root of is the fixed point of The routine fixed_point provides a simple iterative method using Aitkens sequence acceleration to estimate the fixed point of given a starting point. Sets of equations  Finding a root of a set of non-linear equations can be achieved using the root function. Several methods are available, amongst which (the default) and , which, respectively, use the hybrid method of Powell and the Levenberg-Marquardt method from MINPACK. The following example considers the single-variable transcendental equation \[x+2\cos\left(x\right)0,\] a root of which can be found as follows: Consider now a set of non-linear equations \begin{eqnarray*} x_{0}\cos\left(x_{1}\right) &  & 4,\\ x_{0}x_{1}-x_{1} &  & 5. \end{eqnarray*} We define the objective function so that it also returns the Jacobian and indicate this by setting the parameter to . Also, the Levenberg-Marquardt solver is used here. Root finding for large problems  Methods and in root cannot deal with a very large number of variables ( N ), as they need to calculate and invert a dense N x N Jacobian matrix on every Newton step. This becomes rather inefficient when N grows. Consider, for instance, the following problem: we need to solve the following integrodifferential equation on the square : \[(\partial_x^2 + \partial_y^2) P + 5 \left(\int_0^1\int_0^1\cosh(P)\,dx\,dy\right)^2  0\] with the boundary condition on the upper edge and elsewhere on the boundary of the square. This can be done by approximating the continuous function P by its values on a grid, , with a small grid spacing h . The derivatives and integrals can then be approximated; for instance . The problem is then equivalent to finding the root of some function , where is a vector of length . Now, because can be large, methods or in root will take a long time to solve this problem. The solution can, however, be found using one of the large-scale solvers, for example , , or . These use what is known as the inexact Newton method, which instead of computing the Jacobian matrix exactly, forms an approximation for it. The problem we have can now be solved as follows: Still too slow? Preconditioning.  When looking for the zero of the functions , i  1, 2, …, N , the solver spends most of the time inverting the Jacobian matrix, \[J_{ij}  \frac{\partial f_i}{\partial x_j} .\] If you have an approximation for the inverse matrix , you can use it for preconditioning the linear-inversion problem. The idea is that instead of solving one solves : since matrix is “closer” to the identity matrix than is, the equation should be easier for the Krylov method to deal with. The matrix M can be passed to root with method as an option . It can be a (sparse) matrix or a scipy.sparse.linalg.LinearOperator instance. For the problem in the previous section, we note that the function to solve consists of two parts: the first one is the application of the Laplace operator, , and the second is the integral. We can actually easily compute the Jacobian corresponding to the Laplace operator part: we know that in 1-D \[\begin{split}\partial_x^2 \approx \frac{1}{h_x^2} \begin{pmatrix} -2 & 1 & 0 & 0 \cdots \\ 1 & -2 & 1 & 0 \cdots \\ 0 & 1 & -2 & 1 \cdots \\ \ldots \end{pmatrix}  h_x^{-2} L\end{split}\] so that the whole 2-D operator is represented by \[J_1  \partial_x^2 + \partial_y^2 \simeq h_x^{-2} L \otimes I + h_y^{-2} I \otimes L\] The matrix of the Jacobian corresponding to the integral is more difficult to calculate, and since all of it entries are nonzero, it will be difficult to invert. on the other hand is a relatively simple matrix, and can be inverted by scipy.sparse.linalg.splu (or the inverse can be approximated by scipy.sparse.linalg.spilu ). So we are content to take and hope for the best. In the example below, we use the preconditioner . Resulting run, first without preconditioning: and then with preconditioning: Using a preconditioner reduced the number of evaluations of the function by a factor of 4 . For problems where the residual is expensive to compute, good preconditioning can be crucial — it can even decide whether the problem is solvable in practice or not. Preconditioning is an art, science, and industry. Here, we were lucky in making a simple choice that worked reasonably well, but there is a lot more depth to this topic than is shown here. "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Unconstrained minimization of multivariate scalar functions ( minimize )  The minimize function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in scipy.optimize . To demonstrate the minimization function, consider the problem of minimizing the Rosenbrock function of variables: \[f\left(\mathbf{x}\right)\sum_{i1}^{N-1}100\left(x_{i+1}-x_{i}^{2}\right)^{2}+\left(1-x_{i}\right)^{2}.\] The minimum value of this function is 0 which is achieved when Note that the Rosenbrock function and its derivatives are included in scipy.optimize . The implementations shown in the following sections provide examples of how to define an objective function as well as its jacobian and hessian functions. Nelder-Mead Simplex algorithm ( )  In the example below, the minimize routine is used with the Nelder-Mead simplex algorithm (selected through the parameter): The simplex algorithm is probably the simplest way to minimize a fairly well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum. Another optimization algorithm that needs only function calls to find the minimum is Powell ’s method available by setting in minimize . Broyden-Fletcher-Goldfarb-Shanno algorithm ( )  In order to converge more quickly to the solution, this routine uses the gradient of the objective function. If the gradient is not given by the user, then it is estimated using first-differences. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) method typically requires fewer function calls than the simplex algorithm even when the gradient must be estimated. To demonstrate this algorithm, the Rosenbrock function is again used. The gradient of the Rosenbrock function is the vector: \begin{eqnarray*} \frac{\partial f}{\partial x_{j}} &  & \sum_{i1}^{N}200\left(x_{i}-x_{i-1}^{2}\right)\left(\delta_{i,j}-2x_{i-1}\delta_{i-1,j}\right)-2\left(1-x_{i-1}\right)\delta_{i-1,j}.\\ &  & 200\left(x_{j}-x_{j-1}^{2}\right)-400x_{j}\left(x_{j+1}-x_{j}^{2}\right)-2\left(1-x_{j}\right).\end{eqnarray*} This expression is valid for the interior derivatives. Special cases are \begin{eqnarray*} \frac{\partial f}{\partial x_{0}} &  & -400x_{0}\left(x_{1}-x_{0}^{2}\right)-2\left(1-x_{0}\right),\\ \frac{\partial f}{\partial x_{N-1}} &  & 200\left(x_{N-1}-x_{N-2}^{2}\right).\end{eqnarray*} A Python function which computes this gradient is constructed by the code-segment: This gradient information is specified in the minimize function through the parameter as illustrated below. Newton-Conjugate-Gradient algorithm ( )  Newton-Conjugate Gradient algorithm is a modified Newton’s method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian [NW] . Newton’s method is based on fitting the function locally to a quadratic form: \[f\left(\mathbf{x}\right)\approx f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)\cdot\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{T}\mathbf{H}\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right).\] where is a matrix of second-derivatives (the Hessian). If the Hessian is positive definite then the local minimum of this function can be found by setting the gradient of the quadratic form to zero, resulting in \[\mathbf{x}_{\textrm{opt}}\mathbf{x}_{0}-\mathbf{H}^{-1}\nabla f.\] The inverse of the Hessian is evaluated using the conjugate-gradient method. An example of employing this method to minimizing the Rosenbrock function is given below. To take full advantage of the Newton-CG method, a function which computes the Hessian must be provided. The Hessian matrix itself does not need to be constructed, only a vector which is the product of the Hessian with an arbitrary vector needs to be available to the minimization routine. As a result, the user can provide either a function to compute the Hessian matrix, or a function to compute the product of the Hessian with an arbitrary vector. Full Hessian example:  The Hessian of the Rosenbrock function is \begin{eqnarray*} H_{ij}\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}} &  & 200\left(\delta_{i,j}-2x_{i-1}\delta_{i-1,j}\right)-400x_{i}\left(\delta_{i+1,j}-2x_{i}\delta_{i,j}\right)-400\delta_{i,j}\left(x_{i+1}-x_{i}^{2}\right)+2\delta_{i,j},\\ &  & \left(202+1200x_{i}^{2}-400x_{i+1}\right)\delta_{i,j}-400x_{i}\delta_{i+1,j}-400x_{i-1}\delta_{i-1,j},\end{eqnarray*} if with defining the matrix. Other non-zero entries of the matrix are \begin{eqnarray*} \frac{\partial^{2}f}{\partial x_{0}^{2}} &  & 1200x_{0}^{2}-400x_{1}+2,\\ \frac{\partial^{2}f}{\partial x_{0}\partial x_{1}}\frac{\partial^{2}f}{\partial x_{1}\partial x_{0}} &  & -400x_{0},\\ \frac{\partial^{2}f}{\partial x_{N-1}\partial x_{N-2}}\frac{\partial^{2}f}{\partial x_{N-2}\partial x_{N-1}} &  & -400x_{N-2},\\ \frac{\partial^{2}f}{\partial x_{N-1}^{2}} &  & 200.\end{eqnarray*} For example, the Hessian when is \[\begin{split}\mathbf{H}\begin{bmatrix} 1200x_{0}^{2}-400x_{1}+2 & -400x_{0} & 0 & 0 & 0\\ -400x_{0} & 202+1200x_{1}^{2}-400x_{2} & -400x_{1} & 0 & 0\\ 0 & -400x_{1} & 202+1200x_{2}^{2}-400x_{3} & -400x_{2} & 0\\ 0 & & -400x_{2} & 202+1200x_{3}^{2}-400x_{4} & -400x_{3}\\ 0 & 0 & 0 & -400x_{3} & 200\end{bmatrix}.\end{split}\] The code which computes this Hessian along with the code to minimize the function using Newton-CG method is shown in the following example: Hessian product example:  For larger minimization problems, storing the entire Hessian matrix can consume considerable time and memory. The Newton-CG algorithm only needs the product of the Hessian times an arbitrary vector. As a result, the user can supply code to compute this product rather than the full Hessian by giving a function which take the minimization vector as the first argument and the arbitrary vector as the second argument (along with extra arguments passed to the function to be minimized). If possible, using Newton-CG with the Hessian product option is probably the fastest way to minimize the function. In this case, the product of the Rosenbrock Hessian with an arbitrary vector is not difficult to compute. If is the arbitrary vector, then has elements: \[\begin{split}\mathbf{H}\left(\mathbf{x}\right)\mathbf{p}\begin{bmatrix} \left(1200x_{0}^{2}-400x_{1}+2\right)p_{0}-400x_{0}p_{1}\\ \vdots\\ -400x_{i-1}p_{i-1}+\left(202+1200x_{i}^{2}-400x_{i+1}\right)p_{i}-400x_{i}p_{i+1}\\ \vdots\\ -400x_{N-2}p_{N-2}+200p_{N-1}\end{bmatrix}.\end{split}\] Code which makes use of this Hessian product to minimize the Rosenbrock function using minimize follows: According to [NW] p. 170 the algorithm can be inefficient when the Hessian is ill-conditioned because of the poor quality search directions provided by the method in those situations. The method , according to the authors, deals more effectively with this problematic situation and will be described next. Trust-Region Newton-Conjugate-Gradient Algorithm ( )  The method is a line search method: it finds a direction of search minimizing a quadratic approximation of the function and then uses a line search algorithm to find the (nearly) optimal step size in that direction. An alternative approach is to, first, fix the step size limit and then find the optimal step inside the given trust-radius by solving the following quadratic subproblem: \begin{eqnarray*} \min_{\mathbf{p}} f\left(\mathbf{x}_{k}\right)+\nabla f\left(\mathbf{x}_{k}\right)\cdot\mathbf{p}+\frac{1}{2}\mathbf{p}^{T}\mathbf{H}\left(\mathbf{x}_{k}\right)\mathbf{p};&\\ \text{subject to: } \|\mathbf{p}\|\le \Delta.& \end{eqnarray*} The solution is then updated and the trust-radius is adjusted according to the degree of agreement of the quadratic model with the real function. This family of methods is known as trust-region methods. The algorithm is a trust-region method that uses a conjugate gradient algorithm to solve the trust-region subproblem [NW] . Full Hessian example:  Hessian product example:  Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm ( )  Similar to the method, the method is a method suitable for large-scale problems as it uses the hessian only as linear operator by means of matrix-vector products. It solves the quadratic subproblem more accurately than the method. \begin{eqnarray*} \min_{\mathbf{p}} f\left(\mathbf{x}_{k}\right)+\nabla f\left(\mathbf{x}_{k}\right)\cdot\mathbf{p}+\frac{1}{2}\mathbf{p}^{T}\mathbf{H}\left(\mathbf{x}_{k}\right)\mathbf{p};&\\ \text{subject to: } \|\mathbf{p}\|\le \Delta.& \end{eqnarray*} This method wraps the [TRLIB] implementation of the [GLTR] method solving exactly a trust-region subproblem restricted to a truncated Krylov subspace. For indefinite problems it is usually better to use this method as it reduces the number of nonlinear iterations at the expense of few more matrix-vector products per subproblem solve in comparison to the method. Full Hessian example:  Hessian product example:  TRLIB F. Lenders, C. Kirches, A. Potschka: “trlib: A vector-free implementation of the GLTR method for iterative solution of the trust region problem”, https://arxiv.org/abs/1611.04718 GLTR N. Gould, S. Lucidi, M. Roma, P. Toint: “Solving the Trust-Region Subproblem using the Lanczos Method”, SIAM J. Optim., 9(2), 504–525, (1999). https://doi.org/10.1137/S1052623497322735 Trust-Region Nearly Exact Algorithm ( )  All methods , and are suitable for dealing with large-scale problems (problems with thousands of variables). That is because the conjugate gradient algorithm approximately solve the trust-region subproblem (or invert the Hessian) by iterations without the explicit Hessian factorization. Since only the product of the Hessian with an arbitrary vector is needed, the algorithm is specially suited for dealing with sparse Hessians, allowing low storage requirements and significant time savings for those sparse problems. For medium-size problems, for which the storage and factorization cost of the Hessian are not critical, it is possible to obtain a solution within fewer iteration by solving the trust-region subproblems almost exactly. To achieve that, a certain nonlinear equations is solved iteratively for each quadratic subproblem [CGT] . This solution requires usually 3 or 4 Cholesky factorizations of the Hessian matrix. As the result, the method converges in fewer number of iterations and takes fewer evaluations of the objective function than the other implemented trust-region methods. The Hessian product option is not supported by this algorithm. An example using the Rosenbrock function follows: NW ( 1 , 2 , 3 ) J. Nocedal, S.J. Wright “Numerical optimization.” 2nd edition. Springer Science (2006). CGT Conn, A. R., Gould, N. I., & Toint, P. L. “Trust region methods”. Siam. (2000). pp. 169-200. "
"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Univariate function minimizers ( minimize_scalar )  Often only the minimum of an univariate function (i.e., a function that takes a scalar as input) is needed. In these circumstances, other optimization techniques have been developed that can work faster. These are accessible from the minimize_scalar function, which proposes several algorithms. Unconstrained minimization ( )  There are, actually, two methods that can be used to minimize an univariate function: brent and golden , but golden is included only for academic purposes and should rarely be used. These can be respectively selected through the method parameter in minimize_scalar . The brent method uses Brent’s algorithm for locating a minimum. Optimally, a bracket (the bracket parameter) should be given which contains the minimum desired. A bracket is a triple such that and . If this is not given, then alternatively two starting points can be chosen and a bracket will be found from these points using a simple marching algorithm. If these two starting points are not provided, 0 and 1 will be used (this may not be the right choice for your function and result in an unexpected minimum being returned). Here is an example: Bounded minimization ( )  Very often, there are constraints that can be placed on the solution space before minimization occurs. The bounded method in minimize_scalar is an example of a constrained minimization procedure that provides a rudimentary interval constraint for scalar functions. The interval constraint allows the minimization to occur only between two fixed endpoints, specified using the mandatory bounds parameter. For example, to find the minimum of near , minimize_scalar can be called using the interval as a constraint. The result is : "
"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  B-splines  A B-spline is an approximation of a continuous function over a finite- domain in terms of B-spline coefficients and knot points. If the knot- points are equally spaced with spacing , then the B-spline approximation to a 1-D function is the finite-basis expansion. \[y\left(x\right)\approx\sum_{j}c_{j}\beta^{o}\left(\frac{x}{\Delta x}-j\right).\] In two dimensions with knot-spacing and , the function representation is \[z\left(x,y\right)\approx\sum_{j}\sum_{k}c_{jk}\beta^{o}\left(\frac{x}{\Delta x}-j\right)\beta^{o}\left(\frac{y}{\Delta y}-k\right).\] In these expressions, is the space-limited B-spline basis function of order . The requirement of equally-spaced knot-points and equally-spaced data points, allows the development of fast (inverse-filtering) algorithms for determining the coefficients, , from sample-values, . Unlike the general spline interpolation algorithms, these algorithms can quickly find the spline coefficients for large images. The advantage of representing a set of samples via B-spline basis functions is that continuous-domain operators (derivatives, re- sampling, integral, etc.), which assume that the data samples are drawn from an underlying continuous function, can be computed with relative ease from the spline coefficients. For example, the second derivative of a spline is \[y{}^{\prime\prime}\left(x\right)\frac{1}{\Delta x^{2}}\sum_{j}c_{j}\beta^{o\prime\prime}\left(\frac{x}{\Delta x}-j\right).\] Using the property of B-splines that \[\frac{d^{2}\beta^{o}\left(w\right)}{dw^{2}}\beta^{o-2}\left(w+1\right)-2\beta^{o-2}\left(w\right)+\beta^{o-2}\left(w-1\right),\] it can be seen that \[y^{\prime\prime}\left(x\right)\frac{1}{\Delta x^{2}}\sum_{j}c_{j}\left[\beta^{o-2}\left(\frac{x}{\Delta x}-j+1\right)-2\beta^{o-2}\left(\frac{x}{\Delta x}-j\right)+\beta^{o-2}\left(\frac{x}{\Delta x}-j-1\right)\right].\] If , then at the sample points: \begin{eqnarray*} \Delta x^{2}\left.y^{\prime}\left(x\right)\right|_{xn\Delta x} &  & \sum_{j}c_{j}\delta_{n-j+1}-2c_{j}\delta_{n-j}+c_{j}\delta_{n-j-1},\\ &  & c_{n+1}-2c_{n}+c_{n-1}.\end{eqnarray*} Thus, the second-derivative signal can be easily calculated from the spline fit. If desired, smoothing splines can be found to make the second derivative less sensitive to random errors. The savvy reader will have already noticed that the data samples are related to the knot coefficients via a convolution operator, so that simple convolution with the sampled B-spline function recovers the original data from the spline coefficients. The output of convolutions can change depending on how the boundaries are handled (this becomes increasingly more important as the number of dimensions in the dataset increases). The algorithms relating to B-splines in the signal-processing subpackage assume mirror-symmetric boundary conditions. Thus, spline coefficients are computed based on that assumption, and data-samples can be recovered exactly from the spline coefficients by assuming them to be mirror-symmetric also. Currently the package provides functions for determining second- and third- order cubic spline coefficients from equally-spaced samples in one and two dimensions ( qspline1d , qspline2d , cspline1d , cspline2d ). The package also supplies a function ( bspline ) for evaluating the B-spline basis function, for arbitrary order and For large , the B-spline basis function can be approximated well by a zero-mean Gaussian function with standard-deviation equal to : \[\beta^{o}\left(x\right)\approx\frac{1}{\sqrt{2\pi\sigma_{o}^{2}}}\exp\left(-\frac{x^{2}}{2\sigma_{o}}\right).\] A function to compute this Gaussian for arbitrary and is also available ( gauss_spline ). The following code and figure use spline-filtering to compute an edge-image (the second derivative of a smoothed spline) of a raccoon’s face, which is an array returned by the command scipy.misc.face . The command sepfir2d was used to apply a separable 2-D FIR filter with mirror-symmetric boundary conditions to the spline coefficients. This function is ideally-suited for reconstructing samples from spline coefficients and is faster than convolve2d , which convolves arbitrary 2-D filters and allows for choosing mirror-symmetric boundary conditions. Alternatively, we could have done: "
"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  Detrend  SciPy provides the function detrend to remove a constant or linear trend in a data series in order to see effect of higher order. The example below removes the constant and linear trend of a second-order polynomial time series and plots the remaining signal components. References Some further reading and related software: 1 N.R. Lomb “Least-squares frequency analysis of unequally spaced data”, Astrophysics and Space Science, vol 39, pp. 447-462, 1976 2 J.D. Scargle “Studies in astronomical time series analysis. II - Statistical aspects of spectral analysis of unevenly spaced data”, The Astrophysical Journal, vol 263, pp. 835-853, 1982 3 R.H.D. Townsend, “Fast calculation of the Lomb-Scargle periodogram using graphics processing units.”, The Astrophysical Journal Supplement Series, vol 191, pp. 247-253, 2010 "
"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  Filtering  Filtering is a generic name for any system that modifies an input signal in some way. In SciPy, a signal can be thought of as a NumPy array. There are different kinds of filters for different kinds of operations. There are two broad kinds of filtering operations: linear and non-linear. Linear filters can always be reduced to multiplication of the flattened NumPy array by an appropriate matrix resulting in another flattened NumPy array. Of course, this is not usually the best way to compute the filter, as the matrices and vectors involved may be huge. For example, filtering a image with this method would require multiplication of a matrix with a vector. Just trying to store the matrix using a standard NumPy array would require elements. At 4 bytes per element this would require of memory. In most applications, most of the elements of this matrix are zero and a different method for computing the output of the filter is employed. Convolution/Correlation  Many linear filters also have the property of shift-invariance. This means that the filtering operation is the same at different locations in the signal and it implies that the filtering matrix can be constructed from knowledge of one row (or column) of the matrix alone. In this case, the matrix multiplication can be accomplished using Fourier transforms. Let define a 1-D signal indexed by the integer Full convolution of two 1-D signals can be expressed as \[y\left[n\right]\sum_{k-\infty}^{\infty}x\left[k\right]h\left[n-k\right].\] This equation can only be implemented directly if we limit the sequences to finite-support sequences that can be stored in a computer, choose to be the starting point of both sequences, let be that value for which for all and be that value for which for all , then the discrete convolution expression is \[y\left[n\right]\sum_{k\max\left(n-M,0\right)}^{\min\left(n,K\right)}x\left[k\right]h\left[n-k\right].\] For convenience, assume Then, more explicitly, the output of this operation is \begin{eqnarray*} y\left[0\right] &  & x\left[0\right]h\left[0\right]\\ y\left[1\right] &  & x\left[0\right]h\left[1\right]+x\left[1\right]h\left[0\right]\\ y\left[2\right] &  & x\left[0\right]h\left[2\right]+x\left[1\right]h\left[1\right]+x\left[2\right]h\left[0\right]\\ \vdots & \vdots & \vdots\\ y\left[M\right] &  & x\left[0\right]h\left[M\right]+x\left[1\right]h\left[M-1\right]+\cdots+x\left[M\right]h\left[0\right]\\ y\left[M+1\right] &  & x\left[1\right]h\left[M\right]+x\left[2\right]h\left[M-1\right]+\cdots+x\left[M+1\right]h\left[0\right]\\ \vdots & \vdots & \vdots\\ y\left[K\right] &  & x\left[K-M\right]h\left[M\right]+\cdots+x\left[K\right]h\left[0\right]\\ y\left[K+1\right] &  & x\left[K+1-M\right]h\left[M\right]+\cdots+x\left[K\right]h\left[1\right]\\ \vdots & \vdots & \vdots\\ y\left[K+M-1\right] &  & x\left[K-1\right]h\left[M\right]+x\left[K\right]h\left[M-1\right]\\ y\left[K+M\right] &  & x\left[K\right]h\left[M\right].\end{eqnarray*} Thus, the full discrete convolution of two finite sequences of lengths and , respectively, results in a finite sequence of length 1-D convolution is implemented in SciPy with the function convolve . This function takes as inputs the signals , and two optional flags ‘mode’ and ‘method’, and returns the signal The first optional flag, ‘mode’, allows for the specification of which part of the output signal to return. The default value of ‘full’ returns the entire signal. If the flag has a value of ‘same’, then only the middle values are returned, starting at , so that the output has the same length as the first input. If the flag has a value of ‘valid’, then only the middle output values are returned, where depends on all of the values of the smallest input from to In other words, only the values to inclusive are returned. The second optional flag, ‘method’, determines how the convolution is computed, either through the Fourier transform approach with fftconvolve or through the direct method. By default, it selects the expected faster method. The Fourier transform method has order , while the direct method has order . Depending on the big O constant and the value of , one of these two methods may be faster. The default value, ‘auto’, performs a rough calculation and chooses the expected faster method, while the values ‘direct’ and ‘fft’ force computation with the other two methods. The code below shows a simple example for convolution of 2 sequences: This same function convolve can actually take N-D arrays as inputs and will return the N-D convolution of the two arrays, as is shown in the code example below. The same input flags are available for that case as well. Correlation is very similar to convolution except that the minus sign becomes a plus sign. Thus, \[w\left[n\right]\sum_{k-\infty}^{\infty}y\left[k\right]x\left[n+k\right],\] is the (cross) correlation of the signals and For finite-length signals with outside of the range and outside of the range the summation can simplify to \[w\left[n\right]\sum_{k\max\left(0,-n\right)}^{\min\left(K,M-n\right)}y\left[k\right]x\left[n+k\right].\] Assuming again that , this is \begin{eqnarray*} w\left[-K\right] &  & y\left[K\right]x\left[0\right]\\ w\left[-K+1\right] &  & y\left[K-1\right]x\left[0\right]+y\left[K\right]x\left[1\right]\\ \vdots & \vdots & \vdots\\ w\left[M-K\right] &  & y\left[K-M\right]x\left[0\right]+y\left[K-M+1\right]x\left[1\right]+\cdots+y\left[K\right]x\left[M\right]\\ w\left[M-K+1\right] &  & y\left[K-M-1\right]x\left[0\right]+\cdots+y\left[K-1\right]x\left[M\right]\\ \vdots & \vdots & \vdots\\ w\left[-1\right] &  & y\left[1\right]x\left[0\right]+y\left[2\right]x\left[1\right]+\cdots+y\left[M+1\right]x\left[M\right]\\ w\left[0\right] &  & y\left[0\right]x\left[0\right]+y\left[1\right]x\left[1\right]+\cdots+y\left[M\right]x\left[M\right]\\ w\left[1\right] &  & y\left[0\right]x\left[1\right]+y\left[1\right]x\left[2\right]+\cdots+y\left[M-1\right]x\left[M\right]\\ w\left[2\right] &  & y\left[0\right]x\left[2\right]+y\left[1\right]x\left[3\right]+\cdots+y\left[M-2\right]x\left[M\right]\\ \vdots & \vdots & \vdots\\ w\left[M-1\right] &  & y\left[0\right]x\left[M-1\right]+y\left[1\right]x\left[M\right]\\ w\left[M\right] &  & y\left[0\right]x\left[M\right].\end{eqnarray*} The SciPy function correlate implements this operation. Equivalent flags are available for this operation to return the full length sequence (‘full’) or a sequence with the same size as the largest sequence starting at (‘same’) or a sequence where the values depend on all the values of the smallest sequence (‘valid’). This final option returns the values to inclusive. The function correlate can also take arbitrary N-D arrays as input and return the N-D convolution of the two arrays on output. When correlate and/or convolve can be used to construct arbitrary image filters to perform actions such as blurring, enhancing, and edge-detection for an image. Calculating the convolution in the time domain as above is mainly used for filtering when one of the signals is much smaller than the other ( ), otherwise linear filtering is more efficiently calculated in the frequency domain provided by the function fftconvolve . By default, convolve estimates the fastest method using choose_conv_method . If the filter function can be factored according to \[h[n, m]  h_1[n] h_2[m],\] convolution can be calculated by means of the function sepfir2d . As an example, we consider a Gaussian filter gaussian \[h[n, m] \propto e^{-x^2-y^2}  e^{-x^2} e^{-y^2},\] which is often used for blurring. Difference-equation filtering  A general class of linear 1-D filters (that includes convolution filters) are filters described by the difference equation \[\sum_{k0}^{N}a_{k}y\left[n-k\right]\sum_{k0}^{M}b_{k}x\left[n-k\right],\] where is the input sequence and is the output sequence. If we assume initial rest so that for , then this kind of filter can be implemented using convolution. However, the convolution filter sequence could be infinite if for In addition, this general class of linear filter allows initial conditions to be placed on for resulting in a filter that cannot be expressed using convolution. The difference equation filter can be thought of as finding recursively in terms of its previous values \[a_{0}y\left[n\right]-a_{1}y\left[n-1\right]-\cdots-a_{N}y\left[n-N\right]+\cdots+b_{0}x\left[n\right]+\cdots+b_{M}x\left[n-M\right].\] Often, is chosen for normalization. The implementation in SciPy of this general difference equation filter is a little more complicated than would be implied by the previous equation. It is implemented so that only one signal needs to be delayed. The actual implementation equations are (assuming ): \begin{eqnarray*} y\left[n\right] &  & b_{0}x\left[n\right]+z_{0}\left[n-1\right]\\ z_{0}\left[n\right] &  & b_{1}x\left[n\right]+z_{1}\left[n-1\right]-a_{1}y\left[n\right]\\ z_{1}\left[n\right] &  & b_{2}x\left[n\right]+z_{2}\left[n-1\right]-a_{2}y\left[n\right]\\ \vdots & \vdots & \vdots\\ z_{K-2}\left[n\right] &  & b_{K-1}x\left[n\right]+z_{K-1}\left[n-1\right]-a_{K-1}y\left[n\right]\\ z_{K-1}\left[n\right] &  & b_{K}x\left[n\right]-a_{K}y\left[n\right],\end{eqnarray*} where Note that if and if In this way, the output at time depends only on the input at time and the value of at the previous time. This can always be calculated as long as the values are computed and stored at each time step. The difference-equation filter is called using the command lfilter in SciPy. This command takes as inputs the vector the vector, a signal and returns the vector (the same length as ) computed using the equation given above. If is N-D, then the filter is computed along the axis provided. If desired, initial conditions providing the values of to can be provided or else it will be assumed that they are all zero. If initial conditions are provided, then the final conditions on the intermediate variables are also returned. These could be used, for example, to restart the calculation in the same state. Sometimes, it is more convenient to express the initial conditions in terms of the signals and In other words, perhaps you have the values of to and the values of to and would like to determine what values of should be delivered as initial conditions to the difference-equation filter. It is not difficult to show that, for \[z_{m}\left[n\right]\sum_{p0}^{K-m-1}\left(b_{m+p+1}x\left[n-p\right]-a_{m+p+1}y\left[n-p\right]\right).\] Using this formula, we can find the initial-condition vector to given initial conditions on (and ). The command lfiltic performs this function. As an example, consider the following system: \[y[n]  \frac{1}{2} x[n] + \frac{1}{4} x[n-1] + \frac{1}{3} y[n-1]\] The code calculates the signal for a given signal ; first for initial conditions (default case), then for by means of lfiltic . Note that the output signal has the same length as the length as the input signal . Analysis of Linear Systems  Linear system described a linear-difference equation can be fully described by the coefficient vectors and as was done above; an alternative representation is to provide a factor , zeros and poles , respectively, to describe the system by means of its transfer function , according to \[H(z)  k \frac{ (z-z_1)(z-z_2)...(z-z_{N_z})}{ (z-p_1)(z-p_2)...(z-p_{N_p})}.\] This alternative representation can be obtained with the scipy function tf2zpk ; the inverse is provided by zpk2tf . For the above example we have i.e., the system has a zero at and a pole at . The scipy function freqz allows calculation of the frequency response of a system described by the coefficients and . See the help of the freqz function for a comprehensive example. Filter Design  Time-discrete filters can be classified into finite response (FIR) filters and infinite response (IIR) filters. FIR filters can provide a linear phase response, whereas IIR filters cannot. SciPy provides functions for designing both types of filters. FIR Filter  The function firwin designs filters according to the window method. Depending on the provided arguments, the function returns different filter types (e.g., low-pass, band-pass…). The example below designs a low-pass and a band-stop filter, respectively. Note that firwin uses, per default, a normalized frequency defined such that the value corresponds to the Nyquist frequency, whereas the function freqz is defined such that the value corresponds to the Nyquist frequency. The function firwin2 allows design of almost arbitrary frequency responses by specifying an array of corner frequencies and corresponding gains, respectively. The example below designs a filter with such an arbitrary amplitude response. Note the linear scaling of the y-axis and the different definition of the Nyquist frequency in firwin2 and freqz (as explained above). IIR Filter  SciPy provides two functions to directly design IIR iirdesign and iirfilter , where the filter type (e.g., elliptic) is passed as an argument and several more filter design functions for specific filter types, e.g., ellip . The example below designs an elliptic low-pass filter with defined pass-band and stop-band ripple, respectively. Note the much lower filter order (order 4) compared with the FIR filters from the examples above in order to reach the same stop-band attenuation of dB. Filter Coefficients  Filter coefficients can be stored in several different formats: ‘ba’ or ‘tf’  transfer function coefficients ‘zpk’  zeros, poles, and overall gain ‘ss’  state-space system representation ‘sos’  transfer function coefficients of second-order sections Functions, such as tf2zpk and zpk2ss , can convert between them. Transfer function representation  The or format is a 2-tuple representing a transfer function, where b is a length array of coefficients of the M -order numerator polynomial, and a is a length array of coefficients of the N -order denominator, as positive, descending powers of the transfer function variable. So the tuple of and can represent an analog filter of the form: \[H(s)  \frac {b_0 s^M + b_1 s^{(M-1)} + \cdots + b_M} {a_0 s^N + a_1 s^{(N-1)} + \cdots + a_N}  \frac {\sum_{i0}^M b_i s^{(M-i)}} {\sum_{i0}^N a_i s^{(N-i)}}\] or a discrete-time filter of the form: \[H(z)  \frac {b_0 z^M + b_1 z^{(M-1)} + \cdots + b_M} {a_0 z^N + a_1 z^{(N-1)} + \cdots + a_N}  \frac {\sum_{i0}^M b_i z^{(M-i)}} {\sum_{i0}^N a_i z^{(N-i)}}.\] This “positive powers” form is found more commonly in controls engineering. If M and N are equal (which is true for all filters generated by the bilinear transform), then this happens to be equivalent to the “negative powers” discrete-time form preferred in DSP: \[H(z)  \frac {b_0 + b_1 z^{-1} + \cdots + b_M z^{-M}} {a_0 + a_1 z^{-1} + \cdots + a_N z^{-N}}  \frac {\sum_{i0}^M b_i z^{-i}} {\sum_{i0}^N a_i z^{-i}}.\] Although this is true for common filters, remember that this is not true in the general case. If M and N are not equal, the discrete-time transfer function coefficients must first be converted to the “positive powers” form before finding the poles and zeros. This representation suffers from numerical error at higher orders, so other formats are preferred when possible. Zeros and poles representation  The format is a 3-tuple , where z is an M -length array of the complex zeros of the transfer function , p is an N -length array of the complex poles of the transfer function , and k is a scalar gain. These represent the digital transfer function: \[H(z)  k \cdot \frac {(z - z_0) (z - z_1) \cdots (z - z_{(M-1)})} {(z - p_0) (z - p_1) \cdots (z - p_{(N-1)})}  k \frac {\prod_{i0}^{M-1} (z - z_i)} {\prod_{i0}^{N-1} (z - p_i)}\] or the analog transfer function: \[H(s)  k \cdot \frac {(s - z_0) (s - z_1) \cdots (s - z_{(M-1)})} {(s - p_0) (s - p_1) \cdots (s - p_{(N-1)})}  k \frac {\prod_{i0}^{M-1} (s - z_i)} {\prod_{i0}^{N-1} (s - p_i)}.\] Although the sets of roots are stored as ordered NumPy arrays, their ordering does not matter: is the same filter as . State-space system representation  The format is a 4-tuple of arrays representing the state-space of an N -order digital/discrete-time system of the form: \[\begin{split}\mathbf{x}[k+1]  A \mathbf{x}[k] + B \mathbf{u}[k]\\ \mathbf{y}[k]  C \mathbf{x}[k] + D \mathbf{u}[k]\end{split}\] or a continuous/analog system of the form: \[\begin{split}\dot{\mathbf{x}}(t)  A \mathbf{x}(t) + B \mathbf{u}(t)\\ \mathbf{y}(t)  C \mathbf{x}(t) + D \mathbf{u}(t),\end{split}\] with P inputs, Q outputs and N state variables, where: x is the state vector y is the output vector of length Q u is the input vector of length P A is the state matrix, with shape B is the input matrix with shape C is the output matrix with shape D is the feedthrough or feedforward matrix with shape . (In cases where the system does not have a direct feedthrough, all values in D are zero.) State-space is the most general representation and the only one that allows for multiple-input, multiple-output (MIMO) systems. There are multiple state-space representations for a given transfer function. Specifically, the “controllable canonical form” and “observable canonical form” have the same coefficients as the representation, and, therefore, suffer from the same numerical errors. Second-order sections representation  The format is a single 2-D array of shape , representing a sequence of second-order transfer functions which, when cascaded in series, realize a higher-order filter with minimal numerical error. Each row corresponds to a second-order representation, with the first three columns providing the numerator coefficients and the last three providing the denominator coefficients: \[[b_0, b_1, b_2, a_0, a_1, a_2]\] The coefficients are typically normalized, such that is always 1. The section order is usually not important with floating-point computation; the filter output will be the same, regardless of the order. Filter transformations  The IIR filter design functions first generate a prototype analog low-pass filter with a normalized cutoff frequency of 1 rad/sec. This is then transformed into other frequencies and band types using the following substitutions: Type Transformation lp2lp lp2hp lp2bp lp2bs Here, is the new cutoff or center frequency, and is the bandwidth. These preserve symmetry on a logarithmic frequency axis. To convert the transformed analog filter into a digital filter, the bilinear transform is used, which makes the following substitution: \[s \rightarrow \frac{2}{T} \frac{z - 1}{z + 1},\] where T is the sampling time (the inverse of the sampling frequency). Other filters  The signal processing package provides many more filters as well. Median Filter  A median filter is commonly applied when noise is markedly non-Gaussian or when it is desired to preserve edges. The median filter works by sorting all of the array pixel values in a rectangular region surrounding the point of interest. The sample median of this list of neighborhood pixel values is used as the value for the output array. The sample median is the middle-array value in a sorted list of neighborhood values. If there are an even number of elements in the neighborhood, then the average of the middle two values is used as the median. A general purpose median filter that works on N-D arrays is medfilt . A specialized version that works only for 2-D arrays is available as medfilt2d . Order Filter  A median filter is a specific example of a more general class of filters called order filters. To compute the output at a particular pixel, all order filters use the array values in a region surrounding that pixel. These array values are sorted and then one of them is selected as the output value. For the median filter, the sample median of the list of array values is used as the output. A general-order filter allows the user to select which of the sorted values will be used as the output. So, for example, one could choose to pick the maximum in the list or the minimum. The order filter takes an additional argument besides the input array and the region mask that specifies which of the elements in the sorted list of neighbor array values should be used as the output. The command to perform an order filter is order_filter . Wiener filter  The Wiener filter is a simple deblurring filter for denoising images. This is not the Wiener filter commonly described in image-reconstruction problems but, instead, it is a simple, local-mean filter. Let be the input signal, then the output is \[\begin{split}y\left\{ \begin{array}{cc} \frac{\sigma^{2}}{\sigma_{x}^{2}}m_{x}+\left(1-\frac{\sigma^{2}}{\sigma_{x}^{2}}\right)x & \sigma_{x}^{2}\geq\sigma^{2},\\ m_{x} & \sigma_{x}^{2}<\sigma^{2},\end{array}\right.\end{split}\] where is the local estimate of the mean and is the local estimate of the variance. The window for these estimates is an optional input parameter (default is ). The parameter is a threshold noise parameter. If is not given, then it is estimated as the average of the local variances. Hilbert filter  The Hilbert transform constructs the complex-valued analytic signal from a real signal. For example, if , then would return (except near the edges) In the frequency domain, the hilbert transform performs \[YX\cdot H,\] where is for positive frequencies, for negative frequencies, and for zero-frequencies. Analog Filter Design  The functions iirdesign , iirfilter , and the filter design functions for specific filter types (e.g., ellip ) all have a flag analog , which allows the design of analog filters as well. The example below designs an analog (IIR) filter, obtains via tf2zpk the poles and zeros and plots them in the complex s-plane. The zeros at and can be clearly seen in the amplitude response. "
"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  Spectral Analysis  Periodogram Measurements  The scipy function periodogram provides a method to estimate the spectral density using the periodogram method. The example below calculates the periodogram of a sine signal in white Gaussian noise. Spectral Analysis using Welch’s Method  An improved method, especially with respect to noise immunity, is Welch’s method, which is implemented by the scipy function welch . The example below estimates the spectrum using Welch’s method and uses the same parameters as the example above. Note the much smoother noise floor of the spectrogram. Lomb-Scargle Periodograms ( lombscargle )  Least-squares spectral analysis (LSSA) 1 2 is a method of estimating a frequency spectrum, based on a least-squares fit of sinusoids to data samples, similar to Fourier analysis. Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in long-gapped records; LSSA mitigates such problems. The Lomb-Scargle method performs spectral analysis on unevenly-sampled data and is known to be a powerful way to find, and test the significance of, weak periodic signals. For a time series comprising measurements sampled at times , where , assumed to have been scaled and shifted, such that its mean is zero and its variance is unity, the normalized Lomb-Scargle periodogram at frequency is \[P_{n}(f) \frac{1}{2}\left\{\frac{\left[\sum_{j}^{N_{t}}X_{j}\cos\omega(t_{j}-\tau)\right]^{2}}{\sum_{j}^{N_{t}}\cos^{2}\omega(t_{j}-\tau)}+\frac{\left[\sum_{j}^{N_{t}}X_{j}\sin\omega(t_{j}-\tau)\right]^{2}}{\sum_{j}^{N_{t}}\sin^{2}\omega(t_{j}-\tau)}\right\}.\] Here, is the angular frequency. The frequency-dependent time offset is given by \[\tan 2\omega\tau  \frac{\sum_{j}^{N_{t}}\sin 2\omega t_{j}}{\sum_{j}^{N_{t}}\cos 2\omega t_{j}}.\] The lombscargle function calculates the periodogram using a slightly modified algorithm due to Townsend 3 , which allows the periodogram to be calculated using only a single pass through the input arrays for each frequency. The equation is refactored as: \[P_{n}(f)  \frac{1}{2}\left[\frac{(c_{\tau}XC + s_{\tau}XS)^{2}}{c_{\tau}^{2}CC + 2c_{\tau}s_{\tau}CS + s_{\tau}^{2}SS} + \frac{(c_{\tau}XS - s_{\tau}XC)^{2}}{c_{\tau}^{2}SS - 2c_{\tau}s_{\tau}CS + s_{\tau}^{2}CC}\right]\] and \[\tan 2\omega\tau  \frac{2CS}{CC-SS}.\] Here, \[c_{\tau}  \cos\omega\tau,\qquad s_{\tau}  \sin\omega\tau,\] while the sums are \[\begin{split}XC & \sum_{j}^{N_{t}} X_{j}\cos\omega t_{j}\\ XS & \sum_{j}^{N_{t}} X_{j}\sin\omega t_{j}\\ CC & \sum_{j}^{N_{t}} \cos^{2}\omega t_{j}\\ SS & \sum_{j}^{N_{t}} \sin^{2}\omega t_{j}\\ CS & \sum_{j}^{N_{t}} \cos\omega t_{j}\sin\omega t_{j}.\end{split}\] This requires trigonometric function evaluations giving a factor of speed increase over the straightforward implementation. "
"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Basic functionality  ARPACK can solve either standard eigenvalue problems of the form \[A \mathbf{x}  \lambda \mathbf{x}\] or general eigenvalue problems of the form \[A \mathbf{x}  \lambda M \mathbf{x}.\] The power of ARPACK is that it can compute only a specified subset of eigenvalue/eigenvector pairs. This is accomplished through the keyword . The following values of are available: : Eigenvalues with largest magnitude ( , ), that is, largest eigenvalues in the euclidean norm of complex numbers. : Eigenvalues with smallest magnitude ( , ), that is, smallest eigenvalues in the euclidean norm of complex numbers. : Eigenvalues with largest real part ( ). : Eigenvalues with smallest real part ( ). : Eigenvalues with largest imaginary part ( ). : Eigenvalues with smallest imaginary part ( ). : Eigenvalues with largest algebraic value ( ), that is, largest eigenvalues inclusive of any negative sign. : Eigenvalues with smallest algebraic value ( ), that is, smallest eigenvalues inclusive of any negative sign. : Eigenvalues from both ends of the spectrum ( ). Note that ARPACK is generally better at finding extremal eigenvalues, that is, eigenvalues with large magnitudes. In particular, using may lead to slow execution time and/or anomalous results. A better approach is to use shift-invert mode . "
"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Examples  Imagine you’d like to find the smallest and largest eigenvalues and the corresponding eigenvectors for a large matrix. ARPACK can handle many forms of input: dense matrices ,such as numpy.ndarray instances, sparse matrices, such as scipy.sparse.csr_matrix , or a general linear operator derived from scipy.sparse.linalg.LinearOperator . For this example, for simplicity, we’ll construct a symmetric, positive-definite matrix. We now have a symmetric matrix , with which to test the routines. First, compute a standard eigenvalue decomposition using : As the dimension of grows, this routine becomes very slow. Especially, if only a few eigenvectors and eigenvalues are needed, can be a better option. First let’s compute the largest eigenvalues ( ) of and compare them to the known results: The results are as expected. ARPACK recovers the desired eigenvalues and they match the previously known results. Furthermore, the eigenvectors are orthogonal, as we’d expect. Now, let’s attempt to solve for the eigenvalues with smallest magnitude: Oops. We see that, as mentioned above, is not quite as adept at finding small eigenvalues. There are a few ways this problem can be addressed. We could increase the tolerance ( ) to lead to faster convergence: This works, but we lose the precision in the results. Another option is to increase the maximum number of iterations ( ) from 1000 to 5000: We get the results we’d hoped for, but the computation time is much longer. Fortunately, contains a mode that allows a quick determination of non-external eigenvalues: shift-invert mode . As mentioned above, this mode involves transforming the eigenvalue problem to an equivalent problem with different eigenvalues. In this case, we hope to find eigenvalues near zero, so we’ll choose . The transformed eigenvalues will then satisfy , so our small eigenvalues become large eigenvalues . We get the results we were hoping for, with much less computational time. Note that the transformation from takes place entirely in the background. The user need not worry about the details. The shift-invert mode provides more than just a fast way to obtain a few small eigenvalues. Say, you desire to find internal eigenvalues and eigenvectors, e.g., those nearest to . Simply set and ARPACK will take care of the rest: The eigenvalues come out in a different order, but they’re all there. Note that the shift-invert mode requires the internal solution of a matrix inverse. This is taken care of automatically by and eigs , but the operation can also be specified by the user. See the docstring of scipy.sparse.linalg.eigsh and scipy.sparse.linalg.eigs for details. "
"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Introduction  ARPACK 1 is a Fortran package which provides routines for quickly finding a few eigenvalues/eigenvectors of large sparse matrices. In order to find these solutions, it requires only left-multiplication by the matrix in question. This operation is performed through a reverse-communication interface. The result of this structure is that ARPACK is able to find eigenvalues and eigenvectors of any linear function mapping a vector to a vector. All of the functionality provided in ARPACK is contained within the two high-level interfaces scipy.sparse.linalg.eigs and scipy.sparse.linalg.eigsh . eigs provides interfaces for finding the eigenvalues/vectors of real or complex nonsymmetric square matrices, while eigsh provides interfaces for real-symmetric or complex-hermitian matrices. "
"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  References  1 http://www.caam.rice.edu/software/ARPACK/ "
"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Shift-invert mode  Shift-invert mode relies on the following observation. For the generalized eigenvalue problem \[A \mathbf{x}  \lambda M \mathbf{x},\] it can be shown that \[(A - \sigma M)^{-1} M \mathbf{x}  \nu \mathbf{x},\] where \[\nu  \frac{1}{\lambda - \sigma}.\] "
"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Use of LinearOperator  We consider now the case where you’d like to avoid creating a dense matrix and use scipy.sparse.linalg.LinearOperator instead. Our first linear operator applies element-wise multiplication between the input vector and a vector provided by the user to the operator itself. This operator mimics a diagonal matrix with the elements of along the main diagonal and it has the main benefit that the forward and adjoint operations are simple element-wise multiplications other than matrix-vector multiplications. For a diagonal matrix, we expect the eigenvalues to be equal to the elements along the main diagonal, in this case . The eigenvalues and eigenvectors obtained with are compared to those obtained by using when applied to the dense matrix: In this case, we have created a quick and easy operator. The external library PyLops provides similar capabilities in the Diagonal operator, as well as several other operators. Finally, we consider a linear operator that mimics the application of a first-derivative stencil. In this case, the operator is equivalent to a real nonsymmetric matrix. Once again, we compare the estimated eigenvalues and eigenvectors with those from a dense matrix that applies the same first derivative to an input signal: Note that the eigenvalues of this operator are all imaginary. Moreover, the keyword of scipy.sparse.linalg.eigs produces the eigenvalues with largest absolute imaginary part (both positive and negative). Again, a more advanced implementation of the first-derivative operator is available in the PyLops library under the name of FirstDerivative operator. "
"scipy_spatial_data_structures_and_algorithms_scipy_spatial Spatial data structures and algorithms (scipy.spatial) spatial.html  Convex hulls  A convex hull is the smallest convex object containing all points in a given point set. These can be computed via the Qhull wrappers in scipy.spatial as follows: The convex hull is represented as a set of N 1-D simplices, which in 2-D means line segments. The storage scheme is exactly the same as for the simplices in the Delaunay triangulation discussed above. We can illustrate the above result: The same can be achieved with scipy.spatial.convex_hull_plot_2d . "
"scipy_spatial_data_structures_and_algorithms_scipy_spatial Spatial data structures and algorithms (scipy.spatial) spatial.html  Delaunay triangulations  The Delaunay triangulation is a subdivision of a set of points into a non-overlapping set of triangles, such that no point is inside the circumcircle of any triangle. In practice, such triangulations tend to avoid triangles with small angles. Delaunay triangulation can be computed using scipy.spatial as follows: We can visualize it: And add some further decorations: The structure of the triangulation is encoded in the following way: the attribute contains the indices of the points in the array that make up the triangle. For instance: Moreover, neighboring triangles can also be found: What this tells us is that this triangle has triangle #0 as a neighbor, but no other neighbors. Moreover, it tells us that neighbor 0 is opposite the vertex 1 of the triangle: Indeed, from the figure, we see that this is the case. Qhull can also perform tessellations to simplices for higher-dimensional point sets (for instance, subdivision into tetrahedra in 3-D). Coplanar points  It is important to note that not all points necessarily appear as vertices of the triangulation, due to numerical precision issues in forming the triangulation. Consider the above with a duplicated point: Observe that point #4, which is a duplicate, does not occur as a vertex of the triangulation. That this happened is recorded: This means that point 4 resides near triangle 0 and vertex 3, but is not included in the triangulation. Note that such degeneracies can occur not only because of duplicated points, but also for more complicated geometrical reasons, even in point sets that at first sight seem well-behaved. However, Qhull has the “QJ” option, which instructs it to perturb the input data randomly until degeneracies are resolved: Two new triangles appeared. However, we see that they are degenerate and have zero area. "
"scipy_spatial_data_structures_and_algorithms_scipy_spatial Spatial data structures and algorithms (scipy.spatial) spatial.html  Voronoi diagrams  A Voronoi diagram is a subdivision of the space into the nearest neighborhoods of a given set of points. There are two ways to approach this object using scipy.spatial . First, one can use the KDTree to answer the question “which of the points is closest to this one”, and define the regions that way: So the point belongs to region . In color: This does not, however, give the Voronoi diagram as a geometrical object. The representation in terms of lines and points can be again obtained via the Qhull wrappers in scipy.spatial : The Voronoi vertices denote the set of points forming the polygonal edges of the Voronoi regions. In this case, there are 9 different regions: Negative value again indicates a point at infinity. Indeed, only one of the regions, , is bounded. Note here that due to similar numerical precision issues as in Delaunay triangulation above, there may be fewer Voronoi regions than input points. The ridges (lines in 2-D) separating the regions are described as a similar collection of simplices as the convex hull pieces: These numbers present the indices of the Voronoi vertices making up the line segments. is again a point at infinity — only 4 of the 12 lines are a bounded line segment, while others extend to infinity. The Voronoi ridges are perpendicular to the lines drawn between the input points. To which two points each ridge corresponds is also recorded: This information, taken together, is enough to construct the full diagram. We can plot it as follows. First, the points and the Voronoi vertices: Plotting the finite line segments goes as for the convex hull, but now we have to guard for the infinite edges: The ridges extending to infinity require a bit more care: This plot can also be created using scipy.spatial.voronoi_plot_2d . Voronoi diagrams can be used to create interesting generative art. Try playing with the settings of this function to create your own! "
"scipy_special_functions_scipy_special Special functions (scipy.special) special.html  Bessel functions of real order( jv , jn_zeros )  Bessel functions are a family of solutions to Bessel’s differential equation with real or complex order alpha: \[x^2 \frac{d^2 y}{dx^2} + x \frac{dy}{dx} + (x^2 - \alpha^2)y  0\] Among other uses, these functions arise in wave propagation problems, such as the vibrational modes of a thin drum head. Here is an example of a circular drum head anchored at the edge: "
"scipy_special_functions_scipy_special Special functions (scipy.special) special.html  Cython Bindings for Special Functions ( scipy.special.cython_special )  SciPy also offers Cython bindings for scalar, typed versions of many of the functions in special. The following Cython code gives a simple example of how to use these functions: (See the Cython documentation for help with compiling Cython.) In the example the function works essentially like its ufunc counterpart gamma , though it takes C types as arguments instead of NumPy arrays. Note, in particular, that the function is overloaded to support real and complex arguments; the correct variant is selected at compile time. The function works slightly differently from sici ; for the ufunc we could write , whereas in the Cython version multiple return values are passed as pointers. It might help to think of this as analogous to calling a ufunc with an output array: . There are two potential advantages to using the Cython bindings: they avoid Python function overhead they do not require the Python Global Interpreter Lock (GIL) The following sections discuss how to use these advantages to potentially speed up your code, though, of course, one should always profile the code first to make sure putting in the extra effort will be worth it. Avoiding Python Function Overhead  For the ufuncs in special, Python function overhead is avoided by vectorizing, that is, by passing an array to the function. Typically, this approach works quite well, but sometimes it is more convenient to call a special function on scalar inputs inside a loop, for example, when implementing your own ufunc. In this case, the Python function overhead can become significant. Consider the following example: On one computer took about 131 microseconds to run and took about 18.2 microseconds to run. Obviously this example is contrived: one could just call and get results just as fast as in . The point is that if Python function overhead becomes significant in your code, then the Cython bindings might be useful. Releasing the GIL  One often needs to evaluate a special function at many points, and typically the evaluations are trivially parallelizable. Since the Cython bindings do not require the GIL, it is easy to run them in parallel using Cython’s function. For example, suppose that we wanted to compute the fundamental solution to the Helmholtz equation: \[\Delta_x G(x, y) + k^2G(x, y)  \delta(x - y),\] where is the wavenumber and is the Dirac delta function. It is known that in two dimensions the unique (radiating) solution is \[G(x, y)  \frac{i}{4}H_0^{(1)}(k|x - y|),\] where is the Hankel function of the first kind, i.e., the function hankel1 . The following example shows how we could compute this function in parallel: (For help with compiling parallel code in Cython see here .) If the above Cython code is in a file , then we can write an informal benchmark which compares the parallel and serial versions of the function: On one quad-core computer the serial method took 1.29 seconds and the parallel method took 0.29 seconds. "
"scipy_special_functions_scipy_special Special functions (scipy.special) special.html  Functions not in scipy.special  Some functions are not included in special because they are straightforward to implement with existing functions in NumPy and SciPy. To prevent reinventing the wheel, this section provides implementations of several such functions, which hopefully illustrate how to handle similar functions. In all examples NumPy is imported as and special is imported as . The binary entropy function : A rectangular step function on [0, 1]: Translating and scaling can be used to get an arbitrary step function. The ramp function : "
"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Analysing one sample  First, we create some random variables. We set a seed so that in each run we get identical results to look at. As an example we take a sample from the Student t distribution: Here, we set the required shape parameter of the t distribution, which in statistics corresponds to the degrees of freedom, to 10. Using size1000 means that our sample consists of 1000 independently drawn (pseudo) random numbers. Since we did not specify the keyword arguments loc and scale , those are set to their default values zero and one. Descriptive statistics  x is a numpy array, and we have direct access to all array methods, e.g., How do the sample properties compare to their theoretical counterparts? Note: stats.describe uses the unbiased estimator for the variance, while np.var is the biased estimator. For our sample the sample statistics differ a by a small amount from their theoretical counterparts. T-test and KS-test  We can use the t-test to test whether the mean of our sample differs in a statistically significant way from the theoretical expectation. The pvalue is 0.7, this means that with an alpha error of, for example, 10%, we cannot reject the hypothesis that the sample mean is equal to zero, the expectation of the standard t-distribution. As an exercise, we can calculate our ttest also directly without using the provided function, which should give us the same answer, and so it does: The Kolmogorov-Smirnov test can be used to test the hypothesis that the sample comes from the standard t-distribution Again, the p-value is high enough that we cannot reject the hypothesis that the random sample really is distributed according to the t-distribution. In real applications, we don’t know what the underlying distribution is. If we perform the Kolmogorov-Smirnov test of our sample against the standard normal distribution, then we also cannot reject the hypothesis that our sample was generated by the normal distribution given that, in this example, the p-value is almost 40%. However, the standard normal distribution has a variance of 1, while our sample has a variance of 1.29. If we standardize our sample and test it against the normal distribution, then the p-value is again large enough that we cannot reject the hypothesis that the sample came form the normal distribution. Note: The Kolmogorov-Smirnov test assumes that we test against a distribution with given parameters, since, in the last case, we estimated mean and variance, this assumption is violated and the distribution of the test statistic, on which the p-value is based, is not correct. Tails of the distribution  Finally, we can check the upper tail of the distribution. We can use the percent point function ppf, which is the inverse of the cdf function, to obtain the critical values, or, more directly, we can use the inverse of the survival function In all three cases, our sample has more weight in the top tail than the underlying distribution. We can briefly check a larger sample to see if we get a closer match. In this case, the empirical frequency is quite close to the theoretical probability, but if we repeat this several times, the fluctuations are still pretty large. We can also compare it with the tail of the normal distribution, which has less weight in the tails: The chisquare test can be used to test whether for a finite number of bins, the observed frequencies differ significantly from the probabilities of the hypothesized distribution. We see that the standard normal distribution is clearly rejected, while the standard t-distribution cannot be rejected. Since the variance of our sample differs from both standard distributions, we can again redo the test taking the estimate for scale and location into account. The fit method of the distributions can be used to estimate the parameters of the distribution, and the test is repeated using probabilities of the estimated distribution. Taking account of the estimated parameters, we can still reject the hypothesis that our sample came from a normal distribution (at the 5% level), but again, with a p-value of 0.95, we cannot reject the t-distribution. Special tests for normal distributions  Since the normal distribution is the most common distribution in statistics, there are several additional functions available to test whether a sample could have been drawn from a normal distribution. First, we can test if skew and kurtosis of our sample differ significantly from those of a normal distribution: These two tests are combined in the normality test In all three tests, the p-values are very low and we can reject the hypothesis that the our sample has skew and kurtosis of the normal distribution. Since skew and kurtosis of our sample are based on central moments, we get exactly the same results if we test the standardized sample: Because normality is rejected so strongly, we can check whether the normaltest gives reasonable results for other cases: When testing for normality of a small sample of t-distributed observations and a large sample of normal-distributed observations, then in neither case can we reject the null hypothesis that the sample comes from a normal distribution. In the first case, this is because the test is not powerful enough to distinguish a t and a normally distributed random variable in a small sample. "
"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Building specific distributions  The next examples shows how to build your own distributions. Further examples show the usage of the distributions and some statistical tests. Making a continuous distribution, i.e., subclassing  Making continuous distributions is fairly simple. Interestingly, the is now computed automatically: Be aware of the performance issues mentioned in Performance issues and cautionary remarks . The computation of unspecified common methods can become very slow, since only general methods are called, which, by their very nature, cannot use any specific information about the distribution. Thus, as a cautionary example: But this is not correct: the integral over this pdf should be 1. Let’s make the integration interval smaller: This looks better. However, the problem originated from the fact that the pdf is not specified in the class definition of the deterministic distribution. Subclassing  In the following, we use stats.rv_discrete to generate a discrete distribution that has the probabilities of the truncated normal for the intervals centered around the integers. General info From the docstring of rv_discrete, , “You can construct an arbitrary discrete rv where P{Xxk}  pk by passing to the rv_discrete initialization method (through the values keyword) a tuple of sequences (xk, pk) which describes only those values of X (xk) that occur with nonzero probability (pk).” Next to this, there are some further requirements for this approach to work: The keyword name is required. The support points of the distribution xk have to be integers. The number of significant digits (decimals) needs to be specified. In fact, if the last two requirements are not satisfied, an exception may be raised or the resulting numbers may be incorrect. An example Let’s do the work. First: And, finally, we can subclass : Now that we have defined the distribution, we have access to all common methods of discrete distributions. Testing the implementation Let’s generate a random sample and compare observed frequencies with the probabilities. Next, we can test whether our sample was generated by our norm-discrete distribution. This also verifies whether the random numbers were generated correctly. The chisquare test requires that there are a minimum number of observations in each bin. We combine the tail bins into larger bins so that they contain enough observations. The pvalue in this case is high, so we can be quite confident that our random sample was actually generated by the distribution. "
"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Comparing two samples  In the following, we are given two samples, which can come either from the same or from different distribution, and we want to test whether these samples have the same statistical properties. Comparing means  Test with sample with identical means: Test with sample with different means: Kolmogorov-Smirnov test for two samples ks_2samp  For the example, where both samples are drawn from the same distribution, we cannot reject the null hypothesis, since the pvalue is high In the second example, with different location, i.e., means, we can reject the null hypothesis, since the pvalue is below 1% "
"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Introduction  In this tutorial, we discuss many, but certainly not all, features of . The intention here is to provide a user with a working knowledge of this package. We refer to the reference manual for further details. Note: This documentation is work in progress. Discrete Statistical Distributions Continuous Statistical Distributions "
"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Kernel density estimation  A common task in statistics is to estimate the probability density function (PDF) of a random variable from a set of data samples. This task is called density estimation. The most well-known tool to do this is the histogram. A histogram is a useful tool for visualization (mainly because everyone understands it), but doesn’t use the available data very efficiently. Kernel density estimation (KDE) is a more efficient tool for the same task. The gaussian_kde estimator can be used to estimate the PDF of univariate as well as multivariate data. It works best if the data is unimodal. Univariate estimation  We start with a minimal amount of data in order to see how gaussian_kde works and what the different options for bandwidth selection do. The data sampled from the PDF are shown as blue dashes at the bottom of the figure (this is called a rug plot): We see that there is very little difference between Scott’s Rule and Silverman’s Rule, and that the bandwidth selection with a limited amount of data is probably a bit too wide. We can define our own bandwidth function to get a less smoothed-out result. We see that if we set bandwidth to be very narrow, the obtained estimate for the probability density function (PDF) is simply the sum of Gaussians around each data point. We now take a more realistic example and look at the difference between the two available bandwidth selection rules. Those rules are known to work well for (close to) normal distributions, but even for unimodal distributions that are quite strongly non-normal they work reasonably well. As a non-normal distribution we take a Student’s T distribution with 5 degrees of freedom. We now take a look at a bimodal distribution with one wider and one narrower Gaussian feature. We expect that this will be a more difficult density to approximate, due to the different bandwidths required to accurately resolve each feature. As expected, the KDE is not as close to the true PDF as we would like due to the different characteristic size of the two features of the bimodal distribution. By halving the default bandwidth ( ), we can do somewhat better, while using a factor 5 smaller bandwidth than the default doesn’t smooth enough. What we really need, though, in this case, is a non-uniform (adaptive) bandwidth. Multivariate estimation  With gaussian_kde we can perform multivariate, as well as univariate estimation. We demonstrate the bivariate case. First, we generate some random data with a model in which the two variates are correlated. Then we apply the KDE to the data: Finally, we plot the estimated bivariate distribution as a colormap and plot the individual data points on top. Multiscale Graph Correlation (MGC)  With multiscale_graphcorr , we can test for independence on high dimensional and nonlinear data. Before we start, let’s import some useful packages: Let’s use a custom plotting function to plot the data relationship: Let’s look at some linear data first: The simulation relationship can be plotted below: Now, we can see the test statistic, p-value, and MGC map visualized below. The optimal scale is shown on the map as a red “x”: It is clear from here, that MGC is able to determine a relationship between the input data matrices because the p-value is very low and the MGC test statistic is relatively high. The MGC-map indicates a strongly linear relationship . Intuitively, this is because having more neighbors will help in identifying a linear relationship between and . The optimal scale in this case is equivalent to the global scale , marked by a red spot on the map. The same can be done for nonlinear data sets. The following and arrays are derived from a nonlinear simulation: The simulation relationship can be plotted below: Now, we can see the test statistic, p-value, and MGC map visualized below. The optimal scale is shown on the map as a red “x”: It is clear from here, that MGC is able to determine a relationship again because the p-value is very low and the MGC test statistic is relatively high. The MGC-map indicates a strongly nonlinear relationship . The optimal scale in this case is equivalent to the local scale , marked by a red spot on the map. "
"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Random variables  There are two general distribution classes that have been implemented for encapsulating continuous random variables and discrete random variables . Over 80 continuous random variables (RVs) and 10 discrete random variables have been implemented using these classes. Besides this, new routines and distributions can be easily added by the end user. (If you create one, please contribute it.) All of the statistics functions are located in the sub-package scipy.stats and a fairly complete listing of these functions can be obtained using . The list of the random variables available can also be obtained from the docstring for the stats sub-package. In the discussion below, we mostly focus on continuous RVs. Nearly everything also applies to discrete variables, but we point out some differences here: Specific points for discrete distributions . In the code samples below, we assume that the scipy.stats package is imported as and in some cases we assume that individual objects are imported as Getting help  First of all, all distributions are accompanied with help functions. To obtain just some basic information, we print the relevant docstring: . To find the support, i.e., upper and lower bounds of the distribution, call: We can list all methods and properties of the distribution with . As it turns out, some of the methods are private, although they are not named as such (their names do not start with a leading underscore), for example , are only available for internal calculation (those methods will give warnings when one tries to use them, and will be removed at some point). To obtain the real main methods, we list the methods of the frozen distribution. (We explain the meaning of a frozen distribution below). Finally, we can obtain the list of available distribution through introspection: Common methods  The main public methods for continuous RVs are: rvs: Random Variates pdf: Probability Density Function cdf: Cumulative Distribution Function sf: Survival Function (1-CDF) ppf: Percent Point Function (Inverse of CDF) isf: Inverse Survival Function (Inverse of SF) stats: Return mean, variance, (Fisher’s) skew, or (Fisher’s) kurtosis moment: non-central moments of the distribution Let’s take a normal RV as an example. To compute the at a number of points, we can pass a list or a numpy array. Thus, the basic methods, such as pdf , cdf , and so on, are vectorized. Other generally useful methods are supported too: To find the median of a distribution, we can use the percent point function , which is the inverse of the : To generate a sequence of random variates, use the keyword argument: Note that drawing random numbers relies on generators from numpy.random package. In the example above, the specific stream of random numbers is not reproducible across runs. To achieve reproducibility, you can explicitly seed a global variable Relying on a global state is not recommended, though. A better way is to use the random_state parameter, which accepts an instance of numpy.random.RandomState class, or an integer, which is then used to seed an internal object: Don’t think that generates 5 variates: Here, with no keyword is being interpreted as the first possible keyword argument, , which is the first of a pair of keyword arguments taken by all continuous distributions. This brings us to the topic of the next subsection. Shifting and scaling  All continuous distributions take and as keyword parameters to adjust the location and scale of the distribution, e.g., for the standard normal distribution, the location is the mean and the scale is the standard deviation. In many cases, the standardized distribution for a random variable is obtained through the transformation . The default values are and . Smart use of and can help modify the standard distributions in many ways. To illustrate the scaling further, the of an exponentially distributed RV with mean is given by \[F(x)  1 - \exp(-\lambda x)\] By applying the scaling rule above, it can be seen that by taking we get the proper scale. Note Distributions that take shape parameters may require more than simple application of and/or to achieve the desired form. For example, the distribution of 2-D vector lengths given a constant vector of length perturbed by independent N(0, ) deviations in each component is rice( , scale ). The first argument is a shape parameter that needs to be scaled along with . The uniform distribution is also interesting: Finally, recall from the previous paragraph that we are left with the problem of the meaning of . As it turns out, calling a distribution like this, the first argument, i.e., the 5, gets passed to set the parameter. Let’s see: Thus, to explain the output of the example of the last section: generates a single normally distributed random variate with mean , because of the default . We recommend that you set and parameters explicitly, by passing the values as keywords rather than as arguments. Repetition can be minimized when calling more than one method of a given RV by using the technique of Freezing a Distribution , as explained below. Shape parameters  While a general continuous random variable can be shifted and scaled with the and parameters, some distributions require additional shape parameters. For instance, the gamma distribution with density \[\gamma(x, a)  \frac{\lambda (\lambda x)^{a-1}}{\Gamma(a)} e^{-\lambda x}\;,\] requires the shape parameter . Observe that setting can be obtained by setting the keyword to . Let’s check the number and name of the shape parameters of the gamma distribution. (We know from the above that this should be 1.) Now, we set the value of the shape variable to 1 to obtain the exponential distribution, so that we compare easily whether we get the results we expect. Notice that we can also specify shape parameters as keywords: Freezing a distribution  Passing the and keywords time and again can become quite bothersome. The concept of freezing a RV is used to solve such problems. By using we no longer have to include the scale or the shape parameters anymore. Thus, distributions can be used in one of two ways, either by passing all distribution parameters to each method call (such as we did earlier) or by freezing the parameters for the instance of the distribution. Let us check this: This is, indeed, what we should get. Broadcasting  The basic methods , and so on, satisfy the usual numpy broadcasting rules. For example, we can calculate the critical values for the upper tail of the t distribution for different probabilities and degrees of freedom. Here, the first row contains the critical values for 10 degrees of freedom and the second row for 11 degrees of freedom (d.o.f.). Thus, the broadcasting rules give the same result of calling twice: If the array with probabilities, i.e., and the array of degrees of freedom i.e., , have the same array shape, then element-wise matching is used. As an example, we can obtain the 10% tail for 10 d.o.f., the 5% tail for 11 d.o.f. and the 1% tail for 12 d.o.f. by calling Specific points for discrete distributions  Discrete distributions have mostly the same basic methods as the continuous distributions. However is replaced by the probability mass function , no estimation methods, such as fit, are available, and is not a valid keyword parameter. The location parameter, keyword , can still be used to shift the distribution. The computation of the cdf requires some extra attention. In the case of continuous distribution, the cumulative distribution function is, in most standard cases, strictly monotonic increasing in the bounds (a,b) and has, therefore, a unique inverse. The cdf of a discrete distribution, however, is a step function, hence the inverse cdf, i.e., the percent point function, requires a different definition: For further info, see the docs here . We can look at the hypergeometric distribution as an example If we use the cdf at some integer points and then evaluate the ppf at those cdf values, we get the initial integers back, for example If we use values that are not at the kinks of the cdf step function, we get the next higher integer back: Fitting distributions  The main additional methods of the not frozen distribution are related to the estimation of distribution parameters: fit: maximum likelihood estimation of distribution parameters, including location and scale fit_loc_scale: estimation of location and scale when shape parameters are given nnlf: negative log likelihood function expect: calculate the expectation of a function against the pdf or pmf Performance issues and cautionary remarks  The performance of the individual methods, in terms of speed, varies widely by distribution and method. The results of a method are obtained in one of two ways: either by explicit calculation, or by a generic algorithm that is independent of the specific distribution. Explicit calculation, on the one hand, requires that the method is directly specified for the given distribution, either through analytic formulas or through special functions in or for . These are usually relatively fast calculations. The generic methods, on the other hand, are used if the distribution does not specify any explicit calculation. To define a distribution, only one of pdf or cdf is necessary; all other methods can be derived using numeric integration and root finding. However, these indirect methods can be very slow. As an example, creates random variables in a very indirect way and takes about 19 seconds for 100 random variables on my computer, while one million random variables from the standard normal or from the t distribution take just above one second. Remaining issues  The distributions in have recently been corrected and improved and gained a considerable test suite; however, a few issues remain: The distributions have been tested over some range of parameters; however, in some corner ranges, a few incorrect results may remain. The maximum likelihood estimation in fit does not work with default starting parameters for all distributions and the user needs to supply good starting parameters. Also, for some distribution using a maximum likelihood estimator might inherently not be the best choice. "