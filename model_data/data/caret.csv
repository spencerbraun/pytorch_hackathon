text
"caret_11_subsampling_for_class_imbalances 11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html  11.1 Subsampling Techniques To illustrate these methods, let’s simulate some data with a class imbalance using this method. We will simulate a training and test set where each contains 10000 samples and a minority class rate of about 5.9%: Let’s create different versions of the training set prior to model tuning: For these data, we’ll use a bagged classification and estimate the area under the ROC curve using five repeats of 10-fold CV. We will collate the resampling results and create a wrapper to estimate the test set performance: The training and test set estimates for the area under the ROC curve do not appear to correlate. Based on the resampling results, one would infer that up-sampling is nearly perfect and that ROSE does relatively poorly. The reason that up-sampling appears to perform so well is that the samples in the majority class are replicated and have a large potential to be in both the model building and hold-out sets. In essence, the hold-outs here are not truly independent samples. In reality, all of the sampling methods do about the same (based on the test set). The statistics for the basic model fit with no sampling are fairly in-line with one another (0.939 via resampling and 0.925 for the test set). "
"caret_11_subsampling_for_class_imbalances 11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html  11.2 Subsampling During Resampling Recent versions of caret allow the user to specify subsampling when using train so that it is conducted inside of resampling. All four methods shown above can be accessed with the basic package using simple syntax. If you want to use your own technique, or want to change some of the parameters for SMOTE or ROSE , the last section below shows how to use custom subsampling. The way to enable subsampling is to use yet another option in trainControl called sampling . The most basic syntax is to use a character string with the name of the sampling method, either "down" , "up" , "smote" , or "rose" . Note that you will need to have the DMwR and ROSE packages installed to use SMOTE and ROSE, respectively. One complication is related to pre-processing. Should the subsampling occur before or after the pre-processing? For example, if you down-sample the data and using PCA for signal extraction, should the loadings be estimated from the entire training set? The estimate is potentially better since the entire training set is being used but the subsample may happen to capture a small potion of the PCA space. There isn’t any obvious answer. The default behavior is to subsample the data prior to pre-processing. This can be easily changed and an example is given below. Now let’s re-run our bagged tree models while sampling inside of cross-validation: Here are the resampling and test set results: The figure below shows the difference in the area under the ROC curve and the test set results for the approaches shown here. Repeating the subsampling procedures for every resample produces results that are more consistent with the test set. "
"caret_11_subsampling_for_class_imbalances 11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html  11.3 Complications The user should be aware that there are a few things that can happening when subsampling that can cause issues in their code. As previously mentioned, when sampling occurs in relation to pre-processing is one such issue. Others are: Sparsely represented categories in factor variables may turn into zero-variance predictors or may be completely sampled out of the model. The underlying functions that do the sampling (e.g. SMOTE , downSample , etc) operate in very different ways and this can affect your results. For example, SMOTE and ROSE will convert your predictor input argument into a data frame (even if you start with a matrix). Currently, sample weights are not supported with sub-sampling. If you use tuneLength to specify the search grid, understand that the data that is used to determine the grid has not been sampled. In most cases, this will not matter but if the grid creation process is affected by the sample size, you may end up using a sub-optimal tuning grid. For some models that require more samples than parameters, a reduction in the sample size may prevent you from being able to fit the model. "
"caret_11_subsampling_for_class_imbalances 11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html  11.4 Using Custom Subsampling Techniques Users have the ability to create their own type of subsampling procedure. To do this, alternative syntax is used with the sampling argument of the trainControl . Previously, we used a simple string as the value of this argument. Another way to specify the argument is to use a list with three (named) elements: The name value is a character string used when the train object is printed. It can be any string. The func element is a function that does the subsampling. It should have arguments called x and y that will contain the predictors and outcome data, respectively. The function should return a list with elements of the same name. The first element is a single logical value that indicates whether the subsampling should occur first relative to pre-process. A value of FALSE means that the subsampling function will receive the sampled versions of x and y . For example, here is what the list version of the sampling argument looks like when simple down-sampling is used: As another example, suppose we want to use SMOTE but use 10 nearest neighbors instead of the default of 5. To do this, we can create a simple wrapper around the SMOTE function and call this instead: The control object would then be: "
"caret_12_using_recipes_with_train 12 Using Recipes with train using-recipes-with-train.html  12.1 Why Should you learn this? Here are two reasons: 12.1.1 More versatile tools for preprocessing data caret ’s preprocessing tools have a lot of options but the list is not exhaustive and they will only be called in a specific order. If you would like a broader set of options, the ability to write your own preprocessing tools, or to call them in the order that you desire then you can use a recipe to do that. 12.1.2 Using additional data to measure performance In most modeling functions, including train , most variables are consigned to be either predictors or outcomes. For recipes, there are more options. For example, you might want to have specific columns of your data set be available when you compute how well the model is performing, such as: if different stratification variables (e.g. patients, ZIP codes, etc) are required to do correct summaries or ancillary data might be need to compute the expected profit or loss based on the model results. To get these data properly, they need to be made available and handled the same way as all of the other data. This means they should be sub- or resampled as all of the other data. Recipes let you do that. "
"caret_12_using_recipes_with_train 12 Using Recipes with train using-recipes-with-train.html  12.2 An Example The QSARdata package contains several chemistry data sets. These data sets have rows for different potential drugs (called “compounds” here). For each compound, some important characteristic is measured. This illustration will use the AquaticTox data. The outcome is called “Activity” is a measure of how harmful the compound might be to people. We want to predict this during the drug discovery phase in R&D To do this, a set of molecular descriptors are computed based on the compounds formula. There are a lot of different types of these and we will use the 2-dimensional MOE descriptor set. First, lets’ load the package and get the data together: We will build a model on these data to predict the activity. Some notes: A common aspect to chemical descriptors is that they are highly correlated . Many descriptors often measure some variation of the same thing. For example, in these data, there are 56 potential predictors that measure different flavors of surface area. It might be a good idea to reduce the dimensionality of these data by pre-filtering the predictors and/or using a dimension reduction technique. Other descriptors are counts of certain types of aspects of the molecule. For example, one predictor is the number of Bromine atoms. The vast majority of compounds lack Bromine and this leads to a near-zero variance situation discussed previously. It might be a good idea to pre-filter these. Also, to demonstrate the utility of recipes, suppose that we could score potential drugs on the basis of how manufacturable they might be. We might want to build a model on the entire data set but only evaluate it on compounds that could be reasonably manufactured. For illustration, we’ll assume that, as a compounds molecule weight increases, its manufacturability decreases . For this purpose, we create a new variable ( manufacturability ) that is neither an outcome or predictor but will be needed to compute performance. For this analysis, we will compute the RMSE using weights based on the manufacturability column such that a difficult compound has less impact on the RMSE. There is no way to include this extra variable using the default train method or using train.formula . Now, let’s create a recipe incrementally. First, we will use the formula methods to declare the outcome and predictors but change the analysis role of the manufacturability variable so that it will only be available when summarizing the model fit. Using this new role, the manufacturability column will be available when the summary function is executed and the appropriate rows of the data set will be exposed during resampling. For example, if one were to debug the model_stats function during execution of a model, the data object might look like this: More than one variable can have this role so that multiple columns can be made available. Now let’s add some steps to the recipe First, we remove sparse and unbalanced predictors: Note that we have only specified what will happen once the recipe is executed. This is only a specification that uses a generic declaration of all_predictors . As mentioned above, there are a lot of different surface area predictors and they tend to have very high correlations with one another. We’ll add one or more predictors to the model in place of these predictors using principal component analysis. The step will retain the number of components required to capture 95% of the information contained in these 56 predictors. We’ll name these new predictors surf_area_1 , surf_area_2 etc. Now, lets specific that the third step in the recipe is to reduce the number of predictors so that no pair has an absolute correlation greater than 0.90. However, we might want to keep the surface area principal components so we exclude these from the filter (using the minus sign) Finally, we can center and scale all of the predictors that are available at the end of the recipe: Let’s use this recipe to fit a SVM model and pick the tuning parameters that minimize the weighted RMSE value: What variables were generated by the recipe? The trained recipe is available in the train object and now shows specific variables involved in each step: "
"caret_12_using_recipes_with_train 12 Using Recipes with train using-recipes-with-train.html  12.3 Case Weights For models that accept them , case weights can be passed to the model fitting routines using a role of "case weight" . "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.1 Introduction The package contains a large number of predictive model interfaces. However, you may want to create your own because: you are testing out a novel model or the package doesn’t have a model that you are interested in you would like to run an existing model in the package your own way there are pre-processing or sampling steps not contained in the package or you just don’t like the way the package does things You can still get the benefits of the caret infrastructure by creating your own model. Currently, when you specify the type of model that you are interested in (e.g. type  "lda" ), the train function runs another function called getModelInfo to retrieve the specifics of that model from the existing catalog. For example: To use your own model, you can pass a list of these components to type . This page will describe those components in detail. "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.2 Illustrative Example 1: SVMs with Laplacian Kernels The package currently contains support vector machine (SVM) models using linear, polynomial and radial basis function kernels. The kernlab package has other functions, including the Laplacian kernel. We will illustrate the model components for this model, which has two parameters: the standard cost parameter for SVMs and one kernel parameter ( sigma ) "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.3 Model Components You can pass a list of information to the method argument in train . For models that are built-in to the package, you can just pass the method name as before. There are some basic components of the list for custom models. A brief description is below for each then, after setting up and example, each will be described in detail. The list should have the following elements: library is a character vector of package names that will be needed to fit the model or calculate predictions. NULL can also be used. type is a simple character vector with values "Classification" , "Regression" or both. parameters is a data frame with three simple attributes for each tuning parameter (if any): the argument name (e.g. mtry ), the type of data in the parameter grid and textual labels for the parameter. grid is a function that is used to create the tuning grid (unless the user gives the exact values of the parameters via tuneGrid ) fit is a function that fits the model predict is the function that creates predictions prob is a function that can be used to create class probabilities (if applicable) sort is a function that sorts the parameter from most complex to least loop is an optional function for advanced users for models that can create multiple submodel predictions from the same object. levels is an optional function, primarily for classification models using S4 methods to return the factor levels of the outcome. tags is an optional character vector that has subjects associated with the model, such as Tree-Based Model or Embedded Feature Selection . This string is used by the package to create additional documentation pages on the package website. label is an optional character string that names the model (e.g. “Linear Discriminant Analysis”). predictors is an optional function that returns a character vector that contains the names of the predictors that we used in the prediction equation. varImp is an optional function that calculates variable importance metrics for the model (if any). oob is another optional function that calculates out-of-bag performance estimates from the model object. Most models do not have this capability but some (e.g. random forests, bagged models) do. notes is an optional character vector that can be used to document non-obvious aspects of the model. For example, there are two Bayesian lasso models ( blasso and blassoAveraged ) and this field is used to describe the differences between the two models. check is an optional function that can be used to check the system/install to make sure that any atypical software requirements are available to the user. The input is pkg , which is the same character string given by the library . This function is run after the checking function to see if the packages specified in library are installed. As an example, the model pythonKnnReg uses certain python libraries and the user should have python and these libraries installed. The model file demonstrates how to check for python libraries prior to running the R model. In the caret package, the subdirectory models has all the code for each model that train interfaces with and these can be used as prototypes for your model. Let’s create a new model for a classification support vector machin using the Laplacian kernel function. We will use the kernlab package’s ksvm function. The kernel has two parameters: the standard cost parameter for SVMs and one kernel parameter ( sigma ). To start, we’ll create a new list: This model can also be used for regression but we will constrain things here for simplicity. For other SVM models, the type value would be c("Classification", "Regression") . The library value checks to see if this package is installed and loads it whenever it is needed (e.g. before modeling or prediction). Note : caret will check to see if these packages are installed but will not explicitly load them. As such, functions that are used from the package should be referenced by namespace. This is discussed more below when describing the fit function. 13.3.1 The parameters Element We have to create some basic information for the parameters in the form of a data frame. The first column is the name of the parameter. The convention is to use the argument name in the model function (e.g. the ksvm function here). Those values are C and sigma . Each is a number and we can give them labels of "Cost" and "Sigma" , respectively. The parameters element would then be: Now we assign it to the model list: Values of type can indicate numeric, character or logical data types. 13.3.2 The grid Element This should be a function that takes parameters: x and y (for the predictors and outcome data), len (the number of values per tuning parameter) as well as search . len is the value of tuneLength that is potentially passed in through train . search can be either "grid" or "random" . This can be used to setup a grid for searching or random values for random search. The output should be a data frame of tuning parameter combinations with a column for each parameter. The column names should be the parameter name (e.g. the values of prm$parameter ). In our case, let’s vary the cost parameter on the log 2 scale. For the sigma parameter, we can use the kernlab function sigest to pre-estimate the value. Following ksvm we take the average of the low and high estimates. Here is a function we could use: Why did we use kernlab::sigest instead of sigest ? As previously mentioned, caret will not execute library(kernlab) unless you explicitly code it in these functions. Since it is not explicitly loaded, you have to call it using the namespace operator :: . Again, the user can pass their own grid via train ’s tuneGrid option or they can use this code to create a default grid. We assign this function to the overall model list: 13.3.3 The fit Element Here is where we fit the model. This fit function has several arguments: x , y : the current data used to fit the model wts : optional instance weights (not applicable for this particular model) param : the current tuning parameter values lev : the class levels of the outcome (or NULL in regression) last : a logical for whether the current fit is the final fit weights classProbs : a logical for whether class probabilities should be computed. Here is something we could use for this model: A few notes about this: Notice that the package is not loaded in the code. It is loaded prior to this function being called so it won’t hurt if you load it again (but that’s not needed). The ksvm function requires a matrix or predictors. If the original data were a data frame, this would throw and error. The tuning parameters are references in the param data frame. There is always a single row in this data frame. The probability model is fit based on the value of classProbs . This value is determined by the value given in trainControl . The three dots allow the user to pass options in from train to, in this case, the ksvm function. For example, if the use wanted to set the cache size for the function, they could list cache  80 and this argument will be pass from train to ksvm . Any pre-processing that was requested in the call to train have been done. For example, if preProc  "center" was originally requested, the columns of x seen within this function are mean centered. Again, the namespace operator :: is used for rbfdot and ksvm to ensure that the function can be found. 13.3.4 The predict Element This is a function that produces a vector or predictions. In our case these are class predictions but they could be numbers for regression models. The arguments are: modelFit : the model produced by the fit code shown above. newdata : the predictor values of the instances being predicted (e.g. out-of-bag samples) preProc submodels : this an optional list of tuning parameters only used with the loop element discussed below. In most cases, it will be NULL . Our function will be very simple: The function predict.ksvm will automatically create a factor vector as output. The function could also produce character values. Either way, the innards of train will make them factors and ensure that the same levels as the original data are used. 13.3.5 The prob Element If a regression model is being used or if the classification model does not create class probabilities a value of NULL can be used here instead of a function. Otherwise, the function arguments are the same as the pred function. The output should be a matrix or data frame of class probabilities with a column for each class. The column names should be the class levels. We can use: If you look at some of the SVM examples in the models directory, the real functions used by train are much more complicated so that they can deal with model failures, probabilities that do not sum to 1 etc. "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.4 The sort Element This is an optional function that sorts the tuning parameters from the simplest model to the most complex. There are times where this ordering is not obvious. This information is used when the performance values are tied across multiple parameters. We would probably want to choose the least complex model in those cases. Here, we will sort by the cost value. Smaller values of C produce smoother class boundaries than larger values: 13.4.1 The levels Element train ensures that classification models always predict factors with the same levels. To do this at prediction time, the package needs to know the levels from the model object (specifically, the finalModels slot of the train object). For model functions using S3 methods, train automatically attaches a character vector called obsLevels to the object and the package code uses this value. However, this strategy does not work for S4 methods. In these cases, the package will use the code found in the levels slot of the model list. For example, the ksvm function uses S4 methods but, unlike most model functions, has a built–in function called lev that will extract the class levels (if any). In this case, our levels code would be: In most other cases, the levels will beed to be extracted from data contained in the fitted model object. As another example, objects created using the ctree function in the party package would need to use: Again, this slot is only used for classification models using S4 methods. We should now be ready to fit our model. A plot of the data shows that the model doesn’t change when the cost value is above 16. "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.5 Illustrative Example 2: Something More Complicated - LogitBoost ###The loop Element This function can be used to create custom loops for models to tune over. In most cases, the function can just return the existing tuning grid. For example, a LogitBoost model can be trained over the number of boosting iterations. In the caTools package, the LogitBoost function can be used to fit this model. For example: If we were to tune the model evaluating models where the number of iterations was 11, 21, 31, 41 and 51, the grid could be During resampling, train could loop over all five rows in lbGrid and fit five models. However, the predict.LogitBoost function has an argument called nIter that can produce, in this case, predictions from mod for all five models. Instead of train fitting five models, we could fit a single model with nIter  class“hl num”>51 and derive predictions for all five models using only mod`. The terminology used here is that nIter is a sequential tuning parameter (and the other parameters would be considered fixed ). The loop argument for models is used to produce two objects: loop : this is the actual loop that is used by train . submodels is a list that has as many elements as there are rows in loop . The list has all the “extra” parameter settings that can be derived for each model. Going back to the LogitBoost example, we could have: For this case, train first fits the nIter  51 model. When the model is predicted, that code has a for loop that iterates over the elements of submodel[[1]] to get the predictions for the other 4 models. In the end, predictions for all five models (for nIter  seq(11, 51, by  10) ) with a single model fit. There are other models built-in to caret that are used this way. There are a number of models that have multiple sequential tuning parameters. If the loop argument is left NULL the results of tuneGrid are used as the simple loop and is recommended for most situations. Note that the machinery that is used to “derive” the extra predictions is up to the user to create, typically in the predict and prob elements of the custom model object. For the LogitBoost model, some simple code to create these objects would be: For the LogitBoost custom model object, we could use this code in the predict slot: A few more notes: The code in the fit element does not have to change. The prob slot works in the same way. The only difference is that the values saved in the outgoing lists are matrices or data frames of probabilities for each class. After model training (i.e. predicting new samples), the value of submodels is set to NULL and the code produces a single set of predictions. If the model had one sequential parameter and one fixed parameter, the loop data frame would have two columns (one for each parameter). If the model is tuned over more than one value of the fixed parameter, the submodels list would have more than one element. If loop had 10 rows, then length(submodels) would be 10 and loop[i,] would be linked to submodels[[i]] . In this case, the prediction function was called by namespace too (i.e. caTools::predict.LogitBoost ). This may not seem necessary but what functions are available can vary depending on what parallel processing technology is being used. For example, the nature of forking used by doMC and doParallel tends to have easier access to functions while PSOCK methods in doParallel do not. It may be easier to take the safe path of using the namespace operator wherever possible to avoid errors that are difficult to track down. Here is a slimmed down version of the logitBoost code already in the package: Should you care about this? Let’s tune the model over the same data set used for the SVM model above and see how long it takes: On a data set with 157 instances and 60 predictors and a model that is tuned over only 3 parameter values, there is a 1.57-fold speed-up. If the model were more computationally taxing or the data set were larger or the number of tune parameters that were evaluated was larger, the speed-up would increase. Here is a plot of the speed-up for a few more values of tuneLength : The speed-ups show a significant decrease in training time using this method. Note: The previous examples were run using parallel processing. The remainder in this chapter are run sequentially and, for simplicity, the namespace operator is not used in the custom code modules below. "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.6 Illustrative Example 3: Nonstandard Formulas (Note: the previous third illustration (“SMOTE During Resampling”) is no longer needed due to the inclusion of subsampling via train .) One limitation of train is that it requires the use of basic model formulas. There are several functions that use special formulas or operators on predictors that won’t (and perhaps should not) work in the top level call to train . However, we can still fit these models. Here is an example using the mboost function in the mboost package from the help page. We can create a custom model that mimics this code so that we can obtain resampling estimates for this specific model: "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.7 Illustrative Example 4: PLS Feature Extraction Pre-Processing PCA is a common tool for feature extraction prior to modeling but is unsupervised . Partial Least Squares (PLS) is essentially a supervised version of PCA. For some data sets, there may be some benefit to using PLS to generate new features from the original data (the PLS scores) then use those as an input into a different predictive model. PLS requires parameter tuning. In the example below, we use PLS on a data set with highly correlated predictors then use the PLS scores in a random forest model. The “trick” here is to save the PLS loadings along with the random forest model fit so that the loadings can be used on future samples for prediction. Also, the PLS and random forest models are jointly tuned instead of an initial modeling process that finalizes the PLS model, then builds the random forest model separately. In this was we optimize both at once. Another important point is that the resampling results reflect the variability in the random forest and PLS models. If we did PLS up-front then resampled the random forest model, we would under-estimate the noise in the modeling process. The tecator spectroscopy data are used: Here is the model code: We fit the models and look at the resampling results for the joint model: The test set results indicate that these data like the linear model more than anything: "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.8 Illustrative Example 5: Optimizing probability thresholds for class imbalances This description was originally posted on this blog. One of the toughest problems in predictive model occurs when the classes have a severe imbalance. In our book , we spend an entire chapter on this subject itself. One consequence of this is that the performance is generally very biased against the class with the smallest frequencies. For example, if the data have a majority of samples belonging to the first class and very few in the second class, most predictive models will maximize accuracy by predicting everything to be the first class. As a result there’s usually great sensitivity but poor specificity. As a demonstration will use a simulation system described here . By default it has about a 50-50 class frequency but we can change this by altering the function argument called intercept : There is almost a 9:1 imbalance in these data. Let’s use a standard random forest model with these data using the default value of mtry . We’ll also use repeated 10-fold cross validation to get a sense of performance: The area under the ROC curve is very high, indicating that the model has very good predictive power for these data. The plot shows the default probability cut off value of 50%. The sensitivity and specificity values associated with this point indicate that performance is not that good when an actual call needs to be made on a sample. One of the most common ways to deal with this is to determine an alternate probability cut off using the ROC curve. But to do this well, another set of data (not the test set) is needed to set the cut off and the test set is used to validate it. We don’t have a lot of data this is difficult since we will be spending some of our data just to get a single cut off value. Alternatively the model can be tuned, using resampling, to determine any model tuning parameters as well as an appropriate cut off for the probabilities. Suppose the model has one tuning parameter and we want to look at four candidate values for tuning. Suppose we also want to tune the probability cut off over 20 different thresholds. Now we have to look at 20×480 different models (and that is for each resample). One other feature that has been opened up his ability to use sequential parameters: these are tuning parameters that don’t require a completely new model fit to produce predictions. In this case, we can fit one random forest model and get it’s predicted class probabilities and evaluate the candidate probability cutoffs using these same hold-out samples. Here is what the model code looks like: Basically, we define a list of model components (such as the fitting code, the prediction code, etc.) and feed this into the train function instead of using a pre-listed model string (such as method  "rf" ). For this model and these data, there was an 8% increase in training time to evaluate 20 additional values of the probability cut off. How do we optimize this model? Normally we might look at the area under the ROC curve as a metric to choose our final values. In this case the ROC curve is independent of the probability threshold so we have to use something else. A common technique to evaluate a candidate threshold is see how close it is to the perfect model where sensitivity and specificity are one. Our code will use the distance between the current model’s performance and the best possible performance and then have train minimize this distance when choosing it’s parameters. Here is the code that we use to calculate this: Using ggplot(mod1) will show the performance profile. Instead here is a plot of the sensitivity, specificity, and distance to the perfect model: You can see that as we increase the probability cut off for the first class it takes more and more evidence for a sample to be predicted as the first class. As a result the sensitivity goes down when the threshold becomes very large. The upside is that we can increase specificity in the same way. The blue curve shows the distance to the perfect model. The value of 0.89 was found to be optimal. Now we can use the test set ROC curve to validate the cut off we chose by resampling. Here the cut off closest to the perfect model is 0.89. We were able to find a good probability cut off value without setting aside another set of data for tuning the cut off. One great thing about this code is that it will automatically apply the optimized probability threshold when predicting new samples. "
"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.9 Illustrative Example 6: Offsets in Generalized Linear Models Like the mboost example above , a custom method is required since a formula element is used to set the offset variable. Here is an example from ?glm : We can write a small custom method to duplicate this model. Two details of note: If we have factors in the data and do not want train to convert them to dummy variables, the formula method for train should be avoided. We can let glm do that inside the custom method. This would help glm understand that the dummy variable columns came from the same original factor. This will avoid errors in other functions used with glm (e.g. anova ). The slot for x should include any variables that are on the right-hand side of the model formula, including the offset column. Here is the custom model: "
"caret_15_variable_importance 15 Variable Importance variable-importance.html  15.1 Model Specific Metrics The following methods for estimating the contribution of each variable to the model are available: Linear Models : the absolute value of the t -statistic for each model parameter is used. Random Forest : from the R package: “For each tree, the prediction accuracy on the out-of-bag portion of the data is recorded. Then the same is done after permuting each predictor variable. The difference between the two accuracies are then averaged over all trees, and normalized by the standard error. For regression, the MSE is computed on the out-of-bag data for each tree, and then the same computed after permuting a variable. The differences are averaged and normalized by the standard error. If the standard error is equal to 0 for a variable, the division is not done.” Partial Least Squares : the variable importance measure here is based on weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the sums of squares across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the sums of squares. Recursive Partitioning : The reduction in the loss function (e.g. mean squared error) attributed to each variable at each split is tabulated and the sum is returned. Also, since there may be candidate variables that are important but are not used in a split, the top competing variables are also tabulated at each split. This can be turned off using the maxcompete argument in rpart.control . This method does not currently provide class-specific measures of importance when the response is a factor. Bagged Trees : The same methodology as a single tree is applied to all bootstrapped trees and the total importance is returned Boosted Trees : This method uses the same approach as a single tree, but sums the importances over each boosting iteration (see the gbm package vignette). Multivariate Adaptive Regression Splines : MARS models include a backwards elimination feature selection routine that looks at reductions in the generalized cross-validation (GCV) estimate of error. The varImp function tracks the changes in model statistics, such as the GCV, for each predictor and accumulates the reduction in the statistic when each predictor’s feature is added to the model. This total reduction is used as the variable importance measure. If a predictor was never used in any MARS basis function, it has an importance value of zero. There are three statistics that can be used to estimate variable importance in MARS models. Using varImp(object, value  "gcv") tracks the reduction in the generalized cross-validation statistic as terms are added. However, there are some cases when terms are retained in the model that result in an increase in GCV. Negative variable importance values for MARS are set to zero. Terms with non-zero importance that were not included in the final, pruned model are also listed as zero. Alternatively, using varImp(object, value  "rss") monitors the change in the residual sums of squares (RSS) as terms are added, which will never be negative. Also, the option varImp(object, value  "nsubsets") returns the number of times that each variable is involved in a subset (in the final, pruned model). Prior to June 2008, varImp used an internal function to estimate importance for MARS models. Currently, it is a wrapper around the evimp function in the earth package. Nearest shrunken centroids : The difference between the class centroids and the overall centroid is used to measure the variable influence (see pamr.predict ). The larger the difference between the class centroid and the overall center of the data, the larger the separation between the classes. The training set predictions must be supplied when an object of class pamrtrained is given to varImp . Cubist : The Cubist output contains variable usage statistics. It gives the percentage of times where each variable was used in a condition and/or a linear model. Note that this output will probably be inconsistent with the rules shown in the output from summary.cubist . At each split of the tree, Cubist saves a linear model (after feature selection) that is allowed to have terms for each variable used in the current split or any split above it. Quinlan (1992) discusses a smoothing algorithm where each model prediction is a linear combination of the parent and child model along the tree. As such, the final prediction is a function of all the linear models from the initial node to the terminal node. The percentages shown in the Cubist output reflects all the models involved in prediction (as opposed to the terminal models shown in the output). The variable importance used here is a linear combination of the usage in the rule conditions and the model. "
"caret_15_variable_importance 15 Variable Importance variable-importance.html  15.2 Model Independent Metrics If there is no model-specific way to estimate importance (or the argument useModel  FALSE is used in varImp ) the importance of each predictor is evaluated individually using a “filter” approach. For classification, ROC curve analysis is conducted on each predictor. For two class problems, a series of cutoffs is applied to the predictor data to predict the class. The sensitivity and specificity are computed for each cutoff and the ROC curve is computed. The trapezoidal rule is used to compute the area under the ROC curve. This area is used as the measure of variable importance. For multi-class outcomes, the problem is decomposed into all pair-wise problems and the area under the curve is calculated for each class pair (i.e. class 1 vs. class 2, class 2 vs. class 3 etc.). For a specific class, the maximum area under the curve across the relevant pair-wise AUC’s is used as the variable importance measure. For regression, the relationship between each predictor and the outcome is evaluated. An argument, nonpara , is used to pick the model fitting technique. When nonpara  FALSE , a linear model is fit and the absolute value of the t -value for the slope of the predictor is used. Otherwise, a loess smoother is fit between the outcome and the predictor. The R 2 statistic is calculated for this model against the intercept only null model. This number is returned as a relative measure of variable importance. "
"caret_15_variable_importance 15 Variable Importance variable-importance.html  15.3 An Example On the model training web, several models were fit to the example data. The boosted tree model has a built-in variable importance score but neither the support vector machine or the regularized discriminant analysis model do. The function automatically scales the importance scores to be between 0 and 100. Using scale  FALSE avoids this normalization step. To get the area under the ROC curve for each predictor, the filterVarImp function can be used. The area under the ROC curve is computed for each class. Alternatively, for models where no built-in importance score is implemented (or exists), the varImp can still be used to get scores. For SVM classification models, the default behavior is to compute the area under the ROC curve. For importance scores generated from varImp.train , a plot method can be used to visualize the results. In the plot below, the top option is used to make the image more readable. "
"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.1 Yet Another k -Nearest Neighbor Function knn3 is a function for k -nearest neighbor classification. This particular implementation is a modification of the knn C code and returns the vote information for all of the classes ( knn only returns the probability for the winning class). There is a formula interface via There are also print and predict methods. For the Sonar data in the mlbench package, we can fit an 11-nearest neighbor model: Similarly, caret contains a k -nearest neighbor regression function, knnreg . It returns the average outcome for the neighbor. "
"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.2 Partial Least Squares Discriminant Analysis The plsda function is a wrapper for the plsr function in the pls package that does not require a formula interface and can take factor outcomes as arguments. The classes are broken down into dummy variables (one for each class). These 0/1 dummy variables are modeled by partial least squares. From this model, there are two approaches to computing the class predictions and probabilities: the softmax technique can be used on a per-sample basis to normalize the scores so that they are more “probability like”" (i.e. they sum to one and are between zero and one). For a vector of model predictions for each class X , the softmax class probabilities are computed as. The predicted class is simply the class with the largest model prediction, or equivalently, the largest class probability. This is the default behavior for plsda . Bayes rule can be applied to the model predictions to form posterior probabilities. Here, the model predictions for the training set are used along with the training set outcomes to create conditional distributions for each class. When new samples are predicted, the raw model predictions are run through these conditional distributions to produce a posterior probability for each class (along with the prior). Bayes rule can be used by specifying probModel  "Bayes" . An additional parameter, prior , can be used to set prior probabilities for the classes. The advantage to using Bayes rule is that the full training set is used to directly compute the class probabilities (unlike the softmax function which only uses the current sample’s scores). This creates more realistic probability estimates but the disadvantage is that a separate Bayesian model must be created for each value of ncomp , which is more time consuming. For the sonar data set, we can fit two PLS models using each technique and predict the class probabilities for the test set. Similar to plsda , caret also contains a function splsda that allows for classification using sparse PLS. A dummy matrix is created for each class and used with the spls function in the spls package. The same approach to estimating class probabilities is used for plsda and splsda . "
"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.3 Bagged MARS and FDA Multivariate adaptive regression splines (MARS) models, like classification/regression trees, are unstable predictors (Breiman, 1996). This means that small perturbations in the training data might lead to significantly different models. Bagged trees and random forests are effective ways of improving tree models by exploiting these instabilities. caret contains a function, bagEarth , that fits MARS models via the earth function. There are formula and non-formula interfaces. Also, flexible discriminant analysis is a generalization of linear discriminant analysis that can use non-linear features as inputs. One way of doing this is the use MARS-type features to classify samples. The function bagFDA fits FDA models of a set of bootstrap samples and aggregates the predictions to reduce noise. This function is deprecated in favor of the bag function. "
"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.4 Bagging The bag function offers a general platform for bagging classification and regression models. Like rfe and sbf , it is open and models are specified by declaring functions for the model fitting and prediction code (and several built-in sets of functions exist in the package). The function bagControl has options to specify the functions (more details below). The function also has a few non-standard features: The argument var can enable random sampling of the predictors at each bagging iteration. This is to de-correlate the bagged models in the same spirit of random forests (although here the sampling is done once for the whole model). The default is to use all the predictors for each model. The bagControl function has a logical argument called downSample that is useful for classification models with severe class imbalance. The bootstrapped data set is reduced so that the sample sizes for the classes with larger frequencies are the same as the sample size for the minority class. If a parallel backend for the foreach package has been loaded and registered, the bagged models can be trained in parallel. The function’s control function requires the following arguments: 16.4.1 The fit Function Inputs: x : a data frame of the training set predictor data. y : the training set outcomes. ... arguments passed from train to this function The output is the object corresponding to the trained model and any other objects required for prediction. A simple example for a linear discriminant analysis model from the MASS package is: 16.4.2 The pred Function This should be a function that produces predictors for new samples. Inputs: object : the object generated by the fit module. x : a matrix or data frame of predictor data. The output is either a number vector (for regression), a factor (or character) vector for classification or a matrix/data frame of class probabilities. For classification, it is probably better to average class probabilities instead of using the votes of the class predictions. Using the lda example again: 16.4.3 The aggregate Function This should be a function that takes the predictions from the constituent models and converts them to a single prediction per sample. Inputs: x : a list of objects returned by the pred module. type : an optional string that describes the type of output (e.g. “class”, “prob” etc.). The output is either a number vector (for regression), a factor (or character) vector for classification or a matrix/data frame of class probabilities. For the linear discriminant model above, we saved the matrix of class probabilities. To average them and generate a class prediction, we could use: For example, to bag a conditional inference tree (from the party package): "
"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.5 Model Averaged Neural Networks The avNNet fits multiple neural network models to the same data set and predicts using the average of the predictions coming from each constituent model. The models can be different either due to different random number seeds to initialize the network or by fitting the models on bootstrap samples of the original training set (i.e. bagging the neural network). For classification models, the class probabilities are averaged to produce the final class prediction (as opposed to voting from the individual class predictions. As an example, the model can be fit via train : "
"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.6 Neural Networks with a Principal Component Step Neural networks can be affected by severe amounts of multicollinearity in the predictors. The function pcaNNet is a wrapper around the preProcess and nnet functions that will run principal component analysis on the predictors before using them as inputs into a neural network. The function will keep enough components that will capture some pre-defined threshold on the cumulative proportion of variance (see the thresh argument). For new samples, the same transformation is applied to the new predictor values (based on the loadings from the training set). The function is available for both regression and classification. This function is deprecated in favor of the train function using method  "nnet" and preProc  "pca" . "
"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.7 Independent Component Regression The icr function can be used to fit a model analogous to principal component regression (PCR), but using independent component analysis (ICA). The predictor data are centered and projected to the ICA components. These components are then regressed against the outcome. The user needed to specify the number of components to keep. The model uses the preProcess function to compute the latent variables using the fastICA package. Like PCR, there is no guarantee that there will be a correlation between the new latent variable and the outcomes. "
"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.1 Measures for Regression The function postResample can be used to estimate the root mean squared error (RMSE), simple R 2 , and the mean absolute error (MAE) for numeric outcomes. For example: A note about how R 2 is calculated by caret : it takes the straightforward approach of computing the correlation between the observed and predicted values (i.e. R) and squaring the value. When the model is poor, this can lead to differences between this estimator and the more widely known estimate derived form linear regression models. Mostly notably, the correlation approach will not generate negative values of R 2 (which are theoretically invalid). A comparison of these and other estimators can be found in Kvalseth 1985 . "
"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.2 Measures for Predicted Classes Before proceeding, let’s make up some test set data: We would expect that this model will do well on these data: Generating the predicted classes based on the typical 50% cutoff for the probabilities, we can compute the confusion matrix , which shows a cross-tabulation of the observed and predicted classes. The confusionMatrix function can be used to generate these results: For two classes, this function assumes that the class corresponding to an event is the first class level (but this can be changed using the positive argument. Note that there are a number of statistics shown here. The “no-information rate” is the largest proportion of the observed classes (there were more class 2 data than class 1 in this test set). A hypothesis test is also computed to evaluate whether the overall accuracy rate is greater than the rate of the largest class. Also, the prevalence of the “positive event” is computed from the data (unless passed in as an argument), the detection rate (the rate of true events also predicted to be events) and the detection prevalence (the prevalence of predicted events). If the prevalence of the event is different than those seen in the test set, the prevalence option can be used to adjust this. Suppose a 2x2 table: When there are three or more classes, confusionMatrix will show the confusion matrix and a set of “one-versus-all” results. For example, in a three class problem, the sensitivity of the first class is calculated against all the samples in the second and third classes (and so on). The confusionMatrix matrix frames the errors in terms of sensitivity and specificity. In the case of information retrieval, the precision and recall might be more appropriate. In this case, the option mode can be used to get those statistics: Again, the positive argument can be used to control which factor level is associated with a “found” or “important” document or sample. There are individual functions called sensitivity , specificity , posPredValue , negPredValue , precision , recall , and F_meas . Also, a resampled estimate of the training set can also be obtained using confusionMatrix.train . For each resampling iteration, a confusion matrix is created from the hold-out samples and these values can be aggregated to diagnose issues with the model fit. These values are the percentages that hold-out samples landed in the confusion matrix during resampling. There are several methods for normalizing these values. See ?confusionMatrix.train for details. The default performance function used by train is postResample , which generates the accuracy and Kappa statistics: As shown below, another function called twoClassSummary can be used to get the sensitivity and specificity using the default probability cutoff. Another function, multiClassSummary , can do similar calculations when there are three or more classes but both require class probabilities for each class. "
"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.3 Measures for Class Probabilities For data with two classes, there are specialized functions for measuring model performance. First, the twoClassSummary function computes the area under the ROC curve and the specificity and sensitivity under the 50% cutoff. Note that: this function uses the first class level to define the “event” of interest. To change this, use the lev option to the function there must be columns in the data for each of the class probabilities (named the same as the outcome’s class levels) A similar function can be used to get the analugous precision-recall values and the area under the precision-recall curve: This function requires that the MLmetrics package is installed. For multi-class problems, there are additional functions that can be used to calculate performance. One, mnLogLoss computes the negative of the multinomial log-likelihood (smaller is better) based on the class probabilities. This can be used to optimize tuning parameters but can lead to results that are inconsistent with other measures (e.g. accuracy or the area under the ROC curve), especially when the other measures are near their best possible values. The function has similar arguments to the other functions described above. Here is the two-class data from above: Additionally, the function multiClassSummary computes a number of relevant metrics: the overall accuracy and Kappa statistics using the predicted classes the negative of the multinomial log loss (if class probabilities are available) averages of the “one versus all” statistics such as sensitivity, specificity, the area under the ROC curve, etc. "
"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.4 Lift Curves The lift function can be used to evaluate probabilities thresholds that can capture a certain percentage of hits. The function requires a set of sample probability predictions (not from the training set) and the true class labels. For example, we can simulate two-class samples using the twoClassSim function and fit a set of models to the training set: The lift function does the calculations and the corresponding plot function is used to plot the lift curve (although some call this the gain curve ). The value argument creates reference lines: There is also a ggplot method for lift objects: From this we can see that, to find 60 percent of the hits, a little more than 30 percent of the data can be sampled (when ordered by the probability predictions). The LDA model does somewhat worse than the other two models. "
"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.5 Calibration Curves Calibration curves can be used to characterisze how consistent the predicted class probabilities are with the observed event rates. Other functions in the gbm package, the rms package (and others) can also produce calibrartion curves. The format for the function is very similar to the lift function: There is also a ggplot method that shows the confidence intervals for the proportions inside of the subsets: "
"caret_18_feature_selection_overview 18 Feature Selection Overview feature-selection-overview.html  18.1 Models with Built-In Feature Selection Many models that can be accessed using caret ’s train function produce prediction equations that do not necessarily use all the predictors. These models are thought to have built-in feature selection: ada , AdaBag , AdaBoost.M1 , adaboost , bagEarth , bagEarthGCV , bagFDA , bagFDAGCV , bartMachine , blasso , BstLm , bstSm , C5.0 , C5.0Cost , C5.0Rules , C5.0Tree , cforest , chaid , ctree , ctree2 , cubist , deepboost , earth , enet , evtree , extraTrees , fda , gamboost , gbm_h2o , gbm , gcvEarth , glmnet_h2o , glmnet , glmStepAIC , J48 , JRip , lars , lars2 , lasso , LMT , LogitBoost , M5 , M5Rules , msaenet , nodeHarvest , OneR , ordinalNet , ordinalRF , ORFlog , ORFpls , ORFridge , ORFsvm , pam , parRF , PART , penalized , PenalizedLDA , qrf , ranger , Rborist , relaxo , rf , rFerns , rfRules , rotationForest , rotationForestCp , rpart , rpart1SE , rpart2 , rpartCost , rpartScore , rqlasso , rqnc , RRF , RRFglobal , sdwd , smda , sparseLDA , spikeslab , wsrf , xgbDART , xgbLinear , xgbTree . Many of the functions have an ancillary method called predictors that returns a vector indicating which predictors were used in the final model. In many cases, using these models with built-in feature selection will be more efficient than algorithms where the search routine for the right predictors is external to the model. Built-in feature selection typically couples the predictor search algorithm with the parameter estimation and are usually optimized with a single objective function (e.g. error rates or likelihood). "
"caret_18_feature_selection_overview 18 Feature Selection Overview feature-selection-overview.html  18.2 Feature Selection Methods Apart from models with built-in feature selection, most approaches for reducing the number of predictors can be placed into two main categories. Using the terminology of John, Kohavi, and Pfleger (1994) : Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. caret has wrapper methods based on recursive feature elimination , genetic algorithms , and simulated annealing . Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model. Saeys, Inza, and Larranaga (2007) surveys filter methods. caret has a general framework for using univariate filters . Both approaches have advantages and drawbacks. Filter methods are usually more computationally efficient than wrapper methods, but the selection criterion is not directly related to the effectiveness of the model. Also, most filter methods evaluate each predictor separately and, consequently, redundant (i.e. highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified. The downside of the wrapper method is that many models are evaluated (which may also require parameter tuning) and thus an increase in computation time. There is also an increased risk of over-fitting with wrappers. "
"caret_18_feature_selection_overview 18 Feature Selection Overview feature-selection-overview.html  18.3 External Validation It is important to realize that feature selection is part of the model building process and, as such, should be externally validated. Just as parameter tuning can result in over-fitting, feature selection can over-fit to the predictors (especially when search wrappers are used). In each of the caret functions for feature selection, the selection process is included in any resampling loops. See See Ambroise and McLachlan (2002) for a demonstration of this issue. "
"caret_19_feature_selection_using_univariate_filters 19 Feature Selection using Univariate Filters feature-selection-using-univariate-filters.html  19.1 Univariate Filters Another approach to feature selection is to pre-screen the predictors using simple univariate statistical methods then only use those that pass some criterion in the subsequent model steps. Similar to recursive selection, cross-validation of the subsequent models will be biased as the remaining predictors have already been evaluate on the data set. Proper performance estimates via resampling should include the feature selection step. As an example, it has been suggested for classification models, that predictors can be filtered by conducting some sort of k -sample test (where k is the number of classes) to see if the mean of the predictor is different between the classes. Wilcoxon tests, t -tests and ANOVA models are sometimes used. Predictors that have statistically significant differences between the classes are then used for modeling. The caret function sbf (for selection by filter) can be used to cross-validate such feature selection schemes. Similar to rfe , functions can be passed into sbf for the computational components: univariate filtering, model fitting, prediction and performance summaries (details are given below). The function is applied to the entire training set and also to different resampled versions of the data set. From this, generalizable estimates of performance can be computed that properly take into account the feature selection step. Also, the results of the predictor filters can be tracked over resamples to understand the uncertainty in the filtering. "
"caret_19_feature_selection_using_univariate_filters 19 Feature Selection using Univariate Filters feature-selection-using-univariate-filters.html  19.2 Basic Syntax Similar to the rfe function, the syntax for sbf is: In this case, the details are specificed using the sbfControl function. Here, the argument functions dictates what the different components should do. This argument should have elements called filter , fit , pred and summary . 19.2.1 The score Function This function takes as inputs the predictors and the outcome in objects called x and y , respectively. By default, each predictor in x is passed to the score function individually. In this case, the function should return a single score. Alternatively, all the predictors can be exposed to the function using the multivariate argument to sbfControl . In this case, the output should be a named vector of scores where the names correspond to the column names of x . There are two built-in functions called anovaScores and gamScores . anovaScores treats the outcome as the independent variable and the predictor as the outcome. In this way, the null hypothesis is that the mean predictor values are equal across the different classes. For regression, gamScores fits a smoothing spline in the predictor to the outcome using a generalized additive model and tests to see if there is any functional relationship between the two. In each function the p-value is used as the score. 19.2.2 The filter Function This function takes as inputs the scores coming out of the score function (in an argument called score ). The function also has the training set data as inputs (arguments are called x and y ). The output should be a named logical vector where the names correspond to the column names of x . Columns with values of TRUE will be used in the subsequent model. 19.2.3 The fit Function The component is very similar to the rfe -specific function described above. For sbf , there are no first or last arguments. The function should have arguments x , y and ... . The data within x have been filtered using the filter function described above. The output of the fit function should be a fitted model. With some data sets, no predictors will survive the filter. In these cases, a model with predictors cannot be computed, but the lack of viable predictors should not be ignored in the final results. To account for this issue, caret contains a model function called nullModel that fits a simple model that is independent of any of the predictors. For problems where the outcome is numeric, the function predicts every sample using the simple mean of the training set outcomes. For classification, the model predicts all samples using the most prevalent class in the training data. This function can be used in the fit component function to “error-trap” cases where no predictors are selected. For example, there are several built-in functions for some models. The object rfSBF is a set of functions that may be useful for fitting random forest models with filtering. The fit function here uses nullModel to check for cases with no predictors: 19.2.4 The summary and pred Functions The summary function is used to calculate model performance on held-out samples. The pred function is used to predict new samples using the current predictor set. The arguments and outputs for these two functions are identical to the previously discussed summary and pred functions in previously described sections. "
"caret_19_feature_selection_using_univariate_filters 19 Feature Selection using Univariate Filters feature-selection-using-univariate-filters.html  19.3 The Example Returning to the example from (Friedman, 1991), we can fit another random forest model with the predictors pre-filtered using the generalized additive model approach described previously. In this case, the training set indicated that 6 should be used in the random forest model, but the resampling results indicate that there is some variation in this number. Some of the informative predictors are used, but a few others are erroneous retained. Similar to rfe , there are methods for predictors , densityplot , histogram and varImp . "
"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.1 Backwards Selection First, the algorithm fits the model to all predictors. Each predictor is ranked using it’s importance to the model. Let S be a sequence of ordered numbers which are candidate values for the number of predictors to retain ( S 1 > S 2 , …). At each iteration of feature selection, the S i top ranked predictors are retained, the model is refit and performance is assessed. The value of S i with the best performance is determined and the top S i predictors are used to fit the final model. Algorithm 1 has a more complete definition. The algorithm has an optional step (line 1.9) where the predictor rankings are recomputed on the model on the reduced feature set. Svetnik et al (2004) showed that, for random forest models, there was a decrease in performance when the rankings were re-computed at every step. However, in other cases when the initial rankings are not good (e.g. linear models with highly collinear predictors), re-calculation can slightly improve performance. One potential issue over-fitting to the predictor set such that the wrapper procedure could focus on nuances of the training data that are not found in future samples (i.e. over-fitting to predictors and samples). For example, suppose a very large number of uninformative predictors were collected and one such predictor randomly correlated with the outcome. The RFE algorithm would give a good rank to this variable and the prediction error (on the same data set) would be lowered. It would take a different test/validation to find out that this predictor was uninformative. The was referred to as “selection bias” by Ambroise and McLachlan (2002) . In the current RFE algorithm, the training data is being used for at least three purposes: predictor selection, model fitting and performance evaluation. Unless the number of samples is large, especially in relation to the number of variables, one static training set may not be able to fulfill these needs. "
"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.2 Resampling and External Validation Since feature selection is part of the model building process, resampling methods (e.g. cross-validation, the bootstrap) should factor in the variability caused by feature selection when calculating performance. For example, the RFE procedure in Algorithm 1 can estimate the model performance on line 1.7, which during the selection process. Ambroise and McLachlan (2002) and Svetnik et al (2004) showed that improper use of resampling to measure performance will result in models that perform poorly on new samples. To get performance estimates that incorporate the variation due to feature selection, it is suggested that the steps in Algorithm 1 be encapsulated inside an outer layer of resampling (e.g. 10-fold cross-validation). Algorithm 2 shows a version of the algorithm that uses resampling. While this will provide better estimates of performance, it is more computationally burdensome. For users with access to machines with multiple processors, the first For loop in Algorithm 2 (line 2.1) can be easily parallelized. Another complication to using resampling is that multiple lists of the “best” predictors are generated at each iteration. At first this may seem like a disadvantage, but it does provide a more probabilistic assessment of predictor importance than a ranking based on a single fixed data set. At the end of the algorithm, a consensus ranking can be used to determine the best predictors to retain. "
"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.3 Recursive Feature Elimination via caret In caret , Algorithm 1 is implemented by the function rfeIter . The resampling-based Algorithm 2 is in the rfe function. Given the potential selection bias issues, this document focuses on rfe . There are several arguments: x , a matrix or data frame of predictor variables y , a vector (numeric or factor) of outcomes sizes , a integer vector for the specific subset sizes that should be tested (which need not to include ncol(x) ) rfeControl , a list of options that can be used to specify the model and the methods for prediction, ranking etc. For a specific model, a set of functions must be specified in rfeControl$functions . Sections below has descriptions of these sub-functions. There are a number of pre-defined sets of functions for several models, including: linear regression (in the object lmFuncs ), random forests ( rfFuncs ), naive Bayes ( nbFuncs ), bagged trees ( treebagFuncs ) and functions that can be used with caret ’s train function ( caretFuncs ). The latter is useful if the model has tuning parameters that must be determined at each iteration. "
"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.4 An Example To test the algorithm, the “Friedman 1” benchmark (Friedman, 1991) was used. There are five informative variables generated by the equation In the simulation used here: Of the 50 predictors, there are 45 pure noise variables: 5 are uniform on \[0, 1\] and 40 are random univariate standard normals. The predictors are centered and scaled: The simulation will fit models with subset sizes of 25, 20, 15, 10, 5, 4, 3, 2, 1. As previously mentioned, to fit linear models, the lmFuncs set of functions can be used. To do this, a control object is created with the rfeControl function. We also specify that repeated 10-fold cross-validation should be used in line 2.1 of Algorithm 2. The number of folds can be changed via the number argument to rfeControl (defaults to 10). The verbose option prevents copious amounts of output from being produced. The output shows that the best subset size was estimated to be 4 predictors. This set includes informative variables but did not include them all. The predictors function can be used to get a text string of variable names that were picked in the final model. The lmProfile is a list of class "rfe" that contains an object fit that is the final linear model with the remaining terms. The model can be used to get predictions for future or test samples. There are also several plot methods to visualize the results. plot(lmProfile) produces the performance profile across different subset sizes, as shown in the figure below. Also the resampling results are stored in the sub-object lmProfile$resample and can be used with several lattice functions. Univariate lattice functions ( densityplot , histogram ) can be used to plot the resampling distribution while bivariate functions ( xyplot , stripplot ) can be used to plot the distributions for different subset sizes. In the latter case, the option returnResamp``  "all" in rfeControl can be used to save all the resampling results. Example images are shown below for the random forest model. "
"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.5 Helper Functions To use feature elimination for an arbitrary model, a set of functions must be passed to rfe for each of the steps in Algorithm 2. This section defines those functions and uses the existing random forest functions as an illustrative example. caret contains a list called rfFuncs , but this document will use a more simple version that will be better for illustrating the ideas. A set of simplified functions used here and called rfRFE . 20.5.1 The summary Function The summary function takes the observed and predicted values and computes one or more performance metrics (see line 2.14). The input is a data frame with columns obs and pred . The output should be a named vector of numeric variables. Note that the metric argument of the rfe function should reference one of the names of the output of summary . The example function is: Two functions in caret that can be used as the summary funciton are defaultSummary and twoClassSummary (for classification problems with two classes). 20.5.2 The fit Function This function builds the model based on the current data set (lines 2.3, 2.9 and 2.17). The arguments for the function must be: x : the current training set of predictor data with the appropriate subset of variables y : the current outcome data (either a numeric or factor vector) first : a single logical value for whether the current predictor set has all possible variables (e.g. line 2.3) last : similar to first , but TRUE when the last model is fit with the final subset size and predictors. (line 2.17) ... : optional arguments to pass to the fit function in the call to rfe The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: For feature selection without re-ranking at each iteration, the random forest variable importances only need to be computed on the first iterations when all of the predictors are in the model. This can be accomplished using importance``  first . 20.5.3 The pred Function This function returns a vector of predictions (numeric or factors) from the current model (lines 2.4 and 2.10). The input arguments must be object : the model generated by the fit function x : the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 20.5.4 The rank Function This function is used to return the predictors in the order of the most important to the least important (lines 2.5 and 2.11). Inputs are: object : the model generated by the fit function x : the current set of predictor set for the training samples y : the current training outcomes The function should return a data frame with a column called var that has the current variable names. The first row should be the most important predictor etc. Other columns can be included in the output and will be returned in the final rfe object. For random forests, the function below uses caret ’s varImp function to extract the random forest importances and orders them. For classification, randomForest will produce a column of importances for each class. In this case, the default ranking function orders the predictors by the averages importance across the classes. 20.5.5 The selectSize Function This function determines the optimal number of predictors based on the resampling output (line 2.15). Inputs for the function are: x : a matrix with columns for the performance metrics and the number of variables, called Variables metric : a character string of the performance measure to optimize (e.g. RMSE, Accuracy) maximize : a single logical for whether the metric should be maximized This function should return an integer corresponding to the optimal subset size. caret comes with two examples functions for this purpose: pickSizeBest and pickSizeTolerance . The former simply selects the subset size that has the best value. The latter takes into account the whole profile and tries to pick a subset size that is small without sacrificing too much performance. For example, suppose we have computed the RMSE over a series of variables sizes: These are depicted in the figure below. The solid circle identifies the subset size with the absolute smallest RMSE. However, there are many smaller subsets that produce approximately the same performance but with fewer predictors. In this case, we might be able to accept a slightly larger error for less predictors. The pickSizeTolerance determines the absolute best value then the percent difference of the other points to this value. In the case of RMSE, this would be where RMSE {opt} is the absolute best error rate. These “tolerance” values are plotted in the bottom panel. The solid triangle is the smallest subset size that is within 10% of the optimal value. This approach can produce good results for many of the tree based models, such as random forest, where there is a plateau of good performance for larger subset sizes. For trees, this is usually because unimportant variables are infrequently used in splits and do not significantly affect performance. 20.5.6 The selectVar Function After the optimal subset size is determined, this function will be used to calculate the best rankings for each variable across all the resampling iterations (line 2.16). Inputs for the function are: y : a list of variables importance for each resampling iteration and each subset size (generated by the user-defined rank function). In the example, each each of the cross-validation groups the output of the rank function is saved for each of the 10 subset sizes (including the original subset). If the rankings are not recomputed at each iteration, the values will be the same within each cross-validation iteration. size : the integer returned by the selectSize function This function should return a character string of predictor names (of length size ) in the order of most important to least important For random forests, only the first importance calculation (line 2.5) is used since these are the rankings on the full set of predictors. These importances are averaged and the top predictors are returned. Note that if the predictor rankings are recomputed at each iteration (line 2.11) the user will need to write their own selection function to use the other ranks. "
"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.6 The Example For random forest, we fit the same series of model sizes as the linear model. The option to save all the resampling results across subset sizes was changed for this model and are used to show the lattice plot function capabilities in the figures below. The resampling profile can be visualized along with plots of the individual resampling results: "
"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.7 Using a Recipe A recipe can be used to specify the model terms and any preprocessing that may be needed. Instead of using an existing recipe can be used along with a data frame containing the predictors and outcome: The recipe is prepped within each resample in the same manner that train executes the preProc option. However, since a recipe can do a variety of different operations, there are some potentially complicating factors. The main pitfall is that the recipe can involve the creation and deletion of predictors. There are a number of steps that can reduce the number of predictors, such as the ones for pooling factors into an “other” category, PCA signal extraction, as well as filters for near-zero variance predictors and highly correlated predictors. For this reason, it may be difficult to know how many predictors are available for the full model. Also, this number will likely vary between iterations of resampling. To illustrate, let’s use the blood-brain barrier data where there is a high degree of correlation between the predictors. A simple recipe could be Originally, there are 134 predictors and, for the entire data set, the processed version has: When calling rfe , let’s start the maximum subset size at 28: What was the distribution of the maximum number of terms: So… 28ish. Suppose that we used sizes  2:ncol(bbbDescr) when calling rfe . A warning is issued that: "
"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.1 Genetic Algorithms Genetic algorithms (GAs) mimic Darwinian forces of natural selection to find optimal values of some function ( Mitchell, 1998 ). An initial set of candidate solutions are created and their corresponding fitness values are calculated (where larger values are better). This set of solutions is referred to as a population and each solution as an individual . The individuals with the best fitness values are combined randomly to produce offsprings which make up the next population. To do so, individual are selected and undergo cross-over (mimicking genetic reproduction) and also are subject to random mutations. This process is repeated again and again and many generations are produced (i.e. iterations of the search procedure) that should create better and better solutions. For feature selection, the individuals are subsets of predictors that are encoded as binary; a feature is either included or not in the subset. The fitness values are some measure of model performance, such as the RMSE or classification accuracy. One issue with using GAs for feature selection is that the optimization process can be very aggressive and their is potential for the GA to overfit to the predictors (much like the previous discussion for RFE). "
"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.2 Internal and External Performance Estimates The genetic algorithm code in caret conducts the search of the feature space repeatedly within resampling iterations. First, the training data are split be whatever resampling method was specified in the control function. For example, if 10-fold cross-validation is selected, the entire genetic algorithm is conducted 10 separate times. For the first fold, nine tenths of the data are used in the search while the remaining tenth is used to estimate the external performance since these data points were not used in the search. During the genetic algorithm, a measure of fitness is needed to guide the search. This is the internal measure of performance. During the search, the data that are available are the instances selected by the top-level resampling (e.g. the nine tenths mentioned above). A common approach is to conduct another resampling procedure. Another option is to use a holdout set of samples to determine the internal estimate of performance (see the holdout argument of the control function). While this is faster, it is more likely to cause overfitting of the features and should only be used when a large amount of training data are available. Yet another idea is to use a penalized metric (such as the AIC statistic) but this may not exist for some metrics (e.g. the area under the ROC curve). The internal estimates of performance will eventually overfit the subsets to the data. However, since the external estimate is not used by the search, it is able to make better assessments of overfitting. After resampling, this function determines the optimal number of generations for the GA. Finally, the entire data set is used in the last execution of the genetic algorithm search and the final model is built on the predictor subset that is associated with the optimal number of generations determined by resampling (although the update function can be used to manually set the number of generations). "
"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.3 Basic Syntax The most basic usage of the function is: where x : a data frame or matrix of predictor values y : a factor or numeric vector of outcomes iters : the number of generations for the GA This isn’t very specific. All of the action is in the control function. That can be used to specify the model to be fit, how predictions are made and summarized as well as the genetic operations. Suppose that we want to fit a linear regression model. To do this, we can use train as an interface and pass arguments to that function through gafs : Other options, such as preProcess , can be passed in as well. Some important options to gafsControl are: method , number , repeats , index , indexOut , etc: options similar to those for train top control resampling. metric : this is similar to train ’s option but, in this case, the value should be a named vector with values for the internal and external metrics. If none are specified, the first value returned by the summary functions (see details below) are used and a warning is issued. A similar two-element vector for the option maximize is also required. See the last example here for an illustration. holdout : this is a number between [0, 1) that can be used to hold out samples for computing the internal fitness value. Note that this is independent of the external resampling step. Suppose 10-fold CV is being used. Within a resampling iteration, holdout can be used to sample an additional proportion of the 90% resampled data to use for estimating fitness. This may not be a good idea unless you have a very large training set and want to avoid an internal resampling procedure to estimate fitness. allowParallel and genParallel : these are logicals to control where parallel processing should be used (if at all). The former will parallelize the external resampling while the latter parallelizes the fitness calculations within a generation. allowParallel will almost always be more advantageous. There are a few built-in sets of functions to use with gafs : caretGA , rfGA , and treebagGA . The first is a simple interface to train . When using this, as shown above, arguments can be passed to train using the ... structure and the resampling estimates of performance can be used as the internal fitness value. The functions provided by rfGA and treebagGA avoid using train and their internal estimates of fitness come from using the out-of-bag estimates generated from the model. The GA implementation in caret uses the underlying code from the GA package ( Scrucca, 2013 ). "
"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.4 Genetic Algorithm Example Using the example from the previous page where there are five real predictors and 40 noise predictors: We’ll fit a random forest model and use the out-of-bag RMSE estimate as the internal performance metric and use the same repeated 10-fold cross-validation process used with the search. To do this, we’ll use the built-in rfGA object for this purpose. The default GA operators will be used and conduct 200 generations of the algorithm. With 5 repeats of 10-fold cross-validation, the GA was executed 50 times. The average external performance is calculated across resamples and these results are used to determine the optimal number of iterations for the final GA to avoid over-fitting. Across the resamples, an average of 9.3 predictors were selected at the end of each of the algorithms. The plot function is used to monitor the average of the internal out-of-bag RMSE estimates as well as the average of the external performance estimates calculated from the 50 out-of-sample predictions. By default, this function uses ggplot2 package. A black and white theme can be “added” to the output object: Based on these results, the generation associated with the best external RMSE estimate was 2.81. Using the entire training set, the final GA is conducted and, at generation 195, there were 12 that were selected: real1, real2, real3, real4, real5, bogus3, bogus5, bogus7, bogus8, bogus14, bogus17, bogus29. The random forest model with these predictors is created using the entire training set is trained and this is the model that is used when predict.gafs is executed. Note: the correlation between the internal and external fitness values is somewhat atypical for most real-world problems. This is a function of the nature of the simulations (a small number of uncorrelated informative predictors) and that the OOB error estimate from random forest is a product of hundreds of trees. Your mileage may vary. "
"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.5 Customizing the Search 21.5.1 The fit Function This function builds the model based on a proposed current subset. The arguments for the function must be: x : the current training set of predictor data with the appropriate subset of variables y : the current outcome data (either a numeric or factor vector) lev : a character vector with the class levels (or NULL for regression problems) last : a logical that is TRUE when the final GA search is conducted on the entire data set ... : optional arguments to pass to the fit function in the call to gafs The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: 21.5.2 The pred Function This function returns a vector of predictions (numeric or factors) from the current model . The input arguments must be object : the model generated by the fit function x : the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 21.5.3 The fitness_intern Function The fitness_intern function takes the fitted model and computes one or more performance metrics. The inputs to this function are: object : the model generated by the fit function x : the current set of predictor set. If the option gafsControl$holdout is zero, these values will be from the current resample (i.e. the same data used to fit the model). Otherwise, the predictor values are from the hold-out set created by gafsControl$holdout . y : outcome values. See the note for the x argument to understand which data are presented to the function. maximize : a logical from gafsControl that indicates whether the metric should be maximized or minimized p : the total number of possible predictors The output should be a named numeric vector of performance values. In many cases, some resampled measure of performance is used. In the example above using random forest, the OOB error was used. In other cases, the resampled performance from train can be used and, if gafsControl$holdout is not zero, a static hold-out set can be used. This depends on the data and problem at hand. The example function for random forest is: 21.5.4 The fitness_extern Function The fitness_extern function takes the observed and predicted values form the external resampling process and computes one or more performance metrics. The input arguments are: data : a data frame or predictions generated by the fit function. For regression, the predicted values in a column called pred . For classification, pred is a factor vector. Class probabilities are usually attached as columns whose names are the class levels (see the random forest example for the fit function above) lev : a character vector with the class levels (or NULL for regression problems) The output should be a named numeric vector of performance values. The example function for random forest is: Two functions in caret that can be used as the summary function are defaultSummary and twoClassSummary (for classification problems with two classes). 21.5.5 The initial Function This function creates an initial generation. Inputs are: vars : the number of possible predictors popSize : the population size for each generation ... : not currently used The output should be a binary 0/1 matrix where there are vars columns corresponding to the predictors and popSize rows for the individuals in the population. The default function populates the rows randomly with subset sizes varying between 10% and 90% of number of possible predictors. For example: gafs has an argument called suggestions that is similar to the one in the ga function where the initial population can be seeded with specific subsets. 21.5.6 The selection Function This function conducts the genetic selection. Inputs are: population : the indicators for the current population fitness : the corresponding fitness values for the population. Note that if the internal performance value is to be minimized, these are the negatives of the actual values r , q : tuning parameters for specific selection functions. See gafs_lrSelection and gafs_nlrSelection ... : not currently used The output should be a list with named elements. population : the indicators for the selected individuals fitness : the fitness values for the selected individuals The default function is a version of the GA package’s ga_lrSelection function. 21.5.7 The crossover Function This function conducts the genetic crossover. Inputs are: population : the indicators for the current population fitness : the corresponding fitness values for the population. Note that if the internal performance value is to be minimized, these are the negatives of the actual values parents : a matrix with two rows containing indicators for the parent individuals. ... : not currently used The default function is a version of the GA package’s ga_spCrossover function. Another function that is a version of that package’s uniform cross-over function is also available. . The output should be a list with named elements. children : from ?ga_spCrossover : “a matrix of dimension 2 times the number of decision variables containing the generated offsprings”" fitness : “a vector of length 2 containing the fitness values for the offsprings. A value NA is returned if an offspring is different (which is usually the case) from the two parents.”" 21.5.8 The mutation Function This function conducts the genetic mutation. Inputs are: population : the indicators for the current population parents : a vector of indices for where the mutation should occur. ... : not currently used The default function is a version of the GA package’s gabin_raMutation function. . The output should the mutated population. 21.5.9 The selectIter Function This function determines the optimal number of generations based on the resampling output. Inputs for the function are: x : a matrix with columns for the performance metrics averaged over resamples metric : a character string of the performance measure to optimize (e.g. RMSE, Accuracy) maximize : a single logical for whether the metric should be maximized This function should return an integer corresponding to the optimal subset size. "
"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.6 The Example Revisited The previous GA included some of the non-informative predictors. We can cheat a little and try to bias the search to get the right solution. We can try to encourage the algorithm to choose fewer predictors, we can penalize the the RMSE estimate. Normally, a metric like the Akaike information criterion (AIC) statistic would be used. However, with a random forest model, there is no real notion of model degrees of freedom. As an alternative, we can use desirability functions to penalize the RMSE. To do this, two functions are created that translate the number of predictors and the RMSE values to a measure of “desirability”. For the number of predictors, the most desirable property would be a single predictor and the worst situation would be if the model required all 50 predictors. That desirability function is visualized as: For the RMSE, the best case would be zero. Many poor models have values around four. To give the RMSE value more weight in the overall desirability calculation, we use a scale parameter value of 2. This desirability function is: To use the overall desirability to drive the feature selection, the internal function requires replacement. We make a copy of rfGA and add code using the desirability package and the function returns the estimated RMSE and the overall desirability. The gafsControl function also need changes. The metric argument needs to reflect that the overall desirability score should be maximized internally but the RMSE estimate should be minimized externally. Here are the RMSE values for this search: The final GA found 6 that were selected: real1, real2, real3, real4, real5, bogus43. During resampling, the average number of predictors selected was 5.2, indicating that the penalty on the number of predictors was effective. "
"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.7 Using Recipes Like the other feature selection routines, gafs can take a data recipe as an input. This is advantageous when your data needs preprocessing before the model, such as: creation of dummy variables from factors specification of interactions missing data imputation more complex feature engineering methods Like train , the recipe’s preprocessing steps are calculated within each resample. This makes sure that the resampling statistics capture the variation and effect that the preprocessing has on the model. As an example, the Ames housing data is used. These data contain a number of categorical predictors that require conversion to indicators as well as other variables that require processing. To load (and split) the data: Here is a recipe that does differetn types of preprocssing on the predictor set: If this were executed on the training set, it would produce 280 predictor columns out of the original 79. Let’s tune some linear models with gafs and, for the sake of computational time, only use 10 generations of the algorithm: "
"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.1 Simulated Annealing Simulated annealing (SA) is a global search method that makes small random changes (i.e. perturbations) to an initial candidate solution. If the performance value for the perturbed value is better than the previous solution, the new solution is accepted. If not, an acceptance probability is determined based on the difference between the two performance values and the current iteration of the search. From this, a sub-optimal solution can be accepted on the off-change that it may eventually produce a better solution in subsequent iterations. See Kirkpatrick (1984) or Rutenbar (1989) for better descriptions. In the context of feature selection, a solution is a binary vector that describes the current subset. The subset is perturbed by randomly changing a small number of members in the subset. "
"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.2 Internal and External Performance Estimates Much of the discussion on this subject in the genetic algorithm page is relevant here, although SA search is less aggressive than GA search. In any case, the implementation here conducts the SA search inside the resampling loops and uses an external performance estimate to choose how many iterations of the search are appropriate. "
"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.3 Basic Syntax The syntax of this function is very similar to the previous information for genetic algorithm searches. The most basic usage of the function is: where x : a data frame or matrix of predictor values y : a factor or numeric vector of outcomes iters : the number of iterations for the SA This isn’t very specific. All of the action is in the control function. That can be used to specify the model to be fit, how predictions are made and summarized as well as the genetic operations. Suppose that we want to fit a linear regression model. To do this, we can use train as an interface and pass arguments to that function through safs : Other options, such as preProcess , can be passed in as well. Some important options to safsControl are: method , number , repeats , index , indexOut , etc: options similar to those for train top control resampling. metric : this is similar to train ’s option but, in this case, the value should be a named vector with values for the internal and external metrics. If none are specified, the first value returned by the summary functions (see details below) are used and a warning is issued. A similar two-element vector for the option maximize is also required. See the last example here for an illustration. holdout : this is a number between [0, 1) that can be used to hold out samples for computing the internal fitness value. Note that this is independent of the external resampling step. Suppose 10-fold CV is being used. Within a resampling iteration, holdout can be used to sample an additional proportion of the 90% resampled data to use for estimating fitness. This may not be a good idea unless you have a very large training set and want to avoid an internal resampling procedure to estimate fitness. improve : an integer (or infinity) defining how many iterations should pass without an improvement in fitness before the current subset is reset to the last known improvement. allowParallel : should the external resampling loop be run in parallel?. There are a few built-in sets of functions to use with safs : caretSA , rfSA , and treebagSA . The first is a simple interface to train . When using this, as shown above, arguments can be passed to train using the ... structure and the resampling estimates of performance can be used as the internal fitness value. The functions provided by rfSA and treebagSA avoid using train and their internal estimates of fitness come from using the out-of-bag estimates generated from the model. "
"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.4 Simulated Annealing Example Using the example from the previous page where there are five real predictors and 40 noise predictors. We’ll fit a random forest model and use the out-of-bag RMSE estimate as the internal performance metric and use the same repeated 10-fold cross-validation process used with the search. To do this, we’ll use the built-in rfSA object for this purpose. The default SA operators will be used with 1000 iterations of the algorithm. As with the GA, we can plot the internal and external performance over iterations. The performance here isn’t as good as the previous GA or RFE solutions. Based on these results, the iteration associated with the best external RMSE estimate was 212 with a corresponding RMSE estimate of 3.31. Using the entire training set, the final SA is conducted and, at iteration 212, there were 21 selected: real1, real2, real5, bogus1, bogus3, bogus9, bogus10, bogus13, bogus14, bogus15, bogus19, bogus20, bogus23, bogus24, bogus25, bogus26, bogus28, bogus31, bogus33, bogus38, bogus44. The random forest model with these predictors is created using the entire training set is trained and this is the model that is used when predict.safs is executed. "
"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.5 Customizing the Search 22.5.1 The fit Function This function builds the model based on a proposed current subset. The arguments for the function must be: x : the current training set of predictor data with the appropriate subset of variables y : the current outcome data (either a numeric or factor vector) lev : a character vector with the class levels (or NULL for regression problems) last : a logical that is TRUE when the final SA search is conducted on the entire data set ... : optional arguments to pass to the fit function in the call to safs The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: 22.5.2 The pred Function This function returns a vector of predictions (numeric or factors) from the current model. The input arguments must be object : the model generated by the fit function x : the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 22.5.3 The fitness_intern Function The fitness_intern function takes the fitted model and computes one or more performance metrics. The inputs to this function are: object : the model generated by the fit function x : the current set of predictor set. If the option safsControl$holdout is zero, these values will be from the current resample (i.e. the same data used to fit the model). Otherwise, the predictor values are from the hold-out set created by safsControl$holdout . y : outcome values. See the note for the x argument to understand which data are presented to the function. maximize : a logical from safsControl that indicates whether the metric should be maximized or minimized p : the total number of possible predictors The output should be a named numeric vector of performance values. In many cases, some resampled measure of performance is used. In the example above using random forest, the OOB error was used. In other cases, the resampled performance from train can be used and, if safsControl$holdout is not zero, a static hold-out set can be used. This depends on the data and problem at hand. If left The example function for random forest is: 22.5.4 The fitness_extern Function The fitness_extern function takes the observed and predicted values form the external resampling process and computes one or more performance metrics. The input arguments are: data : a data frame or predictions generated by the fit function. For regression, the predicted values in a column called pred . For classification, pred is a factor vector. Class probabilities are usually attached as columns whose names are the class levels (see the random forest example for the fit function above) lev : a character vector with the class levels (or NULL for regression problems) The output should be a named numeric vector of performance values. The example function for random forest is: Two functions in caret that can be used as the summary function are defaultSummary and twoClassSummary (for classification problems with two classes). 22.5.5 The initial Function This function creates an initial subset. Inputs are: vars : the number of possible predictors prob : the probability that a feature is in the subset ... : not currently used The output should be a vector of integers indicating which predictors are in the initial subset. Alternatively, instead of a function, a vector of integers can be used in this slot. 22.5.6 The perturb Function This function perturbs the subset. Inputs are: x : the integers defining the current subset vars : the number of possible predictors number : the number of predictors to randomly change ... : not currently used The output should be a vector of integers indicating which predictors are in the new subset. 22.5.7 The prob Function This function computes the acceptance probability. Inputs are: old : the fitness value for the current subset new : the fitness value for the new subset iteration : the current iteration number or, if the improve argument of safsControl is used, the number of iterations since the last restart ... : not currently used The output should be a numeric value between zero and one. One of the biggest difficulties in using simulated annealing is the specification of the acceptance probability calculation. There are many references on different methods for doing this but the general consensus is that 1) the probability should decrease as the difference between the current and new solution increases and 2) the probability should decrease over iterations. One issue is that the difference in fitness values can be scale-dependent. In this package, the default probability calculations uses the percent difference, i.e. (current - new)/current to normalize the difference. The basic form of the probability simply takes the difference, multiplies by the iteration number and exponentiates this product: To demonstrate this, the plot below shows the probability profile for different fitness values of the current subset and different (absolute) differences. For the example data that were simulated, the RMSE values ranged between values greater than 4 to just under 3. In the plot below, the red curve in the right-hand panel shows how the probability changes over time when comparing a current value of 4 with a new values of 4.5 (smaller values being better). While this difference would likely be accepted in the first few iterations, it is unlikely to be accepted after 30 or 40. Also, larger differences are uniformly disfavored relative to smaller differences. While this is the default, any user-written function can be used to assign probabilities. "
"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.6 Using Recipes Similar to the previous section on genetic algorothms, recipes can be used with safs . Using the same data as before: Let’s again use linear models with the function: "
"caret_23_data_sets 23 Data Sets data-sets.html  23.10 Animal Scat Data Reid (2105) collected data on animal feses in coastal California. The data consist of DNA verified species designations as well as fields related to the time and place of the collection and the scat itself. The data frame scat_orig contains while scat contains data on the three main species. "
"caret_23_data_sets 23 Data Sets data-sets.html  23.1 Blood-Brain Barrier Data Mente and Lombardo (2005) developed models to predict the log of the ratio of the concentration of a compound in the brain and the concentration in blood. For each compound, they computed three sets of molecular descriptors: MOE 2D, rule-of-five and Charge Polar Surface Area (CPSA). In all, 134 descriptors were calculated. Included in this package are 208 non-proprietary literature compounds. The vector logBBB contains the log concentration ratio and the data fame bbbDescr contains the descriptor values. "
"caret_23_data_sets 23 Data Sets data-sets.html  23.2 COX-2 Activity Data From Sutherland, O’Brien, and Weaver (2003) : A set of 467 cyclooxygenase-2 (COX-2) inhibitors has been assembled from the published work of a single research group, with in vitro activities against human recombinant enzyme expressed as IC50 values ranging from 1 nM to >100 uM (53 compounds have indeterminate IC50 values). A set of 255 descriptors (MOE2D and QikProp) were generated. To classify the data, we used a cutoff of 2^{2.5} to determine activity. Using data(cox2) exposes three R objects: cox2Descr is a data frame with the descriptor data, cox2IC50 is a numeric vector of IC50 assay values and cox2Class is a factor vector with the activity results. "
"caret_23_data_sets 23 Data Sets data-sets.html  23.3 DHFR Inhibition Sutherland and Weaver (2004) discuss QSAR models for dihydrofolate reductase (DHFR) inhibition. This data set contains values for 325 compounds. For each compound, 228 molecular descriptors have been calculated. Additionally, each samples is designated as “active” or “inactive”. The data frame dhfr contains a column called Y with the outcome classification. The remainder of the columns are molecular descriptor values. "
"caret_23_data_sets 23 Data Sets data-sets.html  23.4 Tecator NIR Data These data can be found in the datasets section of StatLib. The data consist of 100 near infrared absorbance spectra used to predict the moisture, fat and protein values of chopped meat. From StatLib : These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents. If results from these data are used in a publication we want you to mention the instrument and company name (Tecator) in the publication. In addition, please send a preprint of your article to: Karin Thente, Tecator AB, Box 70, S-263 21 Hoganas, Sweden. One reference for these data is Borggaard and Thodberg (1992). Using data(tecator) loads a 215 x 100 matrix of absorbance spectra and a 215 x 3 matrix of outcomes. "
"caret_23_data_sets 23 Data Sets data-sets.html  23.5 Fatty Acid Composition Data Brodnjak-Voncina et al. (2005) describe a set of data where seven fatty acid compositions were used to classify commercial oils as either pumpkin (labeled A ), sunflower ( B ), peanut ( C ), olive ( D ), soybean ( E ), rapeseed ( F ) and corn ( G ). There were 96 data points contained in their Table 1 with known results. The breakdown of the classes is given in below: As a note, the paper states on page 32 that there are 37 unknown samples while the table on pages 33 and 34 shows that there are 34 unknowns. "
"caret_23_data_sets 23 Data Sets data-sets.html  23.6 German Credit Data Data from Dr. Hans Hofmann of the University of Hamburg and stored at the UC Irvine Machine Learning Repository . These data have two classes for the credit worthiness: good or bad. There are predictors related to attributes, such as: checking account status, duration, credit history, purpose of the loan, amount of the loan, savings accounts or bonds, employment duration, Installment rate in percentage of disposable income, personal information, other debtors/guarantors, residence duration, property, age, other installment plans, housing, number of existing credits, job information, Number of people being liable to provide maintenance for, telephone, and foreign worker status. Many of these predictors are discrete and have been expanded into several 0/1 indicator variables "
"caret_23_data_sets 23 Data Sets data-sets.html  23.7 Kelly Blue Book Resale data for 2005 model year GM cars Kuiper (2008) collected data on Kelly Blue Book resale data for 804 GM cars (2005 model year). cars is data frame of the suggested retail price (column Price ) and various characteristics of each car (columns Mileage , Cylinder , Doors , Cruise , Sound , Leather , Buick , Cadillac , Chevy , Pontiac , Saab , Saturn , convertible , coupe , hatchback , sedan and wagon ) "
"caret_23_data_sets 23 Data Sets data-sets.html  23.8 Cell Body Segmentation Data Hill, LaPan, Li and Haney (2007) develop models to predict which cells in a high content screen were well segmented. The data consists of 119 imaging measurements on 2019. The original analysis used 1009 for training and 1010 as a test set (see the column called Case ). The outcome class is contained in a factor variable called Class with levels PS for poorly segmented and WS for well segmented. "
"caret_23_data_sets 23 Data Sets data-sets.html  23.9 Sacramento House Price Data This data frame contains house and sale price data for 932 homes in Sacramento CA. The original data were obtained from the website for the SpatialKey software . From their website: “The Sacramento real estate transactions file is a list of 985 real estate transactions in the Sacramento area reported over a five-day period, as reported by the Sacramento Bee.” Google was used to fill in missing/incorrect data. "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.10 Class Distance Calculations caret contains functions to generate new predictors variables based on distances to class centroids (similar to how linear discriminant analysis works). For each level of a factor variable, the class centroid and covariance matrix is calculated. For new samples, the Mahalanobis distance to each of the class centroids is computed and can be used as an additional predictor. This can be helpful for non-linear models when the true decision boundary is actually linear. In cases where there are more predictors within a class than samples, the classDist function has arguments called pca and keep arguments that allow for principal components analysis within each class to be used to avoid issues with singular covariance matrices. predict.classDist is then used to generate the class distances. By default, the distances are logged, but this can be changed via the trans argument to predict.classDist . As an example, we can used the MDRR data. This image shows a scatterplot matrix of the class distances for the held-out samples: "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.1 Creating Dummy Variables The function dummyVars can be used to generate a complete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method. For example, the etitanic data set in the earth package includes two factors: pclass (passenger class, with levels 1st, 2nd, 3rd) and sex (with levels female, male). The base R function model.matrix would generate the following variables: Using dummyVars : Note there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as lm . "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.2 Zero- and Near Zero-Variance Predictors In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e. a “zero-variance predictor”). For many models (excluding tree-based models), this may cause the model to crash or the fit to be unstable. Similarly, predictors might have only a handful of unique values that occur with very low frequencies. For example, in the drug resistance data, the nR11 descriptor (number of 11-membered rings) data have a few unique numeric values that are highly unbalanced: The concern here that these predictors may become zero-variance predictors when the data are split into cross-validation/bootstrap sub-samples or that a few samples may have an undue influence on the model. These “near-zero-variance” predictors may need to be identified and eliminated prior to modeling. To identify these types of predictors, the following two metrics can be calculated: the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data and the “percent of unique values’’ is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases If the frequency ratio is greater than a pre-specified threshold and the unique value percentage is less than a threshold, we might consider a predictor to be near zero-variance. We would not want to falsely identify data that have low granularity but are evenly distributed, such as data from a discrete uniform distribution. Using both criteria should not falsely detect such predictors. Looking at the MDRR data, the nearZeroVar function can be used to identify near zero-variance variables (the saveMetrics argument can be used to show the details and usually defaults to FALSE ): By default, nearZeroVar will return the positions of the variables that are flagged to be problematic. "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.3 Identifying Correlated Predictors While there are some models that thrive on correlated predictors (such as pls ), other models may benefit from reducing the level of correlation between the predictors. Given a correlation matrix, the findCorrelation function uses the following algorithm to flag predictors for removal: For the previous MDRR data, there are 65 descriptors that are almost perfectly correlated (|correlation| > 0.999), such as the total information index of atomic composition ( IAC ) and the total information content index (neighborhood symmetry of 0-order) ( TIC0 ) (correlation  1). The code chunk below shows the effect of removing descriptors with absolute correlations above 0.75. "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.4 Linear Dependencies The function findLinearCombos uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). For example, consider the following matrix that is could have been produced by a less-than-full-rank parameterizations of a two-way experimental layout: Note that columns two and three add up to the first column. Similarly, columns four, five and six add up the first column. findLinearCombos will return a list that enumerates these dependencies. For each linear combination, it will incrementally remove columns from the matrix and test to see if the dependencies have been resolved. findLinearCombos will also return a vector of column positions can be removed to eliminate the linear dependencies: These types of dependencies can arise when large numbers of binary chemical fingerprints are used to describe the structure of a molecule. "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.5 The preProcess Function The preProcess class can be used for many operations on predictors, including centering and scaling. The function preProcess estimates the required parameters for each operation and predict.preProcess is used to apply them to specific data sets. This function can also be interfaces when calling the train function. Several types of techniques are described in the next few sections and then another example is used to demonstrate how multiple methods can be used. Note that, in all cases, the preProcess function estimates whatever it requires from a specific data set (e.g. the training set) and then applies these transformations to any data set without recomputing the values "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.6 Centering and Scaling In the example below, the half of the MDRR data are used to estimate the location and scale of the predictors. The function preProcess doesn’t actually pre-process the data. predict.preProcess is used to pre-process this and other data sets. The preProcess option "range" scales the data to the interval between zero and one. "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.7 Imputation preProcess can be used to impute data sets based only on information in the training set. One method of doing this is with K-nearest neighbors. For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predictor is imputed using these values (e.g. using the mean). Using this approach will automatically trigger preProcess to center and scale the data, regardless of what is in the method argument. Alternatively, bagged trees can also be used to impute. For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is used to predict the value. While, in theory, this is a more powerful method of imputing, the computational costs are much higher than the nearest neighbor technique. "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.8 Transforming Predictors In some cases, there is a need to use principal component analysis (PCA) to transform the data to a smaller sub–space where the new variable are uncorrelated with one another. The preProcess class can apply this transformation by including "pca" in the method argument. Doing this will also force scaling of the predictors. Note that when PCA is requested, predict.preProcess changes the column names to PC1 , PC2 and so on. Similarly, independent component analysis (ICA) can also be used to find new variables that are linear combinations of the original set such that the components are independent (as opposed to uncorrelated in PCA). The new variables will be labeled as IC1 , IC2 and so on. The “spatial sign” transformation ( Serneels et al, 2006 ) projects the data for a predictor to the unit circle in p dimensions, where p is the number of predictors. Essentially, a vector of data is divided by its norm. The two figures below show two centered and scaled descriptors from the MDRR data before and after the spatial sign transformation. The predictors should be centered and scaled before applying this transformation. After the spatial sign: Another option, "BoxCox" will estimate a Box–Cox transformation on the predictors if the data are greater than zero. The NA values correspond to the predictors that could not be transformed. This transformation requires the data to be greater than zero. Two similar transformations, the Yeo-Johnson and exponential transformation of Manly (1976) can also be used in preProcess . "
"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.9 Putting It All Together In Applied Predictive Modeling there is a case study where the execution times of jobs in a high performance computing environment are being predicted. The data are: The data are a mix of categorical and numeric predictors. Suppose we want to use the Yeo-Johnson transformation on the continuous predictors then center and scale them. Let’s also suppose that we will be running a tree-based models so we might want to keep the factors as factors (as opposed to creating dummy variables). We run the function on all the columns except the last, which is the outcome. The two predictors labeled as “ignored” in the output are the two factor predictors. These are not altered but the numeric predictors are transformed. However, the predictor for the number of pending jobs, has a very sparse and unbalanced distribution: For some other models, this might be an issue (especially if we resample or down-sample the data). We can add a filter to check for zero- or near zero-variance predictors prior to running the pre-processing calculations: Note that one predictor is labeled as “removed” and the processed data lack the sparse predictor. "
"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.1 Simple Splitting Based on the Outcome The function createDataPartition can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. For example, to create a single 80/20% split of the iris data: The list  FALSE avoids returning the data as a list. This function also has an argument, times , that can create multiple splits at once; the data indices are returned in a list of integer vectors. Similarly, createResample can be used to make simple bootstrap samples and createFolds can be used to generate balanced cross–validation groupings from a set of data. "
"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.2 Splitting Based on the Predictors Also, the function maxDissim can be used to create sub–samples using a maximum dissimilarity approach ( Willett, 1999 ). Suppose there is a data set A with m samples and a larger data set B with n samples. We may want to create a sub–sample from B that is diverse when compared to A . To do this, for each sample in B , the function calculates the m dissimilarities between each point in A . The most dissimilar point in B is added to A and the process continues. There are many methods in R to calculate dissimilarity. caret uses the proxy package. See the manual for that package for a list of available measures. Also, there are many ways to calculate which sample is “most dissimilar”. The argument obj can be used to specify any function that returns a scalar measure. caret includes two functions, minDiss and sumDiss , that can be used to maximize the minimum and total dissimilarities, respectfully. As an example, the figure below shows a scatter plot of two chemical descriptors for the Cox2 data. Using an initial random sample of 5 compounds, we can select 20 more compounds from the data so that the new compounds are most dissimilar from the initial 5 that were specified. The panels in the figure show the results using several combinations of distance metrics and scoring functions. For these data, the distance measure has less of an impact than the scoring method for determining which compounds are most dissimilar. The visualization below shows the data set (small points), the starting samples (larger blue points) and the order in which the other 20 samples are added. "
"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.3 Data Splitting for Time Series Simple random sampling of time series is probably not the best way to resample times series data. Hyndman and Athanasopoulos (2013) discuss rolling forecasting origin techniques that move the training and test sets in time. caret contains a function called createTimeSlices that can create the indices for this type of splitting. The three parameters for this type of splitting are: initialWindow : the initial number of consecutive values in each training set sample horizon : The number of consecutive values in test set sample fixedWindow : A logical: if FALSE , the training set always start at the first sample and the training set size will vary over data splits. As an example, suppose we have a time series with 20 data points. We can fix initialWindow  5 and look at different settings of the other two arguments. In the plot below, rows in each panel correspond to different data splits (i.e. resamples) and the columns correspond to different data points. Also, red indicates samples that are in included in the training set and the blue indicates samples in the test set. "
"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.4 Simple Splitting with Important Groups In some cases there is an important qualitative factor in the data that should be considered during (re)sampling. For example: in clinical trials, there may be hospital-to-hospital differences with longitudinal or repeated measures data, subjects (or general independent experimental unit) may have multiple rows in the data set, etc. There may be an interest in making sure that these groups are not contained in the training and testing set since this may bias the test set performance to be more optimistic. Also, when one or more specific groups are held out, the resampling might capture the “ruggedness” of the model. In the example where clinical data is recorded over multiple sites, the resampling performance estimates partly measure how extensible the model is across sites. To split the data based on groups, groupKFold can be used: The results in folds can be used as inputs into the index argument of the trainControl function. This plot shows how each subject is partitioned between the modeling and holdout sets. Note that since k was less than 20 when folds was created, there are some holdouts with model than one subject. "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.1 Model Training and Parameter Tuning The caret package has several functions that attempt to streamline the model building and evaluation process. The train function can be used to evaluate, using resampling, the effect of model tuning parameters on performance choose the “optimal” model across these parameters estimate model performance from a training set First, a specific model must be chosen. Currently, 238 are available using caret ; see train Model List or train Models By Tag for details. On these pages, there are lists of tuning parameters that can potentially be optimized. User-defined models can also be created. The first step in tuning the model (line 1 in the algorithm below) is to choose a set of parameters to evaluate. For example, if fitting a Partial Least Squares (PLS) model, the number of PLS components to evaluate must be specified. Once the model and tuning parameter values have been defined, the type of resampling should be also be specified. Currently, k -fold cross-validation (once or repeated), leave-one-out cross-validation and bootstrap (simple estimation or the 632 rule) resampling methods can be used by train . After resampling, the process produces a profile of performance measures is available to guide the user as to which tuning parameter values should be chosen. By default, the function automatically chooses the tuning parameters associated with the best value, although different algorithms can be used (see details below). "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.2 An Example The Sonar data are available in the mlbench package. Here, we load the data: The function createDataPartition can be used to create a stratified random sample of the data into training and test sets: We will use these data illustrate functionality on this (and other) pages. "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.3 Basic Parameter Tuning By default, simple bootstrap resampling is used for line 3 in the algorithm above. Others are available, such as repeated K -fold cross-validation, leave-one-out etc. The function trainControl can be used to specifiy the type of resampling: More information about trainControl is given in a section below . The first two arguments to train are the predictor and outcome data objects, respectively. The third argument, method , specifies the type of model (see train Model List or train Models By Tag ). To illustrate, we will fit a boosted tree model via the gbm package. The basic syntax for fitting this model using repeated cross-validation is shown below: For a gradient boosting machine (GBM) model, there are three main tuning parameters: number of iterations, i.e. trees, (called n.trees in the gbm function) complexity of the tree, called interaction.depth learning rate: how quickly the algorithm adapts, called shrinkage the minimum number of training set samples in a node to commence splitting ( n.minobsinnode ) The default values tested for this model are shown in the first two columns ( shrinkage and n.minobsinnode are not shown beause the grid set of candidate models all use a single value for these tuning parameters). The column labeled “ Accuracy ” is the overall agreement rate averaged over cross-validation iterations. The agreement standard deviation is also calculated from the cross-validation results. The column “ Kappa ” is Cohen’s (unweighted) Kappa statistic averaged across the resampling results. train works with specific models (see train Model List or train Models By Tag ). For these models, train can automatically create a grid of tuning parameters. By default, if p is the number of tuning parameters, the grid size is 3^p . As another example, regularized discriminant analysis (RDA) models have two parameters ( gamma and lambda ), both of which lie between zero and one. The default training grid would produce nine combinations in this two-dimensional space. There is additional functionality in train that is described in the next section. "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.4 Notes on Reproducibility Many models utilize random numbers during the phase where parameters are estimated. Also, the resampling indices are chosen using random numbers. There are two main ways to control the randomness in order to assure reproducible results. There are two approaches to ensuring that the same resamples are used between calls to train . The first is to use set.seed just prior to calling train . The first use of random numbers is to create the resampling information. Alternatively, if you would like to use specific splits of the data, the index argument of the trainControl function can be used. This is briefly discussed below. When the models are created inside of resampling , the seeds can also be set. While setting the seed prior to calling train may guarantee that the same random numbers are used, this is unlikely to be the case when parallel processing is used (depending which technology is utilized). To set the model fitting seeds, trainControl has an additional argument called seeds that can be used. The value for this argument is a list of integer vectors that are used as seeds. The help page for trainControl describes the appropriate format for this option. How random numbers are used is highly dependent on the package author. There are rare cases where the underlying model function does not control the random number seed, especially if the computations are conducted in C code. Also, please note that some packages load random numbers when loaded (directly or via namespace) and this may affect reproducibility. "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.5 Customizing the Tuning Process There are a few ways to customize the process of selecting tuning/complexity parameters and building the final model. 5.5.1 Pre-Processing Options As previously mentioned, train can pre-process the data in various ways prior to model fitting. The function preProcess is automatically used. This function can be used for centering and scaling, imputation (see details below), applying the spatial sign transformation and feature extraction via principal component analysis or independent component analysis. To specify what pre-processing should occur, the train function has an argument called preProcess . This argument takes a character string of methods that would normally be passed to the method argument of the preProcess function . Additional options to the preProcess function can be passed via the trainControl function. These processing steps would be applied during any predictions generated using predict.train , extractPrediction or extractProbs (see details later in this document). The pre-processing would not be applied to predictions that directly use the object$finalModel object. For imputation, there are three methods currently implemented: k -nearest neighbors takes a sample with missing values and finds the k closest samples in the training set. The average of the k training set values for that predictor are used as a substitute for the original data. When calculating the distances to the training set samples, the predictors used in the calculation are the ones with no missing values for that sample and no missing values in the training set. another approach is to fit a bagged tree model for each predictor using the training set samples. This is usually a fairly accurate model and can handle missing values. When a predictor for a sample requires imputation, the values for the other predictors are fed through the bagged tree and the prediction is used as the new value. This model can have significant computational cost. the median of the predictor’s training set values can be used to estimate the missing data. If there are missing values in the training set, PCA and ICA models only use complete samples. 5.5.2 Alternate Tuning Grids The tuning parameter grid can be specified by the user. The argument tuneGrid can take a data frame with columns for each tuning parameter. The column names should be the same as the fitting function’s arguments. For the previously mentioned RDA example, the names would be gamma and lambda . train will tune the model over each combination of values in the rows. For the boosted tree model, we can fix the learning rate and evaluate more than three values of n.trees : Another option is to use a random sample of possible tuning parameter combinations, i.e. “random search” (pdf) . This functionality is described on this page . To use a random search, use the option search  "random" in the call to trainControl . In this situation, the tuneLength parameter defines the total number of parameter combinations that will be evaluated. 5.5.3 Plotting the Resampling Profile The plot function can be used to examine the relationship between the estimates of performance and the tuning parameters. For example, a simple invokation of the function shows the results for the first performance measure: Other performance metrics can be shown using the metric option: Other types of plot are also available. See ?plot.train for more details. The code below shows a heatmap of the results: A ggplot method can also be used: There are also plot functions that show more detailed representations of the resampled estimates. See ?xyplot.train for more details. From these plots, a different set of tuning parameters may be desired. To change the final values without starting the whole process again, the update.train can be used to refit the final model. See ?update.train 5.5.4 The trainControl Function The function trainControl generates parameters that further control how models are created, with possible values: method : The resampling method: "boot" , "cv" , "LOOCV" , "LGOCV" , "repeatedcv" , "timeslice" , "none" and "oob" . The last value, out-of-bag estimates, can only be used by random forest, bagged trees, bagged earth, bagged flexible discriminant analysis, or conditional tree forest models. GBM models are not included (the gbm package maintainer has indicated that it would not be a good idea to choose tuning parameter values based on the model OOB error estimates with boosted trees). Also, for leave-one-out cross-validation, no uncertainty estimates are given for the resampled performance measures. number and repeats : number controls with the number of folds in K -fold cross-validation or number of resampling iterations for bootstrapping and leave-group-out cross-validation. repeats applied only to repeated K -fold cross-validation. Suppose that method  "repeatedcv" , number  10 and repeats  3 ,then three separate 10-fold cross-validations are used as the resampling scheme. verboseIter : A logical for printing a training log. returnData : A logical for saving the data into a slot called trainingData . p : For leave-group out cross-validation: the training percentage For method  "timeslice" , trainControl has options initialWindow , horizon and fixedWindow that govern how cross-validation can be used for time series data. classProbs : a logical value determining whether class probabilities should be computed for held-out samples during resample. index and indexOut : optional lists with elements for each resampling iteration. Each list element is the sample rows used for training at that iteration or should be held-out. When these values are not specified, train will generate them. summaryFunction : a function to computed alternate performance summaries. selectionFunction : a function to choose the optimal tuning parameters. and examples. PCAthresh , ICAcomp and k : these are all options to pass to the preProcess function (when used). returnResamp : a character string containing one of the following values: "all" , "final" or "none" . This specifies how much of the resampled performance measures to save. allowParallel : a logical that governs whether train should use parallel processing (if availible). There are several other options not discussed here. 5.5.5 Alternate Performance Metrics The user can change the metric used to determine the best settings. By default, RMSE, R 2 , and the mean absolute error (MAE) are computed for regression while accuracy and Kappa are computed for classification. Also by default, the parameter values are chosen using RMSE and accuracy, respectively for regression and classification. The metric argument of the train function allows the user to control which the optimality criterion is used. For example, in problems where there are a low percentage of samples in one class, using metric  "Kappa" can improve quality of the final model. If none of these parameters are satisfactory, the user can also compute custom performance metrics. The trainControl function has a argument called summaryFunction that specifies a function for computing performance. The function should have these arguments: data is a reference for a data frame or matrix with columns called obs and pred for the observed and predicted outcome values (either numeric data for regression or character values for classification). Currently, class probabilities are not passed to the function. The values in data are the held-out predictions (and their associated reference values) for a single combination of tuning parameters. If the classProbs argument of the trainControl object is set to TRUE , additional columns in data will be present that contains the class probabilities. The names of these columns are the same as the class levels. Also, if weights were specified in the call to train , a column called weights will also be in the data set. Additionally, if the recipe method for train was used (see this section of documentation ), other variables not used in the model will also be included. This can be accomplished by adding a role in the recipe of "performance var" . An example is given in the recipe section of this site. lev is a character string that has the outcome factor levels taken from the training data. For regression, a value of NULL is passed into the function. model is a character string for the model being used (i.e. the value passed to the method argument of train ). The output to the function should be a vector of numeric summary metrics with non-null names. By default, train evaluate classification models in terms of the predicted classes. Optionally, class probabilities can also be used to measure performance. To obtain predicted class probabilities within the resampling process, the argument classProbs in trainControl must be set to TRUE . This merges columns of probabilities into the predictions generated from each resample (there is a column per class and the column names are the class names). As shown in the last section, custom functions can be used to calculate performance scores that are averaged over the resamples. Another built-in function, twoClassSummary , will compute the sensitivity, specificity and area under the ROC curve: To rebuild the boosted tree model using this criterion, we can see the relationship between the tuning parameters and the area under the ROC curve using the following code: In this case, the average area under the ROC curve associated with the optimal tuning parameters was 0.922 across the 100 resamples. "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.6 Choosing the Final Model Another method for customizing the tuning process is to modify the algorithm that is used to select the “best” parameter values, given the performance numbers. By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models). Other schemes for selecting model can be used. Breiman et al (1984) suggested the “one standard error rule” for simple tree-based models. In this case, the model with the best performance value is identified and, using resampling, we can estimate the standard error of performance. The final model used was the simplest model within one standard error of the (empirically) best model. With simple trees this makes sense, since these models will start to over-fit as they become more and more specific to the training data. train allows the user to specify alternate rules for selecting the final model. The argument selectionFunction can be used to supply a function to algorithmically determine the final model. There are three existing functions in the package: best is chooses the largest/smallest value, oneSE attempts to capture the spirit of Breiman et al (1984) and tolerance selects the least complex model within some percent tolerance of the best value. See ?best for more details. User-defined functions can be used, as long as they have the following arguments: x is a data frame containing the tune parameters and their associated performance metrics. Each row corresponds to a different tuning parameter combination. metric a character string indicating which performance metric should be optimized (this is passed in directly from the metric argument of train . maximize is a single logical value indicating whether larger values of the performance metric are better (this is also directly passed from the call to train ). The function should output a single integer indicating which row in x is chosen. As an example, if we chose the previous boosted tree model on the basis of overall accuracy, we would choose: n.trees  1450, interaction.depth  5, shrinkage  0.1, n.minobsinnode  20. However, the scale in this plots is fairly tight, with accuracy values ranging from 0.863 to 0.922. A less complex model (e.g. fewer, more shallow trees) might also yield acceptable accuracy. The tolerance function could be used to find a less complex model based on ( x - x best )/ x best x 100, which is the percent difference. For example, to select parameter values based on a 2% loss of performance: This indicates that we can get a less complex model with an area under the ROC curve of 0.914 (compared to the “pick the best” value of 0.922). The main issue with these functions is related to ordering the models from simplest to complex. In some cases, this is easy (e.g. simple trees, partial least squares), but in cases such as this model, the ordering of models is subjective. For example, is a boosted tree model using 100 iterations and a tree depth of 2 more complex than one with 50 iterations and a depth of 8? The package makes some choices regarding the orderings. In the case of boosted trees, the package assumes that increasing the number of iterations adds complexity at a faster rate than increasing the tree depth, so models are ordered on the number of iterations then ordered with depth. See ?best for more examples for specific models. "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.7 Extracting Predictions and Class Probabilities As previously mentioned, objects produced by the train function contain the “optimized” model in the finalModel sub-object. Predictions can be made from these objects as usual. In some cases, such as pls or gbm objects, additional parameters from the optimized fit may need to be specified. In these cases, the train objects uses the results of the parameter optimization to predict new samples. For example, if predictions were created using predict.gbm , the user would have to specify the number of trees directly (there is no default). Also, for binary classification, the predictions from this function take the form of the probability of one of the classes, so extra steps are required to convert this to a factor vector. predict.train automatically handles these details for this (and for other models). Also, there are very few standard syntaxes for model predictions in R. For example, to get class probabilities, many predict methods have an argument called type that is used to specify whether the classes or probabilities should be generated. Different packages use different values of type , such as "prob" , "posterior" , "response" , "probability" or "raw" . In other cases, completely different syntax is used. For predict.train , the type options are standardized to be "class" and "prob" (the underlying code matches these to the appropriate choices for each model. For example: "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.8 Exploring and Comparing Resampling Distributions 5.8.1 Within-Model There are several lattice functions than can be used to explore relationships between tuning parameters and the resampling results for a specific model: xyplot and stripplot can be used to plot resampling statistics against (numeric) tuning parameters. histogram and densityplot can also be used to look at distributions of the tuning parameters across tuning parameters. For example, the following statements create a density plot: Note that if you are interested in plotting the resampling results across multiple tuning parameters, the option resamples  "all" should be used in the control object. 5.8.2 Between-Models The caret package also includes functions to characterize the differences between models (generated using train , sbf or rfe ) via their resampling distributions. These functions are based on the work of Hothorn et al. (2005) and Eugster et al (2008) . First, a support vector machine model is fit to the Sonar data. The data are centered and scaled using the preProc argument. Note that the same random number seed is set prior to the model that is identical to the seed used for the boosted tree model. This ensures that the same resampling sets are used, which will come in handy when we compare the resampling profiles between models. Also, a regularized discriminant analysis model was fit. Given these models, can we make statistical statements about their performance differences? To do this, we first collect the resampling results using resamples . Note that, in this case, the option resamples  "final" should be user-defined in the control objects. There are several lattice plot methods that can be used to visualize the resampling distributions: density plots, box-whisker plots, scatterplot matrices and scatterplots of summary statistics. For example: Other visualizations are availible in densityplot.resamples and parallel.resamples Since models are fit on the same versions of the training data, it makes sense to make inferences on the differences between models. In this way we reduce the within-resample correlation that may exist. We can compute the differences, then use a simple t -test to evaluate the null hypothesis that there is no difference between models. "
"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.9 Fitting Models Without Parameter Tuning In cases where the model tuning values are known, train can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the method  "none" option in trainControl can be used. For example: Note that plot.train , resamples , confusionMatrix.train and several other functions will not work with this object but predict.train and others will: "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.10 Ensemble Model (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Model Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit Boosted Tree Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst , plyr C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.11 Feature Extraction (back to contents ) Independent Component Regression Type: Regression Tuning parameters: n.comp (#Components) Required packages: fastICA Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Principal Component Analysis Type: Regression Tuning parameters: ncomp (#Components) Required packages: pls Projection Pursuit Regression Type: Regression Tuning parameters: nterms (# Terms) Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Supervised Principal Component Analysis Type: Regression Tuning parameters: threshold (Threshold) n.components (#Components) Required packages: superpc "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.12 Feature Selection Wrapper (back to contents ) Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Regression with Backwards Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Forward Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.13 Gaussian Process (back to contents ) Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.14 Generalized Additive Model (back to contents ) Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Generalized Additive Model using LOESS Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.15 Generalized Linear Model (back to contents ) Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. Generalized Additive Model using LOESS Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Negative Binomial Generalized Linear Model Type: Regression Tuning parameters: link (Link Function) Required packages: MASS A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.16 Handle Missing Predictor Data (back to contents ) AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.17 Implicit Feature Selection (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Elasticnet Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Least Angle Regression Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression Type: Regression Tuning parameters: step (#Steps) Required packages: lars Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Spike and Slab Regression Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab , plyr Notes: Unlike other packages used by train , the spikeslab package is fully loaded when this model is used. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. The lasso Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.18 Kernel Method (back to contents ) Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Polynomial Kernel Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Polynomial Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Radial Basis Function Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS , kernlab Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.19 L1 Regularization (back to contents ) Bayesian Ridge Regression (Model Averaged) Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Elasticnet Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Least Angle Regression Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression Type: Regression Tuning parameters: step (#Steps) Required packages: lars Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. The lasso Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.1 Accepts Case Weights (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Linear Regression Type: Regression Tuning parameters: intercept (intercept) A model-specific variable importance metric is available. Linear Regression with Stepwise Selection Type: Regression No tuning parameters for this model Required packages: MASS Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Negative Binomial Generalized Linear Model Type: Regression Tuning parameters: link (Link Function) Required packages: MASS A model-specific variable importance metric is available. Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Projection Pursuit Regression Type: Regression Tuning parameters: nterms (# Terms) Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Robust Linear Model Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS A model-specific variable importance metric is available. Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.20 L2 Regularization (back to contents ) Bayesian Ridge Regression Type: Regression No tuning parameters for this model Required packages: monomvn Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Polynomial Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Radial Basis Function Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS , kernlab Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Ridge Regression Type: Regression Tuning parameters: lambda (Weight Decay) Required packages: elasticnet Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.21 Linear Classifier (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. Factor-Based Linear Discriminant Analysis Type: Classification Tuning parameters: q (# Factors) Required packages: HiDimDA Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Localized Linear Discriminant Analysis Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Maximum Uncertainty Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: HiDimDA Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Robust SIMCA Type: Classification No tuning parameters for this model Required packages: rrcovHD Notes: Unlike other packages used by train , the rrcovHD package is fully loaded when this model is used. Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Stabilized Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: ipred Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.22 Linear Regression (back to contents ) Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Ridge Regression Type: Regression No tuning parameters for this model Required packages: monomvn Bayesian Ridge Regression (Model Averaged) Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Elasticnet Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Independent Component Regression Type: Regression Tuning parameters: n.comp (#Components) Required packages: fastICA L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Angle Regression Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression Type: Regression Tuning parameters: step (#Steps) Required packages: lars Linear Regression Type: Regression Tuning parameters: intercept (intercept) A model-specific variable importance metric is available. Linear Regression with Backwards Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Forward Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression No tuning parameters for this model Required packages: MASS Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Non-Negative Least Squares Type: Regression No tuning parameters for this model Required packages: nnls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Principal Component Analysis Type: Regression Tuning parameters: ncomp (#Components) Required packages: pls Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Ridge Regression Type: Regression Tuning parameters: lambda (Weight Decay) Required packages: elasticnet Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba Robust Linear Model Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS A model-specific variable importance metric is available. Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Spike and Slab Regression Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab , plyr Notes: Unlike other packages used by train , the spikeslab package is fully loaded when this model is used. Supervised Principal Component Analysis Type: Regression Tuning parameters: threshold (Threshold) n.components (#Components) Required packages: superpc Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. The lasso Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.23 Logic Regression (back to contents ) Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.24 Logistic Regression (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.25 Mixture Model (back to contents ) Adaptive Mixture Discriminant Analysis Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Mixture Discriminant Analysis Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.26 Model Tree (back to contents ) Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.27 Multivariate Adaptive Regression Splines (back to contents ) Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.28 Neural Network (back to contents ) Bayesian Regularized Neural Networks Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Extreme Learning Machine Type: Classification, Regression Tuning parameters: nhid (#Hidden Units) actfun (Activation Function) Required packages: elmNN Notes: The package is no longer on CRAN but can be installed from the archive at https://cran.r-project.org/src/contrib/Archive/elmNN/ Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Monotone Multi-Layer Perceptron Neural Network Type: Classification, Regression Tuning parameters: hidden1 (#Hidden Units) n.ensemble (#Models) Required packages: monmlp Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) Required packages: RSNNS Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, with multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) Required packages: RSNNS Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Dropout Type: Regression, Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) learning.rate (Learning Rate) momentum (Momentum) dropout (Dropout Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) dropout (Dropout Rate) beta1 (beta1) beta2 (beta2) learningrate (Learning Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Users are strongly advised to define num.round themselves. Neural Network Type: Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) Required packages: neuralnet Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Stacked AutoEncoder Deep Neural Network Type: Classification, Regression Tuning parameters: layer1 (Hidden Layer 1) layer2 (Hidden Layer 2) layer3 (Hidden Layer 3) hidden_dropout (Hidden Dropouts) visible_dropout (Visible Dropout) Required packages: deepnet "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.29 Oblique Tree (back to contents ) Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.2 Bagging (back to contents ) Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Model Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.30 Ordinal Outcomes (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.31 Partial Least Squares (back to contents ) Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Generalized Linear Models Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Notes: Unlike other packages used by train , the plsRglm package is fully loaded when this model is used. Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.32 Patient Rule Induction Method (back to contents ) Patient Rule Induction Method Type: Classification Tuning parameters: peel.alpha (peeling quantile) paste.alpha (pasting quantile) mass.min (minimum mass) Required packages: supervisedPRIM "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.33 Polynomial Model (back to contents ) Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Least Squares Support Vector Machine with Polynomial Kernel Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Polynomial Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Robust Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.34 Prototype Models (back to contents ) Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Greedy Prototype Selection Type: Classification Tuning parameters: eps (Ball Size) Minkowski (Distance Order) Required packages: proxy , protoclass k-Nearest Neighbors Type: Regression, Classification Tuning parameters: kmax (Max. #Neighbors) distance (Distance) kernel (Kernel) Required packages: kknn k-Nearest Neighbors Type: Classification, Regression Tuning parameters: k (#Neighbors) Learning Vector Quantization Type: Classification Tuning parameters: size (Codebook Size) k (#Prototypes) Required packages: class Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Optimal Weighted Nearest Neighbor Classifier Type: Classification Tuning parameters: K (#Neighbors) Required packages: snn Stabilized Nearest Neighbor Classifier Type: Classification Tuning parameters: lambda (Stabilization Parameter) Required packages: snn "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.35 Quantile Regression (back to contents ) Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.36 Radial Basis Function (back to contents ) Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Radial Basis Function Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS , kernlab Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.37 Random Forest (back to contents ) Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.38 Regularization (back to contents ) Bayesian Regularized Neural Networks Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.39 Relevance Vector Machines (back to contents ) Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.3 Bayesian Model (back to contents ) Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Bayesian Regularized Neural Networks Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Bayesian Ridge Regression Type: Regression No tuning parameters for this model Required packages: monomvn Bayesian Ridge Regression (Model Averaged) Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. Model Averaged Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Naive Bayes Type: Classification Tuning parameters: laplace (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: naivebayes Naive Bayes Type: Classification Tuning parameters: fL (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: klaR Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Semi-Naive Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Spike and Slab Regression Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab , plyr Notes: Unlike other packages used by train , the spikeslab package is fully loaded when this model is used. The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. Tree Augmented Naive Bayes Classifier Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.40 Ridge Regression (back to contents ) Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.41 Robust Methods (back to contents ) L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.42 Robust Model (back to contents ) Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Linear Model Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS A model-specific variable importance metric is available. Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Robust SIMCA Type: Classification No tuning parameters for this model Required packages: rrcovHD Notes: Unlike other packages used by train , the rrcovHD package is fully loaded when this model is used. SIMCA Type: Classification No tuning parameters for this model Required packages: rrcov , rrcovHD "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.43 ROC Curves (back to contents ) ROC-Based Classifier Type: Classification Tuning parameters: xgenes (#Variables Retained) Required packages: rocc "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.44 Rule-Based Model (back to contents ) Adaptive-Network-Based Fuzzy Inference System Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Dynamic Evolving Neural-Fuzzy Inference System Type: Regression Tuning parameters: Dthr (Threshold) max.iter (Max. Iterations) Required packages: frbs Fuzzy Inference Rules by Descent Method Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Fuzzy Rules Using Chi’s Method Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs Fuzzy Rules Using Genetic Cooperative-Competitive Learning and Pittsburgh Type: Classification Tuning parameters: max.num.rule (Max. #Rules) popu.size (Population Size) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules Using the Structural Learning Algorithm on Vague Environment Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules via MOGUL Type: Regression Tuning parameters: max.gen (Max. Generations) max.iter (Max. Iterations) max.tune (Max. Tuning Iterations) Required packages: frbs Fuzzy Rules via Thrift Type: Regression Tuning parameters: popu.size (Population Size) num.labels (# Fuzzy Labels) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules with Weight Factor Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs Genetic Lateral Tuning and Rule Selection of Linguistic Fuzzy Systems Type: Regression Tuning parameters: popu.size (Population Size) num.labels (# Fuzzy Labels) max.gen (Max. Generations) Required packages: frbs Hybrid Neural Fuzzy Inference System Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Patient Rule Induction Method Type: Classification Tuning parameters: peel.alpha (peeling quantile) paste.alpha (pasting quantile) mass.min (minimum mass) Required packages: supervisedPRIM Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Simplified TSK Fuzzy Rules Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Subtractive Clustering and Fuzzy c-Means Rules Type: Regression Tuning parameters: r.a (Radius) eps.high (Upper Threshold) eps.low (Lower Threshold) Required packages: frbs Wang and Mendel Fuzzy Rules Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.45 Self-Organising Maps (back to contents ) Self-Organizing Maps Type: Classification, Regression Tuning parameters: xdim (Rows) ydim (Columns) user.weights (Layer Weight) topo (Topology) Required packages: kohonen Notes: As of version 3.0.0 of the kohonen package, the argument user.weights replaces the old alpha parameter. user.weights is usually a vector of relative weights such as c(1, 3) but is parameterized here as a proportion such as c(1-.75, .75) where the .75 is the value of the tuning parameter passed to train and indicates that the outcome layer has 3 times the weight as the predictor layer. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.46 String Kernel (back to contents ) Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.47 Support Vector Machines (back to contents ) L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Polynomial Kernel Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.48 Supports Class Probabilities (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Adaptive Mixture Discriminant Analysis Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Model Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Generalized Additive Model using LOESS Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim k-Nearest Neighbors Type: Regression, Classification Tuning parameters: kmax (Max. #Neighbors) distance (Distance) kernel (Kernel) Required packages: kknn k-Nearest Neighbors Type: Classification, Regression Tuning parameters: k (#Neighbors) Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Localized Linear Discriminant Analysis Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Mixture Discriminant Analysis Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Model Averaged Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Monotone Multi-Layer Perceptron Neural Network Type: Classification, Regression Tuning parameters: hidden1 (#Hidden Units) n.ensemble (#Models) Required packages: monmlp Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) Required packages: RSNNS Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, with multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) Required packages: RSNNS Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Dropout Type: Regression, Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Naive Bayes Type: Classification Tuning parameters: laplace (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: naivebayes Naive Bayes Type: Classification Tuning parameters: fL (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: klaR Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) learning.rate (Learning Rate) momentum (Momentum) dropout (Dropout Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) dropout (Dropout Rate) beta1 (beta1) beta2 (beta2) learningrate (Learning Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Users are strongly advised to define num.round themselves. Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Non-Informative Model Type: Classification, Regression No tuning parameters for this model Notes: Since this model always predicts the same value, R-squared values will always be estimated to be NA. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Generalized Linear Models Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Notes: Unlike other packages used by train , the plsRglm package is fully loaded when this model is used. Patient Rule Induction Method Type: Classification Tuning parameters: peel.alpha (peeling quantile) paste.alpha (pasting quantile) mass.min (minimum mass) Required packages: supervisedPRIM Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Robust Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Self-Organizing Maps Type: Classification, Regression Tuning parameters: xdim (Rows) ydim (Columns) user.weights (Layer Weight) topo (Topology) Required packages: kohonen Notes: As of version 3.0.0 of the kohonen package, the argument user.weights replaces the old alpha parameter. user.weights is usually a vector of relative weights such as c(1, 3) but is parameterized here as a proportion such as c(1-.75, .75) where the .75 is the value of the tuning parameter passed to train and indicates that the outcome layer has 3 times the weight as the predictor layer. Semi-Naive Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Stabilized Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: ipred Stacked AutoEncoder Deep Neural Network Type: Classification, Regression Tuning parameters: layer1 (Hidden Layer 1) layer2 (Hidden Layer 2) layer3 (Hidden Layer 3) hidden_dropout (Hidden Dropouts) visible_dropout (Visible Dropout) Required packages: deepnet Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Tree Augmented Naive Bayes Classifier Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.49 Text Mining (back to contents ) Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.4 Binary Predictors Only (back to contents ) Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.50 Tree-Based Model (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit Boosted Tree Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst , plyr C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.51 Two Class Only (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Partial Least Squares Generalized Linear Models Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Notes: Unlike other packages used by train , the plsRglm package is fully loaded when this model is used. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.5 Boosting (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit Boosted Tree Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst , plyr C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.6 Categorical Predictors Only (back to contents ) Model Averaged Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Semi-Naive Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.7 Cost Sensitive Learning (back to contents ) Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.8 Discriminant Analysis (back to contents ) Adaptive Mixture Discriminant Analysis Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Factor-Based Linear Discriminant Analysis Type: Classification Tuning parameters: q (# Factors) Required packages: HiDimDA Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Localized Linear Discriminant Analysis Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Maximum Uncertainty Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: HiDimDA Mixture Discriminant Analysis Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Robust Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Stabilized Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: ipred "
"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.9 Distance Weighted Discrimination (back to contents ) Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. "