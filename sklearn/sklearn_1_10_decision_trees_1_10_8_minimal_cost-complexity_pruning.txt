sklearn_1_10_decision_trees
1.10. Decision Trees
modules/tree.html
 1.10.8. Minimal Cost-Complexity Pruning  Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of [BRE] . This algorithm is parameterized by known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, of a given tree : \[R_\alpha(T)  R(T) + \alpha|T|\] where is the number of terminal nodes in and is traditionally defined as the total misclassification rate of the terminal nodes. Alternatively, scikit-learn uses the total sample weighted impurity of the terminal nodes for . As shown above, the impurity of a node depends on the criterion. Minimal cost-complexity pruning finds the subtree of that minimizes . The cost complexity measure of a single node is . The branch, , is defined to be a tree where node is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, . However, the cost complexity measure of a node, , and its branch, , can be equal depending on . We define the effective of a node to be the value where they are equal, or . A non-terminal node with the smallest value of is the weakest link and will be pruned. This process stops when the pruned treeâ€™s minimal is greater than the parameter. Examples: Post pruning decision trees with cost complexity pruning References: BRE L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984. https://en.wikipedia.org/wiki/Decision_tree_learning https://en.wikipedia.org/wiki/Predictive_analytics J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993. T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009. 