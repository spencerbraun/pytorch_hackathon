sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems
2.5. Decomposing signals in components (matrix factorization problems)
modules/decomposition.html
 2.5.4. Factor Analysis  In unsupervised learning we only have a dataset . How can this dataset be described mathematically? A very simple model for is \[x_i  W h_i + \mu + \epsilon\] The vector is called “latent” because it is unobserved. is considered a noise term distributed according to a Gaussian with mean 0 and covariance (i.e. ), is some arbitrary offset vector. Such a model is called “generative” as it describes how is generated from . If we use all the ’s as columns to form a matrix and all the ’s as columns of a matrix then we can write (with suitably defined and ): \[\mathbf{X}  W \mathbf{H} + \mathbf{M} + \mathbf{E}\] In other words, we decomposed matrix . If is given, the above equation automatically implies the following probabilistic interpretation: \[p(x_i|h_i)  \mathcal{N}(Wh_i + \mu, \Psi)\] For a complete probabilistic model we also need a prior distribution for the latent variable . The most straightforward assumption (based on the nice properties of the Gaussian distribution) is . This yields a Gaussian as the marginal distribution of : \[p(x)  \mathcal{N}(\mu, WW^T + \Psi)\] Now, without any further assumptions the idea of having a latent variable would be superfluous – can be completely modelled with a mean and a covariance. We need to impose some more specific structure on one of these two parameters. A simple additional assumption regards the structure of the error covariance : : This assumption leads to the probabilistic model of PCA . : This model is called FactorAnalysis , a classical statistical model. The matrix W is sometimes called the “factor loading matrix”. Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. FastICA ) if non-Gaussian priors on the latent variables are assumed. Factor analysis can produce similar components (the columns of its loading matrix) to PCA . However, one can not make any general statements about these components (e.g. whether they are orthogonal): The main advantage for Factor Analysis over PCA is that it can model the variance in every direction of the input space independently (heteroscedastic noise): This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise: Examples: Model selection with Probabilistic PCA and Factor Analysis (FA) 