sklearn_1_9_naive_bayes
1.9. Naive Bayes
modules/naive_bayes.html
 1.9.2. Multinomial Naive Bayes  MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors for each class , where is the number of features (in text classification, the size of the vocabulary) and is the probability of feature appearing in a sample belonging to class . The parameters is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting: \[\hat{\theta}_{yi}  \frac{ N_{yi} + \alpha}{N_y + \alpha n}\] where is the number of times feature appears in a sample of class in the training set , and is the total count of all features for class . The smoothing priors accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting is called Laplace smoothing, while is called Lidstone smoothing. 