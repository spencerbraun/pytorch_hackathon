sklearn_1_11_ensemble_methods
1.11. Ensemble methods
modules/ensemble.html
 1.11.8. Stacked generalization  Stacked generalization is a method for combining estimators to reduce their biases [W1992] [HTF] . More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation. The StackingClassifier and StackingRegressor provide such strategies which can be applied to classification and regression problems. The parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators: The will use the predictions of the as input. It needs to be a classifier or a regressor when using StackingClassifier or StackingRegressor , respectively: To train the and , the method needs to be called on the training data: During training, the are fitted on the whole training data . They will be used when calling or . To generalize and avoid over-fitting, the is trained on out-samples using sklearn.model_selection.cross_val_predict internally. For StackingClassifier , note that the output of the is controlled by the parameter and it is called by each estimator. This parameter is either a string, being estimator method names, or which will automatically identify an available method depending on the availability, tested in the order of preference: , and . A StackingRegressor and StackingClassifier can be used as any other regressor or classifier, exposing a , , and methods, e.g.: Note that it is also possible to get the output of the stacked using the method: In practise, a stacking predictor predict as good as the best predictor of the base layer and even sometimes outputperform it by combining the different strength of the these predictors. However, training a stacking predictor is computationally expensive. Note For StackingClassifier , when using , the first column is dropped when the problem is a binary classification problem. Indeed, both probability columns predicted by each estimator are perfectly collinear. Note Multiple stacking layers can be achieved by assigning to a StackingClassifier or StackingRegressor : References W1992 Wolpert, David H. “Stacked generalization.” Neural networks 5.2 (1992): 241-259. 