sklearn_1_6_nearest_neighbors
1.6. Nearest Neighbors
modules/neighbors.html
 1.6.4. Nearest Neighbor Algorithms  1.6.4.1. Brute Force  Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for samples in dimensions, this approach scales as . Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples grows, the brute-force approach quickly becomes infeasible. In the classes within sklearn.neighbors , brute-force neighbors searches are specified using the keyword , and are computed using the routines available in sklearn.metrics.pairwise . 1.6.4.2. K-D Tree  To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point is very distant from point , and point is very close to point , then we know that points and are very distant, without having to explicitly calculate their distance . In this way, the computational cost of a nearest neighbors search can be reduced to or better. This is a significant improvement over brute-force for large . An early approach to taking advantage of this aggregate information was the KD tree data structure (short for K-dimensional tree ), which generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no -dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only distance computations. Though the KD tree approach is very fast for low-dimensional ( ) neighbors searches, it becomes inefficient as grows very large: this is one manifestation of the so-called “curse of dimensionality”. In scikit-learn, KD tree neighbors searches are specified using the keyword , and are computed using the class KDTree . References: “Multidimensional binary search trees used for associative searching” , Bentley, J.L., Communications of the ACM (1975) 1.6.4.3. Ball Tree  To address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions. A ball tree recursively divides the data into nodes defined by a centroid and radius , such that each point in the node lies within the hyper-sphere defined by and . The number of candidate points for a neighbor search is reduced through use of the triangle inequality : \[|x+y| \leq |x| + |y|\] With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword , and are computed using the class sklearn.neighbors.BallTree . Alternatively, the user can work with the BallTree class directly. References: “Five balltree construction algorithms” , Omohundro, S.M., International Computer Science Institute Technical Report (1989) 1.6.4.4. Choice of Nearest Neighbors Algorithm  The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors: number of samples (i.e. ) and dimensionality (i.e. ). Brute force query time grows as Ball tree query time grows as approximately KD tree query time changes with in a way that is difficult to precisely characterise. For small (less than 20 or so) the cost is approximately , and the KD tree query can be very efficient. For larger , the cost increases to nearly , and the overhead due to the tree structure can lead to queries which are slower than brute force. For small data sets ( less than 30 or so), is comparable to , and brute force algorithms can be more efficient than a tree-based approach. Both KDTree and BallTree address this through providing a leaf size parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small . data structure: intrinsic dimensionality of the data and/or sparsity of the data. Intrinsic dimensionality refers to the dimension of a manifold on which the data lies, which can be linearly or non-linearly embedded in the parameter space. Sparsity refers to the degree to which the data fills the parameter space (this is to be distinguished from the concept as used in “sparse” matrices. The data matrix may have no zero entries, but the structure can still be “sparse” in this sense). Brute force query time is unchanged by data structure. Ball tree and KD tree query times can be greatly influenced by data structure. In general, sparser data with a smaller intrinsic dimensionality leads to faster query times. Because the KD tree internal representation is aligned with the parameter axes, it will not generally show as much improvement as ball tree for arbitrarily structured data. Datasets used in machine learning tend to be very structured, and are very well-suited for tree-based queries. number of neighbors requested for a query point. Brute force query time is largely unaffected by the value of Ball tree and KD tree query time will become slower as increases. This is due to two effects: first, a larger leads to the necessity to search a larger portion of the parameter space. Second, using requires internal queueing of results as the tree is traversed. As becomes large compared to , the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient. number of query points. Both the ball tree and the KD Tree require a construction phase. The cost of this construction becomes negligible when amortized over many queries. If only a small number of queries will be performed, however, the construction can make up a significant fraction of the total cost. If very few query points will be required, brute force is better than a tree-based method. Currently, selects if , the input data is sparse, or isn’t in the list for either or . Otherwise, it selects the first out of and that has in its list. This choice is based on the assumption that the number of query points is at least the same order as the number of training points, and that is close to its default value of . 1.6.4.5. Effect of  As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter . This parameter choice has many effects: construction time A larger leads to a faster tree construction time, because fewer nodes need to be created query time Both a large or small can lead to suboptimal query cost. For approaching 1, the overhead involved in traversing nodes can significantly slow query times. For approaching the size of the training set, queries become essentially brute force. A good compromise between these is , the default value of the parameter. memory As increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a -dimensional centroid for each node. The required storage space for BallTree is approximately times the size of the training set. is not referenced for brute force queries. 