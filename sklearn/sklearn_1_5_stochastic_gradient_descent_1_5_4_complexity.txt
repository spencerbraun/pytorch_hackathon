sklearn_1_5_stochastic_gradient_descent
1.5. Stochastic Gradient Descent
modules/sgd.html
 1.5.4. Complexity  The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of , where k is the number of iterations (epochs) and is the average number of non-zero attributes per sample. Recent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase as the training set size increases. 