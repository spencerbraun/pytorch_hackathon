sklearn_1_13_feature_selection
1.13. Feature selection
modules/feature_selection.html
 1.13.4. Feature selection using SelectFromModel  SelectFromModel is a meta-transformer that can be used along with any estimator that has a or attribute after fitting. The features are considered unimportant and removed, if the corresponding or values are below the provided parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the criteria, one can use the parameter to set a limit on the number of features to select. For examples on how it is to be used refer to the sections below. Examples Feature selection using SelectFromModel and LassoCV : Selecting the two most important features from the diabetes dataset without knowing the threshold beforehand. 1.13.4.1. L1-based feature selection  Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with feature_selection.SelectFromModel to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the linear_model.Lasso for regression, and of linear_model.LogisticRegression and svm.LinearSVC for classification: With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected. Examples: Classification of text documents using sparse features : Comparison of different algorithms for document classification including L1-based feature selection. L1-recovery and compressive sensing For a good choice of alpha, the Lasso can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be “sufficiently large”, or L1 models will perform at random, where “sufficiently large” depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated. There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation ( LassoCV or LassoLarsCV ), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC ( LassoLarsIC ) tends, on the opposite, to set high values of alpha. Reference Richard G. Baraniuk “Compressive Sensing”, IEEE Signal Processing Magazine [120] July 2007 http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf 1.13.4.2. Tree-based feature selection  Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute impurity-based feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer): Examples: Feature importances with forests of trees : example on synthetic data showing the recovery of the actually meaningful features. Pixel importances with a parallel forest of trees : example on face recognition data. 