sklearn_1_1_linear_models
1.1. Linear Models
modules/linear_model.html
 1.1.16. Robustness regression: outliers and modeling errors  Robust regression aims to fit a regression model in the presence of corrupt data: either outliers, or error in the model. 1.1.16.1. Different scenario and useful concepts  There are different things to keep in mind when dealing with data corrupted by outliers: Outliers in X or in y ? Outliers in the y direction Outliers in the X direction Fraction of outliers versus amplitude of error The number of outlying points matters, but also how much they are outliers. Small outliers Large outliers An important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data. Note that in general, robust fitting in high-dimensional setting (large ) is very hard. The robust models here will probably not work in these settings. Trade-offs: which estimator? Scikit-learn provides 3 robust regression estimators: RANSAC , Theil Sen and HuberRegressor . HuberRegressor should be faster than RANSAC and Theil Sen unless the number of samples are very large, i.e >> . This is because RANSAC and Theil Sen fit on smaller subsets of the data. However, both Theil Sen and RANSAC are unlikely to be as robust as HuberRegressor for the default parameters. RANSAC is faster than Theil Sen and scales much better with the number of samples. RANSAC will deal better with large outliers in the y direction (most common situation). Theil Sen will cope better with medium-size outliers in the X direction, but this property will disappear in high-dimensional settings. When in doubt, use RANSAC . 1.1.16.2. RANSAC: RANdom SAmple Consensus  RANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers from the complete data set. RANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is dependent on the number of iterations (see parameter). It is typically used for linear and non-linear regression problems and is especially popular in the field of photogrammetric computer vision. The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers. 1.1.16.2.1. Details of the algorithm  Each iteration performs the following steps: Select random samples from the original data and check whether the set of data is valid (see ). Fit a model to the random subset ( ) and check whether the estimated model is valid (see ). Classify all data as inliers or outliers by calculating the residuals to the estimated model ( ) - all data samples with absolute residuals smaller than the are considered as inliers. Save fitted model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has better score. These steps are performed either a maximum number of times ( ) or until one of the special stop criteria are met (see and ). The final model is estimated using all inlier samples (consensus set) of the previously determined best model. The and functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, should be used as it is called prior to fitting the model and thus leading to better computational performance. Examples: Robust linear model estimation using RANSAC Robust linear estimator fitting References: https://en.wikipedia.org/wiki/RANSAC “Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography” Martin A. Fischler and Robert C. Bolles - SRI International (1981) “Performance Evaluation of RANSAC Family” Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009) 1.1.16.3. Theil-Sen estimator: generalized-median-based estimator  The TheilSenRegressor estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It loses its robustness properties and becomes no better than an ordinary least squares in high dimension. Examples: Theil-Sen Regression Robust linear estimator fitting References: https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator 1.1.16.3.1. Theoretical considerations  TheilSenRegressor is comparable to the Ordinary Least Squares (OLS) in terms of asymptotic efficiency and as an unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric method which means it makes no assumption about the underlying distribution of the data. Since Theil-Sen is a median-based estimator, it is more robust against corrupted data aka outliers. In univariate setting, Theil-Sen has a breakdown point of about 29.3% in case of a simple linear regression which means that it can tolerate arbitrary corrupted data of up to 29.3%. The implementation of TheilSenRegressor in scikit-learn follows a generalization to a multivariate linear regression model 12 using the spatial median which is a generalization of the median to multiple dimensions 13 . In terms of time and space complexity, Theil-Sen scales according to \[\binom{n_{\text{samples}}}{n_{\text{subsamples}}}\] which makes it infeasible to be applied exhaustively to problems with a large number of samples and features. Therefore, the magnitude of a subpopulation can be chosen to limit the time and space complexity by considering only a random subset of all possible combinations. Examples: Theil-Sen Regression References: 12 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: Theil-Sen Estimators in a Multiple Linear Regression Model. 13 Kärkkäinen and S. Äyrämö: On Computation of Spatial Median for Robust Data Mining. 1.1.16.4. Huber Regression  The HuberRegressor is different to Ridge because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from TheilSenRegressor and RANSACRegressor because it does not ignore the effect of the outliers but gives a lesser weight to them. The loss function that HuberRegressor minimizes is given by \[\min_{w, \sigma} {\sum_{i1}^n\left(\sigma + H_{\epsilon}\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}^2}\] where \[\begin{split}H_{\epsilon}(z)  \begin{cases} z^2, & \text {if } |z| < \epsilon, \\ 2\epsilon|z| - \epsilon^2, & \text{otherwise} \end{cases}\end{split}\] It is advised to set the parameter to 1.35 to achieve 95% statistical efficiency. 1.1.16.5. Notes  The HuberRegressor differs from using SGDRegressor with loss set to in the following ways. HuberRegressor is scaling invariant. Once is set, scaling and down or up by different values would produce the same robustness to outliers as before. as compared to SGDRegressor where has to be set again when and are scaled. HuberRegressor should be more efficient to use on data with small number of samples while SGDRegressor needs a number of passes on the training data to produce the same robustness. Examples: HuberRegressor vs Ridge on dataset with strong outliers References: Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172 Note that this estimator is different from the R implementation of Robust Regression ( http://www.ats.ucla.edu/stat/r/dae/rreg.htm ) because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold. 