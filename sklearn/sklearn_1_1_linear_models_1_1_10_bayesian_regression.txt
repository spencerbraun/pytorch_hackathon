sklearn_1_1_linear_models
1.1. Linear Models
modules/linear_model.html
 1.1.10. Bayesian Regression  Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. This can be done by introducing uninformative priors over the hyper parameters of the model. The regularization used in Ridge regression and classification is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients with precision . Instead of setting manually, it is possible to treat it as a random variable to be estimated from the data. To obtain a fully probabilistic model, the output is assumed to be Gaussian distributed around : \[p(y|X,w,\alpha)  \mathcal{N}(y|X w,\alpha)\] where is again treated as a random variable that is to be estimated from the data. The advantages of Bayesian Regression are: It adapts to the data at hand. It can be used to include regularization parameters in the estimation procedure. The disadvantages of Bayesian regression include: Inference of the model can be time consuming. References A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning Original Algorithm is detailed in the book by Radford M. Neal 1.1.10.1. Bayesian Ridge Regression  BayesianRidge estimates a probabilistic model of the regression problem as described above. The prior for the coefficient is given by a spherical Gaussian: \[p(w|\lambda)  \mathcal{N}(w|0,\lambda^{-1}\mathbf{I}_{p})\] The priors over and are chosen to be gamma distributions , the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression , and is similar to the classical Ridge . The parameters , and are estimated jointly during the fit of the model, the regularization parameters and being estimated by maximizing the log marginal likelihood . The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters and is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters and . There are four more hyperparameters, , , and of the gamma prior distributions over and . These are usually chosen to be non-informative . By default . Bayesian Ridge Regression is used for regression: After being fitted, the model can then be used to predict new values: The coefficients of the model can be accessed: Due to the Bayesian framework, the weights found are slightly different to the ones found by Ordinary Least Squares . However, Bayesian Ridge Regression is more robust to ill-posed problems. Examples: Bayesian Ridge Regression Curve Fitting with Bayesian Ridge Regression References: Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006 David J. C. MacKay, Bayesian Interpolation , 1992. Michael E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine , 2001. 1.1.10.2. Automatic Relevance Determination - ARD  ARDRegression is very similar to Bayesian Ridge Regression , but can lead to sparser coefficients 1 2 . ARDRegression poses a different prior over , by dropping the assumption of the Gaussian being spherical. Instead, the distribution over is assumed to be an axis-parallel, elliptical Gaussian distribution. This means each coefficient is drawn from a Gaussian distribution, centered on zero and with a precision : \[p(w|\lambda)  \mathcal{N}(w|0,A^{-1})\] with . In contrast to Bayesian Ridge Regression , each coordinate of has its own standard deviation . The prior over all is chosen to be the same gamma distribution given by hyperparameters and . ARD is also known in the literature as Sparse Bayesian Learning and Relevance Vector Machine 3 4 . Examples: Automatic Relevance Determination Regression (ARD) References: 1 Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1 2 David Wipf and Srikantan Nagarajan: A new view of automatic relevance determination 3 Michael E. Tipping: Sparse Bayesian Learning and the Relevance Vector Machine 4 Tristan Fletcher: Relevance Vector Machines explained 