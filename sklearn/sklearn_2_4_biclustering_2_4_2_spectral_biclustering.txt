sklearn_2_4_biclustering
2.4. Biclustering
modules/biclustering.html
 2.4.2. Spectral Biclustering  The SpectralBiclustering algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters. The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix. 2.4.2.1. Mathematical formulation  The input matrix is first normalized to make the checkerboard pattern more obvious. There are three possible methods: Independent row and column normalization , as in Spectral Co-Clustering. This method makes the rows sum to a constant and the columns sum to a different constant. Bistochastization : repeated row and column normalization until convergence. This method makes both rows and columns sum to the same constant. Log normalization : the log of the data matrix is computed: . Then the column mean , row mean , and overall mean of are computed. The final matrix is computed according to the formula \[K_{ij}  L_{ij} - \overline{L_{i \cdot}} - \overline{L_{\cdot j}} + \overline{L_{\cdot \cdot}}\] After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm. If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, and . are discarded. From now on, the “first” singular vectors refers to and except in the case of log normalization. Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered. For instance, if singular vectors were calculated, the best are found as described, where . Let be the matrix with columns the best left singular vectors, and similarly for the right. To partition the rows, the rows of are projected to a dimensional space: . Treating the rows of this matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to and clustering this matrix yields the column labels. Examples: A demo of the Spectral Biclustering algorithm : a simple example showing how to generate a checkerboard matrix and bicluster it. References: Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray data: coclustering genes and conditions . 