sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions
3.3. Metrics and scoring: quantifying the quality of predictions
modules/model_evaluation.html
 3.3.3. Multilabel ranking metrics  In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels. 3.3.3.1. Coverage error  The coverage_error function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels. Note Our implementation’s score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels. Formally, given a binary indicator matrix of the ground truth labels and the score associated with each label , the coverage is defined as \[coverage(y, \hat{f})  \frac{1}{n_{\text{samples}}} \sum_{i0}^{n_{\text{samples}} - 1} \max_{j:y_{ij}  1} \text{rank}_{ij}\] with . Given the rank definition, ties in are broken by giving the maximal rank that would have been assigned to all tied values. Here is a small example of usage of this function: 3.3.3.2. Label ranking average precision  The label_ranking_average_precision_score function implements label ranking average precision (LRAP). This metric is linked to the average_precision_score function, but is based on the notion of label ranking instead of precision and recall. Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank . Formally, given a binary indicator matrix of the ground truth labels and the score associated with each label , the average precision is defined as \[LRAP(y, \hat{f})  \frac{1}{n_{\text{samples}}} \sum_{i0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0} \sum_{j:y_{ij}  1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}\] where , , computes the cardinality of the set (i.e., the number of elements in the set), and is the “norm” (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: 3.3.3.3. Ranking loss  The label_ranking_loss function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero. Formally, given a binary indicator matrix of the ground truth labels and the score associated with each label , the ranking loss is defined as \[ranking\_loss(y, \hat{f})  \frac{1}{n_{\text{samples}}} \sum_{i0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0(n_\text{labels} - ||y_i||_0)} \left|\left\{(k, l): \hat{f}_{ik} \leq \hat{f}_{il}, y_{ik}  1, y_{il}  0 \right\}\right|\] where computes the cardinality of the set (i.e., the number of elements in the set) and is the “norm” (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: References: Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US. 3.3.3.4. Normalized Discounted Cumulative Gain  Discounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain (NDCG) are ranking metrics; they compare a predicted order to ground-truth scores, such as the relevance of answers to a query. From the Wikipedia page for Discounted Cumulative Gain: “Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks” DCG orders the true targets (e.g. relevance of query answers) in the predicted order, then multiplies them by a logarithmic decay and sums the result. The sum can be truncated after the first results, in which case we call it DCG@K. NDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so that it is always between 0 and 1. Usually, NDCG is preferred to DCG. Compared with the ranking loss, NDCG can take into account relevance scores, rather than a ground-truth ranking. So if the ground-truth consists only of an ordering, the ranking loss should be preferred; if the ground-truth consists of actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very relevant), NDCG can be used. For one sample, given the vector of continuous ground-truth values for each target , where is the number of outputs, and the prediction , which induces the ranking function , the DCG score is \[\sum_{r1}^{\min(K, M)}\frac{y_{f(r)}}{\log(1 + r)}\] and the NDCG score is the DCG score divided by the DCG score obtained for . References: Wikipedia entry for Discounted Cumulative Gain Jarvelin, K., & Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4), 422-446. Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May). A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013) McSherry, F., & Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg. 