sklearn_1_10_decision_trees
1.10. Decision Trees
modules/tree.html
 1.10.5. Tips on practical use  Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit. Consider performing dimensionality reduction ( PCA , ICA , or Feature selection ) beforehand to give your tree a better chance of finding features that are discriminative. Understanding the decision tree structure will help in gaining more insights about how the decision tree makes predictions, which is important for understanding the important features in the data. Visualise your tree as you are training by using the function. Use as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth. Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use to control the size of the tree to prevent overfitting. Use or to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While can create arbitrarily small leaves, guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, is often the best choice. Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights ( ) for each class to the same value. Also note that weight-based pre-pruning criteria, such as , will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like . If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as , which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights. All decision trees use arrays internally. If training data is not in this format, a copy of the dataset will be made. If the input matrix X is very sparse, it is recommended to convert to sparse before calling fit and sparse before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples. 