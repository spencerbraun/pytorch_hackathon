sklearn_1_1_linear_models
1.1. Linear Models
modules/linear_model.html
 1.1.2. Ridge regression and classification  1.1.2.1. Regression  Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares: \[\min_{w} || X w - y||_2^2 + \alpha ||w||_2^2\] The complexity parameter controls the amount of shrinkage: the larger the value of , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. As with other linear models, Ridge will take in its method arrays X, y and will store the coefficients of the linear model in its member: 1.1.2.2. Classification  The Ridge regressor has a classifier variant: RidgeClassifier . This classifier first converts binary targets to and then treats the problem as a regression task, optimizing the same objective as above. The predicted class corresponds to the sign of the regressor’s prediction. For multiclass classification, the problem is treated as multi-output regression, and the predicted class corresponds to the output with the highest value. It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However in practice all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the RidgeClassifier allows for a very different choice of the numerical solvers with distinct computational performance profiles. The RidgeClassifier can be significantly faster than e.g. LogisticRegression with a high number of classes, because it is able to compute the projection matrix only once. This classifier is sometimes referred to as a Least Squares Support Vector Machines with a linear kernel. Examples: Plot Ridge coefficients as a function of the regularization Classification of text documents using sparse features Common pitfalls in interpretation of coefficients of linear models 1.1.2.3. Ridge Complexity  This method has the same order of complexity as Ordinary Least Squares . 1.1.2.4. Setting the regularization parameter: generalized Cross-Validation  RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efficient form of leave-one-out cross-validation: Specifying the value of the cv attribute will trigger the use of cross-validation with GridSearchCV , for example for 10-fold cross-validation, rather than Generalized Cross-Validation. References “Notes on Regularized Least Squares”, Rifkin & Lippert ( technical report , course slides ). 