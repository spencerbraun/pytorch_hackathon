sklearn_6_2_feature_extraction
6.2. Feature extraction
modules/feature_extraction.html
 6.2.1. Loading features from dicts  The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python objects to the NumPy/SciPy representation used by scikit-learn estimators. While not particularly fast to process, Python’s has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values. DictVectorizer implements what is called one-of-K or “one-hot” coding for categorical (aka nominal, discrete) features. Categorical features are “attribute-value” pairs where the value is restricted to a list of discrete of possibilities without ordering (e.g. topic identifiers, types of objects, tags, names…). In the following, “city” is a categorical attribute while “temperature” is a traditional numerical feature: DictVectorizer is also a useful representation transformation for training sequence classifiers in Natural Language Processing models that typically work by extracting feature windows around a particular word of interest. For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word ‘sat’ in the sentence ‘The cat sat on the mat.’: This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a text.TfidfTransformer for normalization): As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the class uses a matrix by default instead of a . 