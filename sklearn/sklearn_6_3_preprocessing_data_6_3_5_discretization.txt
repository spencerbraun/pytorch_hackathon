sklearn_6_3_preprocessing_data
6.3. Preprocessing data
modules/preprocessing.html
 6.3.5. Discretization  Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models. 6.3.5.1. K-bins discretization  KBinsDiscretizer discretizes features into bins: By default the output is one-hot encoded into a sparse matrix (See Encoding categorical features ) and this can be configured with the parameter. For each feature, the bin edges are computed during and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as: feature 1: feature 2: feature 3: Based on these bin intervals, is transformed as follows: The resulting dataset contains ordinal attributes which can be further used in a sklearn.pipeline.Pipeline . Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. KBinsDiscretizer implements different binning strategies, which can be selected with the parameter. The ‘uniform’ strategy uses constant-width bins. The ‘quantile’ strategy uses the quantiles values to have equally populated bins in each feature. The ‘kmeans’ strategy defines bins based on a k-means clustering procedure performed on each feature independently. Examples: Using KBinsDiscretizer to discretize continuous features Feature discretization Demonstrating the different strategies of KBinsDiscretizer 6.3.5.2. Feature binarization  Feature binarization is the process of thresholding numerical features to get boolean values . This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution . For instance, this is the case for the sklearn.neural_network.BernoulliRBM . It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice. As for the Normalizer , the utility class Binarizer is meant to be used in the early stages of sklearn.pipeline.Pipeline . The method does nothing as each sample is treated independently of others: It is possible to adjust the threshold of the binarizer: As for the StandardScaler and Normalizer classes, the preprocessing module provides a companion function binarize to be used when the transformer API is not necessary. Note that the Binarizer is similar to the KBinsDiscretizer when , and when the bin edge is at the value . Sparse input binarize and Binarizer accept both dense array-like and sparse matrices from scipy.sparse as input . For sparse input the data is converted to the Compressed Sparse Rows representation (see ). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream. 