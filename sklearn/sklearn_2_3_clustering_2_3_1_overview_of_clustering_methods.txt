sklearn_2_3_clustering
2.3. Clustering
modules/clustering.html
 2.3.1. Overview of clustering methods  A comparison of the clustering algorithms in scikit-learn  Method name Parameters Scalability Usecase Geometry (metric used) K-Means number of clusters Very large , medium with MiniBatch code General-purpose, even cluster size, flat geometry, not too many clusters Distances between points Affinity propagation damping, sample preference Not scalable with n_samples Many clusters, uneven cluster size, non-flat geometry Graph distance (e.g. nearest-neighbor graph) Mean-shift bandwidth Not scalable with Many clusters, uneven cluster size, non-flat geometry Distances between points Spectral clustering number of clusters Medium , small Few clusters, even cluster size, non-flat geometry Graph distance (e.g. nearest-neighbor graph) Ward hierarchical clustering number of clusters or distance threshold Large and Many clusters, possibly connectivity constraints Distances between points Agglomerative clustering number of clusters or distance threshold, linkage type, distance Large and Many clusters, possibly connectivity constraints, non Euclidean distances Any pairwise distance DBSCAN neighborhood size Very large , medium Non-flat geometry, uneven cluster sizes Distances between nearest points OPTICS minimum cluster membership Very large , large Non-flat geometry, uneven cluster sizes, variable cluster density Distances between points Gaussian mixtures many Not scalable Flat geometry, good for density estimation Mahalanobis distances to centers Birch branching factor, threshold, optional global clusterer. Large and Large dataset, outlier removal, data reduction. Euclidean distance between points Non-flat geometry clustering is useful when the clusters have a specific shape, i.e. a non-flat manifold, and the standard euclidean distance is not the right metric. This case arises in the two top rows of the figure above. Gaussian mixture models, useful for clustering, are described in another chapter of the documentation dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component. 