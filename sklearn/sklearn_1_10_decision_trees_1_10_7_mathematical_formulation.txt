sklearn_1_10_decision_trees
1.10. Decision Trees
modules/tree.html
 1.10.7. Mathematical formulation  Given training vectors , i1,…, l and a label vector , a decision tree recursively partitions the space such that the samples with the same labels are grouped together. Let the data at node be represented by . For each candidate split consisting of a feature and threshold , partition the data into and subsets \[ \begin{align}\begin{aligned}Q_{left}(\theta)  {(x, y) | x_j < t_m}\\Q_{right}(\theta)  Q \setminus Q_{left}(\theta)\end{aligned}\end{align} \] The impurity at is computed using an impurity function , the choice of which depends on the task being solved (classification or regression) \[G(Q, \theta)  \frac{n_{left}}{N_m} H(Q_{left}(\theta)) + \frac{n_{right}}{N_m} H(Q_{right}(\theta))\] Select the parameters that minimises the impurity \[\theta^*  \operatorname{argmin}_\theta G(Q, \theta)\] Recurse for subsets and until the maximum allowable depth is reached, or . 1.10.7.1. Classification criteria  If a target is a classification outcome taking on values 0,1,…,K-1, for node , representing a region with observations, let \[p_{mk}  1/ N_m \sum_{x_i \in R_m} I(y_i  k)\] be the proportion of class k observations in node Common measures of impurity are Gini \[H(X_m)  \sum_k p_{mk} (1 - p_{mk})\] Entropy \[H(X_m)  - \sum_k p_{mk} \log(p_{mk})\] and Misclassification \[H(X_m)  1 - \max(p_{mk})\] where is the training data in node 1.10.7.2. Regression criteria  If the target is a continuous value, then for node , representing a region with observations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes. Mean Squared Error: \[ \begin{align}\begin{aligned}\bar{y}_m  \frac{1}{N_m} \sum_{i \in N_m} y_i\\H(X_m)  \frac{1}{N_m} \sum_{i \in N_m} (y_i - \bar{y}_m)^2\end{aligned}\end{align} \] Mean Absolute Error: \[ \begin{align}\begin{aligned}median(y)_m  \underset{i \in N_m}{\mathrm{median}}(y_i)\\H(X_m)  \frac{1}{N_m} \sum_{i \in N_m} |y_i - median(y)_m|\end{aligned}\end{align} \] where is the training data in node 