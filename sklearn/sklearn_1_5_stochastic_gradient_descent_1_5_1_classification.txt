sklearn_1_5_stochastic_gradient_descent
1.5. Stochastic Gradient Descent
modules/sgd.html
 1.5.1. Classification  The class SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a SGDClassifier trained with the hinge loss, equivalent to a linear SVM. As other classifiers, SGD has to be fitted with two arrays: an array of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values (class labels) for the training samples: After being fitted, the model can then be used to predict new values: SGD fits a linear model to the training data. The attribute holds the model parameters: The attribute holds the intercept (aka offset or bias): Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter . The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by SGDClassifier.decision_function : The concrete loss function can be set via the parameter. SGDClassifier supports the following loss functions: : (soft-margin) linear Support Vector Machine, : smoothed hinge loss, : logistic regression, and all regression losses below. In this case the target is encoded as -1 or 1, and the problem is treated as a regression problem. The predicted class then correspond to the sign of the predicted target. Please refer to the mathematical section below for formulas. The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models (i.e. with more zero coefficents), even when L2 penalty is used. Using or enables the method, which gives a vector of probability estimates per sample : The concrete penalty can be set via the parameter. SGD supports the following penalties: : L2 norm penalty on . : L1 norm penalty on . : Convex combination of L2 and L1; . The default setting is . The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net 11 solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter controls the convex combination of L1 and L2 penalty. SGDClassifier supports multi-class classification by combining multiple binary classifiers in a “one versus all” (OVA) scheme. For each of the classes, a binary classifier is learned that discriminates between that and all other classes. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. The Figure below illustrates the OVA approach on the iris dataset. The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers. In the case of multi-class classification is a two-dimensional array of shape (n_classes, n_features) and is a one-dimensional array of shape (n_classes,). The i-th row of holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute ). Note that, in principle, since they allow to create a probability model, and are more suitable for one-vs-all classification. SGDClassifier supports both weighted classes and weighted instances via the fit parameters and . See the examples below and the docstring of SGDClassifier.fit for further information. SGDClassifier supports averaged SGD (ASGD) 10 . Averaging can be enabled by setting . ASGD performs the same updates as the regular SGD (see Mathematical formulation ), but instead of using the last value of the coefficients as the attribute (i.e. the values of the last update), is set instead to the average value of the coefficients across all updates. The same is done for the attribute. When using ASGD the learning rate can be larger and even constant, leading on some datasets to a speed up in training time. For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in LogisticRegression . Examples: SGD: Maximum margin separating hyperplane , Plot multi-class SGD on the iris dataset SGD: Weighted samples Comparing various online solvers SVM: Separating hyperplane for unbalanced classes (See the Note in the example) 