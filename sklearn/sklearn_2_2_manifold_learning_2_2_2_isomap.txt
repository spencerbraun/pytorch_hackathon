sklearn_2_2_manifold_learning
2.2. Manifold learning
modules/manifold.html
 2.2.2. Isomap  One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be performed with the object Isomap . 2.2.2.1. Complexity  The Isomap algorithm comprises three stages: Nearest neighbor search. Isomap uses sklearn.neighbors.BallTree for efficient neighbor search. The cost is approximately , for nearest neighbors of points in dimensions. Shortest-path graph search. The most efficient known algorithms for this are Dijkstra’s Algorithm , which is approximately , or the Floyd-Warshall algorithm , which is . The algorithm can be selected by the user with the keyword of . If unspecified, the code attempts to choose the best algorithm for the input data. Partial eigenvalue decomposition. The embedding is encoded in the eigenvectors corresponding to the largest eigenvalues of the isomap kernel. For a dense solver, the cost is approximately . This cost can often be improved using the solver. The eigensolver can be specified by the user with the keyword of . If unspecified, the code attempts to choose the best algorithm for the input data. The overall complexity of Isomap is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: “A global geometric framework for nonlinear dimensionality reduction” Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. Science 290 (5500) 