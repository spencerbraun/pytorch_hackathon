sklearn_1_17_neural_network_models_supervised
1.17. Neural network models (supervised)
modules/neural_networks_supervised.html
 1.17.5. Algorithms  MLP trains using Stochastic Gradient Descent , Adam , or L-BFGS . Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e. \[w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w} + \frac{\partial Loss}{\partial w})\] where is the learning rate which controls the step-size in the parameter space search. is the loss function used for the network. More details can be found in the documentation of SGD Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments. With SGD or Adam, training supports online and mini-batch learning. L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of L-BFGS . If the selected solver is ‘L-BFGS’, training does not support online nor mini-batch learning. 