sklearn_1_5_stochastic_gradient_descent
1.5. Stochastic Gradient Descent
modules/sgd.html
 1.5.6. Tips on Practical Use  Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be easily done using StandardScaler : If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed. Finding a reasonable regularization term is best done using automatic hyper-parameter search, e.g. GridSearchCV or RandomizedSearchCV , usually in the range . Empirically, we found that SGD converges after observing approximately 10^6 training samples. Thus, a reasonable first guess for the number of iterations is , where is the size of the training set. If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant such that the average L2 norm of the training data equals one. We found that Averaged SGD works best with a larger number of features and a higher eta0 References: “Efficient BackProp” Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998. 