sklearn_1_11_ensemble_methods
1.11. Ensemble methods
modules/ensemble.html
 1.11.5. Histogram-Based Gradient Boosting  Scikit-learn 0.21 introduced two new experimental implementations of gradient boosting trees, namely HistGradientBoostingClassifier and HistGradientBoostingRegressor , inspired by LightGBM (See [LightGBM] ). These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples. They also have built-in support for missing values, which avoids the need for an imputer. These fast estimators first bin the input samples into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. The API of these estimators is slightly different, and some of the features from GradientBoostingClassifier and GradientBoostingRegressor are not yet supported, for instance some loss functions. These estimators are still experimental : their predictions and their API might change without any deprecation cycle. To use them, you need to explicitly import : Examples: Partial Dependence Plots 1.11.5.1. Usage  Most of the parameters are unchanged from GradientBoostingClassifier and GradientBoostingRegressor . One exception is the parameter that replaces , and controls the number of iterations of the boosting process: Available losses for regression are ‘least_squares’, ‘least_absolute_deviation’, which is less sensitive to outliers, and ‘poisson’, which is well suited to model counts and frequencies. For classification, ‘binary_crossentropy’ is used for binary classification and ‘categorical_crossentropy’ is used for multiclass classification. By default the loss is ‘auto’ and will select the appropriate loss depending on y passed to fit . The size of the trees can be controlled through the , , and parameters. The number of bins used to bin the data is controlled with the parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible, which is the default. The parameter is a regularizer on the loss function and corresponds to in equation (2) of [XGBoost] . Note that early-stopping is enabled by default if the number of samples is larger than 10,000 . The early-stopping behaviour is controlled via the , , , , and parameters. It is possible to early-stop using an arbitrary scorer , or just the training or validation loss. Note that for technical reasons, using a scorer is significantly slower than using the loss. By default, early-stopping is performed if there are at least 10,000 samples in the training set, using the validation loss. 1.11.5.2. Missing values support  HistGradientBoostingClassifier and HistGradientBoostingRegressor have built-in support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently: When the missingness pattern is predictive, the splits can be done on whether the feature value is missing or not: If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples. 1.11.5.3. Sample weight support  HistGradientBoostingClassifier and HistGradientBoostingRegressor sample support weights during fit . The following toy example demonstrates how the model ignores the samples with zero sample weights: As you can see, the is comfortably classified as since the first two samples are ignored due to their sample weights. Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights. Note that the binning stage (specifically the quantiles computation) does not take the weights into account. 1.11.5.4. Monotonic Constraints  Depending on the problem at hand, you may have prior knowledge indicating that a given feature should in general have a positive (or negative) effect on the target value. For example, all else being equal, a higher credit score should increase the probability of getting approved for a loan. Monotonic constraints allow you to incorporate such prior knowledge into the model. A positive monotonic constraint is a constraint of the form: , where is the predictor with two features. Similarly, a negative monotonic constraint is of the form: . Note that monotonic constraints only constraint the output “all else being equal”. Indeed, the following relation is not enforced by a positive constraint: . You can specify a monotonic constraint on each feature using the parameter. For each feature, a value of 0 indicates no constraint, while -1 and 1 indicate a negative and positive constraint, respectively: In a binary classification context, imposing a monotonic constraint means that the feature is supposed to have a positive / negative effect on the probability to belong to the positive class. Monotonic constraints are not supported for multiclass context. Examples: Monotonic Constraints 1.11.5.5. Low-level parallelism  HistGradientBoostingClassifier and HistGradientBoostingRegressor have implementations that use OpenMP for parallelization through Cython. For more details on how to control the number of threads, please refer to our Parallelism notes. The following parts are parallelized: mapping samples from real values to integer-valued bins (finding the bin thresholds is however sequential) building histograms is parallelized over features finding the best split point at a node is parallelized over features during fit, mapping samples into the left and right children is parallelized over samples gradient and hessians computations are parallelized over samples predicting is parallelized over samples 1.11.5.6. Why it’s faster  The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs GradientBoostingClassifier and GradientBoostingRegressor ) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of where is the number of samples at the node. HistGradientBoostingClassifier and HistGradientBoostingRegressor , in contrast, do not require sorting the feature values and instead use a data-structure called a histogram, where the samples are implicitly ordered. Building a histogram has a complexity, so the node splitting procedure has a complexity, much smaller than the previous one. In addition, instead of considering split points, we here consider only split points, which is much smaller. In order to build histograms, the input data needs to be binned into integer-valued bins. This binning procedure does require sorting the feature values, but it only happens once at the very beginning of the boosting process (not at each node, like in GradientBoostingClassifier and GradientBoostingRegressor ). Finally, many parts of the implementation of HistGradientBoostingClassifier and HistGradientBoostingRegressor are parallelized. References F1999 Friedmann, Jerome H., 2007, “Stochastic Gradient Boosting” R2007 G. Ridgeway, “Generalized Boosted Models: A guide to the gbm package”, 2007 XGBoost Tianqi Chen, Carlos Guestrin, “XGBoost: A Scalable Tree Boosting System” LightGBM ( 1 , 2 ) Ke et. al. “LightGBM: A Highly Efficient Gradient BoostingDecision Tree” 