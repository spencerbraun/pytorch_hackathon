sklearn_2_6_covariance_estimation
2.6. Covariance estimation
modules/covariance.html
 2.6.3. Sparse inverse covariance  The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as covariance selection . In the small-samples situation, in which is on the order of or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure. The GraphicalLasso estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its parameter, the more sparse the precision matrix. The corresponding GraphicalLassoCV object uses cross-validation to automatically set the parameter. A comparison of maximum likelihood, shrinkage and sparse estimates of the covariance and precision matrix in the very small samples settings.  Note Structure recovery Recovering a graphical structure from correlations in the data is a challenging thing. If you are interested in such recovery keep in mind that: Recovery is easier from a correlation matrix than a covariance matrix: standardize your observations before running GraphicalLasso If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections. If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it. Even if you are in favorable recovery conditions, the alpha parameter chosen by cross-validation (e.g. using the GraphicalLassoCV object) will lead to selecting too many edges. However, the relevant edges will have heavier weights than the irrelevant ones. The mathematical formulation is the following: \[\hat{K}  \mathrm{argmin}_K \big( \mathrm{tr} S K - \mathrm{log} \mathrm{det} K + \alpha \|K\|_1 \big)\] Where is the precision matrix to be estimated, and is the sample covariance matrix. is the sum of the absolute values of off-diagonal coefficients of . The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R package. Examples: Sparse inverse covariance estimation : example on synthetic data showing some recovery of a structure, and comparing to other covariance estimators. Visualizing the stock market structure : example on real stock market data, finding which symbols are most linked. References: Friedman et al, “Sparse inverse covariance estimation with the graphical lasso” , Biostatistics 9, pp 432, 2008 