sklearn_1_4_support_vector_machines
1.4. Support Vector Machines
modules/svm.html
 1.4.5. Tips on Practical Use  Avoiding data copy : For SVC , SVR , NuSVC and NuSVR , if the data passed to certain methods is not C-ordered contiguous and double precision, it will be copied before calling the underlying C implementation. You can check whether a given numpy array is C-contiguous by inspecting its attribute. For LinearSVC (and LogisticRegression ) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use the SGDClassifier class instead. The objective function can be configured to be almost the same as the LinearSVC model. Kernel cache size : For SVC , SVR , NuSVC and NuSVR , the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to set to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB). Setting C : is by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization. LinearSVC and LinearSVR are less sensitive to when it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, larger values will take more time to train, sometimes up to 10 times longer, as shown in 11 . Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data . For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a Pipeline : See section Preprocessing data for more details on scaling and normalization. Regarding the parameter, quoting 12 : We found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance), the code without using shrinking may be much faster Parameter in NuSVC / OneClassSVM / NuSVR approximates the fraction of training errors and support vectors. In SVC , if the data is unbalanced (e.g. many positive and few negative), set and/or try different penalty parameters . Randomness of the underlying implementations : The underlying implementations of SVC and NuSVC use a random number generator only to shuffle the data for probability estimation (when is set to ). This randomness can be controlled with the parameter. If is set to these estimators are not random and has no effect on the results. The underlying OneClassSVM implementation is similar to the ones of SVC and NuSVC . As no probability estimation is provided for OneClassSVM , it is not random. The underlying LinearSVC implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when is set to ). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller parameter. This randomness can also be controlled with the parameter. When is set to the underlying implementation of LinearSVC is not random and has no effect on the results. Using L1 penalization as provided by yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing yields a more complex model (more features are selected). The value that yields a “null” model (all weights equal to zero) can be calculated using l1_min_c . 