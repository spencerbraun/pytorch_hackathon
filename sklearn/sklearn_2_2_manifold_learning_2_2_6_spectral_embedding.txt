sklearn_2_2_manifold_learning
2.2. Manifold learning
modules/manifold.html
 2.2.6. Spectral Embedding  Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed with the function spectral_embedding or its object-oriented counterpart SpectralEmbedding . 2.2.6.1. Complexity  The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages: Weighted Graph Construction . Transform the raw input data into graph representation using affinity (adjacency) matrix representation. Graph Laplacian Construction . unnormalized Graph Laplacian is constructed as for and normalized one as . Partial Eigenvalue Decomposition . Eigenvalue decomposition is done on graph Laplacian The overall complexity of spectral embedding is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation” M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396 