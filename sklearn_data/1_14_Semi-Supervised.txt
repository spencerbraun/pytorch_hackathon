<div class="section" id="semi-supervised">
<span id="id1"></span><h1>1.14. Semi-Supervised<a class="headerlink" href="#semi-supervised" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Semi-supervised_learning">Semi-supervised learning</a> is a situation
in which in your training data some of the samples are not labeled. The
semi-supervised estimators in <a class="reference internal" href="classes.html#module-sklearn.semi_supervised" title="sklearn.semi_supervised"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.semi_supervised</span></code></a> are able to
make use of this additional unlabeled data to better capture the shape of
the underlying data distribution and generalize better to new samples.
These algorithms can perform well when we have a very small amount of
labeled points and a large amount of unlabeled points.</p>
<div class="topic">
<p class="topic-title">Unlabeled entries in <code class="docutils literal notranslate"><span class="pre">y</span></code></p>
<p>It is important to assign an identifier to unlabeled points along with the
labeled data when training the model with the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method. The identifier
that this implementation uses is the integer value <span class="math notranslate nohighlight">\(-1\)</span>.</p>
</div>
<div class="section" id="label-propagation">
<span id="id2"></span><h2>1.14.1. Label Propagation<a class="headerlink" href="#label-propagation" title="Permalink to this headline">¶</a></h2>
<p>Label propagation denotes a few variations of semi-supervised graph
inference algorithms.</p>
<dl class="simple">
<dt>A few features available in this model:</dt><dd><ul class="simple">
<li><p>Can be used for classification and regression tasks</p></li>
<li><p>Kernel methods to project data into alternate dimensional spaces</p></li>
</ul>
</dd>
</dl>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> provides two label propagation models:
<a class="reference internal" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LabelPropagation</span></code></a> and <a class="reference internal" href="generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading" title="sklearn.semi_supervised.LabelSpreading"><code class="xref py py-class docutils literal notranslate"><span class="pre">LabelSpreading</span></code></a>. Both work by
constructing a similarity graph over all items in the input dataset.</p>
<div class="figure align-center" id="id3">
<a class="reference external image-reference" href="../auto_examples/semi_supervised/plot_label_propagation_structure.html"><img alt="../_images/sphx_glr_plot_label_propagation_structure_0011.png" src="../_images/sphx_glr_plot_label_propagation_structure_0011.png" style="width: 510.0px; height: 240.0px;"/></a>
<p class="caption"><span class="caption-text"><strong>An illustration of label-propagation:</strong> <em>the structure of unlabeled
observations is consistent with the class structure, and thus the
class label can be propagated to the unlabeled observations of the
training set.</em></span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LabelPropagation</span></code></a> and <a class="reference internal" href="generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading" title="sklearn.semi_supervised.LabelSpreading"><code class="xref py py-class docutils literal notranslate"><span class="pre">LabelSpreading</span></code></a>
differ in modifications to the similarity matrix that graph and the
clamping effect on the label distributions.
Clamping allows the algorithm to change the weight of the true ground labeled
data to some degree. The <a class="reference internal" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LabelPropagation</span></code></a> algorithm performs hard
clamping of input labels, which means <span class="math notranslate nohighlight">\(\alpha=0\)</span>. This clamping factor
can be relaxed, to say <span class="math notranslate nohighlight">\(\alpha=0.2\)</span>, which means that we will always
retain 80 percent of our original label distribution, but the algorithm gets to
change its confidence of the distribution within 20 percent.</p>
<p><a class="reference internal" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LabelPropagation</span></code></a> uses the raw similarity matrix constructed from
the data with no modifications. In contrast, <a class="reference internal" href="generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading" title="sklearn.semi_supervised.LabelSpreading"><code class="xref py py-class docutils literal notranslate"><span class="pre">LabelSpreading</span></code></a>
minimizes a loss function that has regularization properties, as such it
is often more robust to noise. The algorithm iterates on a modified
version of the original graph and normalizes the edge weights by
computing the normalized graph Laplacian matrix. This procedure is also
used in <a class="reference internal" href="clustering.html#spectral-clustering"><span class="std std-ref">Spectral clustering</span></a>.</p>
<p>Label propagation models have two built-in kernel methods. Choice of kernel
effects both scalability and performance of the algorithms. The following are
available:</p>
<blockquote>
<div><ul class="simple">
<li><p>rbf (<span class="math notranslate nohighlight">\(\exp(-\gamma |x-y|^2), \gamma &gt; 0\)</span>). <span class="math notranslate nohighlight">\(\gamma\)</span> is
specified by keyword gamma.</p></li>
<li><p>knn (<span class="math notranslate nohighlight">\(1[x' \in kNN(x)]\)</span>). <span class="math notranslate nohighlight">\(k\)</span> is specified by keyword
n_neighbors.</p></li>
</ul>
</div></blockquote>
<p>The RBF kernel will produce a fully connected graph which is represented in memory
by a dense matrix. This matrix may be very large and combined with the cost of
performing a full matrix multiplication calculation for each iteration of the
algorithm can lead to prohibitively long running times. On the other hand,
the KNN kernel will produce a much more memory-friendly sparse matrix
which can drastically reduce running times.</p>
<div class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/semi_supervised/plot_label_propagation_versus_svm_iris.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-versus-svm-iris-py"><span class="std std-ref">Decision boundary of label propagation versus SVM on the Iris dataset</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/semi_supervised/plot_label_propagation_structure.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-structure-py"><span class="std std-ref">Label Propagation learning a complex structure</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/semi_supervised/plot_label_propagation_digits.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-py"><span class="std std-ref">Label Propagation digits: Demonstrating performance</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-active-learning-py"><span class="std std-ref">Label Propagation digits active learning</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">References</p>
<p>[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised
Learning (2006), pp. 193-216</p>
<p>[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient
Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005
<a class="reference external" href="https://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf">https://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf</a></p>
</div>
</div>
</div>