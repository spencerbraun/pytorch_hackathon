<div class="section" id="kernel-ridge-regression">
<span id="kernel-ridge"></span><h1>1.3. Kernel ridge regression<a class="headerlink" href="#kernel-ridge-regression" title="Permalink to this headline">¶</a></h1>
<p>Kernel ridge regression (KRR) <a class="reference internal" href="#m2012" id="id1"><span>[M2012]</span></a> combines <a class="reference internal" href="linear_model.html#ridge-regression"><span class="std std-ref">Ridge regression and classification</span></a>
(linear least squares with l2-norm regularization) with the <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_method">kernel trick</a>. It thus learns a linear
function in the space induced by the respective kernel and the data. For
non-linear kernels, this corresponds to a non-linear function in the original
space.</p>
<p>The form of the model learned by <a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> is identical to support
vector regression (<a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a>). However, different loss
functions are used: KRR uses squared error loss while support vector
regression uses <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensitive loss, both combined with l2
regularization. In contrast to <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a>, fitting
<a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> can be done in closed-form and is typically faster for
medium-sized datasets. On the other hand, the learned model is non-sparse and
thus slower than <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a>, which learns a sparse model for
<span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, at prediction-time.</p>
<p>The following figure compares <a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> and
<a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> on an artificial dataset, which consists of a
sinusoidal target function and strong noise added to every fifth datapoint.
The learned model of <a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> and <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> is
plotted, where both complexity/regularization and bandwidth of the RBF kernel
have been optimized using grid-search. The learned functions are very
similar; however, fitting <a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> is approximately seven times
faster than fitting <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> (both with grid-search).
However, prediction of 100000 target values is more than three times faster
with <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> since it has learned a sparse model using only
approximately 1/3 of the 100 training datapoints as support vectors.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/miscellaneous/plot_kernel_ridge_regression.html"><img alt="../_images/sphx_glr_plot_kernel_ridge_regression_0011.png" src="../_images/sphx_glr_plot_kernel_ridge_regression_0011.png"/></a>
</div>
<p>The next figure compares the time for fitting and prediction of
<a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> and <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> for different sizes of the
training set. Fitting <a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> is faster than
<a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> for medium-sized training sets (less than 1000
samples); however, for larger training sets <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> scales
better. With regard to prediction time, <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a> is faster
than <a class="reference internal" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></a> for all sizes of the training set because of the
learned sparse solution. Note that the degree of sparsity and thus the
prediction time depends on the parameters <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(C\)</span> of
the <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal notranslate"><span class="pre">SVR</span></code></a>; <span class="math notranslate nohighlight">\(\epsilon = 0\)</span> would correspond to a
dense model.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/miscellaneous/plot_kernel_ridge_regression.html"><img alt="../_images/sphx_glr_plot_kernel_ridge_regression_0021.png" src="../_images/sphx_glr_plot_kernel_ridge_regression_0021.png"/></a>
</div>
<div class="topic">
<p class="topic-title">References:</p>
<dl class="citation">
<dt class="label" id="m2012"><span class="brackets"><a class="fn-backref" href="#id1">M2012</a></span></dt>
<dd><p>“Machine Learning: A Probabilistic Perspective”
Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012</p>
</dd>
</dl>
</div>
</div>