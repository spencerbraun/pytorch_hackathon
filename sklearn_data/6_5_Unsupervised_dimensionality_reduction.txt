<div class="section" id="unsupervised-dimensionality-reduction">
<span id="data-reduction"></span><h1>6.5. Unsupervised dimensionality reduction<a class="headerlink" href="#unsupervised-dimensionality-reduction" title="Permalink to this headline">¶</a></h1>
<p>If your number of features is high, it may be useful to reduce it with an
unsupervised step prior to supervised steps. Many of the
<a class="reference internal" href="../unsupervised_learning.html#unsupervised-learning"><span class="std std-ref">Unsupervised learning</span></a> methods implement a <code class="docutils literal notranslate"><span class="pre">transform</span></code> method that
can be used to reduce the dimensionality. Below we discuss two specific
example of this pattern that are heavily used.</p>
<div class="topic">
<p class="topic-title"><strong>Pipelining</strong></p>
<p>The unsupervised data reduction and the supervised estimator can be
chained in one step. See <a class="reference internal" href="compose.html#pipeline"><span class="std std-ref">Pipeline: chaining estimators</span></a>.</p>
</div>
<div class="section" id="pca-principal-component-analysis">
<h2>6.5.1. PCA: principal component analysis<a class="headerlink" href="#pca-principal-component-analysis" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">decomposition.PCA</span></code></a> looks for a combination of features that
capture well the variance of the original features. See <a class="reference internal" href="decomposition.html#decompositions"><span class="std std-ref">Decomposing signals in components (matrix factorization problems)</span></a>.</p>
<div class="topic">
<p class="topic-title"><strong>Examples</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="std std-ref">Faces recognition example using eigenfaces and SVMs</span></a></p></li>
</ul>
</div>
</div>
<div class="section" id="random-projections">
<h2>6.5.2. Random projections<a class="headerlink" href="#random-projections" title="Permalink to this headline">¶</a></h2>
<p>The module: <code class="xref py py-mod docutils literal notranslate"><span class="pre">random_projection</span></code> provides several tools for data
reduction by random projections. See the relevant section of the
documentation: <a class="reference internal" href="random_projection.html#random-projection"><span class="std std-ref">Random Projection</span></a>.</p>
<div class="topic">
<p class="topic-title"><strong>Examples</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/miscellaneous/plot_johnson_lindenstrauss_bound.html#sphx-glr-auto-examples-miscellaneous-plot-johnson-lindenstrauss-bound-py"><span class="std std-ref">The Johnson-Lindenstrauss bound for embedding with random projections</span></a></p></li>
</ul>
</div>
</div>
<div class="section" id="feature-agglomeration">
<h2>6.5.3. Feature agglomeration<a class="headerlink" href="#feature-agglomeration" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="xref py py-class docutils literal notranslate"><span class="pre">cluster.FeatureAgglomeration</span></code></a> applies
<a class="reference internal" href="clustering.html#hierarchical-clustering"><span class="std std-ref">Hierarchical clustering</span></a> to group together features that behave
similarly.</p>
<div class="topic">
<p class="topic-title"><strong>Examples</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="std std-ref">Feature agglomeration vs. univariate selection</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/cluster/plot_digits_agglomeration.html#sphx-glr-auto-examples-cluster-plot-digits-agglomeration-py"><span class="std std-ref">Feature agglomeration</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title"><strong>Feature scaling</strong></p>
<p>Note that if features have very different scaling or statistical
properties, <a class="reference internal" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="xref py py-class docutils literal notranslate"><span class="pre">cluster.FeatureAgglomeration</span></code></a> may not be able to
capture the links between related features. Using a
<a class="reference internal" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">preprocessing.StandardScaler</span></code></a> can be useful in these settings.</p>
</div>
</div>
</div>