<div class="section" id="pairwise-metrics-affinities-and-kernels">
<span id="metrics"></span><h1>6.8. Pairwise metrics, Affinities and Kernels<a class="headerlink" href="#pairwise-metrics-affinities-and-kernels" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a> submodule implements utilities to evaluate
pairwise distances or affinity of sets of samples.</p>
<p>This module contains both distance metrics and kernels. A brief summary is
given on the two here.</p>
<p>Distance metrics are functions <code class="docutils literal notranslate"><span class="pre">d(a,</span> <span class="pre">b)</span></code> such that <code class="docutils literal notranslate"><span class="pre">d(a,</span> <span class="pre">b)</span> <span class="pre">&lt;</span> <span class="pre">d(a,</span> <span class="pre">c)</span></code>
if objects <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> are considered “more similar” than objects <code class="docutils literal notranslate"><span class="pre">a</span></code>
and <code class="docutils literal notranslate"><span class="pre">c</span></code>. Two objects exactly alike would have a distance of zero.
One of the most popular examples is Euclidean distance.
To be a ‘true’ metric, it must obey the following four conditions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">a</span> <span class="ow">and</span> <span class="n">b</span>
<span class="mf">2.</span> <span class="n">d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="k">if</span> <span class="ow">and</span> <span class="n">only</span> <span class="k">if</span> <span class="n">a</span> <span class="o">=</span> <span class="n">b</span><span class="p">,</span> <span class="n">positive</span> <span class="n">definiteness</span>
<span class="mf">3.</span> <span class="n">d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="n">d</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">symmetry</span>
<span class="mf">4.</span> <span class="n">d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="n">d</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="n">the</span> <span class="n">triangle</span> <span class="n">inequality</span>
</pre></div>
</div>
<p>Kernels are measures of similarity, i.e. <code class="docutils literal notranslate"><span class="pre">s(a,</span> <span class="pre">b)</span> <span class="pre">&gt;</span> <span class="pre">s(a,</span> <span class="pre">c)</span></code>
if objects <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> are considered “more similar” than objects
<code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">c</span></code>. A kernel must also be positive semi-definite.</p>
<p>There are a number of ways to convert between a distance metric and a
similarity measure, such as a kernel. Let <code class="docutils literal notranslate"><span class="pre">D</span></code> be the distance, and <code class="docutils literal notranslate"><span class="pre">S</span></code> be
the kernel:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">S</span> <span class="pre">=</span> <span class="pre">np.exp(-D</span> <span class="pre">*</span> <span class="pre">gamma)</span></code>, where one heuristic for choosing
<code class="docutils literal notranslate"><span class="pre">gamma</span></code> is <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">num_features</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">S</span> <span class="pre">=</span> <span class="pre">1.</span> <span class="pre">/</span> <span class="pre">(D</span> <span class="pre">/</span> <span class="pre">np.max(D))</span></code></p></li>
</ol>
</div></blockquote>
<p>The distances between the row vectors of <code class="docutils literal notranslate"><span class="pre">X</span></code> and the row vectors of <code class="docutils literal notranslate"><span class="pre">Y</span></code>
can be evaluated using <a class="reference internal" href="generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances" title="sklearn.metrics.pairwise_distances"><code class="xref py py-func docutils literal notranslate"><span class="pre">pairwise_distances</span></code></a>. If <code class="docutils literal notranslate"><span class="pre">Y</span></code> is omitted the
pairwise distances of the row vectors of <code class="docutils literal notranslate"><span class="pre">X</span></code> are calculated. Similarly,
<a class="reference internal" href="generated/sklearn.metrics.pairwise.pairwise_kernels.html#sklearn.metrics.pairwise.pairwise_kernels" title="sklearn.metrics.pairwise.pairwise_kernels"><code class="xref py py-func docutils literal notranslate"><span class="pre">pairwise.pairwise_kernels</span></code></a> can be used to calculate the kernel between <code class="docutils literal notranslate"><span class="pre">X</span></code>
and <code class="docutils literal notranslate"><span class="pre">Y</span></code> using different kernel functions. See the API reference for more
details.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">pairwise_kernels</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'manhattan'</span><span class="p">)</span>
<span class="go">array([[ 4.,  2.],</span>
<span class="go">       [ 7.,  5.],</span>
<span class="go">       [12., 10.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'manhattan'</span><span class="p">)</span>
<span class="go">array([[0., 3., 8.],</span>
<span class="go">       [3., 0., 5.],</span>
<span class="go">       [8., 5., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairwise_kernels</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)</span>
<span class="go">array([[ 2.,  7.],</span>
<span class="go">       [ 3., 11.],</span>
<span class="go">       [ 5., 18.]])</span>
</pre></div>
</div>
<div class="section" id="cosine-similarity">
<span id="id1"></span><h2>6.8.1. Cosine similarity<a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code class="xref py py-func docutils literal notranslate"><span class="pre">cosine_similarity</span></code></a> computes the L2-normalized dot product of vectors.
That is, if <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are row vectors,
their cosine similarity <span class="math notranslate nohighlight">\(k\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[k(x, y) = \frac{x y^\top}{\|x\| \|y\|}\]</div>
<p>This is called cosine similarity, because Euclidean (L2) normalization
projects the vectors onto the unit sphere,
and their dot product is then the cosine of the angle between the points
denoted by the vectors.</p>
<p>This kernel is a popular choice for computing the similarity of documents
represented as tf-idf vectors.
<a class="reference internal" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code class="xref py py-func docutils literal notranslate"><span class="pre">cosine_similarity</span></code></a> accepts <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> matrices.
(Note that the tf-idf functionality in <code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text</span></code>
can produce normalized vectors, in which case <a class="reference internal" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code class="xref py py-func docutils literal notranslate"><span class="pre">cosine_similarity</span></code></a>
is equivalent to <a class="reference internal" href="generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel" title="sklearn.metrics.pairwise.linear_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">linear_kernel</span></code></a>, only slower.)</p>
<div class="topic">
<p class="topic-title">References:</p>
<ul class="simple">
<li><p>C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press.
<a class="reference external" href="https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html</a></p></li>
</ul>
</div>
</div>
<div class="section" id="linear-kernel">
<span id="id2"></span><h2>6.8.2. Linear kernel<a class="headerlink" href="#linear-kernel" title="Permalink to this headline">¶</a></h2>
<p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel" title="sklearn.metrics.pairwise.linear_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">linear_kernel</span></code></a> computes the linear kernel, that is, a
special case of <a class="reference internal" href="generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel" title="sklearn.metrics.pairwise.polynomial_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">polynomial_kernel</span></code></a> with <code class="docutils literal notranslate"><span class="pre">degree=1</span></code> and <code class="docutils literal notranslate"><span class="pre">coef0=0</span></code> (homogeneous).
If <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are column vectors, their linear kernel is:</p>
<div class="math notranslate nohighlight">
\[k(x, y) = x^\top y\]</div>
</div>
<div class="section" id="polynomial-kernel">
<span id="id3"></span><h2>6.8.3. Polynomial kernel<a class="headerlink" href="#polynomial-kernel" title="Permalink to this headline">¶</a></h2>
<p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel" title="sklearn.metrics.pairwise.polynomial_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">polynomial_kernel</span></code></a> computes the degree-d polynomial kernel
between two vectors. The polynomial kernel represents the similarity between two
vectors. Conceptually, the polynomial kernels considers not only the similarity
between vectors under the same dimension, but also across dimensions. When used
in machine learning algorithms, this allows to account for feature interaction.</p>
<p>The polynomial kernel is defined as:</p>
<div class="math notranslate nohighlight">
\[k(x, y) = (\gamma x^\top y +c_0)^d\]</div>
<p>where:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code> are the input vectors</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">d</span></code> is the kernel degree</p></li>
</ul>
</div></blockquote>
<p>If <span class="math notranslate nohighlight">\(c_0 = 0\)</span> the kernel is said to be homogeneous.</p>
</div>
<div class="section" id="sigmoid-kernel">
<span id="id4"></span><h2>6.8.4. Sigmoid kernel<a class="headerlink" href="#sigmoid-kernel" title="Permalink to this headline">¶</a></h2>
<p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.sigmoid_kernel.html#sklearn.metrics.pairwise.sigmoid_kernel" title="sklearn.metrics.pairwise.sigmoid_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">sigmoid_kernel</span></code></a> computes the sigmoid kernel between two
vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer
Perceptron (because, in the neural network field, it is often used as neuron
activation function). It is defined as:</p>
<div class="math notranslate nohighlight">
\[k(x, y) = \tanh( \gamma x^\top y + c_0)\]</div>
<p>where:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code> are the input vectors</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> is known as slope</p></li>
<li><p><span class="math notranslate nohighlight">\(c_0\)</span> is known as intercept</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="rbf-kernel">
<span id="id5"></span><h2>6.8.5. RBF kernel<a class="headerlink" href="#rbf-kernel" title="Permalink to this headline">¶</a></h2>
<p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.rbf_kernel.html#sklearn.metrics.pairwise.rbf_kernel" title="sklearn.metrics.pairwise.rbf_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">rbf_kernel</span></code></a> computes the radial basis function (RBF) kernel
between two vectors. This kernel is defined as:</p>
<div class="math notranslate nohighlight">
\[k(x, y) = \exp( -\gamma \| x-y \|^2)\]</div>
<p>where <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are the input vectors. If <span class="math notranslate nohighlight">\(\gamma = \sigma^{-2}\)</span>
the kernel is known as the Gaussian kernel of variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
</div>
<div class="section" id="laplacian-kernel">
<span id="id6"></span><h2>6.8.6. Laplacian kernel<a class="headerlink" href="#laplacian-kernel" title="Permalink to this headline">¶</a></h2>
<p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.laplacian_kernel.html#sklearn.metrics.pairwise.laplacian_kernel" title="sklearn.metrics.pairwise.laplacian_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">laplacian_kernel</span></code></a> is a variant on the radial basis
function kernel defined as:</p>
<div class="math notranslate nohighlight">
\[k(x, y) = \exp( -\gamma \| x-y \|_1)\]</div>
<p>where <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are the input vectors and <span class="math notranslate nohighlight">\(\|x-y\|_1\)</span> is the
Manhattan distance between the input vectors.</p>
<p>It has proven useful in ML applied to noiseless data.
See e.g. <a class="reference external" href="https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/">Machine learning for quantum mechanics in a nutshell</a>.</p>
</div>
<div class="section" id="chi-squared-kernel">
<span id="chi2-kernel"></span><h2>6.8.7. Chi-squared kernel<a class="headerlink" href="#chi-squared-kernel" title="Permalink to this headline">¶</a></h2>
<p>The chi-squared kernel is a very popular choice for training non-linear SVMs in
computer vision applications.
It can be computed using <a class="reference internal" href="generated/sklearn.metrics.pairwise.chi2_kernel.html#sklearn.metrics.pairwise.chi2_kernel" title="sklearn.metrics.pairwise.chi2_kernel"><code class="xref py py-func docutils literal notranslate"><span class="pre">chi2_kernel</span></code></a> and then passed to an
<a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm.SVC</span></code></a> with <code class="docutils literal notranslate"><span class="pre">kernel="precomputed"</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">chi2_kernel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="n">chi2_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">K</span>
<span class="go">array([[1.        , 0.36787944, 0.89483932, 0.58364548],</span>
<span class="go">       [0.36787944, 1.        , 0.51341712, 0.83822343],</span>
<span class="go">       [0.89483932, 0.51341712, 1.        , 0.7768366 ],</span>
<span class="go">       [0.58364548, 0.83822343, 0.7768366 , 1.        ]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">'precomputed'</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="go">array([0, 1, 0, 1])</span>
</pre></div>
</div>
<p>It can also be directly used as the <code class="docutils literal notranslate"><span class="pre">kernel</span></code> argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">chi2_kernel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 1, 0, 1])</span>
</pre></div>
</div>
<p>The chi squared kernel is given by</p>
<div class="math notranslate nohighlight">
\[k(x, y) = \exp \left (-\gamma \sum_i \frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \right )\]</div>
<p>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one.
The normalization is rationalized with the connection to the chi squared distance,
which is a distance between discrete probability distributions.</p>
<p>The chi squared kernel is most commonly used on histograms (bags) of visual words.</p>
<div class="topic">
<p class="topic-title">References:</p>
<ul class="simple">
<li><p>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.
Local features and kernels for classification of texture and object
categories: A comprehensive study
International Journal of Computer Vision 2007
<a class="reference external" href="https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf">https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf</a></p></li>
</ul>
</div>
</div>
</div>