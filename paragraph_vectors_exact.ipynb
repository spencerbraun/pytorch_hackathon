{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.6/site-packages (from torchtext) (0.1.91)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.6/site-packages (from torchtext) (4.48.2)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext) (1.4.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext) (1.18.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext) (2020.4.5.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import signal\n",
    "from math import ceil\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.random import choice\n",
    "from torchtext.data import Field, TabularDataset\n",
    "\n",
    "# from paragraphvec.utils import DATA_DIR\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(file_name):\n",
    "    \"\"\"Loads contents from a file in the *data* directory into a\n",
    "    torchtext.data.TabularDataset instance.\n",
    "    \"\"\"\n",
    "    file_path = join(DATA_DIR, file_name)\n",
    "    text_field = Field(pad_token=None, tokenize=_tokenize_str)\n",
    "\n",
    "    dataset = TabularDataset(\n",
    "        path=file_path,\n",
    "        format='csv',\n",
    "        fields=[('text', text_field)],\n",
    "        skip_header=True)\n",
    "\n",
    "    text_field.build_vocab(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _tokenize_str(str_):\n",
    "    # keep only alphanumeric and punctations\n",
    "    str_ = re.sub(r'[^A-Za-z0-9(),.!?\\'`]', ' ', str_)\n",
    "    # remove multiple whitespace characters\n",
    "    str_ = re.sub(r'\\s{2,}', ' ', str_)\n",
    "    # punctations to tokens\n",
    "    str_ = re.sub(r'\\(', ' ( ', str_)\n",
    "    str_ = re.sub(r'\\)', ' ) ', str_)\n",
    "    str_ = re.sub(r',', ' , ', str_)\n",
    "    str_ = re.sub(r'\\.', ' . ', str_)\n",
    "    str_ = re.sub(r'!', ' ! ', str_)\n",
    "    str_ = re.sub(r'\\?', ' ? ', str_)\n",
    "    # split contractions into multiple tokens\n",
    "    str_ = re.sub(r'\\'s', ' \\'s', str_)\n",
    "    str_ = re.sub(r'\\'ve', ' \\'ve', str_)\n",
    "    str_ = re.sub(r'n\\'t', ' n\\'t', str_)\n",
    "    str_ = re.sub(r'\\'re', ' \\'re', str_)\n",
    "    str_ = re.sub(r'\\'d', ' \\'d', str_)\n",
    "    str_ = re.sub(r'\\'ll', ' \\'ll', str_)\n",
    "    # lower case\n",
    "    return str_.strip().lower().split()\n",
    "\n",
    "\n",
    "class NCEData(object):\n",
    "    \"\"\"An infinite, parallel (multiprocess) batch generator for\n",
    "    noise-contrastive estimation of word vector models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: torchtext.data.TabularDataset\n",
    "        Dataset from which examples are generated. A column labeled *text*\n",
    "        is expected and should be comprised of a list of tokens. Each row\n",
    "        should represent a single document.\n",
    "\n",
    "    batch_size: int\n",
    "        Number of examples per single gradient update.\n",
    "\n",
    "    context_size: int\n",
    "        Half the size of a neighbourhood of target words (i.e. how many\n",
    "        words left and right are regarded as context).\n",
    "\n",
    "    num_noise_words: int\n",
    "        Number of noise words to sample from the noise distribution.\n",
    "\n",
    "    max_size: int\n",
    "        Maximum number of pre-generated batches.\n",
    "\n",
    "    num_workers: int\n",
    "        Number of jobs to run in parallel. If value is set to -1, total number\n",
    "        of machine CPUs is used.\n",
    "    \"\"\"\n",
    "    # code inspired by parallel generators in https://github.com/fchollet/keras\n",
    "    def __init__(self, dataset, batch_size, context_size,\n",
    "                 num_noise_words, max_size, num_workers):\n",
    "        self.max_size = max_size\n",
    "\n",
    "        self.num_workers = num_workers if num_workers != -1 else os.cpu_count()\n",
    "        if self.num_workers is None:\n",
    "            self.num_workers = 1\n",
    "\n",
    "        self._generator = _NCEGenerator(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            context_size,\n",
    "            num_noise_words,\n",
    "            _NCEGeneratorState(context_size))\n",
    "\n",
    "        self._queue = []\n",
    "        self._stop_event = None\n",
    "        self._processes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._generator)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return self._generator.vocabulary_size()\n",
    "\n",
    "\n",
    "\n",
    "class _NCEGenerator(object):\n",
    "    \"\"\"An infinite, process-safe batch generator for noise-contrastive\n",
    "    estimation of word vector models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state: paragraphvec.data._NCEGeneratorState\n",
    "        Initial (indexing) state of the generator.\n",
    "\n",
    "    For other parameters see the NCEData class.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, context_size,\n",
    "                 num_noise_words, state):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.context_size = context_size\n",
    "        self.num_noise_words = num_noise_words\n",
    "\n",
    "        self._vocabulary = self.dataset.fields['text'].vocab\n",
    "        self._sample_noise = None\n",
    "        self._init_noise_distribution()\n",
    "        self._state = state\n",
    "\n",
    "    def _init_noise_distribution(self):\n",
    "        # we use a unigram distribution raised to the 3/4rd power,\n",
    "        # as proposed by T. Mikolov et al. in Distributed Representations\n",
    "        # of Words and Phrases and their Compositionality\n",
    "        probs = np.zeros(len(self._vocabulary) - 1)\n",
    "\n",
    "        for word, freq in self._vocabulary.freqs.items():\n",
    "            probs[self._word_to_index(word)] = freq\n",
    "\n",
    "        probs = np.power(probs, 0.75)\n",
    "        probs /= np.sum(probs)\n",
    "\n",
    "        self._sample_noise = lambda: choice(\n",
    "            probs.shape[0], self.num_noise_words, p=probs).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        num_examples = sum(self._num_examples_in_doc(d) for d in self.dataset)\n",
    "        return ceil(num_examples / self.batch_size)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return len(self._vocabulary) - 1\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Updates state for the next process in a process-safe manner\n",
    "        and generates the current batch.\"\"\"\n",
    "        prev_doc_id, prev_in_doc_pos = self._state.update_state(\n",
    "            self.dataset,\n",
    "            self.batch_size,\n",
    "            self.context_size,\n",
    "            self._num_examples_in_doc)\n",
    "\n",
    "        # generate the actual batch\n",
    "        batch = _NCEBatch(self.context_size)\n",
    "\n",
    "        while len(batch) < self.batch_size:\n",
    "            if prev_doc_id == len(self.dataset):\n",
    "                # last document exhausted\n",
    "                batch.torch_()\n",
    "                return batch\n",
    "            if prev_in_doc_pos <= (len(self.dataset[prev_doc_id].text) - 1\n",
    "                                   - self.context_size):\n",
    "                # more examples in the current document\n",
    "                self._add_example_to_batch(prev_doc_id, prev_in_doc_pos, batch)\n",
    "                prev_in_doc_pos += 1\n",
    "            else:\n",
    "                # go to the next document\n",
    "                prev_doc_id += 1\n",
    "                prev_in_doc_pos = self.context_size\n",
    "\n",
    "        batch.torch_()\n",
    "        return batch\n",
    "\n",
    "    def _num_examples_in_doc(self, doc, in_doc_pos=None):\n",
    "        if in_doc_pos is not None:\n",
    "            # number of remaining\n",
    "            if len(doc.text) - in_doc_pos >= self.context_size + 1:\n",
    "                return len(doc.text) - in_doc_pos - self.context_size\n",
    "            return 0\n",
    "\n",
    "        if len(doc.text) >= 2 * self.context_size + 1:\n",
    "            # total number\n",
    "            return len(doc.text) - 2 * self.context_size\n",
    "        return 0\n",
    "\n",
    "    def _add_example_to_batch(self, doc_id, in_doc_pos, batch):\n",
    "        doc = self.dataset[doc_id].text\n",
    "        batch.doc_ids.append(doc_id)\n",
    "\n",
    "        # sample from the noise distribution\n",
    "        current_noise = self._sample_noise()\n",
    "        current_noise.insert(0, self._word_to_index(doc[in_doc_pos]))\n",
    "        batch.target_noise_ids.append(current_noise)\n",
    "\n",
    "        if self.context_size == 0:\n",
    "            return\n",
    "\n",
    "        current_context = []\n",
    "        context_indices = (in_doc_pos + diff for diff in\n",
    "                           range(-self.context_size, self.context_size + 1)\n",
    "                           if diff != 0)\n",
    "\n",
    "        for i in context_indices:\n",
    "            context_id = self._word_to_index(doc[i])\n",
    "            current_context.append(context_id)\n",
    "        batch.context_ids.append(current_context)\n",
    "\n",
    "    def _word_to_index(self, word):\n",
    "        return self._vocabulary.stoi[word] - 1\n",
    "\n",
    "\n",
    "class _NCEGeneratorState(object):\n",
    "    \"\"\"Batch generator state that is represented with a document id and\n",
    "    in-document position. It abstracts a process-safe indexing mechanism.\"\"\"\n",
    "    def __init__(self, context_size):\n",
    "        # use raw values because both indices have\n",
    "        # to manually be locked together\n",
    "        self._doc_id = multiprocessing.RawValue('i', 0)\n",
    "        self._in_doc_pos = multiprocessing.RawValue('i', context_size)\n",
    "        self._lock = multiprocessing.Lock()\n",
    "\n",
    "    def update_state(self, dataset, batch_size,\n",
    "                     context_size, num_examples_in_doc):\n",
    "        \"\"\"Returns current indices and computes new indices for the\n",
    "        next process.\"\"\"\n",
    "        with self._lock:\n",
    "            doc_id = self._doc_id.value\n",
    "            in_doc_pos = self._in_doc_pos.value\n",
    "            self._advance_indices(\n",
    "                dataset, batch_size, context_size, num_examples_in_doc)\n",
    "            return doc_id, in_doc_pos\n",
    "\n",
    "    def _advance_indices(self, dataset, batch_size,\n",
    "                         context_size, num_examples_in_doc):\n",
    "        num_examples = num_examples_in_doc(\n",
    "            dataset[self._doc_id.value], self._in_doc_pos.value)\n",
    "\n",
    "        if num_examples > batch_size:\n",
    "            # more examples in the current document\n",
    "            self._in_doc_pos.value += batch_size\n",
    "            return\n",
    "\n",
    "        if num_examples == batch_size:\n",
    "            # just enough examples in the current document\n",
    "            if self._doc_id.value < len(dataset) - 1:\n",
    "                self._doc_id.value += 1\n",
    "            else:\n",
    "                self._doc_id.value = 0\n",
    "            self._in_doc_pos.value = context_size\n",
    "            return\n",
    "\n",
    "        while num_examples < batch_size:\n",
    "            if self._doc_id.value == len(dataset) - 1:\n",
    "                # last document: reset indices\n",
    "                self._doc_id.value = 0\n",
    "                self._in_doc_pos.value = context_size\n",
    "                return\n",
    "\n",
    "            self._doc_id.value += 1\n",
    "            num_examples += num_examples_in_doc(\n",
    "                dataset[self._doc_id.value])\n",
    "\n",
    "        self._in_doc_pos.value = (len(dataset[self._doc_id.value].text)\n",
    "                                  - context_size\n",
    "                                  - (num_examples - batch_size))\n",
    "\n",
    "\n",
    "class _NCEBatch(object):\n",
    "    def __init__(self, context_size):\n",
    "        self.context_ids = [] if context_size > 0 else None\n",
    "        self.doc_ids = []\n",
    "        self.target_noise_ids = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def torch_(self):\n",
    "        if self.context_ids is not None:\n",
    "            self.context_ids = torch.LongTensor(self.context_ids)\n",
    "        self.doc_ids = torch.LongTensor(self.doc_ids)\n",
    "        self.target_noise_ids = torch.LongTensor(self.target_noise_ids)\n",
    "\n",
    "    def cuda_(self):\n",
    "        if self.context_ids is not None:\n",
    "            self.context_ids = self.context_ids.cuda()\n",
    "        self.doc_ids = self.doc_ids.cuda()\n",
    "        self.target_noise_ids = self.target_noise_ids.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NegativeSampling(nn.Module):\n",
    "    \"\"\"Negative sampling loss as proposed by T. Mikolov et al. in Distributed\n",
    "    Representations of Words and Phrases and their Compositionality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(NegativeSampling, self).__init__()\n",
    "        self._log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, scores):\n",
    "        \"\"\"Computes the value of the loss function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores: autograd.Variable of size (batch_size, num_noise_words + 1)\n",
    "            Sparse unnormalized log probabilities. The first element in each\n",
    "            row is the ground truth score (i.e. the target), other elements\n",
    "            are scores of samples from the noise distribution.\n",
    "        \"\"\"\n",
    "        k = scores.size()[1] - 1\n",
    "        return -torch.sum(\n",
    "            self._log_sigmoid(scores[:, 0])\n",
    "            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n",
    "        ) / scores.size()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DBOW(nn.Module):\n",
    "    \"\"\"Distributed Bag of Words version of Paragraph Vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec_dim: int\n",
    "        Dimensionality of vectors to be learned (for paragraphs and words).\n",
    "    num_docs: int\n",
    "        Number of documents in a dataset.\n",
    "    num_words: int\n",
    "        Number of distinct words in a daset (i.e. vocabulary size).\n",
    "    \"\"\"\n",
    "    def __init__(self, vec_dim, num_docs, num_words):\n",
    "        super(DBOW, self).__init__()\n",
    "        # paragraph matrix\n",
    "        self._D = nn.Parameter(\n",
    "            torch.randn(num_docs, vec_dim), requires_grad=True)\n",
    "        # output layer parameters\n",
    "        self._O = nn.Parameter(\n",
    "            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n",
    "\n",
    "    def forward(self, doc_ids, target_noise_ids):\n",
    "        \"\"\"Sparse computation of scores (unnormalized log probabilities)\n",
    "        that should be passed to the negative sampling loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc_ids: torch.Tensor of size (batch_size,)\n",
    "            Document indices of paragraphs.\n",
    "        target_noise_ids: torch.Tensor of size (batch_size, num_noise_words + 1)\n",
    "            Vocabulary indices of target and noise words. The first element in\n",
    "            each row is the ground truth index (i.e. the target), other\n",
    "            elements are indices of samples from the noise distribution.\n",
    "        Returns\n",
    "        -------\n",
    "            autograd.Variable of size (batch_size, num_noise_words + 1)\n",
    "        \"\"\"\n",
    "        # sparse computation of scores (unnormalized log probabilities)\n",
    "        # for negative sampling\n",
    "        return torch.bmm(\n",
    "            self._D[doc_ids, :].unsqueeze(1),\n",
    "            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n",
    "\n",
    "    def get_paragraph_vector(self, index):\n",
    "        return self._D[index, :].data.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import remove\n",
    "from os.path import join, dirname, isfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "_root_dir = \"model_data\"\n",
    "\n",
    "DATA_DIR = join(_root_dir, 'data')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "MODELS_DIR = join(_root_dir, 'models')\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.mkdir(MODELS_DIR)\n",
    "_DIAGNOSTICS_DIR = join(_root_dir, 'diagnostics')\n",
    "if not os.path.exists(_DIAGNOSTICS_DIR):\n",
    "    os.mkdir(_DIAGNOSTICS_DIR)\n",
    "\n",
    "_DM_MODEL_NAME = (\"{:s}_model.{:s}.{:s}_contextsize.{:d}_numnoisewords.{:d}\"\n",
    "                  \"_vecdim.{:d}_batchsize.{:d}_lr.{:f}_epoch.{:d}_loss.{:f}\"\n",
    "                  \".pth.tar\")\n",
    "_DM_DIAGNOSTIC_FILE_NAME = (\"{:s}_model.{:s}.{:s}_contextsize.{:d}\"\n",
    "                            \"_numnoisewords.{:d}_vecdim.{:d}_batchsize.{:d}\"\n",
    "                            \"_lr.{:f}.csv\")\n",
    "_DBOW_MODEL_NAME = (\"{:s}_model.{:s}_numnoisewords.{:d}_vecdim.{:d}\"\n",
    "                    \"_batchsize.{:d}_lr.{:f}_epoch.{:d}_loss.{:f}.pth.tar\")\n",
    "_DBOW_DIAGNOSTIC_FILE_NAME = (\"{:s}_model.{:s}_numnoisewords.{:d}_vecdim.{:d}\"\n",
    "                              \"_batchsize.{:d}_lr.{:f}.csv\")\n",
    "\n",
    "\n",
    "def save_training_state(data_file_name,\n",
    "                        model_ver,\n",
    "                        vec_combine_method,\n",
    "                        context_size,\n",
    "                        num_noise_words,\n",
    "                        vec_dim,\n",
    "                        batch_size,\n",
    "                        lr,\n",
    "                        epoch_i,\n",
    "                        loss,\n",
    "                        model_state,\n",
    "                        save_all,\n",
    "                        generate_plot,\n",
    "                        is_best_loss,\n",
    "                        prev_model_file_path,\n",
    "                        model_ver_is_dbow):\n",
    "    \"\"\"Saves the state of the model. If generate_plot is True, it also\n",
    "    saves current epoch's loss value and generates a plot of all loss\n",
    "    values up to this epoch.\n",
    "    Returns\n",
    "    -------\n",
    "        str representing a model file path from the previous epoch\n",
    "    \"\"\"\n",
    "    if generate_plot:\n",
    "        diagnostic_file_name = _DBOW_DIAGNOSTIC_FILE_NAME.format(\n",
    "            data_file_name[:-4],\n",
    "            model_ver,\n",
    "            num_noise_words,\n",
    "            vec_dim,\n",
    "            batch_size,\n",
    "            lr)\n",
    "\n",
    "        diagnostic_file_path = join(_DIAGNOSTICS_DIR, diagnostic_file_name)\n",
    "\n",
    "        if epoch_i == 0 and isfile(diagnostic_file_path):\n",
    "            remove(diagnostic_file_path)\n",
    "\n",
    "        with open(diagnostic_file_path, 'a') as f:\n",
    "            f.write('{:f}\\n'.format(loss))\n",
    "\n",
    "        # generate a diagnostic loss plot\n",
    "        with open(diagnostic_file_path) as f:\n",
    "            loss_values = [float(l.rstrip()) for l in f.readlines()]\n",
    "\n",
    "        diagnostic_plot_file_path = diagnostic_file_path[:-3] + 'png'\n",
    "        fig = plt.figure()\n",
    "        plt.plot(range(1, epoch_i + 2), loss_values, color='r')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('training loss')\n",
    "        fig.savefig(diagnostic_plot_file_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # save the model\n",
    "\n",
    "    model_file_name = _DBOW_MODEL_NAME.format(\n",
    "        data_file_name[:-4],\n",
    "        model_ver,\n",
    "        num_noise_words,\n",
    "        vec_dim,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        epoch_i + 1,\n",
    "        loss)\n",
    "    \n",
    "\n",
    "    model_file_path = join(MODELS_DIR, model_file_name)\n",
    "\n",
    "    if save_all:\n",
    "        torch.save(model_state, model_file_path)\n",
    "        return None\n",
    "    elif is_best_loss:\n",
    "        if prev_model_file_path is not None:\n",
    "            remove(prev_model_file_path)\n",
    "\n",
    "        torch.save(model_state, model_file_path)\n",
    "        return model_file_path\n",
    "    else:\n",
    "        return prev_model_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def start(data_file_name, model_file_name):\n",
    "    \"\"\"Saves trained paragraph vectors to a csv file in the *data* directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_file_name: str\n",
    "        Name of a file in the *data* directory that was used during training.\n",
    "    model_file_name: str\n",
    "        Name of a file in the *models* directory (a model trained on\n",
    "        the *data_file_name* dataset).\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(data_file_name)\n",
    "\n",
    "    vec_dim = int(re.search('_vecdim\\.(\\d+)_', model_file_name).group(1))\n",
    "\n",
    "    model = _load_model(\n",
    "        model_file_name,\n",
    "        vec_dim,\n",
    "        num_docs=len(dataset),\n",
    "        num_words=len(dataset.fields['text'].vocab) - 1)\n",
    "\n",
    "    _write_to_file(data_file_name, model_file_name, model, vec_dim)\n",
    "\n",
    "\n",
    "def _load_model(model_file_name, vec_dim, num_docs, num_words):\n",
    "    model_ver = re.search('_model\\.(dm|dbow)', model_file_name).group(1)\n",
    "    if model_ver is None:\n",
    "        raise ValueError(\"Model file name contains an invalid\"\n",
    "                         \"version of the model\")\n",
    "\n",
    "    model_file_path = join(MODELS_DIR, model_file_name)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(model_file_path)\n",
    "    except AssertionError:\n",
    "        checkpoint = torch.load(\n",
    "            model_file_path,\n",
    "            map_location=lambda storage, location: storage)\n",
    "\n",
    "    if model_ver == 'dbow':\n",
    "        model = DBOW(vec_dim, num_docs, num_words)\n",
    "    else:\n",
    "        model = DM(vec_dim, num_docs, num_words)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def _write_to_file(data_file_name, model_file_name, model, vec_dim):\n",
    "    result_lines = []\n",
    "\n",
    "    with open(join(DATA_DIR, data_file_name)) as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        for i, line in enumerate(reader):\n",
    "            # skip text\n",
    "            result_line = line[1:]\n",
    "            if i == 0:\n",
    "                # header line\n",
    "                result_line += [\"d{:d}\".format(x) for x in range(vec_dim)]\n",
    "            else:\n",
    "                vector = model.get_paragraph_vector(i - 1)\n",
    "                result_line += [str(x) for x in vector]\n",
    "\n",
    "            result_lines.append(result_line)\n",
    "\n",
    "    result_file_name = model_file_name[:-7] + 'csv'\n",
    "\n",
    "    with open(join(DATA_DIR, result_file_name), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(result_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _print_progress(epoch_i, batch_i, num_batches):\n",
    "    progress = round((batch_i + 1) / num_batches * 100)\n",
    "    print(\"\\rEpoch {:d}\".format(epoch_i + 1), end='')\n",
    "    stdout.write(\" - {:d}%\".format(progress))\n",
    "    stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset comprised of 485 documents.\n",
      "Vocabulary size is 6433.\n",
      "\n",
      "Training started.\n",
      "Epoch 1 - 100% 1% - 1% - 1% - 2% - 2% - 3% - 3% - 4% - 4% - 4% - 4% - 4% - 9% - 11% - 15% - 15% - 15% - 16% - 16% - 19% - 21% - 21% - 21% - 21% - 21% - 21% - 21% - 21% - 23% - 24% - 25% - 25% - 25% - 25% - 27% - 28% - 28% - 28% - 28% - 30% - 31% - 31% - 31% - 32% - 34% - 34% - 35% - 36% - 37% - 37% - 37% - 38% - 38% - 38% - 38% - 38% - 42% - 44% - 45% - 45% - 46% - 46% - 46% - 46% - 46% - 47% - 48% - 48% - 49% - 51% - 51% - 57% - 58% - 58% - 58% - 58% - 58% - 60% - 60% - 60% - 60% - 60% - 61% - 61% - 63% - 64% - 65% - 65% - 68% - 68% - 71% - 71% - 72% - 72% - 72% - 72% - 73% - 78% - 78% - 79% - 79% - 79% - 79% - 81% - 88% - 88% - 88% - 89% - 89% - 89% - 89% - 90% - 91% - 92% - 92% - 92% - 93% - 93% - 95% - 98% - 99% - 99% - 99% - 100% (80s) - loss: 1.3556\n",
      "Epoch 2 - 100% 3% - 4% - 4% - 6% - 7% - 10% - 12% - 14% - 15% - 17% - 17% - 17% - 18% - 18% - 18% - 18% - 19% - 21% - 22% - 22% - 22% - 22% - 23% - 24% - 24% - 24% - 24% - 25% - 26% - 26% - 26% - 26% - 28% - 28% - 29% - 29% - 29% - 29% - 29% - 29% - 30% - 40% - 40% - 40% - 42% - 43% - 45% - 45% - 49% - 50% - 50% - 50% - 51% - 51% - 51% - 51% - 51% - 52% - 53% - 53% - 55% - 55% - 55% - 55% - 55% - 56% - 56% - 56% - 57% - 57% - 57% - 57% - 57% - 57% - 58% - 59% - 60% - 62% - 63% - 70% - 75% - 75% - 75% - 75% - 76% - 76% - 76% - 80% - 80% - 81% - 89% - 91% - 94% - 95% - 97% - 97% - 98% - 98% (78s) - loss: 1.1701\n",
      "Epoch 3 - 100% 4% - 4% - 4% - 7% - 7% - 9% - 9% - 9% - 9% - 12% - 13% - 14% - 18% - 24% - 27% - 28% - 33% - 35% - 35% - 37% - 39% - 39% - 39% - 40% - 41% - 41% - 43% - 43% - 43% - 45% - 47% - 47% - 49% - 51% - 53% - 58% - 60% - 62% - 63% - 64% - 64% - 64% - 64% - 70% - 70% - 70% - 71% - 71% - 71% - 73% - 73% - 73% - 73% - 76% - 76% - 78% - 78% - 90% - 94% - 96% - 97% - 97% - 99% (78s) - loss: 1.0444\n",
      "Epoch 4 - 100% 2% - 2% - 4% - 5% - 13% - 14% - 14% - 15% - 16% - 16% - 23% - 23% - 23% - 25% - 25% - 25% - 28% - 29% - 29% - 29% - 29% - 29% - 29% - 29% - 30% - 31% - 31% - 32% - 32% - 32% - 33% - 36% - 37% - 37% - 39% - 39% - 45% - 48% - 50% - 52% - 53% - 53% - 54% - 56% - 56% - 56% - 57% - 58% - 58% - 59% - 59% - 59% - 59% - 59% - 59% - 59% - 60% - 62% - 63% - 63% - 65% - 65% - 65% - 68% - 68% - 71% - 72% - 72% - 73% - 74% - 75% - 75% - 79% - 79% - 79% - 81% - 81% - 81% - 81% - 85% - 86% - 86% - 86% - 86% - 86% - 87% - 90% - 90% - 90% - 90% - 90% - 90% - 91% - 91% - 92% - 94% - 94% - 94% - 94% - 94% - 94% - 94% - 94% - 96% - 98% - 98% - 99% - 100% - 100% (78s) - loss: 0.9639\n",
      "Epoch 5 - 100% 5% - 5% - 5% - 8% - 12% - 16% - 21% - 21% - 21% - 21% - 22% - 27% - 30% - 37% - 37% - 41% - 43% - 44% - 45% - 45% - 45% - 45% - 51% - 52% - 53% - 53% - 54% - 55% - 55% - 55% - 56% - 56% - 57% - 57% - 58% - 58% - 58% - 60% - 60% - 60% - 60% - 65% - 67% - 71% - 72% - 73% - 81% - 81% - 87% - 88% - 88% - 90% - 90% - 91% - 91% - 91% - 92% - 92% - 93% - 93% - 95% - 95% - 96% - 96% - 96% (78s) - loss: 0.9180\n",
      "Epoch 6 - 100% 1% - 2% - 2% - 2% - 2% - 4% - 4% - 7% - 8% - 8% - 10% - 10% - 10% - 10% - 10% - 10% - 10% - 11% - 12% - 12% - 13% - 13% - 16% - 17% - 18% - 18% - 18% - 19% - 19% - 23% - 24% - 24% - 24% - 24% - 26% - 29% - 30% - 39% - 41% - 42% - 42% - 42% - 43% - 43% - 44% - 44% - 45% - 45% - 45% - 45% - 45% - 45% - 45% - 45% - 45% - 46% - 47% - 48% - 48% - 50% - 51% - 51% - 52% - 57% - 58% - 59% - 64% - 65% - 69% - 69% - 70% - 73% - 73% - 74% - 74% - 74% - 76% - 78% - 78% - 82% - 85% - 87% - 87% - 88% - 91% - 92% - 92% - 92% - 93% - 98% - 99% (78s) - loss: 0.8887\n",
      "Epoch 7 - 100% 1% - 1% - 5% - 6% - 7% - 10% - 10% - 11% - 11% - 11% - 14% - 15% - 15% - 18% - 18% - 18% - 20% - 21% - 21% - 22% - 23% - 29% - 31% - 32% - 40% - 41% - 42% - 42% - 42% - 43% - 45% - 46% - 47% - 49% - 51% - 51% - 51% - 52% - 54% - 55% - 55% - 56% - 57% - 57% - 60% - 61% - 61% - 62% - 62% - 62% - 63% - 67% - 67% - 67% - 79% - 82% - 83% - 83% - 85% - 87% - 87% - 90% - 91% - 91% - 93% - 96% - 96% - 96% - 96% - 99% - 99% - 99% - 99% - 99% - 99% (78s) - loss: 0.8664\n",
      "Epoch 8 - 100% 1% - 1% - 1% - 2% - 4% - 5% - 5% - 5% - 6% - 7% - 9% - 9% - 9% - 9% - 9% - 10% - 11% - 11% - 12% - 12% - 14% - 14% - 14% - 15% - 15% - 16% - 16% - 19% - 20% - 20% - 23% - 25% - 25% - 25% - 26% - 26% - 27% - 27% - 27% - 27% - 27% - 28% - 28% - 28% - 29% - 29% - 30% - 32% - 33% - 34% - 34% - 34% - 35% - 35% - 39% - 39% - 42% - 47% - 47% - 47% - 47% - 47% - 47% - 52% - 54% - 56% - 58% - 60% - 61% - 62% - 62% - 62% - 63% - 63% - 63% - 64% - 66% - 66% - 67% - 67% - 67% - 67% - 67% - 67% - 68% - 68% - 75% - 75% - 75% - 76% - 76% - 76% - 76% - 77% - 77% - 77% - 79% - 79% - 79% - 80% - 83% - 84% - 85% - 87% - 88% - 89% - 90% - 91% - 91% - 91% - 92% - 92% - 93% - 94% - 95% - 96% - 99% (78s) - loss: 0.8529\n",
      "Epoch 9 - 100% 0% - 2% - 3% - 3% - 4% - 4% - 5% - 5% - 5% - 6% - 6% - 6% - 6% - 7% - 7% - 8% - 8% - 9% - 12% - 14% - 16% - 17% - 17% - 18% - 19% - 20% - 23% - 23% - 24% - 26% - 26% - 26% - 26% - 29% - 39% - 39% - 41% - 41% - 42% - 42% - 42% - 42% - 43% - 45% - 45% - 45% - 47% - 47% - 51% - 57% - 57% - 61% - 62% - 64% - 65% - 67% - 69% - 72% - 74% - 77% - 78% - 83% - 83% - 83% - 84% - 84% - 84% - 84% - 84% - 87% - 88% - 88% - 88% - 88% - 89% - 89% - 89% - 91% - 92% - 92% - 92% - 92% - 93% - 93% - 93% - 93% - 93% - 94% - 94% - 94% - 94% - 94% - 96% - 97% (78s) - loss: 0.8407\n",
      "Epoch 10 - 100% 1% - 2% - 5% - 7% - 9% - 9% - 11% - 11% - 12% - 15% - 16% - 16% - 16% - 17% - 17% - 17% - 17% - 19% - 19% - 20% - 20% - 22% - 22% - 23% - 23% - 26% - 26% - 26% - 26% - 26% - 28% - 29% - 29% - 29% - 29% - 31% - 31% - 32% - 32% - 32% - 32% - 32% - 34% - 34% - 34% - 38% - 38% - 39% - 42% - 42% - 43% - 44% - 45% - 46% - 47% - 50% - 50% - 50% - 50% - 50% - 50% - 52% - 52% - 53% - 56% - 56% - 56% - 58% - 58% - 62% - 64% - 67% - 67% - 68% - 68% - 68% - 68% - 68% - 68% - 68% - 68% - 69% - 73% - 74% - 74% - 76% - 76% - 78% - 79% - 80% - 86% - 86% - 87% - 88% - 88% - 88% - 89% - 89% - 94% (78s) - loss: 0.8337\n",
      "Epoch 11 - 100% 1% - 2% - 2% - 2% - 2% - 2% - 2% - 2% - 6% - 6% - 8% - 8% - 8% - 8% - 8% - 8% - 8% - 8% - 8% - 8% - 9% - 9% - 12% - 13% - 13% - 14% - 14% - 14% - 14% - 14% - 14% - 14% - 15% - 15% - 18% - 19% - 20% - 25% - 25% - 25% - 30% - 31% - 34% - 34% - 38% - 39% - 40% - 41% - 41% - 41% - 41% - 42% - 42% - 45% - 45% - 45% - 47% - 47% - 47% - 48% - 55% - 55% - 56% - 57% - 58% - 59% - 62% - 69% - 69% - 69% - 74% - 79% - 88% - 89% - 90% - 90% - 90% - 91% - 91% - 92% - 92% - 92% - 92% - 95% - 95% - 95% - 97% - 98% (78s) - loss: 0.8265\n",
      "Epoch 12 - 100% 2% - 2% - 5% - 5% - 5% - 5% - 7% - 8% - 9% - 9% - 9% - 9% - 9% - 11% - 11% - 13% - 14% - 14% - 15% - 19% - 20% - 20% - 20% - 20% - 20% - 21% - 21% - 21% - 21% - 28% - 28% - 28% - 30% - 30% - 31% - 32% - 36% - 37% - 37% - 41% - 42% - 42% - 42% - 44% - 44% - 47% - 48% - 50% - 51% - 51% - 52% - 52% - 53% - 53% - 53% - 54% - 57% - 57% - 59% - 60% - 63% - 63% - 64% - 64% - 66% - 69% - 70% - 70% - 70% - 72% - 73% - 75% - 76% - 76% - 77% - 78% - 79% - 79% - 79% - 79% - 79% - 79% - 79% - 80% - 80% - 80% - 81% - 83% - 83% - 85% - 86% - 86% - 87% - 90% - 90% - 90% - 91% - 91% - 92% - 94% - 94% - 94% - 95% - 96% - 97% - 97% - 97% - 97% - 97% - 98% - 99% (78s) - loss: 0.8189\n",
      "Epoch 13 - 100% 1% - 1% - 1% - 2% - 4% - 5% - 6% - 10% - 12% - 12% - 13% - 14% - 14% - 14% - 17% - 18% - 18% - 18% - 19% - 19% - 24% - 25% - 26% - 27% - 27% - 30% - 30% - 30% - 30% - 32% - 32% - 32% - 32% - 32% - 32% - 34% - 35% - 35% - 36% - 36% - 37% - 37% - 41% - 41% - 41% - 42% - 42% - 42% - 45% - 45% - 51% - 55% - 55% - 56% - 56% - 57% - 58% - 60% - 63% - 63% - 63% - 73% - 75% - 75% - 75% - 75% - 83% - 83% - 85% - 86% - 86% - 86% - 86% - 86% - 88% - 88% - 88% - 88% - 89% - 91% - 92% - 92% - 93% - 94% - 95% - 98% - 99% (78s) - loss: 0.8140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - 100% 0% - 2% - 4% - 4% - 4% - 4% - 5% - 5% - 5% - 5% - 5% - 6% - 7% - 8% - 11% - 11% - 11% - 12% - 12% - 12% - 15% - 15% - 15% - 15% - 19% - 23% - 26% - 28% - 28% - 28% - 34% - 34% - 35% - 37% - 37% - 39% - 40% - 40% - 41% - 41% - 41% - 41% - 41% - 41% - 46% - 47% - 47% - 48% - 49% - 50% - 52% - 55% - 56% - 56% - 56% - 57% - 58% - 58% - 60% - 60% - 60% - 60% - 60% - 60% - 63% - 63% - 63% - 65% - 66% - 66% - 67% - 67% - 67% - 68% - 68% - 68% - 69% - 70% - 71% - 71% - 74% - 75% - 76% - 77% - 78% - 78% - 78% - 80% - 80% - 80% - 81% - 82% - 82% - 82% - 83% - 83% - 83% - 83% - 84% - 84% - 85% - 85% - 85% - 86% - 87% - 88% - 90% - 90% - 92% - 93% - 93% - 93% - 93% - 94% - 94% - 94% - 95% - 95% - 95% - 95% - 95% - 95% - 96% - 96% - 96% - 96% - 97% - 99% - 99% (79s) - loss: 0.8093\n",
      "Epoch 15 - 100% 0% - 1% - 2% - 2% - 4% - 4% - 5% - 6% - 8% - 9% - 9% - 9% - 9% - 9% - 9% - 10% - 12% - 12% - 13% - 15% - 15% - 15% - 16% - 17% - 17% - 17% - 17% - 17% - 17% - 17% - 21% - 23% - 23% - 24% - 26% - 27% - 27% - 28% - 28% - 28% - 28% - 28% - 28% - 33% - 36% - 37% - 38% - 38% - 39% - 39% - 42% - 42% - 42% - 42% - 45% - 46% - 46% - 49% - 52% - 52% - 55% - 55% - 60% - 63% - 63% - 66% - 68% - 68% - 71% - 71% - 71% - 75% - 75% - 76% - 78% - 78% - 78% - 78% - 78% - 78% - 78% - 78% - 79% - 79% - 80% - 80% - 81% - 81% - 82% - 82% - 84% - 86% - 87% - 89% - 89% - 89% - 90% - 90% - 90% - 91% - 93% - 93% - 93% - 93% - 93% - 93% - 94% - 95% - 97% - 97% - 99% - 99% - 99% - 99% (78s) - loss: 0.8061\n",
      "Epoch 16 - 100% 2% - 2% - 3% - 3% - 4% - 4% - 4% - 5% - 5% - 6% - 8% - 11% - 14% - 15% - 15% - 18% - 18% - 19% - 29% - 31% - 33% - 33% - 33% - 33% - 34% - 52% - 52% - 55% - 55% - 56% - 56% - 56% - 56% - 57% - 59% - 59% - 61% - 62% - 63% - 63% - 63% - 64% - 64% - 64% - 66% - 67% - 68% - 68% - 69% - 72% - 80% - 80% - 80% - 80% - 81% - 81% - 82% - 82% - 83% - 86% - 88% - 88% - 88% - 88% - 96% - 96% - 96% - 97% - 97% (77s) - loss: 0.8004\n",
      "Epoch 17 - 100% 2% - 2% - 3% - 3% - 4% - 4% - 4% - 7% - 8% - 10% - 14% - 17% - 18% - 20% - 20% - 22% - 24% - 24% - 24% - 24% - 25% - 29% - 29% - 29% - 29% - 31% - 35% - 36% - 36% - 36% - 39% - 40% - 40% - 43% - 43% - 45% - 45% - 48% - 48% - 50% - 51% - 51% - 51% - 51% - 51% - 54% - 54% - 55% - 56% - 57% - 58% - 61% - 62% - 63% - 64% - 64% - 64% - 64% - 64% - 66% - 67% - 71% - 72% - 73% - 73% - 73% - 73% - 75% - 75% - 75% - 75% - 75% - 75% - 76% - 76% - 78% - 79% - 81% - 82% - 84% - 84% - 85% - 85% - 85% - 85% - 87% - 87% - 87% - 88% - 88% - 88% - 88% - 92% - 92% - 92% - 93% - 93% - 97% - 97% (78s) - loss: 0.7990\n",
      "Epoch 18 - 100% 1% - 1% - 1% - 1% - 1% - 1% - 6% - 10% - 10% - 10% - 10% - 10% - 17% - 18% - 21% - 24% - 26% - 26% - 27% - 27% - 27% - 29% - 29% - 31% - 31% - 31% - 32% - 34% - 34% - 34% - 35% - 35% - 37% - 39% - 41% - 41% - 42% - 42% - 42% - 42% - 42% - 42% - 43% - 43% - 43% - 45% - 45% - 49% - 49% - 51% - 52% - 52% - 52% - 53% - 58% - 61% - 62% - 64% - 65% - 67% - 67% - 68% - 73% - 77% - 77% - 77% - 77% - 77% - 77% - 81% - 87% - 88% - 90% - 94% - 94% - 94% - 95% - 95% - 95% - 98% (78s) - loss: 0.7964\n",
      "Epoch 19 - 100% 3% - 3% - 3% - 3% - 3% - 5% - 6% - 7% - 7% - 9% - 15% - 15% - 16% - 16% - 18% - 23% - 23% - 23% - 23% - 25% - 25% - 25% - 29% - 29% - 29% - 30% - 30% - 32% - 32% - 33% - 33% - 34% - 36% - 36% - 36% - 36% - 37% - 37% - 37% - 38% - 41% - 42% - 42% - 43% - 43% - 52% - 55% - 57% - 57% - 64% - 64% - 67% - 67% - 67% - 67% - 68% - 69% - 69% - 69% - 69% - 71% - 72% - 76% - 78% - 81% - 82% - 82% - 82% - 82% - 82% - 82% - 82% - 87% - 89% - 91% - 94% - 95% - 97% (78s) - loss: 0.7913\n",
      "Epoch 20 - 100% 1% - 1% - 1% - 1% - 1% - 3% - 4% - 4% - 5% - 6% - 6% - 7% - 7% - 10% - 12% - 12% - 12% - 14% - 14% - 14% - 15% - 15% - 16% - 18% - 19% - 20% - 21% - 22% - 23% - 23% - 23% - 23% - 24% - 25% - 25% - 25% - 26% - 27% - 27% - 27% - 29% - 29% - 29% - 30% - 31% - 31% - 32% - 32% - 33% - 33% - 33% - 35% - 36% - 36% - 37% - 38% - 38% - 39% - 39% - 39% - 39% - 39% - 41% - 41% - 42% - 42% - 43% - 44% - 44% - 45% - 48% - 48% - 48% - 49% - 49% - 49% - 49% - 50% - 50% - 50% - 51% - 51% - 52% - 53% - 53% - 53% - 53% - 53% - 55% - 55% - 55% - 55% - 56% - 56% - 56% - 57% - 58% - 58% - 59% - 60% - 60% - 60% - 60% - 61% - 61% - 61% - 61% - 62% - 62% - 64% - 65% - 66% - 66% - 66% - 66% - 67% - 67% - 68% - 68% - 71% - 72% - 74% - 74% - 74% - 74% - 80% - 80% - 85% - 87% - 87% - 87% - 90% - 90% - 91% - 92% - 93% - 94% - 96% - 96% - 100% - 100% (78s) - loss: 0.7889\n",
      "Epoch 21 - 100% 3% - 3% - 6% - 11% - 11% - 11% - 11% - 12% - 18% - 18% - 21% - 23% - 26% - 26% - 27% - 28% - 30% - 38% - 42% - 42% - 42% - 43% - 43% - 45% - 49% - 51% - 55% - 60% - 60% - 60% - 60% - 60% - 60% - 63% - 64% - 64% - 64% - 65% - 67% - 67% - 68% - 70% - 71% - 73% - 73% - 73% - 74% - 74% - 75% - 75% - 76% - 82% - 82% - 83% - 84% - 84% - 85% - 85% - 86% - 87% - 88% - 89% - 91% - 93% - 93% - 93% - 93% - 94% - 95% - 95% - 95% - 96% - 96% (78s) - loss: 0.7885\n",
      "Epoch 22 - 100% 1% - 2% - 7% - 7% - 11% - 13% - 13% - 13% - 13% - 13% - 13% - 14% - 14% - 14% - 18% - 18% - 19% - 20% - 22% - 22% - 22% - 26% - 28% - 28% - 28% - 28% - 29% - 29% - 29% - 30% - 30% - 30% - 30% - 32% - 37% - 39% - 39% - 39% - 39% - 42% - 44% - 44% - 44% - 46% - 47% - 47% - 48% - 48% - 50% - 50% - 53% - 53% - 54% - 54% - 54% - 54% - 55% - 55% - 56% - 56% - 56% - 58% - 59% - 59% - 61% - 62% - 62% - 63% - 64% - 65% - 65% - 65% - 65% - 67% - 67% - 67% - 67% - 67% - 70% - 73% - 74% - 74% - 74% - 74% - 75% - 76% - 77% - 77% - 77% - 77% - 78% - 81% - 83% - 86% - 87% - 87% - 87% - 87% - 88% - 89% - 89% - 89% - 89% - 89% - 89% - 92% - 93% - 93% - 98% - 98% - 99% (78s) - loss: 0.7857\n",
      "Epoch 23 - 100% 2% - 3% - 4% - 4% - 4% - 4% - 5% - 6% - 6% - 7% - 11% - 12% - 12% - 13% - 14% - 14% - 16% - 17% - 19% - 21% - 21% - 21% - 21% - 21% - 21% - 24% - 28% - 28% - 29% - 29% - 30% - 30% - 30% - 31% - 31% - 31% - 31% - 31% - 31% - 31% - 34% - 34% - 34% - 36% - 36% - 36% - 37% - 39% - 40% - 40% - 42% - 44% - 46% - 46% - 47% - 51% - 51% - 56% - 56% - 56% - 56% - 58% - 58% - 59% - 59% - 59% - 59% - 60% - 64% - 65% - 65% - 66% - 66% - 66% - 67% - 67% - 70% - 70% - 70% - 73% - 73% - 74% - 74% - 74% - 76% - 76% - 77% - 77% - 79% - 80% - 80% - 80% - 80% - 83% - 88% - 89% - 92% - 95% - 99% - 100% (78s) - loss: 0.7835\n",
      "Epoch 24 - 100% 1% - 1% - 1% - 1% - 3% - 3% - 3% - 4% - 4% - 4% - 4% - 5% - 6% - 7% - 9% - 10% - 12% - 12% - 12% - 12% - 12% - 12% - 14% - 15% - 15% - 16% - 16% - 16% - 16% - 17% - 18% - 23% - 25% - 25% - 25% - 29% - 34% - 35% - 35% - 35% - 35% - 37% - 42% - 42% - 43% - 43% - 45% - 46% - 46% - 47% - 47% - 49% - 50% - 50% - 50% - 50% - 50% - 50% - 50% - 51% - 52% - 53% - 53% - 54% - 56% - 56% - 56% - 58% - 58% - 58% - 59% - 60% - 60% - 63% - 64% - 64% - 65% - 66% - 66% - 68% - 68% - 69% - 70% - 71% - 76% - 77% - 78% - 79% - 79% - 80% - 80% - 81% - 81% - 81% - 81% - 81% - 82% - 82% - 82% - 82% - 83% - 83% - 84% - 85% - 88% - 88% - 89% - 89% - 90% - 90% - 90% - 91% - 91% - 92% - 93% - 93% - 93% - 93% - 94% - 94% - 99% (78s) - loss: 0.7792\n",
      "Epoch 25 - 100% 1% - 1% - 4% - 4% - 4% - 6% - 6% - 11% - 14% - 14% - 14% - 16% - 17% - 17% - 18% - 18% - 18% - 19% - 21% - 22% - 22% - 22% - 22% - 22% - 22% - 28% - 28% - 28% - 28% - 29% - 29% - 29% - 29% - 29% - 29% - 32% - 33% - 33% - 34% - 36% - 36% - 36% - 36% - 37% - 37% - 37% - 37% - 37% - 37% - 40% - 40% - 41% - 42% - 43% - 45% - 45% - 47% - 50% - 55% - 58% - 59% - 65% - 66% - 66% - 66% - 67% - 67% - 67% - 67% - 68% - 69% - 70% - 70% - 72% - 72% - 72% - 72% - 72% - 72% - 73% - 74% - 74% - 75% - 75% - 75% - 79% - 79% - 80% - 81% - 81% - 81% - 81% - 81% - 81% - 82% - 82% - 83% - 85% - 85% - 88% - 89% - 90% - 90% - 90% - 90% - 90% - 90% - 90% - 91% - 93% - 94% - 95% - 95% - 95% - 95% - 100% - 100% - 100% (78s) - loss: 0.7810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - 100% 2% - 2% - 3% - 3% - 3% - 3% - 5% - 7% - 7% - 7% - 8% - 8% - 9% - 10% - 11% - 11% - 11% - 12% - 12% - 12% - 13% - 13% - 15% - 16% - 18% - 18% - 18% - 18% - 21% - 21% - 22% - 22% - 23% - 23% - 24% - 26% - 30% - 30% - 33% - 34% - 34% - 35% - 36% - 38% - 39% - 39% - 40% - 42% - 42% - 42% - 44% - 46% - 46% - 46% - 46% - 46% - 46% - 47% - 48% - 56% - 56% - 56% - 57% - 57% - 57% - 57% - 60% - 61% - 61% - 62% - 62% - 62% - 63% - 63% - 63% - 64% - 64% - 65% - 65% - 65% - 65% - 66% - 66% - 68% - 68% - 68% - 70% - 72% - 72% - 74% - 74% - 74% - 74% - 75% - 80% - 80% - 80% - 81% - 81% - 82% - 82% - 82% - 83% - 84% - 86% - 87% - 87% - 88% - 88% - 89% - 89% - 90% - 90% - 90% - 91% - 91% - 92% - 92% - 92% - 93% - 95% - 95% - 97% - 98% (79s) - loss: 0.7788\n",
      "Epoch 27 - 100% 0% - 0% - 1% - 1% - 1% - 2% - 2% - 3% - 3% - 3% - 3% - 4% - 4% - 4% - 9% - 9% - 10% - 10% - 11% - 12% - 12% - 12% - 13% - 17% - 17% - 17% - 19% - 20% - 21% - 22% - 22% - 22% - 22% - 22% - 23% - 23% - 26% - 27% - 27% - 29% - 29% - 29% - 34% - 34% - 34% - 36% - 36% - 36% - 37% - 37% - 38% - 38% - 39% - 39% - 40% - 43% - 43% - 43% - 43% - 44% - 44% - 46% - 46% - 47% - 48% - 49% - 49% - 49% - 50% - 50% - 52% - 54% - 55% - 56% - 56% - 60% - 61% - 61% - 61% - 61% - 61% - 63% - 63% - 64% - 64% - 65% - 67% - 72% - 79% - 81% - 81% - 81% - 82% - 82% - 83% - 85% - 85% - 85% - 86% - 86% - 87% - 87% - 91% - 94% - 94% - 94% - 95% - 97% - 97% - 99% (78s) - loss: 0.7771\n",
      "Epoch 28 - 100% 0% - 0% - 5% - 6% - 6% - 9% - 10% - 10% - 13% - 14% - 17% - 17% - 20% - 20% - 20% - 20% - 22% - 22% - 23% - 23% - 27% - 28% - 28% - 31% - 31% - 31% - 32% - 34% - 34% - 34% - 37% - 37% - 37% - 37% - 37% - 38% - 40% - 40% - 41% - 42% - 43% - 47% - 52% - 52% - 53% - 53% - 62% - 62% - 62% - 62% - 62% - 62% - 66% - 67% - 68% - 70% - 73% - 74% - 75% - 75% - 76% - 80% - 80% - 81% - 81% - 84% - 86% - 89% - 90% - 91% - 91% - 91% - 92% - 92% - 92% - 92% - 92% - 96% - 96% - 97% - 98% - 98% - 98% - 99% (78s) - loss: 0.7756\n",
      "Epoch 29 - 100% 3% - 5% - 5% - 6% - 6% - 6% - 10% - 12% - 15% - 19% - 19% - 19% - 21% - 21% - 23% - 24% - 26% - 28% - 28% - 28% - 28% - 29% - 31% - 32% - 32% - 32% - 33% - 33% - 33% - 38% - 38% - 38% - 38% - 40% - 41% - 41% - 42% - 45% - 47% - 47% - 47% - 47% - 59% - 61% - 61% - 61% - 61% - 67% - 68% - 69% - 70% - 71% - 72% - 72% - 72% - 74% - 75% - 78% - 78% - 79% - 79% - 80% - 81% - 85% - 86% - 88% - 88% - 88% - 88% - 88% - 88% - 90% - 90% - 91% - 95% - 96% - 96% - 96% - 96% - 97% - 98% - 98% - 99% (78s) - loss: 0.7734\n",
      "Epoch 30 - 100% 0% - 0% - 1% - 2% - 2% - 4% - 6% - 13% - 16% - 16% - 18% - 20% - 20% - 21% - 21% - 22% - 22% - 23% - 23% - 23% - 29% - 30% - 31% - 31% - 31% - 31% - 31% - 32% - 34% - 34% - 35% - 36% - 36% - 38% - 38% - 38% - 39% - 39% - 40% - 41% - 43% - 43% - 43% - 43% - 45% - 45% - 45% - 46% - 46% - 48% - 49% - 49% - 51% - 52% - 52% - 52% - 52% - 53% - 53% - 53% - 53% - 53% - 53% - 53% - 53% - 53% - 53% - 54% - 54% - 54% - 54% - 55% - 55% - 55% - 55% - 57% - 57% - 58% - 59% - 60% - 62% - 62% - 62% - 65% - 65% - 65% - 65% - 65% - 71% - 75% - 75% - 75% - 76% - 77% - 81% - 87% - 87% - 88% - 88% - 88% - 88% - 90% - 90% - 95% - 95% - 95% - 99% - 99% - 100% (78s) - loss: 0.7739\n",
      "Epoch 31 - 100% 9% - 11% - 11% - 12% - 12% - 14% - 15% - 16% - 16% - 17% - 18% - 19% - 19% - 19% - 19% - 20% - 20% - 21% - 21% - 21% - 22% - 23% - 23% - 24% - 24% - 27% - 27% - 28% - 29% - 29% - 29% - 30% - 30% - 31% - 31% - 32% - 33% - 33% - 34% - 35% - 41% - 41% - 45% - 45% - 47% - 48% - 48% - 50% - 50% - 50% - 53% - 55% - 57% - 57% - 58% - 59% - 59% - 65% - 66% - 66% - 66% - 66% - 66% - 66% - 68% - 71% - 71% - 72% - 73% - 73% - 74% - 75% - 75% - 79% - 80% - 81% - 83% - 83% - 87% - 88% - 89% - 90% - 94% - 94% - 95% - 96% - 97% - 99% (78s) - loss: 0.7735\n",
      "Epoch 32 - 100% 2% - 2% - 2% - 3% - 3% - 5% - 5% - 5% - 7% - 8% - 9% - 10% - 11% - 11% - 11% - 12% - 12% - 13% - 18% - 19% - 20% - 21% - 21% - 22% - 22% - 22% - 24% - 24% - 24% - 24% - 24% - 24% - 25% - 26% - 27% - 30% - 31% - 31% - 32% - 32% - 32% - 32% - 32% - 34% - 36% - 36% - 36% - 36% - 36% - 37% - 37% - 38% - 38% - 41% - 41% - 41% - 41% - 41% - 43% - 43% - 54% - 56% - 56% - 59% - 68% - 70% - 70% - 71% - 72% - 72% - 73% - 74% - 74% - 75% - 75% - 79% - 81% - 82% - 83% - 87% - 88% - 90% - 95% - 96% (78s) - loss: 0.7705\n",
      "Epoch 33 - 100% 1% - 1% - 2% - 3% - 6% - 6% - 6% - 6% - 6% - 7% - 9% - 20% - 20% - 20% - 22% - 22% - 22% - 22% - 25% - 29% - 29% - 34% - 34% - 35% - 38% - 40% - 40% - 41% - 41% - 41% - 45% - 45% - 46% - 48% - 50% - 52% - 52% - 55% - 55% - 55% - 58% - 58% - 61% - 63% - 64% - 64% - 64% - 64% - 70% - 70% - 73% - 73% - 73% - 78% - 78% - 78% - 79% - 79% - 79% - 79% - 79% - 84% - 87% - 89% - 91% - 91% - 91% - 92% - 92% (78s) - loss: 0.7695\n",
      "Epoch 34 - 100% 6% - 7% - 7% - 14% - 18% - 23% - 26% - 26% - 26% - 29% - 29% - 30% - 30% - 30% - 32% - 33% - 33% - 33% - 37% - 38% - 38% - 42% - 44% - 45% - 45% - 46% - 48% - 49% - 51% - 53% - 53% - 56% - 57% - 58% - 58% - 58% - 58% - 59% - 59% - 59% - 59% - 59% - 64% - 64% - 65% - 65% - 66% - 66% - 66% - 67% - 69% - 74% - 74% - 74% - 75% - 75% - 75% - 76% - 76% - 76% - 79% - 84% - 84% - 84% - 84% - 84% - 85% - 87% - 89% - 89% - 90% - 92% - 92% - 92% - 93% - 93% - 94% - 95% - 95% - 95% - 95% - 95% - 95% - 95% - 97% - 99% (78s) - loss: 0.7704\n",
      "Epoch 35 - 100% 4% - 4% - 6% - 6% - 7% - 7% - 9% - 15% - 15% - 15% - 16% - 16% - 21% - 21% - 21% - 21% - 22% - 23% - 24% - 25% - 26% - 26% - 26% - 26% - 28% - 29% - 29% - 29% - 29% - 30% - 31% - 31% - 31% - 31% - 35% - 35% - 38% - 38% - 40% - 50% - 54% - 58% - 58% - 58% - 60% - 61% - 61% - 64% - 64% - 67% - 68% - 73% - 77% - 78% - 78% - 78% - 78% - 78% - 78% - 79% - 79% - 80% - 84% - 85% - 85% - 85% - 87% - 87% - 91% - 91% - 91% - 93% - 93% - 97% - 97% - 97% - 97% - 99% (78s) - loss: 0.7688\n",
      "Epoch 36 - 100% 2% - 2% - 2% - 6% - 6% - 8% - 8% - 8% - 8% - 9% - 9% - 9% - 9% - 10% - 10% - 11% - 12% - 12% - 12% - 13% - 13% - 15% - 17% - 17% - 17% - 17% - 17% - 18% - 19% - 20% - 20% - 20% - 21% - 21% - 21% - 21% - 21% - 21% - 22% - 22% - 25% - 25% - 25% - 27% - 27% - 29% - 32% - 32% - 32% - 33% - 34% - 34% - 36% - 36% - 37% - 37% - 37% - 41% - 42% - 42% - 43% - 44% - 45% - 45% - 45% - 47% - 48% - 49% - 50% - 50% - 51% - 51% - 51% - 52% - 56% - 56% - 56% - 56% - 57% - 57% - 62% - 65% - 67% - 68% - 68% - 70% - 71% - 72% - 74% - 79% - 79% - 79% - 81% - 81% - 82% - 82% - 82% - 82% - 83% - 83% - 84% - 84% - 85% - 87% - 87% - 87% - 87% - 87% - 87% - 90% - 93% - 93% - 93% - 94% - 96% - 96% - 97% - 97% - 98% - 99% (78s) - loss: 0.7687\n",
      "Epoch 37 - 100% 3% - 3% - 3% - 4% - 4% - 7% - 8% - 10% - 13% - 13% - 15% - 15% - 17% - 17% - 18% - 18% - 20% - 23% - 27% - 27% - 27% - 27% - 27% - 27% - 27% - 30% - 32% - 32% - 32% - 32% - 32% - 33% - 36% - 39% - 39% - 39% - 39% - 41% - 43% - 44% - 45% - 45% - 46% - 46% - 46% - 48% - 48% - 48% - 49% - 51% - 52% - 54% - 57% - 57% - 57% - 58% - 58% - 58% - 58% - 59% - 60% - 60% - 61% - 61% - 65% - 70% - 70% - 71% - 72% - 73% - 73% - 74% - 74% - 74% - 76% - 76% - 78% - 80% - 80% - 80% - 83% - 88% - 88% - 92% - 92% - 92% - 93% - 94% - 94% - 95% - 95% - 97% - 99% (78s) - loss: 0.7676\n",
      "Epoch 38 - 100% 4% - 5% - 5% - 5% - 5% - 5% - 6% - 6% - 7% - 7% - 8% - 9% - 10% - 10% - 10% - 11% - 12% - 14% - 14% - 14% - 16% - 17% - 17% - 18% - 18% - 19% - 30% - 31% - 31% - 31% - 31% - 31% - 31% - 31% - 35% - 36% - 36% - 38% - 41% - 41% - 43% - 45% - 46% - 48% - 52% - 52% - 53% - 54% - 54% - 54% - 54% - 54% - 55% - 55% - 55% - 55% - 56% - 57% - 57% - 57% - 61% - 61% - 61% - 62% - 62% - 62% - 63% - 65% - 65% - 66% - 66% - 68% - 68% - 68% - 71% - 72% - 79% - 81% - 82% - 82% - 83% - 83% - 83% - 83% - 84% - 88% - 90% - 90% - 90% - 92% - 94% - 95% - 95% - 96% - 96% (78s) - loss: 0.7659\n",
      "Epoch 39 - 100% 3% - 4% - 12% - 16% - 16% - 16% - 16% - 16% - 16% - 16% - 18% - 20% - 21% - 21% - 21% - 26% - 33% - 34% - 35% - 35% - 37% - 41% - 41% - 43% - 43% - 47% - 48% - 50% - 54% - 54% - 56% - 59% - 62% - 62% - 63% - 63% - 63% - 64% - 64% - 64% - 64% - 64% - 65% - 65% - 66% - 67% - 72% - 74% - 77% - 77% - 81% - 82% - 88% - 88% - 88% - 88% - 93% - 98% - 99% - 100% (77s) - loss: 0.7659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - 100% 5% - 5% - 5% - 5% - 8% - 8% - 10% - 10% - 10% - 11% - 12% - 12% - 13% - 14% - 15% - 15% - 16% - 16% - 18% - 19% - 20% - 21% - 23% - 26% - 29% - 31% - 33% - 33% - 34% - 34% - 34% - 34% - 35% - 35% - 36% - 36% - 38% - 39% - 44% - 52% - 52% - 53% - 53% - 54% - 55% - 55% - 55% - 55% - 55% - 56% - 57% - 57% - 57% - 57% - 58% - 59% - 59% - 60% - 64% - 64% - 65% - 67% - 67% - 70% - 70% - 71% - 71% - 71% - 71% - 72% - 72% - 72% - 74% - 79% - 80% - 80% - 83% - 83% - 84% - 84% - 84% - 84% - 88% - 88% - 91% - 91% - 93% - 97% - 98% (78s) - loss: 0.7657\n",
      "Epoch 41 - 100% 2% - 3% - 5% - 5% - 5% - 6% - 6% - 6% - 6% - 7% - 7% - 7% - 7% - 12% - 15% - 17% - 17% - 17% - 19% - 29% - 29% - 30% - 35% - 35% - 35% - 38% - 39% - 40% - 40% - 40% - 42% - 44% - 45% - 47% - 55% - 58% - 58% - 59% - 60% - 61% - 62% - 62% - 62% - 63% - 64% - 65% - 65% - 71% - 73% - 76% - 78% - 79% - 80% - 80% - 86% - 87% - 87% - 88% - 88% - 88% - 91% - 91% - 91% - 92% - 93% - 93% - 98% - 98% - 98% - 98% (78s) - loss: 0.7638\n",
      "Epoch 42 - 100% 4% - 5% - 5% - 7% - 8% - 10% - 10% - 11% - 11% - 12% - 12% - 12% - 13% - 15% - 21% - 22% - 22% - 23% - 24% - 26% - 29% - 31% - 31% - 31% - 32% - 33% - 34% - 35% - 35% - 36% - 36% - 36% - 38% - 38% - 41% - 41% - 42% - 42% - 42% - 44% - 45% - 45% - 46% - 47% - 47% - 47% - 48% - 49% - 49% - 49% - 49% - 51% - 51% - 52% - 53% - 54% - 54% - 54% - 54% - 54% - 54% - 55% - 55% - 55% - 56% - 59% - 60% - 61% - 62% - 62% - 63% - 64% - 64% - 65% - 65% - 65% - 65% - 66% - 67% - 67% - 67% - 68% - 68% - 68% - 69% - 74% - 75% - 75% - 75% - 76% - 76% - 76% - 76% - 76% - 79% - 79% - 79% - 81% - 87% - 88% - 88% - 89% - 91% - 91% - 91% - 91% - 91% - 92% - 94% - 94% - 94% - 94% - 96% - 97% (79s) - loss: 0.7647\n",
      "Epoch 43 - 100% 1% - 3% - 6% - 9% - 9% - 9% - 9% - 10% - 10% - 13% - 13% - 17% - 18% - 18% - 18% - 18% - 18% - 18% - 19% - 23% - 23% - 24% - 25% - 26% - 27% - 27% - 27% - 27% - 28% - 30% - 30% - 30% - 30% - 30% - 31% - 31% - 31% - 32% - 33% - 34% - 34% - 35% - 35% - 36% - 38% - 38% - 39% - 39% - 40% - 40% - 40% - 40% - 41% - 42% - 42% - 43% - 45% - 46% - 51% - 51% - 51% - 52% - 52% - 54% - 56% - 57% - 60% - 62% - 66% - 67% - 67% - 67% - 68% - 72% - 75% - 77% - 79% - 80% - 80% - 83% - 84% - 84% - 85% - 85% - 86% - 86% - 95% - 95% - 95% - 95% - 96% - 96% - 96% - 96% - 96% - 96% - 96% - 100% (78s) - loss: 0.7632\n",
      "Epoch 44 - 100% 2% - 5% - 5% - 7% - 7% - 9% - 10% - 11% - 14% - 14% - 14% - 15% - 15% - 15% - 20% - 21% - 21% - 21% - 21% - 22% - 22% - 24% - 24% - 25% - 29% - 29% - 29% - 32% - 32% - 33% - 34% - 34% - 35% - 36% - 36% - 37% - 40% - 40% - 41% - 41% - 44% - 44% - 45% - 45% - 45% - 45% - 45% - 45% - 50% - 50% - 50% - 50% - 52% - 54% - 54% - 56% - 56% - 57% - 57% - 60% - 60% - 60% - 60% - 60% - 60% - 60% - 60% - 61% - 64% - 66% - 66% - 66% - 67% - 71% - 73% - 75% - 77% - 80% - 80% - 80% - 82% - 82% - 86% - 87% - 95% - 95% - 95% - 96% - 97% - 98% - 98% - 98% - 98% (78s) - loss: 0.7618\n",
      "Epoch 45 - 100% 1% - 1% - 3% - 11% - 13% - 14% - 14% - 20% - 21% - 21% - 23% - 25% - 27% - 29% - 29% - 37% - 38% - 39% - 40% - 40% - 40% - 43% - 43% - 43% - 43% - 44% - 45% - 45% - 45% - 46% - 48% - 50% - 55% - 56% - 56% - 60% - 60% - 63% - 63% - 65% - 70% - 70% - 70% - 70% - 73% - 74% - 74% - 76% - 76% - 85% - 90% - 91% - 94% - 95% - 96% - 96% - 96% - 96% - 96% - 99% (78s) - loss: 0.7616\n",
      "Epoch 46 - 100% 0% - 1% - 3% - 3% - 3% - 3% - 4% - 4% - 4% - 4% - 9% - 9% - 9% - 13% - 13% - 13% - 13% - 15% - 18% - 19% - 19% - 19% - 19% - 19% - 19% - 21% - 21% - 21% - 22% - 22% - 22% - 25% - 26% - 26% - 29% - 29% - 32% - 33% - 33% - 37% - 39% - 40% - 40% - 42% - 42% - 43% - 43% - 43% - 45% - 45% - 46% - 46% - 46% - 46% - 46% - 46% - 46% - 50% - 51% - 51% - 51% - 53% - 53% - 54% - 55% - 59% - 59% - 65% - 70% - 74% - 74% - 75% - 75% - 77% - 78% - 78% - 78% - 80% - 81% - 81% - 81% - 81% - 81% - 81% - 81% - 82% - 83% - 84% - 84% - 85% - 85% - 85% - 85% - 86% - 86% - 86% - 87% - 87% - 88% - 90% - 90% - 91% - 92% - 93% - 93% - 93% - 93% - 94% - 95% - 95% - 95% (78s) - loss: 0.7621\n",
      "Epoch 47 - 100% 1% - 2% - 4% - 4% - 4% - 5% - 5% - 5% - 5% - 7% - 8% - 8% - 8% - 8% - 14% - 16% - 16% - 17% - 17% - 17% - 17% - 17% - 17% - 17% - 18% - 18% - 18% - 18% - 18% - 18% - 21% - 24% - 24% - 24% - 25% - 25% - 25% - 25% - 25% - 26% - 26% - 27% - 27% - 27% - 27% - 28% - 28% - 28% - 30% - 31% - 31% - 31% - 33% - 33% - 33% - 35% - 36% - 36% - 39% - 39% - 40% - 41% - 41% - 42% - 44% - 45% - 45% - 45% - 45% - 45% - 45% - 47% - 47% - 47% - 47% - 49% - 49% - 51% - 53% - 56% - 56% - 59% - 60% - 63% - 63% - 64% - 64% - 66% - 66% - 66% - 67% - 67% - 68% - 69% - 71% - 71% - 73% - 75% - 75% - 75% - 77% - 78% - 80% - 80% - 80% - 80% - 81% - 81% - 81% - 83% - 83% - 83% - 87% - 87% - 87% - 90% - 91% - 91% - 93% - 94% - 94% - 94% - 99% (78s) - loss: 0.7621\n",
      "Epoch 48 - 100% 0% - 1% - 1% - 1% - 2% - 2% - 2% - 2% - 3% - 7% - 7% - 10% - 11% - 11% - 11% - 14% - 14% - 16% - 16% - 16% - 16% - 17% - 18% - 18% - 22% - 23% - 23% - 23% - 23% - 30% - 30% - 30% - 30% - 30% - 40% - 42% - 42% - 42% - 47% - 51% - 52% - 52% - 52% - 56% - 56% - 56% - 56% - 57% - 57% - 59% - 60% - 60% - 61% - 62% - 63% - 63% - 65% - 68% - 68% - 69% - 69% - 73% - 73% - 74% - 75% - 75% - 76% - 77% - 78% - 78% - 79% - 79% - 83% - 83% - 83% - 83% - 83% - 84% - 84% - 84% - 86% - 86% - 86% - 87% - 88% - 88% - 89% - 89% - 89% - 89% - 90% - 90% - 96% - 98% - 99% - 99% - 99% - 99% - 99% (78s) - loss: 0.7605\n",
      "Epoch 49 - 100% 1% - 2% - 2% - 2% - 2% - 3% - 3% - 3% - 7% - 8% - 9% - 15% - 20% - 26% - 26% - 27% - 36% - 40% - 42% - 45% - 45% - 48% - 48% - 48% - 48% - 49% - 49% - 49% - 49% - 49% - 52% - 54% - 55% - 55% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 56% - 57% - 57% - 57% - 57% - 58% - 59% - 59% - 59% - 61% - 64% - 64% - 64% - 68% - 68% - 68% - 68% - 68% - 68% - 69% - 69% - 70% - 70% - 74% - 75% - 76% - 77% - 81% - 86% - 86% - 87% - 87% - 88% - 88% - 95% - 96% - 98% - 99% - 100% (78s) - loss: 0.7610\n",
      "Epoch 50 - 100% 3% - 5% - 11% - 14% - 15% - 15% - 20% - 24% - 24% - 26% - 28% - 28% - 28% - 28% - 28% - 28% - 34% - 34% - 34% - 34% - 34% - 34% - 38% - 38% - 38% - 43% - 43% - 43% - 47% - 47% - 47% - 51% - 53% - 54% - 56% - 57% - 59% - 60% - 60% - 64% - 65% - 65% - 66% - 68% - 69% - 70% - 70% - 71% - 71% - 71% - 71% - 72% - 74% - 74% - 74% - 85% - 85% - 85% - 85% - 85% - 89% - 89% - 90% - 90% - 90% - 90% - 90% - 91% - 91% - 92% - 99% - 99% - 100% (77s) - loss: 0.7605\n",
      "Epoch 51 - 100% 5% - 10% - 12% - 14% - 16% - 18% - 18% - 20% - 21% - 21% - 22% - 23% - 23% - 23% - 24% - 24% - 24% - 25% - 26% - 26% - 26% - 26% - 26% - 27% - 27% - 28% - 29% - 32% - 32% - 35% - 36% - 39% - 40% - 41% - 41% - 41% - 41% - 41% - 44% - 44% - 44% - 44% - 45% - 47% - 48% - 49% - 56% - 58% - 58% - 58% - 58% - 60% - 63% - 65% - 67% - 67% - 77% - 78% - 78% - 79% - 79% - 81% - 83% - 85% - 85% - 86% - 86% - 87% - 87% - 87% - 88% - 88% - 88% - 88% - 88% - 88% - 90% - 91% - 91% - 93% - 94% - 94% - 94% - 94% - 98% - 99% - 99% - 100% (78s) - loss: 0.7595\n",
      "Epoch 52 - 100% 0% - 3% - 3% - 4% - 5% - 18% - 20% - 24% - 26% - 27% - 27% - 28% - 28% - 30% - 31% - 34% - 37% - 37% - 37% - 38% - 41% - 42% - 42% - 44% - 44% - 48% - 50% - 51% - 52% - 52% - 53% - 54% - 54% - 54% - 55% - 55% - 55% - 60% - 60% - 60% - 60% - 62% - 63% - 63% - 63% - 64% - 64% - 65% - 65% - 65% - 66% - 67% - 69% - 70% - 70% - 70% - 71% - 71% - 71% - 71% - 72% - 72% - 73% - 74% - 75% - 75% - 75% - 76% - 78% - 78% - 78% - 78% - 78% - 78% - 79% - 79% - 79% - 79% - 79% - 81% - 82% - 82% - 83% - 84% - 84% - 88% - 91% - 92% - 92% - 93% - 93% - 93% - 94% - 98% - 98% - 98% - 99% - 99% (78s) - loss: 0.7582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 - 100% 1% - 1% - 2% - 5% - 7% - 9% - 10% - 15% - 16% - 16% - 16% - 19% - 21% - 22% - 23% - 23% - 24% - 24% - 24% - 24% - 25% - 26% - 26% - 26% - 27% - 27% - 28% - 28% - 28% - 29% - 30% - 30% - 32% - 33% - 33% - 33% - 34% - 36% - 37% - 37% - 38% - 39% - 40% - 40% - 41% - 45% - 45% - 46% - 46% - 46% - 47% - 47% - 47% - 48% - 49% - 52% - 54% - 55% - 55% - 56% - 58% - 60% - 60% - 60% - 61% - 62% - 62% - 62% - 62% - 63% - 63% - 64% - 64% - 65% - 67% - 67% - 67% - 68% - 68% - 69% - 75% - 75% - 75% - 77% - 77% - 77% - 78% - 78% - 78% - 78% - 78% - 78% - 79% - 79% - 80% - 80% - 81% - 83% - 83% - 83% - 84% - 84% - 84% - 84% - 86% - 86% - 88% - 88% - 89% - 91% - 91% - 91% - 94% - 94% - 94% - 94% - 96% - 97% - 97% - 97% - 97% - 99% - 99% - 99% - 99% (78s) - loss: 0.7569\n",
      "Epoch 54 - 100% 1% - 2% - 2% - 5% - 5% - 7% - 7% - 7% - 7% - 9% - 9% - 9% - 9% - 9% - 9% - 10% - 10% - 11% - 11% - 14% - 14% - 14% - 14% - 15% - 18% - 21% - 24% - 24% - 24% - 24% - 26% - 26% - 26% - 28% - 28% - 29% - 30% - 31% - 33% - 34% - 38% - 38% - 41% - 41% - 41% - 41% - 41% - 41% - 41% - 43% - 44% - 47% - 48% - 48% - 50% - 50% - 50% - 50% - 50% - 50% - 52% - 52% - 54% - 56% - 57% - 59% - 59% - 60% - 61% - 61% - 61% - 61% - 64% - 65% - 66% - 69% - 69% - 70% - 70% - 71% - 72% - 73% - 75% - 75% - 76% - 77% - 77% - 79% - 79% - 79% - 86% - 87% - 89% - 90% - 91% - 93% - 95% - 95% - 95% - 96% - 97% - 97% - 97% - 97% (78s) - loss: 0.7579\n",
      "Epoch 55 - 100% 1% - 1% - 2% - 3% - 3% - 3% - 3% - 4% - 4% - 8% - 8% - 8% - 16% - 16% - 16% - 16% - 18% - 21% - 21% - 22% - 22% - 23% - 23% - 27% - 27% - 35% - 35% - 39% - 39% - 44% - 49% - 49% - 51% - 51% - 52% - 53% - 54% - 58% - 58% - 58% - 58% - 60% - 62% - 62% - 62% - 62% - 62% - 63% - 63% - 67% - 68% - 68% - 68% - 68% - 70% - 70% - 70% - 71% - 72% - 75% - 76% - 77% - 79% - 82% - 83% - 83% - 83% - 83% - 83% - 85% - 86% - 86% - 88% - 88% - 88% - 90% - 93% - 93% - 94% - 94% - 96% - 96% - 96% - 99% (77s) - loss: 0.7565\n",
      "Epoch 56 - 100% 3% - 3% - 4% - 11% - 15% - 15% - 16% - 21% - 22% - 22% - 22% - 22% - 22% - 22% - 22% - 22% - 22% - 24% - 28% - 32% - 32% - 34% - 38% - 38% - 38% - 38% - 38% - 38% - 41% - 43% - 43% - 45% - 46% - 46% - 46% - 46% - 46% - 49% - 50% - 50% - 50% - 50% - 50% - 50% - 51% - 53% - 53% - 58% - 58% - 59% - 61% - 61% - 61% - 61% - 62% - 64% - 65% - 65% - 67% - 68% - 68% - 70% - 72% - 72% - 73% - 73% - 73% - 73% - 73% - 74% - 76% - 76% - 76% - 79% - 79% - 79% - 81% - 81% - 81% - 81% - 82% - 82% - 82% - 82% - 82% - 82% - 86% - 86% - 86% - 86% - 86% - 86% - 89% - 89% - 91% - 93% - 93% - 93% - 96% - 96% - 97% - 98% - 98% - 98% - 99% (78s) - loss: 0.7567\n",
      "Epoch 57 - 100% 0% - 1% - 1% - 1% - 1% - 1% - 4% - 7% - 8% - 8% - 10% - 10% - 12% - 12% - 13% - 17% - 18% - 19% - 19% - 19% - 19% - 19% - 20% - 22% - 23% - 23% - 24% - 25% - 25% - 25% - 25% - 25% - 31% - 33% - 37% - 37% - 37% - 37% - 37% - 38% - 41% - 41% - 41% - 43% - 43% - 48% - 48% - 49% - 50% - 50% - 51% - 52% - 56% - 56% - 60% - 60% - 60% - 61% - 62% - 62% - 62% - 63% - 68% - 68% - 68% - 68% - 68% - 68% - 69% - 70% - 70% - 73% - 74% - 74% - 74% - 74% - 77% - 77% - 78% - 78% - 78% - 79% - 79% - 79% - 80% - 80% - 82% - 82% - 85% - 89% - 89% - 89% - 89% - 89% - 90% - 98% - 99% - 99% (78s) - loss: 0.7589\n",
      "Epoch 58 - 100% 1% - 1% - 2% - 2% - 3% - 3% - 4% - 4% - 4% - 5% - 5% - 5% - 6% - 8% - 10% - 10% - 11% - 11% - 11% - 12% - 12% - 12% - 12% - 12% - 13% - 14% - 14% - 14% - 14% - 14% - 14% - 14% - 15% - 15% - 16% - 16% - 16% - 16% - 16% - 16% - 18% - 19% - 21% - 21% - 21% - 21% - 23% - 24% - 24% - 24% - 25% - 26% - 27% - 28% - 30% - 31% - 31% - 32% - 32% - 34% - 34% - 35% - 35% - 35% - 35% - 35% - 38% - 39% - 40% - 40% - 40% - 40% - 40% - 41% - 42% - 42% - 45% - 45% - 46% - 46% - 46% - 46% - 47% - 47% - 49% - 50% - 50% - 51% - 51% - 51% - 51% - 52% - 52% - 54% - 54% - 55% - 55% - 56% - 56% - 58% - 58% - 59% - 59% - 61% - 61% - 61% - 62% - 63% - 63% - 64% - 64% - 66% - 66% - 66% - 67% - 67% - 68% - 68% - 69% - 76% - 76% - 76% - 77% - 77% - 78% - 80% - 81% - 81% - 83% - 86% - 86% - 86% - 87% - 87% - 88% - 88% - 92% - 92% - 94% - 94% - 96% - 99% - 99% - 100% (79s) - loss: 0.7589\n",
      "Epoch 59 - 100% 1% - 2% - 3% - 3% - 4% - 4% - 5% - 5% - 5% - 8% - 8% - 9% - 10% - 10% - 10% - 11% - 11% - 11% - 12% - 12% - 13% - 13% - 14% - 15% - 15% - 15% - 16% - 16% - 16% - 17% - 18% - 18% - 18% - 18% - 19% - 19% - 19% - 19% - 20% - 22% - 22% - 25% - 25% - 28% - 29% - 29% - 29% - 29% - 30% - 32% - 32% - 32% - 32% - 32% - 33% - 34% - 34% - 34% - 34% - 35% - 35% - 35% - 35% - 36% - 37% - 37% - 39% - 42% - 43% - 45% - 46% - 47% - 48% - 48% - 48% - 49% - 49% - 50% - 50% - 50% - 50% - 50% - 52% - 52% - 53% - 54% - 57% - 57% - 58% - 59% - 60% - 60% - 61% - 61% - 63% - 63% - 64% - 65% - 65% - 66% - 67% - 67% - 69% - 69% - 70% - 72% - 72% - 73% - 73% - 73% - 74% - 74% - 74% - 76% - 78% - 78% - 78% - 80% - 82% - 85% - 85% - 85% - 86% - 88% - 89% - 91% - 92% - 95% - 95% - 96% - 96% - 96% - 96% - 98% - 98% - 98% - 98% - 100% (78s) - loss: 0.7575\n",
      "Epoch 60 - 100% 1% - 1% - 1% - 3% - 7% - 10% - 10% - 10% - 10% - 11% - 11% - 11% - 11% - 13% - 13% - 18% - 19% - 21% - 22% - 23% - 23% - 23% - 34% - 35% - 35% - 36% - 36% - 36% - 36% - 36% - 40% - 41% - 41% - 41% - 41% - 42% - 42% - 42% - 43% - 43% - 43% - 44% - 44% - 44% - 47% - 50% - 50% - 50% - 53% - 53% - 54% - 54% - 54% - 57% - 57% - 59% - 59% - 59% - 59% - 59% - 63% - 65% - 67% - 67% - 67% - 67% - 69% - 69% - 70% - 70% - 70% - 70% - 70% - 73% - 74% - 74% - 74% - 74% - 74% - 75% - 76% - 82% - 82% - 82% - 82% - 83% - 84% - 88% - 91% - 92% - 96% (78s) - loss: 0.7562\n",
      "Epoch 61 - 100% 0% - 3% - 4% - 7% - 11% - 13% - 14% - 16% - 16% - 25% - 29% - 29% - 30% - 32% - 34% - 36% - 36% - 36% - 36% - 37% - 37% - 38% - 39% - 40% - 40% - 42% - 42% - 49% - 49% - 49% - 52% - 59% - 61% - 61% - 61% - 61% - 61% - 63% - 63% - 69% - 69% - 69% - 69% - 70% - 70% - 71% - 71% - 71% - 74% - 79% - 79% - 80% - 83% - 86% - 86% - 86% - 86% - 87% - 87% - 90% - 93% - 93% - 95% - 95% - 95% - 98% - 98% - 99% - 99% - 99% - 99% (78s) - loss: 0.7571\n",
      "Epoch 62 - 100% 0% - 1% - 4% - 4% - 4% - 6% - 8% - 8% - 12% - 12% - 12% - 13% - 13% - 13% - 13% - 13% - 14% - 15% - 17% - 19% - 19% - 19% - 19% - 20% - 21% - 22% - 23% - 24% - 24% - 24% - 24% - 25% - 25% - 26% - 32% - 32% - 33% - 33% - 33% - 33% - 35% - 36% - 38% - 39% - 39% - 41% - 50% - 50% - 51% - 53% - 53% - 53% - 53% - 54% - 54% - 54% - 54% - 57% - 60% - 60% - 62% - 62% - 62% - 62% - 63% - 64% - 65% - 65% - 66% - 67% - 67% - 68% - 70% - 70% - 70% - 71% - 71% - 71% - 71% - 72% - 72% - 72% - 73% - 73% - 74% - 74% - 75% - 76% - 76% - 76% - 77% - 77% - 77% - 79% - 80% - 81% - 84% - 86% - 86% - 86% - 86% - 86% - 87% - 89% - 90% - 90% - 91% - 91% - 91% - 92% - 92% - 92% - 93% - 93% - 94% - 94% - 96% - 96% - 96% - 96% - 96% - 96% - 97% - 100% (78s) - loss: 0.7561\n",
      "Epoch 63 - 100% 1% - 1% - 1% - 3% - 6% - 16% - 18% - 18% - 18% - 19% - 20% - 20% - 20% - 20% - 27% - 27% - 28% - 28% - 28% - 29% - 30% - 32% - 33% - 36% - 37% - 37% - 37% - 39% - 39% - 40% - 40% - 40% - 40% - 43% - 43% - 43% - 43% - 43% - 43% - 43% - 44% - 45% - 48% - 49% - 49% - 49% - 50% - 51% - 57% - 57% - 61% - 64% - 67% - 70% - 71% - 72% - 72% - 76% - 76% - 80% - 81% - 82% - 83% - 84% - 84% - 87% - 87% - 87% - 92% - 97% - 99% (78s) - loss: 0.7569\n",
      "Epoch 64 - 100% 1% - 2% - 2% - 2% - 2% - 3% - 3% - 3% - 3% - 3% - 6% - 8% - 11% - 11% - 12% - 13% - 25% - 27% - 27% - 29% - 33% - 34% - 34% - 34% - 35% - 35% - 35% - 39% - 40% - 41% - 46% - 47% - 47% - 47% - 47% - 47% - 51% - 51% - 56% - 58% - 59% - 59% - 61% - 62% - 65% - 67% - 70% - 70% - 71% - 71% - 72% - 75% - 81% - 81% - 81% - 81% - 81% - 85% - 85% - 86% - 86% - 86% - 86% - 94% - 94% - 94% - 94% - 94% - 94% - 96% - 97% - 97% - 97% - 99% - 99% - 99% - 99% (77s) - loss: 0.7573\n",
      "Epoch 65 - 100% 0% - 4% - 4% - 5% - 5% - 6% - 8% - 8% - 10% - 10% - 11% - 16% - 16% - 16% - 16% - 17% - 17% - 17% - 18% - 21% - 21% - 22% - 24% - 26% - 27% - 27% - 29% - 31% - 37% - 37% - 38% - 42% - 45% - 49% - 52% - 52% - 52% - 58% - 59% - 60% - 71% - 71% - 71% - 72% - 72% - 72% - 76% - 76% - 79% - 79% - 81% - 81% - 81% - 81% - 81% - 82% - 84% - 84% - 86% - 86% - 88% - 88% - 88% - 88% - 88% - 88% - 88% - 88% - 89% - 89% - 89% - 89% - 94% - 95% - 98% - 99% (78s) - loss: 0.7556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 - 100% 2% - 3% - 3% - 6% - 10% - 10% - 11% - 11% - 11% - 11% - 11% - 14% - 15% - 16% - 17% - 17% - 17% - 18% - 19% - 21% - 23% - 24% - 24% - 25% - 25% - 25% - 26% - 26% - 27% - 27% - 28% - 29% - 30% - 30% - 30% - 30% - 30% - 31% - 32% - 34% - 36% - 36% - 39% - 39% - 40% - 40% - 40% - 40% - 41% - 44% - 45% - 45% - 47% - 48% - 48% - 49% - 51% - 51% - 52% - 52% - 52% - 52% - 52% - 54% - 54% - 54% - 54% - 54% - 55% - 55% - 55% - 57% - 58% - 58% - 58% - 60% - 60% - 61% - 61% - 63% - 63% - 64% - 64% - 64% - 64% - 64% - 65% - 65% - 66% - 69% - 69% - 71% - 73% - 75% - 75% - 76% - 77% - 77% - 80% - 80% - 80% - 81% - 81% - 81% - 81% - 81% - 81% - 81% - 82% - 82% - 82% - 83% - 84% - 84% - 84% - 85% - 86% - 88% - 88% - 88% - 88% - 88% - 89% - 89% - 89% - 89% - 90% - 90% - 93% - 95% - 95% - 99% - 99% - 100% (78s) - loss: 0.7548\n",
      "Epoch 67 - 100% 0% - 0% - 0% - 0% - 1% - 1% - 1% - 1% - 2% - 2% - 2% - 3% - 3% - 3% - 5% - 6% - 7% - 7% - 9% - 14% - 16% - 17% - 18% - 18% - 18% - 18% - 19% - 20% - 21% - 21% - 23% - 23% - 23% - 24% - 24% - 25% - 25% - 25% - 25% - 25% - 26% - 27% - 30% - 30% - 30% - 31% - 31% - 33% - 34% - 34% - 34% - 34% - 35% - 39% - 39% - 39% - 39% - 39% - 43% - 45% - 46% - 46% - 47% - 47% - 50% - 50% - 50% - 53% - 53% - 54% - 54% - 57% - 58% - 65% - 66% - 67% - 68% - 68% - 69% - 69% - 71% - 74% - 75% - 76% - 76% - 76% - 80% - 80% - 80% - 81% - 82% - 86% - 86% - 87% - 87% - 87% - 87% - 87% - 87% - 91% - 93% - 95% - 99% (78s) - loss: 0.7531\n",
      "Epoch 68 - 100% 0% - 0% - 0% - 1% - 1% - 1% - 1% - 1% - 2% - 2% - 2% - 3% - 6% - 6% - 6% - 6% - 6% - 8% - 8% - 8% - 8% - 18% - 19% - 20% - 23% - 24% - 24% - 24% - 33% - 33% - 38% - 38% - 42% - 43% - 43% - 43% - 43% - 43% - 45% - 45% - 47% - 47% - 49% - 49% - 49% - 51% - 51% - 51% - 53% - 53% - 53% - 59% - 61% - 61% - 64% - 65% - 65% - 68% - 69% - 69% - 69% - 70% - 70% - 71% - 71% - 71% - 72% - 73% - 74% - 78% - 81% - 81% - 84% - 85% - 85% - 89% - 89% - 91% - 91% - 91% - 95% - 96% - 98% (78s) - loss: 0.7542\n",
      "Epoch 69 - 100% 2% - 4% - 5% - 11% - 11% - 12% - 13% - 13% - 13% - 15% - 17% - 17% - 18% - 19% - 22% - 23% - 23% - 23% - 23% - 24% - 28% - 28% - 28% - 29% - 29% - 32% - 32% - 32% - 33% - 33% - 33% - 33% - 34% - 34% - 34% - 34% - 35% - 36% - 37% - 37% - 38% - 38% - 39% - 39% - 40% - 40% - 41% - 41% - 41% - 44% - 47% - 48% - 49% - 50% - 50% - 50% - 54% - 57% - 58% - 58% - 58% - 58% - 58% - 58% - 58% - 58% - 58% - 61% - 62% - 67% - 67% - 67% - 67% - 68% - 70% - 71% - 71% - 71% - 71% - 71% - 72% - 73% - 73% - 73% - 73% - 75% - 76% - 76% - 76% - 76% - 77% - 79% - 79% - 82% - 82% - 83% - 84% - 85% - 86% - 86% - 86% - 86% - 86% - 87% - 87% - 87% - 87% - 88% - 89% - 93% - 93% - 98% - 100% (78s) - loss: 0.7551\n",
      "Epoch 70 - 100% 4% - 5% - 6% - 6% - 6% - 6% - 6% - 7% - 9% - 11% - 11% - 13% - 13% - 16% - 18% - 20% - 20% - 20% - 25% - 25% - 26% - 26% - 27% - 27% - 28% - 29% - 31% - 31% - 31% - 32% - 32% - 32% - 33% - 33% - 34% - 35% - 35% - 36% - 37% - 37% - 39% - 39% - 41% - 44% - 46% - 47% - 48% - 49% - 50% - 50% - 51% - 55% - 55% - 55% - 56% - 57% - 59% - 60% - 60% - 60% - 62% - 62% - 62% - 62% - 62% - 63% - 70% - 72% - 72% - 73% - 73% - 74% - 75% - 76% - 76% - 82% - 82% - 82% - 83% - 84% - 87% - 89% - 90% - 90% - 90% - 91% - 91% - 93% - 93% - 94% - 94% - 97% - 98% - 99% - 99% (78s) - loss: 0.7534\n",
      "Epoch 71 - 100% 1% - 3% - 3% - 3% - 3% - 5% - 6% - 8% - 9% - 11% - 12% - 12% - 13% - 15% - 15% - 16% - 16% - 20% - 20% - 21% - 23% - 26% - 31% - 31% - 32% - 39% - 40% - 40% - 40% - 40% - 40% - 42% - 42% - 42% - 44% - 44% - 48% - 49% - 51% - 51% - 51% - 51% - 53% - 53% - 56% - 56% - 56% - 57% - 57% - 60% - 60% - 61% - 63% - 63% - 63% - 63% - 63% - 63% - 68% - 72% - 72% - 72% - 72% - 72% - 73% - 75% - 75% - 75% - 80% - 80% - 80% - 80% - 81% - 81% - 83% - 84% - 84% - 84% - 85% - 87% - 89% - 90% - 95% - 95% - 95% - 97% - 97% (78s) - loss: 0.7545\n",
      "Epoch 72 - 100% 0% - 5% - 7% - 9% - 12% - 12% - 12% - 12% - 12% - 12% - 12% - 12% - 13% - 13% - 14% - 14% - 17% - 19% - 20% - 20% - 27% - 28% - 30% - 30% - 31% - 31% - 32% - 39% - 39% - 39% - 42% - 46% - 48% - 48% - 48% - 48% - 50% - 50% - 50% - 52% - 52% - 54% - 54% - 54% - 54% - 57% - 61% - 61% - 62% - 62% - 62% - 62% - 64% - 68% - 69% - 69% - 70% - 70% - 73% - 79% - 79% - 79% - 81% - 81% - 82% - 82% - 82% - 82% - 82% - 90% - 90% - 95% - 95% - 96% - 98% - 98% - 99% - 100% (78s) - loss: 0.7533\n",
      "Epoch 73 - 100% 1% - 15% - 15% - 19% - 20% - 22% - 22% - 23% - 23% - 27% - 29% - 29% - 29% - 31% - 34% - 36% - 36% - 36% - 36% - 37% - 37% - 40% - 41% - 41% - 41% - 45% - 46% - 47% - 47% - 47% - 47% - 47% - 47% - 48% - 48% - 49% - 49% - 49% - 50% - 51% - 51% - 52% - 52% - 52% - 52% - 53% - 54% - 57% - 57% - 57% - 57% - 57% - 58% - 61% - 61% - 61% - 61% - 61% - 61% - 61% - 62% - 63% - 64% - 65% - 65% - 66% - 66% - 66% - 68% - 68% - 69% - 69% - 70% - 72% - 72% - 73% - 76% - 78% - 80% - 80% - 81% - 81% - 82% - 85% - 85% - 85% - 86% - 87% - 88% - 88% - 89% - 90% - 91% - 92% - 92% - 93% - 93% - 94% - 94% - 94% - 94% - 95% - 95% - 95% - 95% - 95% - 97% - 97% - 98% - 98% - 99% - 99% - 99% - 100% - 100% (79s) - loss: 0.7537\n",
      "Epoch 74 - 100% 0% - 1% - 2% - 5% - 5% - 6% - 6% - 6% - 7% - 7% - 9% - 9% - 9% - 11% - 12% - 12% - 12% - 12% - 13% - 14% - 15% - 15% - 15% - 15% - 16% - 16% - 18% - 18% - 19% - 20% - 20% - 20% - 20% - 21% - 22% - 22% - 22% - 23% - 23% - 24% - 24% - 25% - 27% - 28% - 28% - 28% - 28% - 28% - 31% - 32% - 32% - 32% - 32% - 33% - 33% - 33% - 33% - 34% - 34% - 35% - 36% - 36% - 38% - 39% - 41% - 42% - 42% - 42% - 44% - 45% - 46% - 47% - 47% - 47% - 48% - 50% - 50% - 50% - 51% - 53% - 56% - 56% - 56% - 56% - 56% - 59% - 59% - 59% - 60% - 60% - 60% - 61% - 61% - 61% - 62% - 62% - 63% - 63% - 63% - 63% - 64% - 64% - 66% - 71% - 72% - 72% - 72% - 73% - 73% - 74% - 74% - 79% - 80% - 80% - 82% - 82% - 82% - 83% - 83% - 83% - 83% - 83% - 84% - 84% - 85% - 85% - 86% - 86% - 86% - 86% - 87% - 87% - 92% - 92% - 92% - 94% - 94% - 94% - 95% - 96% - 96% - 97% - 98% - 99% (79s) - loss: 0.7532\n",
      "Epoch 75 - 100% 3% - 5% - 5% - 7% - 7% - 7% - 8% - 8% - 10% - 10% - 11% - 11% - 12% - 13% - 15% - 15% - 17% - 17% - 17% - 17% - 17% - 18% - 19% - 25% - 26% - 27% - 27% - 27% - 28% - 28% - 32% - 35% - 37% - 40% - 40% - 40% - 42% - 43% - 43% - 43% - 44% - 44% - 45% - 45% - 48% - 48% - 48% - 50% - 52% - 53% - 55% - 56% - 56% - 56% - 59% - 59% - 61% - 62% - 62% - 68% - 68% - 68% - 68% - 71% - 71% - 72% - 72% - 72% - 76% - 76% - 76% - 77% - 77% - 79% - 83% - 83% - 84% - 86% - 87% - 96% - 96% - 97% - 97% - 98% - 98% - 98% - 99% - 99% - 99% (78s) - loss: 0.7548\n",
      "Epoch 76 - 100% 1% - 1% - 1% - 2% - 4% - 7% - 10% - 17% - 17% - 18% - 19% - 19% - 23% - 23% - 23% - 25% - 28% - 28% - 28% - 32% - 37% - 38% - 39% - 39% - 40% - 40% - 42% - 43% - 43% - 45% - 45% - 46% - 46% - 46% - 46% - 48% - 49% - 52% - 56% - 56% - 57% - 61% - 64% - 66% - 68% - 69% - 69% - 70% - 72% - 72% - 73% - 73% - 74% - 75% - 78% - 78% - 78% - 78% - 80% - 83% - 84% - 84% - 84% - 88% - 89% - 89% - 89% - 89% - 91% - 91% - 91% - 91% - 96% - 98% - 98% - 99% - 100% (78s) - loss: 0.7545\n",
      "Epoch 77 - 100% 0% - 3% - 3% - 3% - 5% - 5% - 5% - 5% - 5% - 5% - 6% - 9% - 11% - 12% - 12% - 14% - 15% - 20% - 23% - 23% - 23% - 23% - 23% - 23% - 24% - 26% - 27% - 27% - 34% - 34% - 35% - 35% - 35% - 36% - 40% - 40% - 41% - 42% - 42% - 42% - 43% - 47% - 47% - 47% - 47% - 47% - 47% - 50% - 51% - 51% - 51% - 55% - 55% - 57% - 57% - 58% - 58% - 58% - 58% - 58% - 58% - 58% - 61% - 61% - 63% - 65% - 65% - 66% - 66% - 68% - 68% - 68% - 70% - 71% - 72% - 72% - 72% - 73% - 74% - 77% - 77% - 81% - 82% - 83% - 83% - 83% - 84% - 85% - 87% - 87% - 89% - 89% - 91% - 91% - 91% - 97% - 99% - 99% - 99% - 99% - 99% - 99% - 100% (78s) - loss: 0.7527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 - 100% 1% - 2% - 2% - 2% - 3% - 4% - 6% - 6% - 9% - 9% - 9% - 11% - 12% - 12% - 15% - 19% - 22% - 25% - 26% - 27% - 27% - 28% - 28% - 29% - 30% - 30% - 30% - 30% - 31% - 31% - 31% - 34% - 37% - 38% - 38% - 39% - 40% - 40% - 40% - 40% - 40% - 41% - 41% - 41% - 41% - 41% - 41% - 45% - 45% - 47% - 47% - 47% - 47% - 48% - 49% - 50% - 53% - 54% - 55% - 56% - 56% - 57% - 59% - 60% - 60% - 60% - 60% - 60% - 61% - 62% - 63% - 64% - 68% - 69% - 69% - 70% - 71% - 72% - 72% - 72% - 73% - 73% - 73% - 75% - 75% - 75% - 75% - 75% - 75% - 77% - 77% - 77% - 78% - 78% - 79% - 79% - 82% - 82% - 82% - 84% - 85% - 86% - 88% - 88% - 88% - 89% - 89% - 89% - 89% - 89% - 90% - 91% - 93% - 94% - 94% - 94% - 94% - 97% - 99% - 100% (78s) - loss: 0.7539\n",
      "Epoch 79 - 100% 2% - 5% - 5% - 5% - 5% - 5% - 8% - 8% - 9% - 9% - 9% - 9% - 10% - 12% - 12% - 12% - 12% - 18% - 19% - 20% - 20% - 23% - 26% - 27% - 27% - 28% - 29% - 29% - 31% - 33% - 33% - 33% - 41% - 41% - 43% - 43% - 44% - 48% - 48% - 51% - 52% - 54% - 54% - 54% - 54% - 54% - 54% - 54% - 55% - 56% - 56% - 58% - 59% - 59% - 59% - 59% - 60% - 60% - 62% - 62% - 63% - 63% - 63% - 63% - 64% - 64% - 65% - 65% - 65% - 67% - 67% - 67% - 67% - 67% - 67% - 68% - 69% - 69% - 70% - 70% - 70% - 70% - 71% - 72% - 74% - 74% - 74% - 80% - 80% - 81% - 81% - 81% - 82% - 82% - 82% - 82% - 83% - 85% - 86% - 86% - 86% - 92% - 95% - 95% - 97% - 98% (78s) - loss: 0.7519\n",
      "Epoch 80 - 100% 2% - 2% - 2% - 3% - 3% - 6% - 6% - 6% - 8% - 8% - 8% - 11% - 11% - 13% - 13% - 16% - 16% - 18% - 18% - 21% - 21% - 21% - 21% - 21% - 25% - 25% - 29% - 29% - 30% - 31% - 31% - 38% - 42% - 44% - 45% - 49% - 53% - 55% - 56% - 56% - 57% - 60% - 62% - 64% - 64% - 65% - 65% - 66% - 66% - 73% - 74% - 74% - 79% - 79% - 80% - 81% - 85% - 88% - 89% - 95% - 95% - 98% - 99% - 99% (77s) - loss: 0.7537\n",
      "Epoch 81 - 100% 1% - 4% - 7% - 7% - 7% - 7% - 7% - 8% - 9% - 9% - 9% - 11% - 12% - 14% - 14% - 15% - 15% - 16% - 18% - 18% - 21% - 21% - 22% - 31% - 31% - 33% - 34% - 34% - 35% - 35% - 38% - 38% - 38% - 40% - 40% - 42% - 42% - 46% - 48% - 51% - 53% - 54% - 54% - 55% - 57% - 58% - 58% - 60% - 60% - 60% - 60% - 61% - 62% - 63% - 63% - 65% - 67% - 67% - 70% - 71% - 72% - 72% - 73% - 73% - 74% - 75% - 75% - 75% - 75% - 77% - 77% - 78% - 79% - 82% - 82% - 83% - 83% - 83% - 83% - 84% - 90% - 90% - 91% - 91% - 92% - 92% - 94% - 99% - 99% - 99% - 99% (78s) - loss: 0.7535\n",
      "Epoch 82 - 100% 3% - 3% - 4% - 4% - 4% - 7% - 7% - 7% - 7% - 7% - 7% - 8% - 8% - 8% - 8% - 9% - 9% - 10% - 12% - 17% - 17% - 17% - 17% - 18% - 19% - 20% - 20% - 20% - 20% - 20% - 21% - 22% - 23% - 24% - 24% - 24% - 27% - 27% - 27% - 28% - 28% - 28% - 28% - 28% - 28% - 31% - 34% - 35% - 36% - 36% - 39% - 39% - 39% - 40% - 40% - 40% - 42% - 43% - 45% - 45% - 49% - 49% - 51% - 52% - 57% - 59% - 60% - 61% - 61% - 61% - 63% - 63% - 64% - 64% - 65% - 65% - 65% - 67% - 68% - 69% - 69% - 70% - 70% - 70% - 70% - 71% - 72% - 72% - 73% - 74% - 74% - 74% - 76% - 76% - 77% - 77% - 78% - 80% - 81% - 81% - 83% - 83% - 84% - 86% - 86% - 86% - 86% - 89% - 89% - 93% - 95% - 95% - 95% - 96% - 97% - 99% - 99% - 99% - 99% - 99% (78s) - loss: 0.7524\n",
      "Epoch 83 - 100% 3% - 4% - 5% - 5% - 6% - 6% - 6% - 7% - 7% - 7% - 7% - 9% - 9% - 15% - 15% - 15% - 15% - 15% - 16% - 16% - 17% - 18% - 19% - 19% - 19% - 22% - 22% - 23% - 23% - 24% - 25% - 25% - 27% - 27% - 27% - 27% - 27% - 28% - 34% - 34% - 35% - 37% - 37% - 38% - 38% - 38% - 39% - 39% - 40% - 43% - 43% - 43% - 44% - 46% - 50% - 51% - 51% - 52% - 55% - 58% - 58% - 63% - 63% - 64% - 64% - 64% - 66% - 67% - 69% - 72% - 72% - 72% - 72% - 72% - 72% - 72% - 72% - 73% - 75% - 80% - 85% - 89% - 93% - 93% - 93% - 93% - 95% - 95% - 95% - 98% - 99% (78s) - loss: 0.7502\n",
      "Epoch 84 - 100% 4% - 6% - 7% - 7% - 8% - 12% - 12% - 12% - 13% - 13% - 13% - 13% - 15% - 16% - 16% - 16% - 17% - 17% - 17% - 17% - 17% - 17% - 18% - 18% - 18% - 18% - 18% - 18% - 19% - 19% - 20% - 21% - 21% - 22% - 22% - 22% - 24% - 25% - 27% - 27% - 28% - 33% - 34% - 35% - 35% - 35% - 35% - 36% - 36% - 38% - 42% - 42% - 43% - 44% - 44% - 45% - 45% - 45% - 45% - 45% - 47% - 48% - 48% - 48% - 49% - 51% - 51% - 53% - 54% - 55% - 57% - 58% - 58% - 58% - 58% - 59% - 59% - 60% - 60% - 60% - 61% - 61% - 66% - 68% - 68% - 71% - 71% - 71% - 71% - 73% - 73% - 74% - 74% - 78% - 78% - 81% - 82% - 84% - 88% - 88% - 88% - 88% - 90% - 90% - 92% - 94% - 96% - 97% - 97% - 98% - 99% (78s) - loss: 0.7522\n",
      "Epoch 85 - 100% 2% - 2% - 5% - 6% - 6% - 6% - 8% - 10% - 10% - 11% - 11% - 12% - 12% - 13% - 13% - 14% - 15% - 17% - 18% - 18% - 18% - 18% - 19% - 20% - 20% - 20% - 20% - 20% - 20% - 21% - 23% - 23% - 25% - 25% - 25% - 25% - 25% - 26% - 29% - 29% - 30% - 30% - 30% - 30% - 30% - 30% - 30% - 31% - 31% - 32% - 34% - 36% - 36% - 36% - 36% - 36% - 37% - 38% - 38% - 38% - 39% - 39% - 46% - 46% - 47% - 48% - 48% - 48% - 48% - 48% - 48% - 49% - 50% - 51% - 53% - 56% - 56% - 56% - 56% - 56% - 58% - 59% - 59% - 59% - 60% - 62% - 62% - 62% - 62% - 63% - 63% - 63% - 63% - 63% - 66% - 66% - 70% - 72% - 75% - 75% - 76% - 77% - 78% - 78% - 81% - 82% - 83% - 83% - 83% - 86% - 89% - 90% - 91% - 91% - 92% - 92% - 94% - 94% - 94% - 94% - 95% - 99% (78s) - loss: 0.7529\n",
      "Epoch 86 - 100% 1% - 2% - 6% - 7% - 7% - 12% - 15% - 19% - 19% - 21% - 21% - 26% - 26% - 26% - 27% - 27% - 31% - 32% - 32% - 32% - 33% - 34% - 34% - 34% - 34% - 34% - 35% - 37% - 37% - 37% - 42% - 43% - 43% - 45% - 46% - 48% - 48% - 48% - 48% - 48% - 48% - 48% - 48% - 50% - 50% - 50% - 51% - 51% - 51% - 53% - 53% - 54% - 57% - 58% - 60% - 66% - 68% - 69% - 72% - 72% - 73% - 74% - 75% - 75% - 76% - 78% - 79% - 81% - 81% - 82% - 82% - 82% - 82% - 83% - 83% - 84% - 84% - 85% - 85% - 87% - 90% - 90% - 91% - 92% - 93% - 94% - 94% - 96% - 98% - 98% - 98% - 99% - 99% - 100% (78s) - loss: 0.7524\n",
      "Epoch 87 - 100% 0% - 0% - 1% - 2% - 2% - 3% - 5% - 5% - 8% - 8% - 8% - 8% - 9% - 9% - 11% - 11% - 13% - 13% - 13% - 14% - 14% - 14% - 15% - 15% - 15% - 17% - 17% - 18% - 18% - 18% - 19% - 19% - 21% - 21% - 29% - 30% - 36% - 36% - 37% - 37% - 38% - 41% - 41% - 41% - 43% - 45% - 45% - 45% - 47% - 47% - 47% - 47% - 48% - 48% - 48% - 49% - 49% - 49% - 50% - 55% - 60% - 60% - 61% - 64% - 64% - 64% - 64% - 64% - 66% - 68% - 72% - 76% - 79% - 81% - 82% - 82% - 82% - 82% - 82% - 82% - 84% - 86% - 86% - 86% - 86% - 86% - 86% - 86% - 91% - 91% - 91% - 91% - 92% - 92% - 92% - 93% - 95% - 97% - 98% - 98% - 99% (78s) - loss: 0.7522\n",
      "Epoch 88 - 100% 2% - 2% - 4% - 4% - 4% - 6% - 7% - 7% - 13% - 13% - 15% - 15% - 15% - 15% - 15% - 16% - 19% - 20% - 22% - 22% - 26% - 26% - 27% - 27% - 28% - 33% - 34% - 36% - 36% - 38% - 41% - 47% - 48% - 48% - 51% - 51% - 51% - 52% - 53% - 58% - 58% - 58% - 59% - 59% - 59% - 60% - 61% - 62% - 63% - 63% - 63% - 67% - 69% - 69% - 69% - 70% - 70% - 70% - 70% - 71% - 71% - 73% - 73% - 74% - 75% - 76% - 77% - 78% - 83% - 84% - 84% - 85% - 87% - 87% - 88% - 88% - 89% - 90% - 91% - 94% - 96% (78s) - loss: 0.7525\n",
      "Epoch 89 - 100% 3% - 4% - 5% - 7% - 8% - 8% - 10% - 17% - 17% - 18% - 19% - 19% - 19% - 24% - 24% - 27% - 29% - 32% - 32% - 35% - 36% - 39% - 42% - 42% - 42% - 42% - 42% - 42% - 48% - 49% - 51% - 52% - 52% - 52% - 53% - 53% - 53% - 53% - 55% - 55% - 57% - 59% - 61% - 65% - 65% - 67% - 67% - 67% - 68% - 69% - 71% - 73% - 77% - 77% - 77% - 77% - 78% - 78% - 78% - 79% - 81% - 81% - 81% - 82% - 85% - 85% - 86% - 86% - 86% - 86% - 86% - 86% - 87% - 87% - 87% - 88% - 89% - 89% - 89% - 92% - 94% - 94% - 95% - 95% - 96% - 99% (78s) - loss: 0.7524\n",
      "Epoch 90 - 100% 2% - 3% - 5% - 5% - 6% - 6% - 6% - 6% - 6% - 7% - 7% - 9% - 9% - 13% - 13% - 18% - 18% - 18% - 19% - 20% - 21% - 21% - 21% - 21% - 21% - 23% - 26% - 26% - 28% - 28% - 28% - 30% - 33% - 33% - 34% - 35% - 36% - 36% - 36% - 36% - 36% - 38% - 43% - 45% - 45% - 45% - 46% - 48% - 49% - 50% - 50% - 50% - 50% - 55% - 56% - 57% - 59% - 59% - 62% - 63% - 63% - 63% - 64% - 66% - 66% - 66% - 66% - 66% - 67% - 67% - 67% - 69% - 69% - 70% - 70% - 70% - 70% - 70% - 70% - 71% - 71% - 73% - 75% - 76% - 80% - 80% - 80% - 83% - 84% - 86% - 96% - 96% - 96% - 96% - 97% - 97% - 97% (78s) - loss: 0.7531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 - 100%- 10% - 10% - 10% - 12% - 12% - 13% - 18% - 19% - 21% - 24% - 25% - 25% - 25% - 25% - 26% - 28% - 30% - 30% - 31% - 31% - 32% - 33% - 34% - 35% - 36% - 38% - 41% - 41% - 41% - 42% - 42% - 42% - 42% - 44% - 45% - 45% - 46% - 48% - 50% - 52% - 52% - 52% - 52% - 53% - 56% - 58% - 60% - 60% - 60% - 64% - 65% - 65% - 66% - 70% - 70% - 72% - 72% - 78% - 79% - 79% - 79% - 79% - 79% - 81% - 82% - 82% - 86% - 87% - 87% - 87% - 87% - 88% - 89% - 90% - 93% - 96% - 97% - 97% - 98% - 100% (78s) - loss: 0.7510\n",
      "Epoch 92 - 100% 1% - 1% - 5% - 9% - 10% - 10% - 19% - 22% - 22% - 22% - 23% - 28% - 28% - 31% - 32% - 32% - 32% - 32% - 33% - 33% - 34% - 34% - 36% - 39% - 40% - 40% - 40% - 40% - 45% - 46% - 46% - 54% - 56% - 57% - 58% - 58% - 59% - 59% - 60% - 60% - 60% - 63% - 63% - 64% - 64% - 64% - 64% - 65% - 66% - 66% - 68% - 68% - 69% - 69% - 71% - 71% - 73% - 81% - 81% - 81% - 81% - 81% - 81% - 85% - 85% - 88% - 89% - 90% - 90% - 90% - 90% - 95% - 96% - 96% - 96% - 97% (77s) - loss: 0.7522\n",
      "Epoch 93 - 100% 6% - 6% - 6% - 8% - 8% - 8% - 8% - 11% - 13% - 15% - 15% - 15% - 20% - 23% - 25% - 33% - 34% - 37% - 38% - 39% - 39% - 41% - 44% - 46% - 46% - 47% - 47% - 54% - 54% - 56% - 56% - 57% - 61% - 61% - 64% - 64% - 69% - 70% - 70% - 71% - 72% - 73% - 75% - 75% - 75% - 78% - 81% - 81% - 83% - 84% - 85% - 87% - 87% - 87% - 90% - 91% - 91% - 96% - 97% - 98% - 98% - 98% (78s) - loss: 0.7502\n",
      "Epoch 94 - 100% 2% - 2% - 3% - 4% - 5% - 5% - 5% - 6% - 6% - 6% - 8% - 9% - 10% - 10% - 10% - 10% - 11% - 11% - 12% - 12% - 13% - 14% - 15% - 15% - 16% - 16% - 17% - 17% - 17% - 20% - 20% - 21% - 22% - 22% - 23% - 23% - 24% - 26% - 26% - 27% - 27% - 27% - 28% - 28% - 28% - 29% - 30% - 31% - 33% - 35% - 35% - 36% - 36% - 36% - 37% - 38% - 38% - 39% - 39% - 39% - 39% - 39% - 42% - 42% - 42% - 42% - 42% - 43% - 45% - 46% - 46% - 46% - 49% - 50% - 51% - 51% - 54% - 54% - 54% - 57% - 58% - 58% - 58% - 58% - 58% - 61% - 62% - 62% - 62% - 63% - 63% - 63% - 63% - 63% - 64% - 65% - 66% - 69% - 71% - 71% - 73% - 73% - 73% - 73% - 74% - 74% - 75% - 75% - 76% - 81% - 82% - 83% - 83% - 83% - 86% - 86% - 87% - 89% - 90% - 91% - 91% - 92% - 92% - 92% - 92% - 93% - 93% - 94% - 94% - 94% - 95% - 96% - 99% (79s) - loss: 0.7514\n",
      "Epoch 95 - 100% 0% - 2% - 2% - 2% - 3% - 3% - 3% - 3% - 3% - 4% - 4% - 5% - 5% - 5% - 5% - 6% - 7% - 9% - 9% - 10% - 12% - 12% - 12% - 15% - 16% - 16% - 16% - 17% - 17% - 18% - 18% - 18% - 18% - 18% - 18% - 18% - 18% - 18% - 18% - 19% - 19% - 19% - 19% - 19% - 22% - 24% - 26% - 28% - 28% - 28% - 29% - 32% - 33% - 34% - 44% - 46% - 47% - 49% - 49% - 51% - 53% - 55% - 56% - 58% - 58% - 58% - 58% - 58% - 58% - 58% - 58% - 61% - 61% - 61% - 62% - 62% - 64% - 71% - 75% - 76% - 76% - 76% - 77% - 81% - 81% - 81% - 82% - 82% - 84% - 84% - 84% - 84% - 90% - 90% - 90% - 91% - 93% - 94% - 94% - 100% (78s) - loss: 0.7512\n",
      "Epoch 96 - 100% 0% - 1% - 1% - 2% - 2% - 4% - 14% - 14% - 15% - 19% - 20% - 20% - 22% - 22% - 25% - 31% - 33% - 33% - 42% - 44% - 44% - 44% - 44% - 44% - 44% - 47% - 54% - 55% - 55% - 55% - 55% - 55% - 58% - 58% - 61% - 61% - 61% - 65% - 75% - 75% - 79% - 80% - 80% - 80% - 81% - 81% - 81% - 83% - 87% - 87% - 88% - 88% - 88% - 89% - 92% - 94% - 94% - 95% - 96% - 96% - 97% - 98% - 99% - 100% (77s) - loss: 0.7515\n",
      "Epoch 97 - 100% 0% - 2% - 3% - 3% - 3% - 3% - 4% - 4% - 4% - 4% - 5% - 6% - 7% - 7% - 8% - 8% - 10% - 10% - 11% - 11% - 11% - 12% - 12% - 13% - 13% - 13% - 14% - 14% - 19% - 20% - 23% - 24% - 30% - 32% - 32% - 32% - 33% - 34% - 34% - 34% - 34% - 35% - 35% - 41% - 41% - 41% - 41% - 41% - 43% - 43% - 47% - 47% - 47% - 47% - 47% - 47% - 47% - 48% - 48% - 48% - 48% - 49% - 50% - 52% - 53% - 55% - 55% - 56% - 57% - 58% - 59% - 59% - 59% - 68% - 68% - 70% - 71% - 72% - 72% - 72% - 73% - 73% - 73% - 73% - 73% - 73% - 74% - 74% - 74% - 74% - 74% - 76% - 76% - 76% - 76% - 79% - 80% - 80% - 81% - 81% - 81% - 82% - 83% - 83% - 84% - 86% - 87% - 88% - 90% - 95% - 100% (78s) - loss: 0.7501\n",
      "Epoch 98 - 100% 1% - 1% - 3% - 5% - 5% - 5% - 5% - 5% - 11% - 11% - 14% - 15% - 15% - 19% - 23% - 23% - 24% - 28% - 28% - 28% - 31% - 33% - 34% - 34% - 37% - 37% - 38% - 38% - 42% - 43% - 47% - 47% - 48% - 48% - 54% - 57% - 58% - 58% - 61% - 62% - 63% - 64% - 64% - 67% - 67% - 67% - 68% - 72% - 74% - 74% - 78% - 85% - 85% - 89% - 90% - 90% - 90% - 90% - 91% - 91% - 97% - 97% (78s) - loss: 0.7516\n",
      "Epoch 99 - 100% 1% - 2% - 6% - 6% - 6% - 9% - 11% - 12% - 12% - 12% - 13% - 14% - 14% - 14% - 14% - 14% - 16% - 16% - 20% - 20% - 21% - 21% - 21% - 22% - 22% - 24% - 26% - 32% - 32% - 36% - 40% - 40% - 44% - 44% - 44% - 49% - 49% - 50% - 52% - 53% - 53% - 55% - 60% - 63% - 66% - 66% - 70% - 72% - 72% - 72% - 72% - 73% - 73% - 73% - 75% - 75% - 75% - 76% - 77% - 78% - 78% - 82% - 82% - 82% - 82% - 82% - 83% - 84% - 86% - 87% - 87% - 87% - 87% - 89% - 91% - 91% - 91% - 94% - 94% - 94% - 95% - 95% - 95% - 95% - 95% - 97% - 98% - 98% - 98% - 98% - 99% - 99% (77s) - loss: 0.7517\n",
      "Epoch 100 - 100% 0% - 2% - 3% - 3% - 3% - 4% - 9% - 9% - 9% - 11% - 22% - 24% - 25% - 25% - 27% - 28% - 28% - 28% - 31% - 33% - 33% - 33% - 33% - 34% - 34% - 35% - 36% - 38% - 39% - 39% - 40% - 41% - 41% - 41% - 42% - 42% - 42% - 42% - 44% - 47% - 47% - 48% - 48% - 48% - 48% - 49% - 49% - 49% - 49% - 49% - 49% - 52% - 52% - 52% - 57% - 59% - 64% - 65% - 65% - 66% - 66% - 67% - 67% - 67% - 67% - 68% - 68% - 69% - 75% - 76% - 77% - 81% - 83% - 83% - 83% - 83% - 85% - 87% - 87% - 87% - 89% - 91% - 93% - 94% - 94% - 94% - 94% - 97% - 97% - 98% - 98% - 98% - 99% - 99% - 99% (78s) - loss: 0.7515\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sys import float_info, stdout\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "data_file_name = 'all_data.csv' \n",
    "\n",
    "num_epochs = 100 \n",
    "batch_size = 32 \n",
    "num_noise_words = 2 \n",
    "vec_dim = 100 \n",
    "lr = 1e-3\n",
    "\n",
    "model_ver_is_dbow = True\n",
    "model_ver = 'dbow'\n",
    "\n",
    "context_size=0\n",
    "num_workers=1\n",
    "\n",
    "vec_combine_method='sum'\n",
    "save_all=False\n",
    "generate_plot=True\n",
    "max_generated_batches=5\n",
    "num_workers=1\n",
    "\n",
    "if vec_combine_method not in ('sum', 'concat'):\n",
    "    raise ValueError(\"Invalid method for combining paragraph and word \"\n",
    "                     \"vectors when using dm\")\n",
    "\n",
    "\n",
    "dataset = load_dataset(data_file_name)\n",
    "nce_data = NCEData(\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    context_size,\n",
    "    num_noise_words,\n",
    "    max_generated_batches,\n",
    "    num_workers)\n",
    "\n",
    "data_generator = nce_data._generator\n",
    "\n",
    "\n",
    "num_batches = len(nce_data)\n",
    "vocabulary_size = nce_data.vocabulary_size()\n",
    "\n",
    "model = DBOW(vec_dim, num_docs=len(dataset), num_words=vocabulary_size)\n",
    "cost_func = NegativeSampling()\n",
    "optimizer = Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "print(\"Dataset comprised of {:d} documents.\".format(len(dataset)))\n",
    "print(\"Vocabulary size is {:d}.\\n\".format(vocabulary_size))\n",
    "print(\"Training started.\")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "prev_model_file_path = None\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss = []\n",
    "\n",
    "    for batch_i in range(num_batches):\n",
    "        batch = data_generator.next()\n",
    "        if torch.cuda.is_available():\n",
    "            batch.cuda_()\n",
    "\n",
    "\n",
    "        x = model.forward(batch.doc_ids, batch.target_noise_ids)\n",
    "        x = cost_func.forward(x)\n",
    "\n",
    "        loss.append(x.item())\n",
    "        model.zero_grad()\n",
    "        x.backward()\n",
    "        optimizer.step()\n",
    "        _print_progress(epoch_i, batch_i, num_batches)\n",
    "\n",
    "    # end of epoch\n",
    "    loss = torch.mean(torch.FloatTensor(loss))\n",
    "    is_best_loss = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "\n",
    "    state = {\n",
    "        'epoch': epoch_i + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    prev_model_file_path = save_training_state(\n",
    "        data_file_name,\n",
    "        model_ver,\n",
    "        vec_combine_method,\n",
    "        context_size,\n",
    "        num_noise_words,\n",
    "        vec_dim,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        epoch_i,\n",
    "        loss,\n",
    "        state,\n",
    "        save_all,\n",
    "        generate_plot,\n",
    "        is_best_loss,\n",
    "        prev_model_file_path,\n",
    "        model_ver_is_dbow)\n",
    "\n",
    "    epoch_total_time = round(time.time() - epoch_start_time)\n",
    "    print(\" ({:d}s) - loss: {:.4f}\".format(epoch_total_time, loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = os.listdir(MODELS_DIR)[0]\n",
    "model_root = model_file_name.replace(\".pth.tar\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "data_file_name = 'all_data.csv' \n",
    "start(data_file_name, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_data/data/all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.750086.csv',\n",
       " 'model_data/data/sklearn.csv',\n",
       " 'model_data/data/caret.csv',\n",
       " 'model_data/data/all_data.csv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(DATA_DIR + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.750086'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 100 fields in line 130, saw 144\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7922ce890ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"/{model_root}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 100 fields in line 130, saw 144\n"
     ]
    }
   ],
   "source": [
    "doc_matrix = pd.read_csv(DATA_DIR + f\"/{model_root}.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-b934292ce9c6>:12: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x972 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_pca = PCA(n_components=5).fit_transform(doc_matrix)\n",
    "tsne = TSNE(n_components=2, perplexity=5).fit_transform(doc_pca)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsne[:,0], tsne[:,1],s=3)\n",
    "# for x, y, token in zip(tsne[:,0],tsne[:,1],mft):\n",
    "#     ax.annotate(token, xy=(x,y), size=10)\n",
    "\n",
    "fig.set_size_inches(15,13.5)\n",
    "# fig.savefig('./plots/word_embeddings.pdf',dpi=300)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
