{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import signal\n",
    "from math import ceil\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.random import choice\n",
    "from torchtext.data import Field, TabularDataset\n",
    "\n",
    "# from paragraphvec.utils import DATA_DIR\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(file_name):\n",
    "    \"\"\"Loads contents from a file in the *data* directory into a\n",
    "    torchtext.data.TabularDataset instance.\n",
    "    \"\"\"\n",
    "    file_path = join(DATA_DIR, file_name)\n",
    "    text_field = Field(pad_token=None, tokenize=_tokenize_str)\n",
    "\n",
    "    dataset = TabularDataset(\n",
    "        path=file_path,\n",
    "        format='csv',\n",
    "        fields=[('text', text_field)],\n",
    "        skip_header=True)\n",
    "\n",
    "    text_field.build_vocab(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _tokenize_str(str_):\n",
    "    # keep only alphanumeric and punctations\n",
    "    str_ = re.sub(r'[^A-Za-z0-9(),.!?\\'`]', ' ', str_)\n",
    "    # remove multiple whitespace characters\n",
    "    str_ = re.sub(r'\\s{2,}', ' ', str_)\n",
    "    # punctations to tokens\n",
    "    str_ = re.sub(r'\\(', ' ( ', str_)\n",
    "    str_ = re.sub(r'\\)', ' ) ', str_)\n",
    "    str_ = re.sub(r',', ' , ', str_)\n",
    "    str_ = re.sub(r'\\.', ' . ', str_)\n",
    "    str_ = re.sub(r'!', ' ! ', str_)\n",
    "    str_ = re.sub(r'\\?', ' ? ', str_)\n",
    "    # split contractions into multiple tokens\n",
    "    str_ = re.sub(r'\\'s', ' \\'s', str_)\n",
    "    str_ = re.sub(r'\\'ve', ' \\'ve', str_)\n",
    "    str_ = re.sub(r'n\\'t', ' n\\'t', str_)\n",
    "    str_ = re.sub(r'\\'re', ' \\'re', str_)\n",
    "    str_ = re.sub(r'\\'d', ' \\'d', str_)\n",
    "    str_ = re.sub(r'\\'ll', ' \\'ll', str_)\n",
    "    # lower case\n",
    "    return str_.strip().lower().split()\n",
    "\n",
    "\n",
    "class NCEData(object):\n",
    "    \"\"\"An infinite, parallel (multiprocess) batch generator for\n",
    "    noise-contrastive estimation of word vector models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: torchtext.data.TabularDataset\n",
    "        Dataset from which examples are generated. A column labeled *text*\n",
    "        is expected and should be comprised of a list of tokens. Each row\n",
    "        should represent a single document.\n",
    "\n",
    "    batch_size: int\n",
    "        Number of examples per single gradient update.\n",
    "\n",
    "    context_size: int\n",
    "        Half the size of a neighbourhood of target words (i.e. how many\n",
    "        words left and right are regarded as context).\n",
    "\n",
    "    num_noise_words: int\n",
    "        Number of noise words to sample from the noise distribution.\n",
    "\n",
    "    max_size: int\n",
    "        Maximum number of pre-generated batches.\n",
    "\n",
    "    num_workers: int\n",
    "        Number of jobs to run in parallel. If value is set to -1, total number\n",
    "        of machine CPUs is used.\n",
    "    \"\"\"\n",
    "    # code inspired by parallel generators in https://github.com/fchollet/keras\n",
    "    def __init__(self, dataset, batch_size, context_size,\n",
    "                 num_noise_words, max_size, num_workers):\n",
    "        self.max_size = max_size\n",
    "\n",
    "        self.num_workers = num_workers if num_workers != -1 else os.cpu_count()\n",
    "        if self.num_workers is None:\n",
    "            self.num_workers = 1\n",
    "\n",
    "        self._generator = _NCEGenerator(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            context_size,\n",
    "            num_noise_words,\n",
    "            _NCEGeneratorState(context_size))\n",
    "\n",
    "        self._queue = []\n",
    "        self._stop_event = None\n",
    "        self._processes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._generator)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return self._generator.vocabulary_size()\n",
    "\n",
    "\n",
    "\n",
    "class _NCEGenerator(object):\n",
    "    \"\"\"An infinite, process-safe batch generator for noise-contrastive\n",
    "    estimation of word vector models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state: paragraphvec.data._NCEGeneratorState\n",
    "        Initial (indexing) state of the generator.\n",
    "\n",
    "    For other parameters see the NCEData class.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, context_size,\n",
    "                 num_noise_words, state):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.context_size = context_size\n",
    "        self.num_noise_words = num_noise_words\n",
    "\n",
    "        self._vocabulary = self.dataset.fields['text'].vocab\n",
    "        self._sample_noise = None\n",
    "        self._init_noise_distribution()\n",
    "        self._state = state\n",
    "\n",
    "    def _init_noise_distribution(self):\n",
    "        # we use a unigram distribution raised to the 3/4rd power,\n",
    "        # as proposed by T. Mikolov et al. in Distributed Representations\n",
    "        # of Words and Phrases and their Compositionality\n",
    "        probs = np.zeros(len(self._vocabulary) - 1)\n",
    "\n",
    "        for word, freq in self._vocabulary.freqs.items():\n",
    "            probs[self._word_to_index(word)] = freq\n",
    "\n",
    "        probs = np.power(probs, 0.75)\n",
    "        probs /= np.sum(probs)\n",
    "\n",
    "        self._sample_noise = lambda: choice(\n",
    "            probs.shape[0], self.num_noise_words, p=probs).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        num_examples = sum(self._num_examples_in_doc(d) for d in self.dataset)\n",
    "        return ceil(num_examples / self.batch_size)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return len(self._vocabulary) - 1\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Updates state for the next process in a process-safe manner\n",
    "        and generates the current batch.\"\"\"\n",
    "        prev_doc_id, prev_in_doc_pos = self._state.update_state(\n",
    "            self.dataset,\n",
    "            self.batch_size,\n",
    "            self.context_size,\n",
    "            self._num_examples_in_doc)\n",
    "\n",
    "        # generate the actual batch\n",
    "        batch = _NCEBatch(self.context_size)\n",
    "\n",
    "        while len(batch) < self.batch_size:\n",
    "            if prev_doc_id == len(self.dataset):\n",
    "                # last document exhausted\n",
    "                batch.torch_()\n",
    "                return batch\n",
    "            if prev_in_doc_pos <= (len(self.dataset[prev_doc_id].text) - 1\n",
    "                                   - self.context_size):\n",
    "                # more examples in the current document\n",
    "                self._add_example_to_batch(prev_doc_id, prev_in_doc_pos, batch)\n",
    "                prev_in_doc_pos += 1\n",
    "            else:\n",
    "                # go to the next document\n",
    "                prev_doc_id += 1\n",
    "                prev_in_doc_pos = self.context_size\n",
    "\n",
    "        batch.torch_()\n",
    "        return batch\n",
    "\n",
    "    def _num_examples_in_doc(self, doc, in_doc_pos=None):\n",
    "        if in_doc_pos is not None:\n",
    "            # number of remaining\n",
    "            if len(doc.text) - in_doc_pos >= self.context_size + 1:\n",
    "                return len(doc.text) - in_doc_pos - self.context_size\n",
    "            return 0\n",
    "\n",
    "        if len(doc.text) >= 2 * self.context_size + 1:\n",
    "            # total number\n",
    "            return len(doc.text) - 2 * self.context_size\n",
    "        return 0\n",
    "\n",
    "    def _add_example_to_batch(self, doc_id, in_doc_pos, batch):\n",
    "        doc = self.dataset[doc_id].text\n",
    "        batch.doc_ids.append(doc_id)\n",
    "\n",
    "        # sample from the noise distribution\n",
    "        current_noise = self._sample_noise()\n",
    "        current_noise.insert(0, self._word_to_index(doc[in_doc_pos]))\n",
    "        batch.target_noise_ids.append(current_noise)\n",
    "\n",
    "        if self.context_size == 0:\n",
    "            return\n",
    "\n",
    "        current_context = []\n",
    "        context_indices = (in_doc_pos + diff for diff in\n",
    "                           range(-self.context_size, self.context_size + 1)\n",
    "                           if diff != 0)\n",
    "\n",
    "        for i in context_indices:\n",
    "            context_id = self._word_to_index(doc[i])\n",
    "            current_context.append(context_id)\n",
    "        batch.context_ids.append(current_context)\n",
    "\n",
    "    def _word_to_index(self, word):\n",
    "        return self._vocabulary.stoi[word] - 1\n",
    "\n",
    "\n",
    "class _NCEGeneratorState(object):\n",
    "    \"\"\"Batch generator state that is represented with a document id and\n",
    "    in-document position. It abstracts a process-safe indexing mechanism.\"\"\"\n",
    "    def __init__(self, context_size):\n",
    "        # use raw values because both indices have\n",
    "        # to manually be locked together\n",
    "        self._doc_id = multiprocessing.RawValue('i', 0)\n",
    "        self._in_doc_pos = multiprocessing.RawValue('i', context_size)\n",
    "        self._lock = multiprocessing.Lock()\n",
    "\n",
    "    def update_state(self, dataset, batch_size,\n",
    "                     context_size, num_examples_in_doc):\n",
    "        \"\"\"Returns current indices and computes new indices for the\n",
    "        next process.\"\"\"\n",
    "        with self._lock:\n",
    "            doc_id = self._doc_id.value\n",
    "            in_doc_pos = self._in_doc_pos.value\n",
    "            self._advance_indices(\n",
    "                dataset, batch_size, context_size, num_examples_in_doc)\n",
    "            return doc_id, in_doc_pos\n",
    "\n",
    "    def _advance_indices(self, dataset, batch_size,\n",
    "                         context_size, num_examples_in_doc):\n",
    "        num_examples = num_examples_in_doc(\n",
    "            dataset[self._doc_id.value], self._in_doc_pos.value)\n",
    "\n",
    "        if num_examples > batch_size:\n",
    "            # more examples in the current document\n",
    "            self._in_doc_pos.value += batch_size\n",
    "            return\n",
    "\n",
    "        if num_examples == batch_size:\n",
    "            # just enough examples in the current document\n",
    "            if self._doc_id.value < len(dataset) - 1:\n",
    "                self._doc_id.value += 1\n",
    "            else:\n",
    "                self._doc_id.value = 0\n",
    "            self._in_doc_pos.value = context_size\n",
    "            return\n",
    "\n",
    "        while num_examples < batch_size:\n",
    "            if self._doc_id.value == len(dataset) - 1:\n",
    "                # last document: reset indices\n",
    "                self._doc_id.value = 0\n",
    "                self._in_doc_pos.value = context_size\n",
    "                return\n",
    "\n",
    "            self._doc_id.value += 1\n",
    "            num_examples += num_examples_in_doc(\n",
    "                dataset[self._doc_id.value])\n",
    "\n",
    "        self._in_doc_pos.value = (len(dataset[self._doc_id.value].text)\n",
    "                                  - context_size\n",
    "                                  - (num_examples - batch_size))\n",
    "\n",
    "\n",
    "class _NCEBatch(object):\n",
    "    def __init__(self, context_size):\n",
    "        self.context_ids = [] if context_size > 0 else None\n",
    "        self.doc_ids = []\n",
    "        self.target_noise_ids = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def torch_(self):\n",
    "        if self.context_ids is not None:\n",
    "            self.context_ids = torch.LongTensor(self.context_ids)\n",
    "        self.doc_ids = torch.LongTensor(self.doc_ids)\n",
    "        self.target_noise_ids = torch.LongTensor(self.target_noise_ids)\n",
    "\n",
    "    def cuda_(self):\n",
    "        if self.context_ids is not None:\n",
    "            self.context_ids = self.context_ids.cuda()\n",
    "        self.doc_ids = self.doc_ids.cuda()\n",
    "        self.target_noise_ids = self.target_noise_ids.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NegativeSampling(nn.Module):\n",
    "    \"\"\"Negative sampling loss as proposed by T. Mikolov et al. in Distributed\n",
    "    Representations of Words and Phrases and their Compositionality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(NegativeSampling, self).__init__()\n",
    "        self._log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, scores):\n",
    "        \"\"\"Computes the value of the loss function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores: autograd.Variable of size (batch_size, num_noise_words + 1)\n",
    "            Sparse unnormalized log probabilities. The first element in each\n",
    "            row is the ground truth score (i.e. the target), other elements\n",
    "            are scores of samples from the noise distribution.\n",
    "        \"\"\"\n",
    "        k = scores.size()[1] - 1\n",
    "        return -torch.sum(\n",
    "            self._log_sigmoid(scores[:, 0])\n",
    "            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n",
    "        ) / scores.size()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DBOW(nn.Module):\n",
    "    \"\"\"Distributed Bag of Words version of Paragraph Vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec_dim: int\n",
    "        Dimensionality of vectors to be learned (for paragraphs and words).\n",
    "    num_docs: int\n",
    "        Number of documents in a dataset.\n",
    "    num_words: int\n",
    "        Number of distinct words in a daset (i.e. vocabulary size).\n",
    "    \"\"\"\n",
    "    def __init__(self, vec_dim, num_docs, num_words):\n",
    "        super(DBOW, self).__init__()\n",
    "        # paragraph matrix\n",
    "        self._D = nn.Parameter(\n",
    "            torch.randn(num_docs, vec_dim), requires_grad=True)\n",
    "        # output layer parameters\n",
    "        self._O = nn.Parameter(\n",
    "            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n",
    "\n",
    "    def forward(self, doc_ids, target_noise_ids):\n",
    "        \"\"\"Sparse computation of scores (unnormalized log probabilities)\n",
    "        that should be passed to the negative sampling loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc_ids: torch.Tensor of size (batch_size,)\n",
    "            Document indices of paragraphs.\n",
    "        target_noise_ids: torch.Tensor of size (batch_size, num_noise_words + 1)\n",
    "            Vocabulary indices of target and noise words. The first element in\n",
    "            each row is the ground truth index (i.e. the target), other\n",
    "            elements are indices of samples from the noise distribution.\n",
    "        Returns\n",
    "        -------\n",
    "            autograd.Variable of size (batch_size, num_noise_words + 1)\n",
    "        \"\"\"\n",
    "        # sparse computation of scores (unnormalized log probabilities)\n",
    "        # for negative sampling\n",
    "        return torch.bmm(\n",
    "            self._D[doc_ids, :].unsqueeze(1),\n",
    "            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n",
    "\n",
    "    def get_paragraph_vector(self, index):\n",
    "        return self._D[index, :].data.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import remove\n",
    "from os.path import join, dirname, isfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "_root_dir = \"model_data\"\n",
    "\n",
    "DATA_DIR = join(_root_dir, 'data')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "MODELS_DIR = join(_root_dir, 'models')\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.mkdir(MODELS_DIR)\n",
    "_DIAGNOSTICS_DIR = join(_root_dir, 'diagnostics')\n",
    "if not os.path.exists(_DIAGNOSTICS_DIR):\n",
    "    os.mkdir(_DIAGNOSTICS_DIR)\n",
    "\n",
    "_DM_MODEL_NAME = (\"{:s}_model.{:s}.{:s}_contextsize.{:d}_numnoisewords.{:d}\"\n",
    "                  \"_vecdim.{:d}_batchsize.{:d}_lr.{:f}_epoch.{:d}_loss.{:f}\"\n",
    "                  \".pth.tar\")\n",
    "_DM_DIAGNOSTIC_FILE_NAME = (\"{:s}_model.{:s}.{:s}_contextsize.{:d}\"\n",
    "                            \"_numnoisewords.{:d}_vecdim.{:d}_batchsize.{:d}\"\n",
    "                            \"_lr.{:f}.csv\")\n",
    "_DBOW_MODEL_NAME = (\"{:s}_model.{:s}_numnoisewords.{:d}_vecdim.{:d}\"\n",
    "                    \"_batchsize.{:d}_lr.{:f}_epoch.{:d}_loss.{:f}.pth.tar\")\n",
    "_DBOW_DIAGNOSTIC_FILE_NAME = (\"{:s}_model.{:s}_numnoisewords.{:d}_vecdim.{:d}\"\n",
    "                              \"_batchsize.{:d}_lr.{:f}.csv\")\n",
    "\n",
    "\n",
    "def save_training_state(data_file_name,\n",
    "                        model_ver,\n",
    "                        vec_combine_method,\n",
    "                        context_size,\n",
    "                        num_noise_words,\n",
    "                        vec_dim,\n",
    "                        batch_size,\n",
    "                        lr,\n",
    "                        epoch_i,\n",
    "                        loss,\n",
    "                        model_state,\n",
    "                        save_all,\n",
    "                        generate_plot,\n",
    "                        is_best_loss,\n",
    "                        prev_model_file_path,\n",
    "                        model_ver_is_dbow):\n",
    "    \"\"\"Saves the state of the model. If generate_plot is True, it also\n",
    "    saves current epoch's loss value and generates a plot of all loss\n",
    "    values up to this epoch.\n",
    "    Returns\n",
    "    -------\n",
    "        str representing a model file path from the previous epoch\n",
    "    \"\"\"\n",
    "    if generate_plot:\n",
    "        diagnostic_file_name = _DBOW_DIAGNOSTIC_FILE_NAME.format(\n",
    "            data_file_name[:-4],\n",
    "            model_ver,\n",
    "            num_noise_words,\n",
    "            vec_dim,\n",
    "            batch_size,\n",
    "            lr)\n",
    "\n",
    "        diagnostic_file_path = join(_DIAGNOSTICS_DIR, diagnostic_file_name)\n",
    "\n",
    "        if epoch_i == 0 and isfile(diagnostic_file_path):\n",
    "            remove(diagnostic_file_path)\n",
    "\n",
    "        with open(diagnostic_file_path, 'a') as f:\n",
    "            f.write('{:f}\\n'.format(loss))\n",
    "\n",
    "        # generate a diagnostic loss plot\n",
    "        with open(diagnostic_file_path) as f:\n",
    "            loss_values = [float(l.rstrip()) for l in f.readlines()]\n",
    "\n",
    "        diagnostic_plot_file_path = diagnostic_file_path[:-3] + 'png'\n",
    "        fig = plt.figure()\n",
    "        plt.plot(range(1, epoch_i + 2), loss_values, color='r')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('training loss')\n",
    "        fig.savefig(diagnostic_plot_file_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # save the model\n",
    "\n",
    "    model_file_name = _DBOW_MODEL_NAME.format(\n",
    "        data_file_name[:-4],\n",
    "        model_ver,\n",
    "        num_noise_words,\n",
    "        vec_dim,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        epoch_i + 1,\n",
    "        loss)\n",
    "    \n",
    "\n",
    "    model_file_path = join(MODELS_DIR, model_file_name)\n",
    "\n",
    "    if save_all:\n",
    "        torch.save(model_state, model_file_path)\n",
    "        return None\n",
    "    elif is_best_loss:\n",
    "        if prev_model_file_path is not None:\n",
    "            remove(prev_model_file_path)\n",
    "\n",
    "        torch.save(model_state, model_file_path)\n",
    "        return model_file_path\n",
    "    else:\n",
    "        return prev_model_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from os.path import join\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "\n",
    "\n",
    "def start(data_file_name, model_file_name):\n",
    "    \"\"\"Saves trained paragraph vectors to a csv file in the *data* directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_file_name: str\n",
    "        Name of a file in the *data* directory that was used during training.\n",
    "    model_file_name: str\n",
    "        Name of a file in the *models* directory (a model trained on\n",
    "        the *data_file_name* dataset).\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(data_file_name)\n",
    "\n",
    "    vec_dim = int(re.search('_vecdim\\.(\\d+)_', model_file_name).group(1))\n",
    "\n",
    "    model = _load_model(\n",
    "        model_file_name,\n",
    "        vec_dim,\n",
    "        num_docs=len(dataset),\n",
    "        num_words=len(dataset.fields['text'].vocab) - 1)\n",
    "\n",
    "    _write_to_file(data_file_name, model_file_name, model, vec_dim)\n",
    "\n",
    "\n",
    "def _load_model(model_file_name, vec_dim, num_docs, num_words):\n",
    "    model_ver = re.search('_model\\.(dm|dbow)', model_file_name).group(1)\n",
    "    if model_ver is None:\n",
    "        raise ValueError(\"Model file name contains an invalid\"\n",
    "                         \"version of the model\")\n",
    "\n",
    "    model_file_path = join(MODELS_DIR, model_file_name)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(model_file_path)\n",
    "    except AssertionError:\n",
    "        checkpoint = torch.load(\n",
    "            model_file_path,\n",
    "            map_location=lambda storage, location: storage)\n",
    "\n",
    "    if model_ver == 'dbow':\n",
    "        model = DBOW(vec_dim, num_docs, num_words)\n",
    "    else:\n",
    "        model = DM(vec_dim, num_docs, num_words)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def _write_to_file(data_file_name, model_file_name, model, vec_dim):\n",
    "    result_lines = []\n",
    "\n",
    "    with open(join(DATA_DIR, data_file_name)) as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        for i, line in enumerate(reader):\n",
    "            # skip text\n",
    "            result_line = line[1:]\n",
    "            if i == 0:\n",
    "                # header line\n",
    "                result_line += [\"d{:d}\".format(x) for x in range(vec_dim)]\n",
    "            else:\n",
    "                vector = model.get_paragraph_vector(i - 1)\n",
    "                result_line += [str(x) for x in vector]\n",
    "\n",
    "            result_lines.append(result_line)\n",
    "\n",
    "    result_file_name = model_file_name[:-7] + 'csv'\n",
    "\n",
    "    with open(join(DATA_DIR, result_file_name), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(result_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _print_progress(epoch_i, batch_i, num_batches):\n",
    "    progress = round((batch_i + 1) / num_batches * 100)\n",
    "    print(\"\\rEpoch {:d}\".format(epoch_i + 1), end='')\n",
    "    stdout.write(\" - {:d}%\".format(progress))\n",
    "    stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset comprised of 485 documents.\n",
      "Vocabulary size is 6433.\n",
      "\n",
      "Training started.\n",
      "Epoch 1 - 18%- 0% - 1% - 2% - 2% - 3% - 4% - 4% - 4% - 4% - 5% - 5% - 5% - 6% - 6% - 6% - 7% - 7% - 7% - 8% - 8% - 9% - 10% - 10% - 11% - 11% - 11% - 11% - 11% - 11% - 11% - 12% - 12% - 13% - 13% - 14% - 14% - 16% - 17% - 17% - 17% - 17% - 17% - 18% - 18%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0698cfa25d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-94947d14d304>\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             if prev_in_doc_pos <= (len(self.dataset[prev_doc_id].text) - 1\n\u001b[0m\u001b[1;32m    160\u001b[0m                                    - self.context_size):\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# more examples in the current document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sys import float_info, stdout\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "data_file_name = 'all_data.csv' \n",
    "\n",
    "num_epochs = 100 \n",
    "batch_size = 32 \n",
    "num_noise_words = 2 \n",
    "vec_dim = 100 \n",
    "lr = 1e-3\n",
    "\n",
    "model_ver_is_dbow = True\n",
    "model_ver = 'dbow'\n",
    "\n",
    "context_size=0\n",
    "num_workers=1\n",
    "\n",
    "vec_combine_method='sum'\n",
    "save_all=False\n",
    "generate_plot=True\n",
    "max_generated_batches=5\n",
    "num_workers=1\n",
    "\n",
    "if vec_combine_method not in ('sum', 'concat'):\n",
    "    raise ValueError(\"Invalid method for combining paragraph and word \"\n",
    "                     \"vectors when using dm\")\n",
    "\n",
    "\n",
    "dataset = load_dataset(data_file_name)\n",
    "nce_data = NCEData(\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    context_size,\n",
    "    num_noise_words,\n",
    "    max_generated_batches,\n",
    "    num_workers)\n",
    "\n",
    "data_generator = nce_data._generator\n",
    "\n",
    "\n",
    "num_batches = len(nce_data)\n",
    "vocabulary_size = nce_data.vocabulary_size()\n",
    "\n",
    "model = DBOW(vec_dim, num_docs=len(dataset), num_words=vocabulary_size)\n",
    "cost_func = NegativeSampling()\n",
    "optimizer = Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "print(\"Dataset comprised of {:d} documents.\".format(len(dataset)))\n",
    "print(\"Vocabulary size is {:d}.\\n\".format(vocabulary_size))\n",
    "print(\"Training started.\")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "prev_model_file_path = None\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss = []\n",
    "\n",
    "    for batch_i in range(num_batches):\n",
    "        batch = data_generator.next()\n",
    "        if torch.cuda.is_available():\n",
    "            batch.cuda_()\n",
    "\n",
    "\n",
    "        x = model.forward(batch.doc_ids, batch.target_noise_ids)\n",
    "        x = cost_func.forward(x)\n",
    "\n",
    "        loss.append(x.item())\n",
    "        model.zero_grad()\n",
    "        x.backward()\n",
    "        optimizer.step()\n",
    "        _print_progress(epoch_i, batch_i, num_batches)\n",
    "\n",
    "    # end of epoch\n",
    "    loss = torch.mean(torch.FloatTensor(loss))\n",
    "    is_best_loss = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "\n",
    "    state = {\n",
    "        'epoch': epoch_i + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    prev_model_file_path = save_training_state(\n",
    "        data_file_name,\n",
    "        model_ver,\n",
    "        vec_combine_method,\n",
    "        context_size,\n",
    "        num_noise_words,\n",
    "        vec_dim,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        epoch_i,\n",
    "        loss,\n",
    "        state,\n",
    "        save_all,\n",
    "        generate_plot,\n",
    "        is_best_loss,\n",
    "        prev_model_file_path,\n",
    "        model_ver_is_dbow)\n",
    "\n",
    "    epoch_total_time = round(time.time() - epoch_start_time)\n",
    "    print(\" ({:d}s) - loss: {:.4f}\".format(epoch_total_time, loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"sklearn_test_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.879499.pth.tar\"\n",
    "data_file_name = 'test.csv'\n",
    "start(data_file_name, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: paragraph_vectors/models/sklearn_test_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.43_loss.0.900334.pth.tar: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! ls paragraph_vectors/models/sklearn_test_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.43_loss.0.900334.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.67_loss.0.822061.pth.tar\r\n",
      "example_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.71_loss.0.881631.pth.tar\r\n",
      "sklearn_test_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.879499.pth.tar\r\n"
     ]
    }
   ],
   "source": [
    "! ls paragraph_vectors/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix = pd.read_csv(DATA_DIR + \"/sklearn_test_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.879499.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-b934292ce9c6>:12: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x972 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_pca = PCA(n_components=5).fit_transform(doc_matrix)\n",
    "tsne = TSNE(n_components=2, perplexity=5).fit_transform(doc_pca)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsne[:,0], tsne[:,1],s=3)\n",
    "# for x, y, token in zip(tsne[:,0],tsne[:,1],mft):\n",
    "#     ax.annotate(token, xy=(x,y), size=10)\n",
    "\n",
    "fig.set_size_inches(15,13.5)\n",
    "# fig.savefig('./plots/word_embeddings.pdf',dpi=300)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3",
   "language": "python",
   "name": ".venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
