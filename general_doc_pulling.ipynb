{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceID(title, package):\n",
    "    s = re.sub(\"[():,]\", \" \", title)\n",
    "    s = \"_\".join([\n",
    "        x for x in s.split(\" \")\n",
    "#         if not re.match('[0-9]', x)\n",
    "    ])\n",
    "    \n",
    "    file_name = (\n",
    "        s\n",
    "        .replace(\".\",\"_\")\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"__\",\"_\")\n",
    "        .replace(\"-\",\"\")\n",
    "        .strip(\"_\")\n",
    "        .lower()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    full_name = f'{package}_{file_name}'\n",
    "    \n",
    "    return full_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SymPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_case = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caret_base = \"https://topepo.github.io/caret/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(caret_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index.html\n",
      "1 Introduction\n",
      "visualizations.html\n",
      "2 Visualizations\n",
      "pre-processing.html\n",
      "3 Pre-Processing\n",
      "data-splitting.html\n",
      "4 Data Splitting\n",
      "model-training-and-tuning.html\n",
      "5 Model Training and Tuning\n",
      "available-models.html\n",
      "6 Available Models\n",
      "train-models-by-tag.html\n",
      "7 train Models By Tag\n",
      "models-clustered-by-tag-similarity.html\n",
      "8 Models Clustered by Tag Similarity\n",
      "parallel-processing.html\n",
      "9 Parallel Processing\n",
      "random-hyperparameter-search.html\n",
      "10 Random Hyperparameter Search\n",
      "subsampling-for-class-imbalances.html\n",
      "11 Subsampling For Class Imbalances\n",
      "using-recipes-with-train.html\n",
      "12 Using Recipes with train\n",
      "using-your-own-model-in-train.html\n",
      "13 Using Your Own Model in train\n",
      "adaptive-resampling.html\n",
      "14 Adaptive Resampling\n",
      "variable-importance.html\n",
      "15 Variable Importance\n",
      "miscellaneous-model-functions.html\n",
      "16 Miscellaneous Model Functions\n",
      "measuring-performance.html\n",
      "17 Measuring Performance\n",
      "feature-selection-overview.html\n",
      "18 Feature Selection Overview\n",
      "feature-selection-using-univariate-filters.html\n",
      "19 Feature Selection using Univariate Filters\n",
      "recursive-feature-elimination.html\n",
      "20 Recursive Feature Elimination\n",
      "feature-selection-using-genetic-algorithms.html\n",
      "21 Feature Selection using Genetic Algorithms\n",
      "feature-selection-using-simulated-annealing.html\n",
      "22 Feature Selection using Simulated Annealing\n",
      "data-sets.html\n",
      "23 Data Sets\n",
      "session-information.html\n",
      "24 Session Information\n"
     ]
    }
   ],
   "source": [
    "caret_sections = []\n",
    "\n",
    "for li in soup.find_all('li', {'class': 'chapter'}):\n",
    "\n",
    "    children = li.findChildren(\"a\" , recursive=False)\n",
    "    for child in children:\n",
    "        if \"#\" in child['href']:\n",
    "            continue\n",
    "        print(child['href'])\n",
    "        print(child.text)\n",
    "        id_ = produceID(child.text, 'caret')\n",
    "        caret_sections.append((id_, child.text, child['href']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = getPage(caret_base + caret_sections[3][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = splitSections(test, '<h2>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = []\n",
    "for name, text in sections:\n",
    "    processed.append(processhtml(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 4.1 Simple Splitting Based on the Outcome The function createDataPartition can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. For example, to create a single 80/20% split of the iris data: The list  FALSE avoids returning the data as a list. This function also has an argument, times , that can create multiple splits at once; the data indices are returned in a list of integer vectors. Similarly, createResample can be used to make simple bootstrap samples and createFolds can be used to generate balanced cross–validation groupings from a set of data. ',\n",
       " ' 4.2 Splitting Based on the Predictors Also, the function maxDissim can be used to create sub–samples using a maximum dissimilarity approach ( Willett, 1999 ). Suppose there is a data set A with m samples and a larger data set B with n samples. We may want to create a sub–sample from B that is diverse when compared to A . To do this, for each sample in B , the function calculates the m dissimilarities between each point in A . The most dissimilar point in B is added to A and the process continues. There are many methods in R to calculate dissimilarity. caret uses the proxy package. See the manual for that package for a list of available measures. Also, there are many ways to calculate which sample is “most dissimilar”. The argument obj can be used to specify any function that returns a scalar measure. caret includes two functions, minDiss and sumDiss , that can be used to maximize the minimum and total dissimilarities, respectfully. As an example, the figure below shows a scatter plot of two chemical descriptors for the Cox2 data. Using an initial random sample of 5 compounds, we can select 20 more compounds from the data so that the new compounds are most dissimilar from the initial 5 that were specified. The panels in the figure show the results using several combinations of distance metrics and scoring functions. For these data, the distance measure has less of an impact than the scoring method for determining which compounds are most dissimilar. The visualization below shows the data set (small points), the starting samples (larger blue points) and the order in which the other 20 samples are added. ',\n",
       " ' 4.3 Data Splitting for Time Series Simple random sampling of time series is probably not the best way to resample times series data. Hyndman and Athanasopoulos (2013) discuss rolling forecasting origin techniques that move the training and test sets in time. caret contains a function called createTimeSlices that can create the indices for this type of splitting. The three parameters for this type of splitting are: initialWindow : the initial number of consecutive values in each training set sample horizon : The number of consecutive values in test set sample fixedWindow : A logical: if FALSE , the training set always start at the first sample and the training set size will vary over data splits. As an example, suppose we have a time series with 20 data points. We can fix initialWindow  5 and look at different settings of the other two arguments. In the plot below, rows in each panel correspond to different data splits (i.e.\\xa0resamples) and the columns correspond to different data points. Also, red indicates samples that are in included in the training set and the blue indicates samples in the test set. ',\n",
       " ' 4.4 Simple Splitting with Important Groups In some cases there is an important qualitative factor in the data that should be considered during (re)sampling. For example: in clinical trials, there may be hospital-to-hospital differences with longitudinal or repeated measures data, subjects (or general independent experimental unit) may have multiple rows in the data set, etc. There may be an interest in making sure that these groups are not contained in the training and testing set since this may bias the test set performance to be more optimistic. Also, when one or more specific groups are held out, the resampling might capture the “ruggedness” of the model. In the example where clinical data is recorded over multiple sites, the resampling performance estimates partly measure how extensible the model is across sites. To split the data based on groups, groupKFold can be used: The results in folds can be used as inputs into the index argument of the trainControl function. This plot shows how each subject is partitioned between the modeling and holdout sets. Note that since k was less than 20 when folds was created, there are some holdouts with model than one subject. ']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content)\n",
    "    content_text = soup.find('div', {'role':\"main\"})\n",
    "    \n",
    "    return content_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, data in enumerate(test.prettify().split(header)):\n",
    "#     if idx == 0:\n",
    "#         continue\n",
    "\n",
    "#     data_string = f'{header}{data}'\n",
    "\n",
    "#     section_name = (\n",
    "#         re.sub( \"_+\", \"_\",produceID(\n",
    "#             '_'.join(BeautifulSoup(data_string)\n",
    "#             .find(header.strip(\"<>\"))\n",
    "#             .text\n",
    "#             .strip()\n",
    "#             .split('\\n')[:3]),\n",
    "#         package=''\n",
    "#         ))\n",
    "#     )\n",
    "#     print(section_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSections(page, header):\n",
    "\n",
    "    if isinstance(page, str):\n",
    "        soup = BeautifulSoup(page)\n",
    "    else:\n",
    "        soup = page\n",
    "        \n",
    "    html_sections = []\n",
    "    for idx, data in enumerate(soup.prettify().split(header)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "\n",
    "        data_string = f'{header}{data}'\n",
    "        section_name = (\n",
    "            re.sub( \"_+\", \"_\",produceID(\n",
    "                '_'.join(BeautifulSoup(data_string)\n",
    "                .find(header.strip(\"<>\"))\n",
    "                .text\n",
    "                .strip()\n",
    "                .split('\\n')[:3]),\n",
    "            package=''\n",
    "            ))\n",
    "        )\n",
    "        \n",
    "        html_sections.append((section_name, data_string))\n",
    "        \n",
    "    return html_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processhtml(section):\n",
    "    \n",
    "    if isinstance(section, str):\n",
    "        soup = BeautifulSoup(section)\n",
    "    else:\n",
    "        soup = section\n",
    "    \n",
    "    for div in soup.find_all(\"span\", {'class':'math notranslate nohighlight'}): \n",
    "        div.decompose()\n",
    "        \n",
    "    for div in soup.find_all(\"code\", {'class':'docutils literal notranslate'}): \n",
    "        div.decompose()\n",
    "        \n",
    "    for div in soup.find_all(\"pre\"): \n",
    "        div.decompose()\n",
    "        \n",
    "    rem_newlines = re.sub(r'[\\n]{1,}', ' ', soup.text)\n",
    "    x = re.sub(r'[\\ ]{2,}', ' ', rem_newlines)\n",
    "    x = x.replace('=', '')\n",
    "    x = x.replace('>>>', '')\n",
    "    x = x.replace('¶', '')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction index.html\n",
      "2 Visualizations visualizations.html\n",
      "3 Pre-Processing pre-processing.html\n",
      "caret_3_preprocessing _3_1_creating_dummy_variables\n",
      "caret_3_preprocessing _3_2_zero_and_near_zerovariance_predictors\n",
      "caret_3_preprocessing _3_3_identifying_correlated_predictors\n",
      "caret_3_preprocessing _3_4_linear_dependencies\n",
      "caret_3_preprocessing _3_5_the\n",
      "caret_3_preprocessing _3_6_centering_and_scaling\n",
      "caret_3_preprocessing _3_7_imputation\n",
      "caret_3_preprocessing _3_8_transforming_predictors\n",
      "caret_3_preprocessing _3_9_putting_it_all_together\n",
      "caret_3_preprocessing _3_10_class_distance_calculations\n",
      "4 Data Splitting data-splitting.html\n",
      "caret_4_data_splitting _4_1_simple_splitting_based_on_the_outcome\n",
      "caret_4_data_splitting _4_2_splitting_based_on_the_predictors\n",
      "caret_4_data_splitting _4_3_data_splitting_for_time_series\n",
      "caret_4_data_splitting _4_4_simple_splitting_with_important_groups\n",
      "5 Model Training and Tuning model-training-and-tuning.html\n",
      "caret_5_model_training_and_tuning _5_1_model_training_and_parameter_tuning\n",
      "caret_5_model_training_and_tuning _5_2_an_example\n",
      "caret_5_model_training_and_tuning _5_3_basic_parameter_tuning\n",
      "caret_5_model_training_and_tuning _5_4_notes_on_reproducibility\n",
      "caret_5_model_training_and_tuning _5_5_customizing_the_tuning_process\n",
      "caret_5_model_training_and_tuning _5_6_choosing_the_final_model\n",
      "caret_5_model_training_and_tuning _5_7_extracting_predictions_and_class_probabilities\n",
      "caret_5_model_training_and_tuning _5_8_exploring_and_comparing_resampling_distributions\n",
      "caret_5_model_training_and_tuning _5_9_fitting_models_without_parameter_tuning\n",
      "6 Available Models available-models.html\n",
      "7 train Models By Tag train-models-by-tag.html\n",
      "caret_7_train_models_by_tag _7_0_1_accepts_case_weights\n",
      "caret_7_train_models_by_tag _7_0_2_bagging\n",
      "caret_7_train_models_by_tag _7_0_3_bayesian_model\n",
      "caret_7_train_models_by_tag _7_0_4_binary_predictors_only\n",
      "caret_7_train_models_by_tag _7_0_5_boosting\n",
      "caret_7_train_models_by_tag _7_0_6_categorical_predictors_only\n",
      "caret_7_train_models_by_tag _7_0_7_cost_sensitive_learning\n",
      "caret_7_train_models_by_tag _7_0_8_discriminant_analysis\n",
      "caret_7_train_models_by_tag _7_0_9_distance_weighted_discrimination\n",
      "caret_7_train_models_by_tag _7_0_10_ensemble_model\n",
      "caret_7_train_models_by_tag _7_0_11_feature_extraction\n",
      "caret_7_train_models_by_tag _7_0_12_feature_selection_wrapper\n",
      "caret_7_train_models_by_tag _7_0_13_gaussian_process\n",
      "caret_7_train_models_by_tag _7_0_14_generalized_additive_model\n",
      "caret_7_train_models_by_tag _7_0_15_generalized_linear_model\n",
      "caret_7_train_models_by_tag _7_0_16_handle_missing_predictor_data\n",
      "caret_7_train_models_by_tag _7_0_17_implicit_feature_selection\n",
      "caret_7_train_models_by_tag _7_0_18_kernel_method\n",
      "caret_7_train_models_by_tag _7_0_19_l1_regularization\n",
      "caret_7_train_models_by_tag _7_0_20_l2_regularization\n",
      "caret_7_train_models_by_tag _7_0_21_linear_classifier\n",
      "caret_7_train_models_by_tag _7_0_22_linear_regression\n",
      "caret_7_train_models_by_tag _7_0_23_logic_regression\n",
      "caret_7_train_models_by_tag _7_0_24_logistic_regression\n",
      "caret_7_train_models_by_tag _7_0_25_mixture_model\n",
      "caret_7_train_models_by_tag _7_0_26_model_tree\n",
      "caret_7_train_models_by_tag _7_0_27_multivariate_adaptive_regression_splines\n",
      "caret_7_train_models_by_tag _7_0_28_neural_network\n",
      "caret_7_train_models_by_tag _7_0_29_oblique_tree\n",
      "caret_7_train_models_by_tag _7_0_30_ordinal_outcomes\n",
      "caret_7_train_models_by_tag _7_0_31_partial_least_squares\n",
      "caret_7_train_models_by_tag _7_0_32_patient_rule_induction_method\n",
      "caret_7_train_models_by_tag _7_0_33_polynomial_model\n",
      "caret_7_train_models_by_tag _7_0_34_prototype_models\n",
      "caret_7_train_models_by_tag _7_0_35_quantile_regression\n",
      "caret_7_train_models_by_tag _7_0_36_radial_basis_function\n",
      "caret_7_train_models_by_tag _7_0_37_random_forest\n",
      "caret_7_train_models_by_tag _7_0_38_regularization\n",
      "caret_7_train_models_by_tag _7_0_39_relevance_vector_machines\n",
      "caret_7_train_models_by_tag _7_0_40_ridge_regression\n",
      "caret_7_train_models_by_tag _7_0_41_robust_methods\n",
      "caret_7_train_models_by_tag _7_0_42_robust_model\n",
      "caret_7_train_models_by_tag _7_0_43_roc_curves\n",
      "caret_7_train_models_by_tag _7_0_44_rulebased_model\n",
      "caret_7_train_models_by_tag _7_0_45_selforganising_maps\n",
      "caret_7_train_models_by_tag _7_0_46_string_kernel\n",
      "caret_7_train_models_by_tag _7_0_47_support_vector_machines\n",
      "caret_7_train_models_by_tag _7_0_48_supports_class_probabilities\n",
      "caret_7_train_models_by_tag _7_0_49_text_mining\n",
      "caret_7_train_models_by_tag _7_0_50_treebased_model\n",
      "caret_7_train_models_by_tag _7_0_51_two_class_only\n",
      "8 Models Clustered by Tag Similarity models-clustered-by-tag-similarity.html\n",
      "9 Parallel Processing parallel-processing.html\n",
      "10 Random Hyperparameter Search random-hyperparameter-search.html\n",
      "11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html\n",
      "caret_11_subsampling_for_class_imbalances _11_1_subsampling_techniques\n",
      "caret_11_subsampling_for_class_imbalances _11_2_subsampling_during_resampling\n",
      "caret_11_subsampling_for_class_imbalances _11_3_complications\n",
      "caret_11_subsampling_for_class_imbalances _11_4_using_custom_subsampling_techniques\n",
      "12 Using Recipes with train using-recipes-with-train.html\n",
      "caret_12_using_recipes_with_train _12_1_why_should_you_learn_this?\n",
      "caret_12_using_recipes_with_train _12_2_an_example\n",
      "caret_12_using_recipes_with_train _12_3_case_weights\n",
      "13 Using Your Own Model in train using-your-own-model-in-train.html\n",
      "caret_13_using_your_own_model_in_train _13_1_introduction\n",
      "caret_13_using_your_own_model_in_train _13_2_illustrative_example_1_svms_with_laplacian_kernels\n",
      "caret_13_using_your_own_model_in_train _13_3_model_components\n",
      "caret_13_using_your_own_model_in_train _13_4_the_sort_element\n",
      "caret_13_using_your_own_model_in_train _13_5_illustrative_example_2_something_more_complicated\n",
      "caret_13_using_your_own_model_in_train _13_6_illustrative_example_3_nonstandard_formulas\n",
      "caret_13_using_your_own_model_in_train _13_7_illustrative_example_4_pls_feature_extraction_preprocessing\n",
      "caret_13_using_your_own_model_in_train _13_8_illustrative_example_5_optimizing_probability_thresholds_for_class_imbalances\n",
      "caret_13_using_your_own_model_in_train _13_9_illustrative_example_6_offsets_in_generalized_linear_models\n",
      "14 Adaptive Resampling adaptive-resampling.html\n",
      "15 Variable Importance variable-importance.html\n",
      "caret_15_variable_importance _15_1_model_specific_metrics\n",
      "caret_15_variable_importance _15_2_model_independent_metrics\n",
      "caret_15_variable_importance _15_3_an_example\n",
      "16 Miscellaneous Model Functions miscellaneous-model-functions.html\n",
      "caret_16_miscellaneous_model_functions _16_1_yet_another\n",
      "caret_16_miscellaneous_model_functions _16_2_partial_least_squares_discriminant_analysis\n",
      "caret_16_miscellaneous_model_functions _16_3_bagged_mars_and_fda\n",
      "caret_16_miscellaneous_model_functions _16_4_bagging\n",
      "caret_16_miscellaneous_model_functions _16_5_model_averaged_neural_networks\n",
      "caret_16_miscellaneous_model_functions _16_6_neural_networks_with_a_principal_component_step\n",
      "caret_16_miscellaneous_model_functions _16_7_independent_component_regression\n",
      "17 Measuring Performance measuring-performance.html\n",
      "caret_17_measuring_performance _17_1_measures_for_regression\n",
      "caret_17_measuring_performance _17_2_measures_for_predicted_classes\n",
      "caret_17_measuring_performance _17_3_measures_for_class_probabilities\n",
      "caret_17_measuring_performance _17_4_lift_curves\n",
      "caret_17_measuring_performance _17_5_calibration_curves\n",
      "18 Feature Selection Overview feature-selection-overview.html\n",
      "caret_18_feature_selection_overview _18_1_models_with_builtin_feature_selection\n",
      "caret_18_feature_selection_overview _18_2_feature_selection_methods\n",
      "caret_18_feature_selection_overview _18_3_external_validation\n",
      "19 Feature Selection using Univariate Filters feature-selection-using-univariate-filters.html\n",
      "caret_19_feature_selection_using_univariate_filters _19_1_univariate_filters\n",
      "caret_19_feature_selection_using_univariate_filters _19_2_basic_syntax\n",
      "caret_19_feature_selection_using_univariate_filters _19_3_the_example\n",
      "20 Recursive Feature Elimination recursive-feature-elimination.html\n",
      "caret_20_recursive_feature_elimination _20_1_backwards_selection\n",
      "caret_20_recursive_feature_elimination _20_2_resampling_and_external_validation\n",
      "caret_20_recursive_feature_elimination _20_3_recursive_feature_elimination_via\n",
      "caret_20_recursive_feature_elimination _20_4_an_example\n",
      "caret_20_recursive_feature_elimination _20_5_helper_functions\n",
      "caret_20_recursive_feature_elimination _20_6_the_example\n",
      "caret_20_recursive_feature_elimination _20_7_using_a_recipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html\n",
      "caret_21_feature_selection_using_genetic_algorithms _21_1_genetic_algorithms\n",
      "caret_21_feature_selection_using_genetic_algorithms _21_2_internal_and_external_performance_estimates\n",
      "caret_21_feature_selection_using_genetic_algorithms _21_3_basic_syntax\n",
      "caret_21_feature_selection_using_genetic_algorithms _21_4_genetic_algorithm_example\n",
      "caret_21_feature_selection_using_genetic_algorithms _21_5_customizing_the_search\n",
      "caret_21_feature_selection_using_genetic_algorithms _21_6_the_example_revisited\n",
      "caret_21_feature_selection_using_genetic_algorithms _21_7_using_recipes\n",
      "22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html\n",
      "caret_22_feature_selection_using_simulated_annealing _22_1_simulated_annealing\n",
      "caret_22_feature_selection_using_simulated_annealing _22_2_internal_and_external_performance_estimates\n",
      "caret_22_feature_selection_using_simulated_annealing _22_3_basic_syntax\n",
      "caret_22_feature_selection_using_simulated_annealing _22_4_simulated_annealing_example\n",
      "caret_22_feature_selection_using_simulated_annealing _22_5_customizing_the_search\n",
      "caret_22_feature_selection_using_simulated_annealing _22_6_using_recipes\n",
      "23 Data Sets data-sets.html\n",
      "caret_23_data_sets _23_1_bloodbrain_barrier_data\n",
      "caret_23_data_sets _23_2_cox2_activity_data\n",
      "caret_23_data_sets _23_3_dhfr_inhibition\n",
      "caret_23_data_sets _23_4_tecator_nir_data\n",
      "caret_23_data_sets _23_5_fatty_acid_composition_data\n",
      "caret_23_data_sets _23_6_german_credit_data\n",
      "caret_23_data_sets _23_7_kelly_blue_book\n",
      "caret_23_data_sets _23_8_cell_body_segmentation_data\n",
      "caret_23_data_sets _23_9_sacramento_house_price_data\n",
      "caret_23_data_sets _23_10_animal_scat_data\n",
      "24 Session Information session-information.html\n"
     ]
    }
   ],
   "source": [
    "for pageid, title, link in caret_sections:\n",
    "    print(title, link)\n",
    "    \n",
    "    if not os.path.exists('caret'):\n",
    "        os.mkdir('caret')\n",
    "    \n",
    "    try:\n",
    "        page = getPage(caret_base + link)\n",
    "        splits = splitSections(page, '<h2>')\n",
    "        if not splits:\n",
    "            splits = splitSections(page, '<h3>')\n",
    "\n",
    "        for section_name, section_data in splits:\n",
    "            text = processhtml(section_data)\n",
    "            print(pageid, section_name)\n",
    "            with open(f'caret/{pageid}{section_name}.txt', 'w') as f:\n",
    "                f.write(f\"{pageid}\\n{title}\\n{link}\\n\") \n",
    "                f.write(str(text))\n",
    "    except:\n",
    "        print(f\"Error retrieving page {title}\")\n",
    "        \n",
    "    sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
