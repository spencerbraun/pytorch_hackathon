{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.7.0-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from torchtext) (1.6.0)\n",
      "Requirement already satisfied: requests in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from torchtext) (2.24.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from torchtext) (1.19.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp38-cp38-macosx_10_6_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 24.4 MB/s eta 0:00:01     |███████                         | 225 kB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from torch->torchtext) (0.18.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from requests->torchtext) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from requests->torchtext) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages (from requests->torchtext) (2.10)\n",
      "Installing collected packages: tqdm, sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0 tqdm-4.48.0\n"
     ]
    }
   ],
   "source": [
    "! pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torchtext.data import Field, TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 7.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _tokenize_str(str_):\n",
    "    # keep only alphanumeric and punctations\n",
    "    str_ = re.sub(r'[^A-Za-z0-9(),.!?\\'`]', ' ', str_)\n",
    "    # remove multiple whitespace characters\n",
    "    str_ = re.sub(r'\\s{2,}', ' ', str_)\n",
    "    # punctations to tokens\n",
    "    str_ = re.sub(r'\\(', ' ( ', str_)\n",
    "    str_ = re.sub(r'\\)', ' ) ', str_)\n",
    "    str_ = re.sub(r',', ' , ', str_)\n",
    "    str_ = re.sub(r'\\.', ' . ', str_)\n",
    "    str_ = re.sub(r'!', ' ! ', str_)\n",
    "    str_ = re.sub(r'\\?', ' ? ', str_)\n",
    "    # split contractions into multiple tokens\n",
    "    str_ = re.sub(r'\\'s', ' \\'s', str_)\n",
    "    str_ = re.sub(r'\\'ve', ' \\'ve', str_)\n",
    "    str_ = re.sub(r'n\\'t', ' n\\'t', str_)\n",
    "    str_ = re.sub(r'\\'re', ' \\'re', str_)\n",
    "    str_ = re.sub(r'\\'d', ' \\'d', str_)\n",
    "    str_ = re.sub(r'\\'ll', ' \\'ll', str_)\n",
    "    # lower case\n",
    "    return str_.strip().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_path = 'example.csv'\n",
    "text_field = Field(pad_token=None, tokenize=_tokenize_str)\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path=file_path,\n",
    "    format='csv',\n",
    "    fields=[('text', text_field)],\n",
    "    skip_header=True)\n",
    "\n",
    "text_field.build_vocab(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spencerbraun/dev/virtualenvs/venv3/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator = Iterator(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var):\n",
    "        self.dl = dl \n",
    "        self.x_var = x_var\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            yield(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iterator, \"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DBOW(nn.Module):\n",
    "    \"\"\"Distributed Bag of Words version of Paragraph Vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec_dim: int\n",
    "        Dimensionality of vectors to be learned (for paragraphs and words).\n",
    "    num_docs: int\n",
    "        Number of documents in a dataset.\n",
    "    num_words: int\n",
    "        Number of distinct words in a daset (i.e. vocabulary size).\n",
    "    \"\"\"\n",
    "    def __init__(self, vec_dim, num_docs, num_words):\n",
    "        super(DBOW, self).__init__()\n",
    "        # paragraph matrix\n",
    "        self._D = nn.Parameter(\n",
    "            torch.randn(num_docs, vec_dim), requires_grad=True)\n",
    "        # output layer parameters\n",
    "        self._O = nn.Parameter(\n",
    "            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n",
    "\n",
    "    def forward(self, doc_ids, target_noise_ids):\n",
    "        \"\"\"Sparse computation of scores (unnormalized log probabilities)\n",
    "        that should be passed to the negative sampling loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc_ids: torch.Tensor of size (batch_size,)\n",
    "            Document indices of paragraphs.\n",
    "        target_noise_ids: torch.Tensor of size (batch_size, num_noise_words + 1)\n",
    "            Vocabulary indices of target and noise words. The first element in\n",
    "            each row is the ground truth index (i.e. the target), other\n",
    "            elements are indices of samples from the noise distribution.\n",
    "        Returns\n",
    "        -------\n",
    "            autograd.Variable of size (batch_size, num_noise_words + 1)\n",
    "        \"\"\"\n",
    "        # sparse computation of scores (unnormalized log probabilities)\n",
    "        # for negative sampling\n",
    "        return torch.bmm(\n",
    "            self._D[doc_ids, :].unsqueeze(1),\n",
    "            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n",
    "\n",
    "    def get_paragraph_vector(self, index):\n",
    "        return self._D[index, :].data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NegativeSampling(nn.Module):\n",
    "    \"\"\"Negative sampling loss as proposed by T. Mikolov et al. in Distributed\n",
    "    Representations of Words and Phrases and their Compositionality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(NegativeSampling, self).__init__()\n",
    "        self._log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, scores):\n",
    "        \"\"\"Computes the value of the loss function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores: autograd.Variable of size (batch_size, num_noise_words + 1)\n",
    "            Sparse unnormalized log probabilities. The first element in each\n",
    "            row is the ground truth score (i.e. the target), other elements\n",
    "            are scores of samples from the noise distribution.\n",
    "        \"\"\"\n",
    "        k = scores.size()[1] - 1\n",
    "        return -torch.sum(\n",
    "            self._log_sigmoid(scores[:, 0])\n",
    "            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n",
    "        ) / scores.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "vocab_size = len(text_field.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBOW(64, num_docs=len(dataset), num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset comprised of 4 documents.\n",
      "Vocabulary size is 110.\n",
      "\n",
      "Training started.\n"
     ]
    }
   ],
   "source": [
    "cost_func = NegativeSampling()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "print(\"Dataset comprised of {:d} documents.\".format(len(dataset)))\n",
    "print(\"Vocabulary size is {:d}.\\n\".format(vocab_size))\n",
    "print(\"Training started.\")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "prev_model_file_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "model.train() # turn on training mode\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss = []\n",
    "    \n",
    "    \n",
    "    for batch_i in tqdm.tqdm(train_dl): \n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(batch_i)\n",
    "        loss = loss_func(y, preds)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data[0] * x.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "\n",
    "    for batch_i in range(num_batches):\n",
    "        batch = next(data_generator)\n",
    "        if torch.cuda.is_available():\n",
    "            batch.cuda_()\n",
    "\n",
    "        if model_ver_is_dbow:\n",
    "            x = model.forward(batch.doc_ids, batch.target_noise_ids)\n",
    "        else:\n",
    "            x = model.forward(\n",
    "                batch.context_ids,\n",
    "                batch.doc_ids,\n",
    "                batch.target_noise_ids)\n",
    "\n",
    "        x = cost_func.forward(x)\n",
    "\n",
    "        loss.append(x.item())\n",
    "        model.zero_grad()\n",
    "        x.backward()\n",
    "        optimizer.step()\n",
    "        _print_progress(epoch_i, batch_i, num_batches)\n",
    "\n",
    "    # end of epoch\n",
    "    loss = torch.mean(torch.FloatTensor(loss))\n",
    "    is_best_loss = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "\n",
    "    state = {\n",
    "        'epoch': epoch_i + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    prev_model_file_path = save_training_state(\n",
    "        data_file_name,\n",
    "        model_ver,\n",
    "        vec_combine_method,\n",
    "        context_size,\n",
    "        num_noise_words,\n",
    "        vec_dim,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        epoch_i,\n",
    "        loss,\n",
    "        state,\n",
    "        save_all,\n",
    "        generate_plot,\n",
    "        is_best_loss,\n",
    "        prev_model_file_path,\n",
    "        model_ver_is_dbow)\n",
    "\n",
    "    epoch_total_time = round(time.time() - epoch_start_time)\n",
    "    print(\" ({:d}s) - loss: {:.4f}\".format(epoch_total_time, loss))\n",
    "\n",
    "\n",
    "def _print_progress(epoch_i, batch_i, num_batches):\n",
    "progress = round((batch_i + 1) / num_batches * 100)\n",
    "print(\"\\rEpoch {:d}\".format(epoch_i + 1), end='')\n",
    "stdout.write(\" - {:d}%\".format(progress))\n",
    "stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sys import float_info, stdout\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "def start(data_file_name,\n",
    "          num_noise_words,\n",
    "          vec_dim,\n",
    "          num_epochs,\n",
    "          batch_size,\n",
    "          lr,\n",
    "          model_ver='dbow',\n",
    "          context_size=0,\n",
    "          vec_combine_method='sum',\n",
    "          save_all=False,\n",
    "          generate_plot=True,\n",
    "          max_generated_batches=5,\n",
    "          num_workers=1):\n",
    "    \"\"\"Trains a new model. The latest checkpoint and the best performing\n",
    "    model are saved in the *models* directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_file_name: str\n",
    "        Name of a file in the *data* directory.\n",
    "    model_ver: str, one of ('dm', 'dbow'), default='dbow'\n",
    "        Version of the model as proposed by Q. V. Le et al., Distributed\n",
    "        Representations of Sentences and Documents. 'dbow' stands for\n",
    "        Distributed Bag Of Words, 'dm' stands for Distributed Memory.\n",
    "    vec_combine_method: str, one of ('sum', 'concat'), default='sum'\n",
    "        Method for combining paragraph and word vectors when model_ver='dm'.\n",
    "        Currently only the 'sum' operation is implemented.\n",
    "    context_size: int, default=0\n",
    "        Half the size of a neighbourhood of target words when model_ver='dm'\n",
    "        (i.e. how many words left and right are regarded as context). When\n",
    "        model_ver='dm' context_size has to greater than 0, when\n",
    "        model_ver='dbow' context_size has to be 0.\n",
    "    num_noise_words: int\n",
    "        Number of noise words to sample from the noise distribution.\n",
    "    vec_dim: int\n",
    "        Dimensionality of vectors to be learned (for paragraphs and words).\n",
    "    num_epochs: int\n",
    "        Number of iterations to train the model (i.e. number\n",
    "        of times every example is seen during training).\n",
    "    batch_size: int\n",
    "        Number of examples per single gradient update.\n",
    "    lr: float\n",
    "        Learning rate of the Adam optimizer.\n",
    "    save_all: bool, default=False\n",
    "        Indicates whether a checkpoint is saved after each epoch.\n",
    "        If false, only the best performing model is saved.\n",
    "    generate_plot: bool, default=True\n",
    "        Indicates whether a diagnostic plot displaying loss value over\n",
    "        epochs is generated after each epoch.\n",
    "    max_generated_batches: int, default=5\n",
    "        Maximum number of pre-generated batches.\n",
    "    num_workers: int, default=1\n",
    "        Number of batch generator jobs to run in parallel. If value is set\n",
    "        to -1 number of machine cores are used.\n",
    "    \"\"\"\n",
    "    if model_ver not in ('dm', 'dbow'):\n",
    "        raise ValueError(\"Invalid version of the model\")\n",
    "\n",
    "    model_ver_is_dbow = model_ver == 'dbow'\n",
    "\n",
    "    if model_ver_is_dbow and context_size != 0:\n",
    "        raise ValueError(\"Context size has to be zero when using dbow\")\n",
    "    if not model_ver_is_dbow:\n",
    "        if vec_combine_method not in ('sum', 'concat'):\n",
    "            raise ValueError(\"Invalid method for combining paragraph and word \"\n",
    "                             \"vectors when using dm\")\n",
    "        if context_size <= 0:\n",
    "            raise ValueError(\"Context size must be positive when using dm\")\n",
    "\n",
    "    dataset = load_dataset(data_file_name)\n",
    "    nce_data = NCEData(\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        context_size,\n",
    "        num_noise_words,\n",
    "        max_generated_batches,\n",
    "        num_workers)\n",
    "    nce_data.start()\n",
    "\n",
    "    try:\n",
    "        _run(data_file_name, dataset, nce_data.get_generator(), len(nce_data),\n",
    "             nce_data.vocabulary_size(), context_size, num_noise_words, vec_dim,\n",
    "             num_epochs, batch_size, lr, model_ver, vec_combine_method,\n",
    "             save_all, generate_plot, model_ver_is_dbow)\n",
    "    except KeyboardInterrupt:\n",
    "        nce_data.stop()\n",
    "\n",
    "\n",
    "def _run(data_file_name,\n",
    "         dataset,\n",
    "         data_generator,\n",
    "         num_batches,\n",
    "         vocabulary_size,\n",
    "         context_size,\n",
    "         num_noise_words,\n",
    "         vec_dim,\n",
    "         num_epochs,\n",
    "         batch_size,\n",
    "         lr,\n",
    "         model_ver,\n",
    "         vec_combine_method,\n",
    "         save_all,\n",
    "         generate_plot,\n",
    "         model_ver_is_dbow):\n",
    "\n",
    "    if model_ver_is_dbow:\n",
    "        model = DBOW(vec_dim, num_docs=len(dataset), num_words=vocabulary_size)\n",
    "    else:\n",
    "        model = DM(vec_dim, num_docs=len(dataset), num_words=vocabulary_size)\n",
    "\n",
    "    cost_func = NegativeSampling()\n",
    "    optimizer = Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    print(\"Dataset comprised of {:d} documents.\".format(len(dataset)))\n",
    "    print(\"Vocabulary size is {:d}.\\n\".format(vocabulary_size))\n",
    "    print(\"Training started.\")\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    prev_model_file_path = None\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        loss = []\n",
    "\n",
    "        for batch_i in range(num_batches):\n",
    "            batch = next(data_generator)\n",
    "            if torch.cuda.is_available():\n",
    "                batch.cuda_()\n",
    "\n",
    "            if model_ver_is_dbow:\n",
    "                x = model.forward(batch.doc_ids, batch.target_noise_ids)\n",
    "            else:\n",
    "                x = model.forward(\n",
    "                    batch.context_ids,\n",
    "                    batch.doc_ids,\n",
    "                    batch.target_noise_ids)\n",
    "\n",
    "            x = cost_func.forward(x)\n",
    "\n",
    "            loss.append(x.item())\n",
    "            model.zero_grad()\n",
    "            x.backward()\n",
    "            optimizer.step()\n",
    "            _print_progress(epoch_i, batch_i, num_batches)\n",
    "\n",
    "        # end of epoch\n",
    "        loss = torch.mean(torch.FloatTensor(loss))\n",
    "        is_best_loss = loss < best_loss\n",
    "        best_loss = min(loss, best_loss)\n",
    "\n",
    "        state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        prev_model_file_path = save_training_state(\n",
    "            data_file_name,\n",
    "            model_ver,\n",
    "            vec_combine_method,\n",
    "            context_size,\n",
    "            num_noise_words,\n",
    "            vec_dim,\n",
    "            batch_size,\n",
    "            lr,\n",
    "            epoch_i,\n",
    "            loss,\n",
    "            state,\n",
    "            save_all,\n",
    "            generate_plot,\n",
    "            is_best_loss,\n",
    "            prev_model_file_path,\n",
    "            model_ver_is_dbow)\n",
    "\n",
    "        epoch_total_time = round(time.time() - epoch_start_time)\n",
    "        print(\" ({:d}s) - loss: {:.4f}\".format(epoch_total_time, loss))\n",
    "\n",
    "\n",
    "def _print_progress(epoch_i, batch_i, num_batches):\n",
    "    progress = round((batch_i + 1) / num_batches * 100)\n",
    "    print(\"\\rEpoch {:d}\".format(epoch_i + 1), end='')\n",
    "    stdout.write(\" - {:d}%\".format(progress))\n",
    "    stdout.flush()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3",
   "language": "python",
   "name": ".venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
