scipy_integration_scipy_integrate
Integration (scipy.integrate)
integrate.html
 Ordinary differential equations ( solve_ivp )  Integrating a set of ordinary differential equations (ODEs) given initial conditions is another useful example. The function solve_ivp is available in SciPy for integrating a first-order vector differential equation: \[\frac{d\mathbf{y}}{dt}\mathbf{f}\left(\mathbf{y},t\right),\] given initial conditions , where is a length vector and is a mapping from to A higher-order ordinary differential equation can always be reduced to a differential equation of this type by introducing intermediate derivatives into the vector. For example, suppose it is desired to find the solution to the following second-order differential equation: \[\frac{d^{2}w}{dz^{2}}-zw(z)0\] with initial conditions and It is known that the solution to this differential equation with these boundary conditions is the Airy function \[w\textrm{Ai}\left(z\right),\] which gives a means to check the integrator using special.airy . First, convert this ODE into standard form by setting and . Thus, the differential equation becomes \[\begin{split}\frac{d\mathbf{y}}{dt}\left[\begin{array}{c} ty_{1}\\ y_{0}\end{array}\right]\left[\begin{array}{cc} 0 & t\\ 1 & 0\end{array}\right]\left[\begin{array}{c} y_{0}\\ y_{1}\end{array}\right]\left[\begin{array}{cc} 0 & t\\ 1 & 0\end{array}\right]\mathbf{y}.\end{split}\] In other words, \[\mathbf{f}\left(\mathbf{y},t\right)\mathbf{A}\left(t\right)\mathbf{y}.\] As an interesting reminder, if commutes with under matrix multiplication, then this linear differential equation has an exact solution using the matrix exponential: \[\mathbf{y}\left(t\right)\exp\left(\int_{0}^{t}\mathbf{A}\left(\tau\right)d\tau\right)\mathbf{y}\left(0\right),\] However, in this case, and its integral do not commute. This differential equation can be solved using the function solve_ivp . It requires the derivative, fprime , the time span [t_start, t_end] and the initial conditions vector, y0 , as input arguments and returns an object whose y field is an array with consecutive solution values as columns. The initial conditions are therefore given in the first output column. As it can be seen solve_ivp determines its time steps automatically if not specified otherwise. To compare the solution of solve_ivp with the airy function the time vector created by solve_ivp is passed to the airy function. The solution of solve_ivp with its standard parameters shows a big deviation to the airy function. To minimize this deviation, relative and absolute tolerances can be used. To specify user defined time points for the solution of solve_ivp , solve_ivp offers two possibilities that can also be used complementarily. By passing the t_eval option to the function call solve_ivp returns the solutions of these time points of t_eval in its output. If the jacobian matrix of function is known, it can be passed to the solve_ivp to achieve better results. Please be aware however that the default integration method RK45 does not support jacobian matrices and thereby another integration method has to be chosen. One of the integration methods that support a jacobian matrix is the for example the Radau method of following example. Solving a system with a banded Jacobian matrix  odeint can be told that the Jacobian is banded . For a large system of differential equations that are known to be stiff, this can improve performance significantly. As an example, we’ll solve the 1-D Gray-Scott partial differential equations using the method of lines [MOL] . The Gray-Scott equations for the functions and on the interval are \[\begin{split}\begin{split} \frac{\partial u}{\partial t}  D_u \frac{\partial^2 u}{\partial x^2} - uv^2 + f(1-u) \\ \frac{\partial v}{\partial t}  D_v \frac{\partial^2 v}{\partial x^2} + uv^2 - (f + k)v \\ \end{split}\end{split}\] where and are the diffusion coefficients of the components and , respectively, and and are constants. (For more information about the system, see http://groups.csail.mit.edu/mac/projects/amorphous/GrayScott/ ) We’ll assume Neumann (i.e., “no flux”) boundary conditions: \[\frac{\partial u}{\partial x}(0,t)  0, \quad \frac{\partial v}{\partial x}(0,t)  0, \quad \frac{\partial u}{\partial x}(L,t)  0, \quad \frac{\partial v}{\partial x}(L,t)  0\] To apply the method of lines, we discretize the variable by defining the uniformly spaced grid of points , with and . We define and , and replace the derivatives with finite differences. That is, \[\frac{\partial^2 u}{\partial x^2}(x_j, t) \rightarrow \frac{u_{j-1}(t) - 2 u_{j}(t) + u_{j+1}(t)}{(\Delta x)^2}\] We then have a system of ordinary differential equations: (1)  \[\begin{split} \begin{split} \frac{du_j}{dt}  \frac{D_u}{(\Delta x)^2} \left(u_{j-1} - 2 u_{j} + u_{j+1}\right) -u_jv_j^2 + f(1 - u_j) \\ \frac{dv_j}{dt}  \frac{D_v}{(\Delta x)^2} \left(v_{j-1} - 2 v_{j} + v_{j+1}\right) + u_jv_j^2 - (f + k)v_j \end{split}\end{split}\] For convenience, the arguments have been dropped. To enforce the boundary conditions, we introduce “ghost” points and , and define , ; and are defined analogously. Then (2)  \[\begin{split} \begin{split} \frac{du_0}{dt}  \frac{D_u}{(\Delta x)^2} \left(2u_{1} - 2 u_{0}\right) -u_0v_0^2 + f(1 - u_0) \\ \frac{dv_0}{dt}  \frac{D_v}{(\Delta x)^2} \left(2v_{1} - 2 v_{0}\right) + u_0v_0^2 - (f + k)v_0 \end{split}\end{split}\] and (3)  \[\begin{split} \begin{split} \frac{du_{N-1}}{dt}  \frac{D_u}{(\Delta x)^2} \left(2u_{N-2} - 2 u_{N-1}\right) -u_{N-1}v_{N-1}^2 + f(1 - u_{N-1}) \\ \frac{dv_{N-1}}{dt}  \frac{D_v}{(\Delta x)^2} \left(2v_{N-2} - 2 v_{N-1}\right) + u_{N-1}v_{N-1}^2 - (f + k)v_{N-1} \end{split}\end{split}\] Our complete system of ordinary differential equations is (1) for , along with (2) and (3) . We can now starting implementing this system in code. We must combine and into a single vector of length . The two obvious choices are and . Mathematically, it does not matter, but the choice affects how efficiently odeint can solve the system. The reason is in how the order affects the pattern of the nonzero elements of the Jacobian matrix. When the variables are ordered as , the pattern of nonzero elements of the Jacobian matrix is \[\begin{split}\begin{smallmatrix} * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\ * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 \\ 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 \\ 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 \\ 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 \\ 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 \\ 0 & 0 & 0 & 0 & 0 & * & * & 0 & 0 & 0 & 0 & 0 & 0 & * \\ * & 0 & 0 & 0 & 0 & 0 & 0 & * & * & 0 & 0 & 0 & 0 & 0 \\ 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 \\ 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 \\ 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 \\ 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 \\ 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * \\ 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & ) & * & * \\ \end{smallmatrix}\end{split}\] The Jacobian pattern with variables interleaved as is \[\begin{split}\begin{smallmatrix} * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * \\ \end{smallmatrix}\end{split}\] In both cases, there are just five nontrivial diagonals, but when the variables are interleaved, the bandwidth is much smaller. That is, the main diagonal and the two diagonals immediately above and the two immediately below the main diagonal are the nonzero diagonals. This is important, because the inputs and of odeint are the upper and lower bandwidths of the Jacobian matrix. When the variables are interleaved, and are 2. When the variables are stacked with following , the upper and lower bandwidths are . With that decision made, we can write the function that implements the system of differential equations. First, we define the functions for the source and reaction terms of the system: Next, we define the function that computes the right-hand side of the system of differential equations: We won’t implement a function to compute the Jacobian, but we will tell odeint that the Jacobian matrix is banded. This allows the underlying solver (LSODA) to avoid computing values that it knows are zero. For a large system, this improves the performance significantly, as demonstrated in the following ipython session. First, we define the required inputs: Time the computation without taking advantage of the banded structure of the Jacobian matrix: Now set and , so odeint knows that the Jacobian matrix is banded: That is quite a bit faster! Let’s ensure that they have computed the same result: References  WPR https://en.wikipedia.org/wiki/Romberg’s_method MOL https://en.wikipedia.org/wiki/Method_of_lines 