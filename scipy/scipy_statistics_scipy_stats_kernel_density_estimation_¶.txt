scipy_statistics_scipy_stats
Statistics (scipy.stats)
stats.html
 Kernel density estimation  A common task in statistics is to estimate the probability density function (PDF) of a random variable from a set of data samples. This task is called density estimation. The most well-known tool to do this is the histogram. A histogram is a useful tool for visualization (mainly because everyone understands it), but doesn’t use the available data very efficiently. Kernel density estimation (KDE) is a more efficient tool for the same task. The gaussian_kde estimator can be used to estimate the PDF of univariate as well as multivariate data. It works best if the data is unimodal. Univariate estimation  We start with a minimal amount of data in order to see how gaussian_kde works and what the different options for bandwidth selection do. The data sampled from the PDF are shown as blue dashes at the bottom of the figure (this is called a rug plot): We see that there is very little difference between Scott’s Rule and Silverman’s Rule, and that the bandwidth selection with a limited amount of data is probably a bit too wide. We can define our own bandwidth function to get a less smoothed-out result. We see that if we set bandwidth to be very narrow, the obtained estimate for the probability density function (PDF) is simply the sum of Gaussians around each data point. We now take a more realistic example and look at the difference between the two available bandwidth selection rules. Those rules are known to work well for (close to) normal distributions, but even for unimodal distributions that are quite strongly non-normal they work reasonably well. As a non-normal distribution we take a Student’s T distribution with 5 degrees of freedom. We now take a look at a bimodal distribution with one wider and one narrower Gaussian feature. We expect that this will be a more difficult density to approximate, due to the different bandwidths required to accurately resolve each feature. As expected, the KDE is not as close to the true PDF as we would like due to the different characteristic size of the two features of the bimodal distribution. By halving the default bandwidth ( ), we can do somewhat better, while using a factor 5 smaller bandwidth than the default doesn’t smooth enough. What we really need, though, in this case, is a non-uniform (adaptive) bandwidth. Multivariate estimation  With gaussian_kde we can perform multivariate, as well as univariate estimation. We demonstrate the bivariate case. First, we generate some random data with a model in which the two variates are correlated. Then we apply the KDE to the data: Finally, we plot the estimated bivariate distribution as a colormap and plot the individual data points on top. Multiscale Graph Correlation (MGC)  With multiscale_graphcorr , we can test for independence on high dimensional and nonlinear data. Before we start, let’s import some useful packages: Let’s use a custom plotting function to plot the data relationship: Let’s look at some linear data first: The simulation relationship can be plotted below: Now, we can see the test statistic, p-value, and MGC map visualized below. The optimal scale is shown on the map as a red “x”: It is clear from here, that MGC is able to determine a relationship between the input data matrices because the p-value is very low and the MGC test statistic is relatively high. The MGC-map indicates a strongly linear relationship . Intuitively, this is because having more neighbors will help in identifying a linear relationship between and . The optimal scale in this case is equivalent to the global scale , marked by a red spot on the map. The same can be done for nonlinear data sets. The following and arrays are derived from a nonlinear simulation: The simulation relationship can be plotted below: Now, we can see the test statistic, p-value, and MGC map visualized below. The optimal scale is shown on the map as a red “x”: It is clear from here, that MGC is able to determine a relationship again because the p-value is very low and the MGC test statistic is relatively high. The MGC-map indicates a strongly nonlinear relationship . The optimal scale in this case is equivalent to the local scale , marked by a red spot on the map. 