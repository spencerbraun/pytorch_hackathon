scipy_optimization_scipy_optimize
Optimization (scipy.optimize)
optimize.html
 Least-squares minimization ( least_squares )  SciPy is capable of solving robustified bound-constrained nonlinear least-squares problems: \begin{align} &\min_\mathbf{x} \frac{1}{2} \sum_{i  1}^m \rho\left(f_i(\mathbf{x})^2\right) \\ &\text{subject to }\mathbf{lb} \leq \mathbf{x} \leq \mathbf{ub} \end{align} Here are smooth functions from to , we refer to them as residuals. The purpose of a scalar-valued function is to reduce the influence of outlier residuals and contribute to robustness of the solution, we refer to it as a loss function. A linear loss function gives a standard least-squares problem. Additionally, constraints in a form of lower and upper bounds on some of are allowed. All methods specific to least-squares minimization utilize a matrix of partial derivatives called Jacobian and defined as . It is highly recommended to compute this matrix analytically and pass it to least_squares , otherwise, it will be estimated by finite differences, which takes a lot of additional time and can be very inaccurate in hard cases. Function least_squares can be used for fitting a function to empirical data . To do this, one should simply precompute residuals as , where are weights assigned to each observation. Example of solving a fitting problem  Here we consider an enzymatic reaction 1 . There are 11 residuals defined as \[f_i(x)  \frac{x_0 (u_i^2 + u_i x_1)}{u_i^2 + u_i x_2 + x_3} - y_i, \quad i  0, \ldots, 10,\] where are measurement values and are values of the independent variable. The unknown vector of parameters is . As was said previously, it is recommended to compute Jacobian matrix in a closed form: \begin{align} &J_{i0}  \frac{\partial f_i}{\partial x_0}  \frac{u_i^2 + u_i x_1}{u_i^2 + u_i x_2 + x_3} \\ &J_{i1}  \frac{\partial f_i}{\partial x_1}  \frac{u_i x_0}{u_i^2 + u_i x_2 + x_3} \\ &J_{i2}  \frac{\partial f_i}{\partial x_2}  -\frac{x_0 (u_i^2 + u_i x_1) u_i}{(u_i^2 + u_i x_2 + x_3)^2} \\ &J_{i3}  \frac{\partial f_i}{\partial x_3}  -\frac{x_0 (u_i^2 + u_i x_1)}{(u_i^2 + u_i x_2 + x_3)^2} \end{align} We are going to use the “hard” starting point defined in 2 . To find a physically meaningful solution, avoid potential division by zero and assure convergence to the global minimum we impose constraints . The code below implements least-squares estimation of and finally plots the original data and the fitted model function: 1 J. Kowalik and J. F. Morrison, “Analysis of kinetic data for allosteric enzyme reactions as a nonlinear regression problem”, Math. Biosci., vol. 2, pp. 57-66, 1968. 2 Averick et al., “The MINPACK-2 Test Problem Collection”. Further examples  Three interactive examples below illustrate usage of least_squares in greater detail. Large-scale bundle adjustment in scipy demonstrates large-scale capabilities of least_squares and how to efficiently compute finite difference approximation of sparse Jacobian. Robust nonlinear regression in scipy shows how to handle outliers with a robust loss function in a nonlinear regression. Solving a discrete boundary-value problem in scipy examines how to solve a large system of equations and use bounds to achieve desired properties of the solution. For the details about mathematical algorithms behind the implementation refer to documentation of least_squares . 