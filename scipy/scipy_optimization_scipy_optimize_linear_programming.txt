scipy_optimization_scipy_optimize
Optimization (scipy.optimize)
optimize.html
 Linear programming ( linprog )  The function linprog can minimize a linear objective function subject to linear equality and inequality constraints. This kind of problem is well known as linear programming. Linear programming solves problems of the following form: \[\begin{split}\min_x \ & c^T x \\ \mbox{such that} \ & A_{ub} x \leq b_{ub},\\ & A_{eq} x  b_{eq},\\ & l \leq x \leq u ,\end{split}\] where is a vector of decision variables; , , , , and are vectors; and and are matrices. In this tutorial, we will try to solve a typical linear programming problem using linprog . Linear programming example  Consider the following simple linear programming problem: \[\begin{split}\max_{x_1, x_2, x_3, x_4} \ & 29x_1 + 45x_2 \\ \mbox{such that} \ & x_1 -x_2 -3x_3 \leq 5\\ & 2x_1 -3x_2 -7x_3 + 3x_4 \geq 10\\ & 2x_1 + 8x_2 + x_3  60\\ & 4x_1 + 4x_2 + x_4  60\\ & 0 \leq x_0\\ & 0 \leq x_1 \leq 5\\ & x_2 \leq 0.5\\ & -3 \leq x_3\\\end{split}\] We need some mathematical manipulations to convert the target problem to the form accepted by linprog . First of all, let’s consider the objective function. We want to maximize the objective function, but linprog can only accept a minimization problem. This is easily remedied by converting the maximize to minimizing . Also, are not shown in the objective function. That means the weights corresponding with are zero. So, the objective function can be converted to: \[\min_{x_1, x_2, x_3, x_4} \ -29x_1 -45x_2 + 0x_3 + 0x_4\] If we define the vector of decision variables , the objective weights vector of linprog in this problem should be \[c  [-29, -45, 0, 0]^T\] Next, let’s consider the two inequality constraints. The first one is a “less than” inequality, so it is already in the form accepted by linprog . The second one is a “greater than” inequality, so we need to multiply both sides by to convert it to a “less than” inequality. Explicitly showing zero coefficients, we have: \[\begin{split}x_1 -x_2 -3x_3 + 0x_4 &\leq 5\\ -2x_1 + 3x_2 + 7x_3 - 3x_4 &\leq -10\\\end{split}\] These equations can be converted to matrix form: \[\begin{split}A_{ub} x \leq b_{ub}\\\end{split}\] where \begin{equation*} A_{ub}  \begin{bmatrix} 1 & -1 & -3 & 0 \\ -2 & 3 & 7 & -3 \end{bmatrix} \end{equation*} \begin{equation*} b_{ub}  \begin{bmatrix} 5 \\ -10 \end{bmatrix} \end{equation*} Next, let’s consider the two equality constraints. Showing zero weights explicitly, these are: \[\begin{split}2x_1 + 8x_2 + 1x_3 + 0x_4 & 60\\ 4x_1 + 4x_2 + 0x_3 + 1x_4 & 60\\\end{split}\] These equations can be converted to matrix form: \[\begin{split}A_{eq} x  b_{eq}\\\end{split}\] where \begin{equation*} A_{eq}  \begin{bmatrix} 2 & 8 & 1 & 0 \\ 4 & 4 & 0 & 1 \end{bmatrix} \end{equation*} \begin{equation*} b_{eq}  \begin{bmatrix} 60 \\ 60 \end{bmatrix} \end{equation*} Lastly, let’s consider the separate inequality constraints on individual decision variables, which are known as “box constraints” or “simple bounds”. These constraints can be applied using the bounds argument of linprog . As noted in the linprog documentation, the default value of bounds is , meaning that the lower bound on each decision variable is 0, and the upper bound on each decision variable is infinity: all the decision variables are non-negative. Our bounds are different, so we will need to specify the lower and upper bound on each decision variable as a tuple and group these tuples into a list. Finally, we can solve the transformed problem using linprog . The result states that our problem is infeasible, meaning that there is no solution vector that satisfies all the constraints. That doesn’t necessarily mean we did anything wrong; some problems truly are infeasible. Suppose, however, that we were to decide that our bound constraint on was too tight and that it could be loosened to . After adjusting our code to reflect the change and executing it again: The result shows the optimization was successful. We can check the objective value ( ) is same as : We can also check that all constraints are satisfied within reasonable tolerances: If we need greater accuracy, typically at the expense of speed, we can solve using the method: References Some further reading and related software, such as Newton-Krylov [KK] , PETSc [PP] , and PyAMG [AMG] : KK D.A. Knoll and D.E. Keyes, “Jacobian-free Newton-Krylov methods”, J. Comp. Phys. 193, 357 (2004). doi:10.1016/j.jcp.2003.08.010 PP PETSc https://www.mcs.anl.gov/petsc/ and its Python bindings https://bitbucket.org/petsc/petsc4py/ AMG PyAMG (algebraic multigrid preconditioners/solvers) https://github.com/pyamg/pyamg/issues 