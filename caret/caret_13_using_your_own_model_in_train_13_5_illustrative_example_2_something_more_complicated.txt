caret_13_using_your_own_model_in_train
13 Using Your Own Model in train
using-your-own-model-in-train.html
 13.5 Illustrative Example 2: Something More Complicated - LogitBoost ###The loop Element This function can be used to create custom loops for models to tune over. In most cases, the function can just return the existing tuning grid. For example, a LogitBoost model can be trained over the number of boosting iterations. In the caTools package, the LogitBoost function can be used to fit this model. For example: If we were to tune the model evaluating models where the number of iterations was 11, 21, 31, 41 and 51, the grid could be During resampling, train could loop over all five rows in lbGrid and fit five models. However, the predict.LogitBoost function has an argument called nIter that can produce, in this case, predictions from mod for all five models. Instead of train fitting five models, we could fit a single model with nIter  class“hl num”>51 and derive predictions for all five models using only mod`. The terminology used here is that nIter is a sequential tuning parameter (and the other parameters would be considered fixed ). The loop argument for models is used to produce two objects: loop : this is the actual loop that is used by train . submodels is a list that has as many elements as there are rows in loop . The list has all the “extra” parameter settings that can be derived for each model. Going back to the LogitBoost example, we could have: For this case, train first fits the nIter  51 model. When the model is predicted, that code has a for loop that iterates over the elements of submodel[[1]] to get the predictions for the other 4 models. In the end, predictions for all five models (for nIter  seq(11, 51, by  10) ) with a single model fit. There are other models built-in to caret that are used this way. There are a number of models that have multiple sequential tuning parameters. If the loop argument is left NULL the results of tuneGrid are used as the simple loop and is recommended for most situations. Note that the machinery that is used to “derive” the extra predictions is up to the user to create, typically in the predict and prob elements of the custom model object. For the LogitBoost model, some simple code to create these objects would be: For the LogitBoost custom model object, we could use this code in the predict slot: A few more notes: The code in the fit element does not have to change. The prob slot works in the same way. The only difference is that the values saved in the outgoing lists are matrices or data frames of probabilities for each class. After model training (i.e. predicting new samples), the value of submodels is set to NULL and the code produces a single set of predictions. If the model had one sequential parameter and one fixed parameter, the loop data frame would have two columns (one for each parameter). If the model is tuned over more than one value of the fixed parameter, the submodels list would have more than one element. If loop had 10 rows, then length(submodels) would be 10 and loop[i,] would be linked to submodels[[i]] . In this case, the prediction function was called by namespace too (i.e. caTools::predict.LogitBoost ). This may not seem necessary but what functions are available can vary depending on what parallel processing technology is being used. For example, the nature of forking used by doMC and doParallel tends to have easier access to functions while PSOCK methods in doParallel do not. It may be easier to take the safe path of using the namespace operator wherever possible to avoid errors that are difficult to track down. Here is a slimmed down version of the logitBoost code already in the package: Should you care about this? Let’s tune the model over the same data set used for the SVM model above and see how long it takes: On a data set with 157 instances and 60 predictors and a model that is tuned over only 3 parameter values, there is a 1.57-fold speed-up. If the model were more computationally taxing or the data set were larger or the number of tune parameters that were evaluated was larger, the speed-up would increase. Here is a plot of the speed-up for a few more values of tuneLength : The speed-ups show a significant decrease in training time using this method. Note: The previous examples were run using parallel processing. The remainder in this chapter are run sequentially and, for simplicity, the namespace operator is not used in the custom code modules below. 