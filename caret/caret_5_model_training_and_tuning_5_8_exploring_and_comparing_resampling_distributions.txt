caret_5_model_training_and_tuning
5 Model Training and Tuning
model-training-and-tuning.html
 5.8 Exploring and Comparing Resampling Distributions 5.8.1 Within-Model There are several lattice functions than can be used to explore relationships between tuning parameters and the resampling results for a specific model: xyplot and stripplot can be used to plot resampling statistics against (numeric) tuning parameters. histogram and densityplot can also be used to look at distributions of the tuning parameters across tuning parameters. For example, the following statements create a density plot: Note that if you are interested in plotting the resampling results across multiple tuning parameters, the option resamples  "all" should be used in the control object. 5.8.2 Between-Models The caret package also includes functions to characterize the differences between models (generated using train , sbf or rfe ) via their resampling distributions. These functions are based on the work of Hothorn et al. (2005) and Eugster et al (2008) . First, a support vector machine model is fit to the Sonar data. The data are centered and scaled using the preProc argument. Note that the same random number seed is set prior to the model that is identical to the seed used for the boosted tree model. This ensures that the same resampling sets are used, which will come in handy when we compare the resampling profiles between models. Also, a regularized discriminant analysis model was fit. Given these models, can we make statistical statements about their performance differences? To do this, we first collect the resampling results using resamples . Note that, in this case, the option resamples  "final" should be user-defined in the control objects. There are several lattice plot methods that can be used to visualize the resampling distributions: density plots, box-whisker plots, scatterplot matrices and scatterplots of summary statistics. For example: Other visualizations are availible in densityplot.resamples and parallel.resamples Since models are fit on the same versions of the training data, it makes sense to make inferences on the differences between models. In this way we reduce the within-resample correlation that may exist. We can compute the differences, then use a simple t -test to evaluate the null hypothesis that there is no difference between models. 