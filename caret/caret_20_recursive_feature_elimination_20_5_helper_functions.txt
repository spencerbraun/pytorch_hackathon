caret_20_recursive_feature_elimination
20 Recursive Feature Elimination
recursive-feature-elimination.html
 20.5 Helper Functions To use feature elimination for an arbitrary model, a set of functions must be passed to rfe for each of the steps in Algorithm 2. This section defines those functions and uses the existing random forest functions as an illustrative example. caret contains a list called rfFuncs , but this document will use a more simple version that will be better for illustrating the ideas. A set of simplified functions used here and called rfRFE . 20.5.1 The summary Function The summary function takes the observed and predicted values and computes one or more performance metrics (see line 2.14). The input is a data frame with columns obs and pred . The output should be a named vector of numeric variables. Note that the metric argument of the rfe function should reference one of the names of the output of summary . The example function is: Two functions in caret that can be used as the summary funciton are defaultSummary and twoClassSummary (for classification problems with two classes). 20.5.2 The fit Function This function builds the model based on the current data set (lines 2.3, 2.9 and 2.17). The arguments for the function must be: x : the current training set of predictor data with the appropriate subset of variables y : the current outcome data (either a numeric or factor vector) first : a single logical value for whether the current predictor set has all possible variables (e.g. line 2.3) last : similar to first , but TRUE when the last model is fit with the final subset size and predictors. (line 2.17) ... : optional arguments to pass to the fit function in the call to rfe The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: For feature selection without re-ranking at each iteration, the random forest variable importances only need to be computed on the first iterations when all of the predictors are in the model. This can be accomplished using importance``  first . 20.5.3 The pred Function This function returns a vector of predictions (numeric or factors) from the current model (lines 2.4 and 2.10). The input arguments must be object : the model generated by the fit function x : the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 20.5.4 The rank Function This function is used to return the predictors in the order of the most important to the least important (lines 2.5 and 2.11). Inputs are: object : the model generated by the fit function x : the current set of predictor set for the training samples y : the current training outcomes The function should return a data frame with a column called var that has the current variable names. The first row should be the most important predictor etc. Other columns can be included in the output and will be returned in the final rfe object. For random forests, the function below uses caret ’s varImp function to extract the random forest importances and orders them. For classification, randomForest will produce a column of importances for each class. In this case, the default ranking function orders the predictors by the averages importance across the classes. 20.5.5 The selectSize Function This function determines the optimal number of predictors based on the resampling output (line 2.15). Inputs for the function are: x : a matrix with columns for the performance metrics and the number of variables, called Variables metric : a character string of the performance measure to optimize (e.g. RMSE, Accuracy) maximize : a single logical for whether the metric should be maximized This function should return an integer corresponding to the optimal subset size. caret comes with two examples functions for this purpose: pickSizeBest and pickSizeTolerance . The former simply selects the subset size that has the best value. The latter takes into account the whole profile and tries to pick a subset size that is small without sacrificing too much performance. For example, suppose we have computed the RMSE over a series of variables sizes: These are depicted in the figure below. The solid circle identifies the subset size with the absolute smallest RMSE. However, there are many smaller subsets that produce approximately the same performance but with fewer predictors. In this case, we might be able to accept a slightly larger error for less predictors. The pickSizeTolerance determines the absolute best value then the percent difference of the other points to this value. In the case of RMSE, this would be where RMSE {opt} is the absolute best error rate. These “tolerance” values are plotted in the bottom panel. The solid triangle is the smallest subset size that is within 10% of the optimal value. This approach can produce good results for many of the tree based models, such as random forest, where there is a plateau of good performance for larger subset sizes. For trees, this is usually because unimportant variables are infrequently used in splits and do not significantly affect performance. 20.5.6 The selectVar Function After the optimal subset size is determined, this function will be used to calculate the best rankings for each variable across all the resampling iterations (line 2.16). Inputs for the function are: y : a list of variables importance for each resampling iteration and each subset size (generated by the user-defined rank function). In the example, each each of the cross-validation groups the output of the rank function is saved for each of the 10 subset sizes (including the original subset). If the rankings are not recomputed at each iteration, the values will be the same within each cross-validation iteration. size : the integer returned by the selectSize function This function should return a character string of predictor names (of length size ) in the order of most important to least important For random forests, only the first importance calculation (line 2.5) is used since these are the rankings on the full set of predictors. These importances are averaged and the top predictors are returned. Note that if the predictor rankings are recomputed at each iteration (line 2.11) the user will need to write their own selection function to use the other ranks. 