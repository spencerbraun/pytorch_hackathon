caret_17_measuring_performance
17 Measuring Performance
measuring-performance.html
 17.2 Measures for Predicted Classes Before proceeding, let’s make up some test set data: We would expect that this model will do well on these data: Generating the predicted classes based on the typical 50% cutoff for the probabilities, we can compute the confusion matrix , which shows a cross-tabulation of the observed and predicted classes. The confusionMatrix function can be used to generate these results: For two classes, this function assumes that the class corresponding to an event is the first class level (but this can be changed using the positive argument. Note that there are a number of statistics shown here. The “no-information rate” is the largest proportion of the observed classes (there were more class 2 data than class 1 in this test set). A hypothesis test is also computed to evaluate whether the overall accuracy rate is greater than the rate of the largest class. Also, the prevalence of the “positive event” is computed from the data (unless passed in as an argument), the detection rate (the rate of true events also predicted to be events) and the detection prevalence (the prevalence of predicted events). If the prevalence of the event is different than those seen in the test set, the prevalence option can be used to adjust this. Suppose a 2x2 table: When there are three or more classes, confusionMatrix will show the confusion matrix and a set of “one-versus-all” results. For example, in a three class problem, the sensitivity of the first class is calculated against all the samples in the second and third classes (and so on). The confusionMatrix matrix frames the errors in terms of sensitivity and specificity. In the case of information retrieval, the precision and recall might be more appropriate. In this case, the option mode can be used to get those statistics: Again, the positive argument can be used to control which factor level is associated with a “found” or “important” document or sample. There are individual functions called sensitivity , specificity , posPredValue , negPredValue , precision , recall , and F_meas . Also, a resampled estimate of the training set can also be obtained using confusionMatrix.train . For each resampling iteration, a confusion matrix is created from the hold-out samples and these values can be aggregated to diagnose issues with the model fit. These values are the percentages that hold-out samples landed in the confusion matrix during resampling. There are several methods for normalizing these values. See ?confusionMatrix.train for details. The default performance function used by train is postResample , which generates the accuracy and Kappa statistics: As shown below, another function called twoClassSummary can be used to get the sensitivity and specificity using the default probability cutoff. Another function, multiClassSummary , can do similar calculations when there are three or more classes but both require class probabilities for each class. 