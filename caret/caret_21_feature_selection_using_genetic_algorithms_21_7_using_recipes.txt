caret_21_feature_selection_using_genetic_algorithms
21 Feature Selection using Genetic Algorithms
feature-selection-using-genetic-algorithms.html
 21.7 Using Recipes Like the other feature selection routines, gafs can take a data recipe as an input. This is advantageous when your data needs preprocessing before the model, such as: creation of dummy variables from factors specification of interactions missing data imputation more complex feature engineering methods Like train , the recipe’s preprocessing steps are calculated within each resample. This makes sure that the resampling statistics capture the variation and effect that the preprocessing has on the model. As an example, the Ames housing data is used. These data contain a number of categorical predictors that require conversion to indicators as well as other variables that require processing. To load (and split) the data: Here is a recipe that does differetn types of preprocssing on the predictor set: If this were executed on the training set, it would produce 280 predictor columns out of the original 79. Let’s tune some linear models with gafs and, for the sake of computational time, only use 10 generations of the algorithm: 