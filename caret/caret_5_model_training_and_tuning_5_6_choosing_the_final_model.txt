caret_5_model_training_and_tuning
5 Model Training and Tuning
model-training-and-tuning.html
 5.6 Choosing the Final Model Another method for customizing the tuning process is to modify the algorithm that is used to select the “best” parameter values, given the performance numbers. By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models). Other schemes for selecting model can be used. Breiman et al (1984) suggested the “one standard error rule” for simple tree-based models. In this case, the model with the best performance value is identified and, using resampling, we can estimate the standard error of performance. The final model used was the simplest model within one standard error of the (empirically) best model. With simple trees this makes sense, since these models will start to over-fit as they become more and more specific to the training data. train allows the user to specify alternate rules for selecting the final model. The argument selectionFunction can be used to supply a function to algorithmically determine the final model. There are three existing functions in the package: best is chooses the largest/smallest value, oneSE attempts to capture the spirit of Breiman et al (1984) and tolerance selects the least complex model within some percent tolerance of the best value. See ?best for more details. User-defined functions can be used, as long as they have the following arguments: x is a data frame containing the tune parameters and their associated performance metrics. Each row corresponds to a different tuning parameter combination. metric a character string indicating which performance metric should be optimized (this is passed in directly from the metric argument of train . maximize is a single logical value indicating whether larger values of the performance metric are better (this is also directly passed from the call to train ). The function should output a single integer indicating which row in x is chosen. As an example, if we chose the previous boosted tree model on the basis of overall accuracy, we would choose: n.trees  1450, interaction.depth  5, shrinkage  0.1, n.minobsinnode  20. However, the scale in this plots is fairly tight, with accuracy values ranging from 0.863 to 0.922. A less complex model (e.g. fewer, more shallow trees) might also yield acceptable accuracy. The tolerance function could be used to find a less complex model based on ( x - x best )/ x best x 100, which is the percent difference. For example, to select parameter values based on a 2% loss of performance: This indicates that we can get a less complex model with an area under the ROC curve of 0.914 (compared to the “pick the best” value of 0.922). The main issue with these functions is related to ordering the models from simplest to complex. In some cases, this is easy (e.g. simple trees, partial least squares), but in cases such as this model, the ordering of models is subjective. For example, is a boosted tree model using 100 iterations and a tree depth of 2 more complex than one with 50 iterations and a depth of 8? The package makes some choices regarding the orderings. In the case of boosted trees, the package assumes that increasing the number of iterations adds complexity at a faster rate than increasing the tree depth, so models are ordered on the number of iterations then ordered with depth. See ?best for more examples for specific models. 