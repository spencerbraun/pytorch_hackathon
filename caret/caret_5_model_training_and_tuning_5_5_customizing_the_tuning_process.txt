caret_5_model_training_and_tuning
5 Model Training and Tuning
model-training-and-tuning.html
 5.5 Customizing the Tuning Process There are a few ways to customize the process of selecting tuning/complexity parameters and building the final model. 5.5.1 Pre-Processing Options As previously mentioned, train can pre-process the data in various ways prior to model fitting. The function preProcess is automatically used. This function can be used for centering and scaling, imputation (see details below), applying the spatial sign transformation and feature extraction via principal component analysis or independent component analysis. To specify what pre-processing should occur, the train function has an argument called preProcess . This argument takes a character string of methods that would normally be passed to the method argument of the preProcess function . Additional options to the preProcess function can be passed via the trainControl function. These processing steps would be applied during any predictions generated using predict.train , extractPrediction or extractProbs (see details later in this document). The pre-processing would not be applied to predictions that directly use the object$finalModel object. For imputation, there are three methods currently implemented: k -nearest neighbors takes a sample with missing values and finds the k closest samples in the training set. The average of the k training set values for that predictor are used as a substitute for the original data. When calculating the distances to the training set samples, the predictors used in the calculation are the ones with no missing values for that sample and no missing values in the training set. another approach is to fit a bagged tree model for each predictor using the training set samples. This is usually a fairly accurate model and can handle missing values. When a predictor for a sample requires imputation, the values for the other predictors are fed through the bagged tree and the prediction is used as the new value. This model can have significant computational cost. the median of the predictor’s training set values can be used to estimate the missing data. If there are missing values in the training set, PCA and ICA models only use complete samples. 5.5.2 Alternate Tuning Grids The tuning parameter grid can be specified by the user. The argument tuneGrid can take a data frame with columns for each tuning parameter. The column names should be the same as the fitting function’s arguments. For the previously mentioned RDA example, the names would be gamma and lambda . train will tune the model over each combination of values in the rows. For the boosted tree model, we can fix the learning rate and evaluate more than three values of n.trees : Another option is to use a random sample of possible tuning parameter combinations, i.e. “random search” (pdf) . This functionality is described on this page . To use a random search, use the option search  "random" in the call to trainControl . In this situation, the tuneLength parameter defines the total number of parameter combinations that will be evaluated. 5.5.3 Plotting the Resampling Profile The plot function can be used to examine the relationship between the estimates of performance and the tuning parameters. For example, a simple invokation of the function shows the results for the first performance measure: Other performance metrics can be shown using the metric option: Other types of plot are also available. See ?plot.train for more details. The code below shows a heatmap of the results: A ggplot method can also be used: There are also plot functions that show more detailed representations of the resampled estimates. See ?xyplot.train for more details. From these plots, a different set of tuning parameters may be desired. To change the final values without starting the whole process again, the update.train can be used to refit the final model. See ?update.train 5.5.4 The trainControl Function The function trainControl generates parameters that further control how models are created, with possible values: method : The resampling method: "boot" , "cv" , "LOOCV" , "LGOCV" , "repeatedcv" , "timeslice" , "none" and "oob" . The last value, out-of-bag estimates, can only be used by random forest, bagged trees, bagged earth, bagged flexible discriminant analysis, or conditional tree forest models. GBM models are not included (the gbm package maintainer has indicated that it would not be a good idea to choose tuning parameter values based on the model OOB error estimates with boosted trees). Also, for leave-one-out cross-validation, no uncertainty estimates are given for the resampled performance measures. number and repeats : number controls with the number of folds in K -fold cross-validation or number of resampling iterations for bootstrapping and leave-group-out cross-validation. repeats applied only to repeated K -fold cross-validation. Suppose that method  "repeatedcv" , number  10 and repeats  3 ,then three separate 10-fold cross-validations are used as the resampling scheme. verboseIter : A logical for printing a training log. returnData : A logical for saving the data into a slot called trainingData . p : For leave-group out cross-validation: the training percentage For method  "timeslice" , trainControl has options initialWindow , horizon and fixedWindow that govern how cross-validation can be used for time series data. classProbs : a logical value determining whether class probabilities should be computed for held-out samples during resample. index and indexOut : optional lists with elements for each resampling iteration. Each list element is the sample rows used for training at that iteration or should be held-out. When these values are not specified, train will generate them. summaryFunction : a function to computed alternate performance summaries. selectionFunction : a function to choose the optimal tuning parameters. and examples. PCAthresh , ICAcomp and k : these are all options to pass to the preProcess function (when used). returnResamp : a character string containing one of the following values: "all" , "final" or "none" . This specifies how much of the resampled performance measures to save. allowParallel : a logical that governs whether train should use parallel processing (if availible). There are several other options not discussed here. 5.5.5 Alternate Performance Metrics The user can change the metric used to determine the best settings. By default, RMSE, R 2 , and the mean absolute error (MAE) are computed for regression while accuracy and Kappa are computed for classification. Also by default, the parameter values are chosen using RMSE and accuracy, respectively for regression and classification. The metric argument of the train function allows the user to control which the optimality criterion is used. For example, in problems where there are a low percentage of samples in one class, using metric  "Kappa" can improve quality of the final model. If none of these parameters are satisfactory, the user can also compute custom performance metrics. The trainControl function has a argument called summaryFunction that specifies a function for computing performance. The function should have these arguments: data is a reference for a data frame or matrix with columns called obs and pred for the observed and predicted outcome values (either numeric data for regression or character values for classification). Currently, class probabilities are not passed to the function. The values in data are the held-out predictions (and their associated reference values) for a single combination of tuning parameters. If the classProbs argument of the trainControl object is set to TRUE , additional columns in data will be present that contains the class probabilities. The names of these columns are the same as the class levels. Also, if weights were specified in the call to train , a column called weights will also be in the data set. Additionally, if the recipe method for train was used (see this section of documentation ), other variables not used in the model will also be included. This can be accomplished by adding a role in the recipe of "performance var" . An example is given in the recipe section of this site. lev is a character string that has the outcome factor levels taken from the training data. For regression, a value of NULL is passed into the function. model is a character string for the model being used (i.e. the value passed to the method argument of train ). The output to the function should be a vector of numeric summary metrics with non-null names. By default, train evaluate classification models in terms of the predicted classes. Optionally, class probabilities can also be used to measure performance. To obtain predicted class probabilities within the resampling process, the argument classProbs in trainControl must be set to TRUE . This merges columns of probabilities into the predictions generated from each resample (there is a column per class and the column names are the class names). As shown in the last section, custom functions can be used to calculate performance scores that are averaged over the resamples. Another built-in function, twoClassSummary , will compute the sensitivity, specificity and area under the ROC curve: To rebuild the boosted tree model using this criterion, we can see the relationship between the tuning parameters and the area under the ROC curve using the following code: In this case, the average area under the ROC curve associated with the optimal tuning parameters was 0.922 across the 100 resamples. 