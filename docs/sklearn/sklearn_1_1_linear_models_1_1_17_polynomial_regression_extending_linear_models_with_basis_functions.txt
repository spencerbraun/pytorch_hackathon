sklearn_1_1_linear_models
1.1. Linear Models
modules/linear_model.html
 1.1.17. Polynomial regression: extending linear models with basis functions  One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data. For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data: \[\hat{y}(w, x)  w_0 + w_1 x_1 + w_2 x_2\] If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this: \[\hat{y}(w, x)  w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\] The (sometimes surprising) observation is that this is still a linear model : to see this, imagine creating a new set of features \[z  [x_1, x_2, x_1 x_2, x_1^2, x_2^2]\] With this re-labeling of the data, our problem can be written \[\hat{y}(w, z)  w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\] We see that the resulting polynomial regression is in the same class of linear models we considered above (i.e. the model is linear in ) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data. Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees: This figure is created using the PolynomialFeatures transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows: The features of have been transformed from to , and can now be used within any linear model. This sort of preprocessing can be streamlined with the Pipeline tools. A single object representing a simple polynomial regression can be created and used as follows: The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients. In some cases it’s not necessary to include higher powers of any single feature, but only the so-called interaction features that multiply together at most distinct features. These can be gotten from PolynomialFeatures with the setting . For example, when dealing with boolean features, for all and is therefore useless; but represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier: And the classifier “predictions” are perfect: 