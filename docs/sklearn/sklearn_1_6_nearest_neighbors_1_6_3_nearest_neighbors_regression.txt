sklearn_1_6_nearest_neighbors
1.6. Nearest Neighbors
modules/neighbors.html
 1.6.3. Nearest Neighbors Regression  Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors. scikit-learn implements two different neighbors regressors: KNeighborsRegressor implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user. RadiusNeighborsRegressor implements learning based on the neighbors within a fixed radius of the query point, where is a floating-point value specified by the user. The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the keyword. The default value, , assigns equal weights to all points. assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights. The use of multi-output nearest neighbors for regression is demonstrated in Face completion with a multi-output estimators . In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces. Examples: Nearest Neighbors regression : an example of regression using nearest neighbors. Face completion with a multi-output estimators : an example of multi-output regression using nearest neighbors. 