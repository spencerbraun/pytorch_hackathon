sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions
3.3. Metrics and scoring: quantifying the quality of predictions
modules/model_evaluation.html
 3.3.1. The parameter: defining model evaluation rules  Model selection and evaluation using tools, such as model_selection.GridSearchCV and model_selection.cross_val_score , take a parameter that controls what metric they apply to the estimators evaluated. 3.3.1.1. Common cases: predefined values  For the most common use cases, you can designate a scorer object with the parameter; the table below shows all possible values. All scorer objects follow the convention that higher return values are better than lower return values . Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error , are available as neg_mean_squared_error which return the negated value of the metric. Scoring Function Comment Classification ‘accuracy’ metrics.accuracy_score ‘balanced_accuracy’ metrics.balanced_accuracy_score ‘average_precision’ metrics.average_precision_score ‘neg_brier_score’ metrics.brier_score_loss ‘f1’ metrics.f1_score for binary targets ‘f1_micro’ metrics.f1_score micro-averaged ‘f1_macro’ metrics.f1_score macro-averaged ‘f1_weighted’ metrics.f1_score weighted average ‘f1_samples’ metrics.f1_score by multilabel sample ‘neg_log_loss’ metrics.log_loss requires support ‘precision’ etc. metrics.precision_score suffixes apply as with ‘f1’ ‘recall’ etc. metrics.recall_score suffixes apply as with ‘f1’ ‘jaccard’ etc. metrics.jaccard_score suffixes apply as with ‘f1’ ‘roc_auc’ metrics.roc_auc_score ‘roc_auc_ovr’ metrics.roc_auc_score ‘roc_auc_ovo’ metrics.roc_auc_score ‘roc_auc_ovr_weighted’ metrics.roc_auc_score ‘roc_auc_ovo_weighted’ metrics.roc_auc_score Clustering ‘adjusted_mutual_info_score’ metrics.adjusted_mutual_info_score ‘adjusted_rand_score’ metrics.adjusted_rand_score ‘completeness_score’ metrics.completeness_score ‘fowlkes_mallows_score’ metrics.fowlkes_mallows_score ‘homogeneity_score’ metrics.homogeneity_score ‘mutual_info_score’ metrics.mutual_info_score ‘normalized_mutual_info_score’ metrics.normalized_mutual_info_score ‘v_measure_score’ metrics.v_measure_score Regression ‘explained_variance’ metrics.explained_variance_score ‘max_error’ metrics.max_error ‘neg_mean_absolute_error’ metrics.mean_absolute_error ‘neg_mean_squared_error’ metrics.mean_squared_error ‘neg_root_mean_squared_error’ metrics.mean_squared_error ‘neg_mean_squared_log_error’ metrics.mean_squared_log_error ‘neg_median_absolute_error’ metrics.median_absolute_error ‘r2’ metrics.r2_score ‘neg_mean_poisson_deviance’ metrics.mean_poisson_deviance ‘neg_mean_gamma_deviance’ metrics.mean_gamma_deviance Usage examples: Note The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary . 3.3.1.2. Defining your scoring strategy from metric functions  The module sklearn.metrics also exposes a set of simple functions measuring a prediction error given ground truth and prediction: functions ending with return a value to maximize, the higher the better. functions ending with or return a value to minimize, the lower the better. When converting into a scorer object using make_scorer , set the parameter to False (True by default; see the parameter description below). Metrics available for various machine learning tasks are detailed in sections below. Many metrics are not given names to be used as values, sometimes because they require additional parameters, such as fbeta_score . In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using make_scorer . That function converts metrics into callables that can be used for model evaluation. One typical use case is to wrap an existing metric function from the library with non-default values for its parameters, such as the parameter for the fbeta_score function: The second use case is to build a completely custom scorer object from a simple python function using make_scorer , which can take several parameters: the python function you want to use ( in the example below) whether the python function returns a score ( , the default) or a loss ( ). If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return higher values for better models. for classification metrics only: whether the python function you provided requires continuous decision certainties ( ). The default value is False. any additional parameters, such as or in f1_score . Here is an example of building custom scorers, and of using the parameter: 3.3.1.3. Implementing your own scoring object  You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the make_scorer factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules: It can be called with parameters , where is the model that should be evaluated, is validation data, and is the ground truth target for (in the supervised case) or (in the unsupervised case). It returns a floating point number that quantifies the prediction quality on , with reference to . Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated. Note Using custom scorers in functions where n_jobs > 1 While defining the custom scoring function alongside the calling function should work out of the box with the default joblib backend (loky), importing it from another module will be a more robust approach and work independently of the joblib backend. For example, to use greater than 1 in the example below, function is saved in a user-created module ( ) and imported: 3.3.1.4. Using multiple metric evaluation  Scikit-learn also permits evaluation of multiple metrics in , and . There are two ways to specify multiple scoring metrics for the parameter: As an iterable of string metrics:: As a mapping the scorer name to the scoring function:: Note that the dict values can either be scorer functions or one of the predefined metric strings. Currently only those scorer functions that return a single score can be passed inside the dict. Scorer functions that return multiple values are not permitted and will require a wrapper to return a single metric: 