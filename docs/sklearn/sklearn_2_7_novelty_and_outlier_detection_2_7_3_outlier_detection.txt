sklearn_2_7_novelty_and_outlier_detection
2.7. Novelty and Outlier Detection
modules/outlier_detection.html
 2.7.3. Outlier Detection  Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called outliers . Yet, in the case of outlier detection, we don’t have a clean data set representing the population of regular observations that can be used to train any tool. 2.7.3.1. Fitting an elliptic envelope  One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the “shape” of the data, and can define outlying observations as observations which stand far enough from the fit shape. The scikit-learn provides an object covariance.EllipticEnvelope that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode. For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below. Examples: See Robust covariance estimation and Mahalanobis distances relevance for an illustration of the difference between using a standard ( covariance.EmpiricalCovariance ) or a robust estimate ( covariance.MinCovDet ) of location and covariance to assess the degree of outlyingness of an observation. References: Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum covariance determinant estimator” Technometrics 41(3), 212 (1999) 2.7.3.2. Isolation Forest  One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The ensemble.IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies. The implementation of ensemble.IsolationForest is based on an ensemble of tree.ExtraTreeRegressor . Following Isolation Forest original paper, the maximum depth of each tree is set to where is the number of samples used to build the tree (see (Liu et al., 2008) for more details). This algorithm is illustrated below. The ensemble.IsolationForest supports which allows you to add more trees to an already fitted model: Examples: See IsolationForest example for an illustration of the use of IsolationForest. See Comparing anomaly detection algorithms for outlier detection on toy datasets for a comparison of ensemble.IsolationForest with neighbors.LocalOutlierFactor , svm.OneClassSVM (tuned to perform like an outlier detection method) and a covariance-based outlier detection with covariance.EllipticEnvelope . References: Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.” Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on. 2.7.3.3. Local Outlier Factor  Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm. The neighbors.LocalOutlierFactor (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors. In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density. The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors35 in the example below). The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood. When applying LOF for outlier detection, there are no , and methods but only a method. The scores of abnormality of the training samples are accessible through the attribute. Note that , and can be used on new unseen data when LOF is applied for novelty detection, i.e. when the parameter is set to . See Novelty detection with Local Outlier Factor . This strategy is illustrated below. Examples: See Outlier detection with Local Outlier Factor (LOF) for an illustration of the use of neighbors.LocalOutlierFactor . See Comparing anomaly detection algorithms for outlier detection on toy datasets for a comparison with other anomaly detection methods. References: Breunig, Kriegel, Ng, and Sander (2000) LOF: identifying density-based local outliers. Proc. ACM SIGMOD 