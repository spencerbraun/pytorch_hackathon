sklearn_1_4_support_vector_machines
1.4. Support Vector Machines
modules/svm.html
 1.4.7. Mathematical formulation  A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”: In general, when the problem isn’t linearly separable, the support vectors are the samples within the margin boundaries. We recommend 13 and 14 as good references for the theory and practicalities of SVMs. 1.4.7.1. SVC  Given training vectors , i1,…, n, in two classes, and a vector , our goal is to find and such that the prediction given by is correct for most samples. SVC solves the following primal problem: \[ \begin{align}\begin{aligned}\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i1}^{n} \zeta_i\\\begin{split}\textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\ & \zeta_i \geq 0, i1, ..., n\end{split}\end{aligned}\end{align} \] Intuitively, we’re trying to maximize the margin (by minimizing ), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value would be for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance from their correct margin boundary. The penalty term controls the strengh of this penalty, and as a result, acts as an inverse regularization parameter (see note below). The dual problem to the primal is \[ \begin{align}\begin{aligned}\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha\\\begin{split} \textrm {subject to } & y^T \alpha  0\\ & 0 \leq \alpha_i \leq C, i1, ..., n\end{split}\end{aligned}\end{align} \] where is the vector of all ones, and is an by positive semidefinite matrix, , where is the kernel. The terms are called the dual coefficients, and they are upper-bounded by . This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function : see kernel trick . Once the optimization problem is solved, the output of decision_function for a given sample becomes: \[\sum_{i\in SV} y_i \alpha_i K(x_i, x) + b,\] and the predicted class correspond to its sign. We only need to sum over the support vectors (i.e. the samples that lie within the margin) because the dual coefficients are zero for the other samples. These parameters can be accessed through the attributes which holds the product , which holds the support vectors, and which holds the independent term Note While SVM models derived from libsvm and liblinear use as regularization parameter, most other estimators use . The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is sklearn.linear_model.Ridge regression, the relation between them is given as . 1.4.7.2. LinearSVC  The primal problem can be equivalently formulated as \[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i1}\max(0, y_i (w^T \phi(x_i) + b)),\] where we make use of the hinge loss . This is the form that is directly optimized by LinearSVC , but unlike the dual form, this one does not involve inner products between samples, so the famous kernel trick cannot be applied. This is why only the linear kernel is supported by LinearSVC ( is the identity function). 1.4.7.3. NuSVC  The -SVC formulation 15 is a reparameterization of the -SVC and therefore mathematically equivalent. We introduce a new parameter (instead of ) which controls the number of support vectors and margin errors : is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin. 1.4.7.4. SVR  Given training vectors , i1,…, n, and a vector -SVR solves the following primal problem: \[ \begin{align}\begin{aligned}\min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i1}^{n} (\zeta_i + \zeta_i^*)\\\begin{split}\textrm {subject to } & y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\ & w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\ & \zeta_i, \zeta_i^* \geq 0, i1, ..., n\end{split}\end{aligned}\end{align} \] Here, we are penalizing samples whose prediction is at least away from their true target. These samples penalize the objective by or , depending on whether their predictions lie above or below the tube. The dual problem is \[ \begin{align}\begin{aligned}\min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)\\\begin{split} \textrm {subject to } & e^T (\alpha - \alpha^*)  0\\ & 0 \leq \alpha_i, \alpha_i^* \leq C, i1, ..., n\end{split}\end{aligned}\end{align} \] where is the vector of all ones, is an by positive semidefinite matrix, is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function . The prediction is: \[\sum_{i \in SV}(\alpha_i - \alpha_i^*) K(x_i, x) + b\] These parameters can be accessed through the attributes which holds the difference , which holds the support vectors, and which holds the independent term 1.4.7.5. LinearSVR  The primal problem can be equivalently formulated as \[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i1}\max(0, |y_i - (w^T \phi(x_i) + b)| - \varepsilon),\] where we make use of the epsilon-insensitive loss, i.e. errors of less than are ignored. This is the form that is directly optimized by LinearSVR . 