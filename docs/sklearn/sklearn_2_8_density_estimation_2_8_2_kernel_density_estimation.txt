sklearn_2_8_density_estimation
2.8. Density Estimation
modules/density.html
 2.8.2. Kernel Density Estimation  Kernel density estimation in scikit-learn is implemented in the sklearn.neighbors.KernelDensity estimator, which uses the Ball Tree or KD Tree for efficient queries (see Nearest Neighbors for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions. In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels: It’s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows: Here we have used , as seen above. Mathematically, a kernel is a positive function which is controlled by the bandwidth parameter . Given this kernel form, the density estimate at a point within a group of points is given by: \[\rho_K(y)  \sum_{i1}^{N} K(y - x_i; h)\] The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution. sklearn.neighbors.KernelDensity implements several common kernel forms, which are shown in the following figure: The form of these kernels is as follows: Gaussian kernel ( ) Tophat kernel ( ) if Epanechnikov kernel ( ) Exponential kernel ( ) Linear kernel ( ) if Cosine kernel ( ) if The kernel density estimator can be used with any of the valid distance metrics (see sklearn.neighbors.DistanceMetric for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the Haversine distance which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent: One other useful application of kernel density estimation is to learn a non-parametric generative model of a dataset in order to efficiently draw new samples from this generative model. Here is an example of using this process to create a new set of hand-written digits, using a Gaussian kernel learned on a PCA projection of the data: The “new” data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model. Examples: Simple 1D Kernel Density Estimation : computation of simple kernel density estimates in one dimension. Kernel Density Estimation : an example of using Kernel Density estimation to learn a generative model of the hand-written digits data, and drawing new samples from this model. Kernel Density Estimate of Species Distributions : an example of Kernel Density estimation using the Haversine distance metric to visualize geospatial data 