sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions
3.3. Metrics and scoring: quantifying the quality of predictions
modules/model_evaluation.html
 3.3.6. Dummy estimators  When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. DummyClassifier implements several such simple strategies for classification: generates random predictions by respecting the training set class distribution. always predicts the most frequent label in the training set. always predicts the class that maximizes the class prior (like ) and returns the class prior. generates predictions uniformly at random. always predicts a constant label that is provided by the user. A major motivation of this method is F1-scoring, when the positive class is in the minority. Note that with all these strategies, the method completely ignores the input data! To illustrate DummyClassifier , first let’s create an imbalanced dataset: Next, let’s compare the accuracy of and : We see that doesn’t do much better than a dummy classifier. Now, let’s change the kernel: We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the Cross-validation: evaluating estimator performance section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator section for details. More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc… DummyRegressor also implements four simple rules of thumb for regression: always predicts the mean of the training targets. always predicts the median of the training targets. always predicts a user provided quantile of the training targets. always predicts a constant value that is provided by the user. In all these strategies, the method completely ignores the input data. 