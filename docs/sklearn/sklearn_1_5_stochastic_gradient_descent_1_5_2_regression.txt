sklearn_1_5_stochastic_gradient_descent
1.5. Stochastic Gradient Descent
modules/sgd.html
 1.5.2. Regression  The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. SGDRegressor is well suited for regression problems with a large number of training samples (> 10.000), for other problems we recommend Ridge , Lasso , or ElasticNet . The concrete loss function can be set via the parameter. SGDRegressor supports the following loss functions: : Ordinary least squares, : Huber loss for robust regression, : linear Support Vector Regression. Please refer to the mathematical section below for formulas. The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter . This parameter depends on the scale of the target variables. The parameter determines the regularization to be used (see description above in the classification section). SGDRegressor also supports averaged SGD 10 (here again, see description above in the classification section). For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in Ridge . 