sklearn_3_2_tuning_the_hyper-parameters_of_an_estimator
3.2. Tuning the hyper-parameters of an estimator
modules/grid_search.html
 3.2.3. Tips for parameter search  3.2.3.1. Specifying an objective metric  By default, parameter search uses the function of the estimator to evaluate a parameter setting. These are the sklearn.metrics.accuracy_score for classification and sklearn.metrics.r2_score for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the parameter to GridSearchCV , RandomizedSearchCV and many of the specialized cross-validation tools described below. See The scoring parameter: defining model evaluation rules for more details. 3.2.3.2. Specifying multiple metrics for evaluation  and allow specifying multiple metrics for the parameter. Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See Using multiple metric evaluation for more details. When specifying multiple metrics, the parameter must be set to the metric (string) for which the will be found and used to build the on the whole dataset. If the search should not be refit, set . Leaving refit to the default value will result in an error when using multiple metrics. See Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV for an example usage. 3.2.3.3. Composite estimators and parameter spaces  GridSearchCV and RandomizedSearchCV allow searching over parameters of composite or nested estimators such as Pipeline , ColumnTransformer , VotingClassifier or CalibratedClassifierCV using a dedicated syntax: Here, is the parameter name of the nested estimator, in this case . If the meta-estimator is constructed as a collection of estimators as in , then refers to the name of the estimator, see Nested parameters . In practice, there can be several levels of nesting: 3.2.3.4. Model selection: development and evaluation  Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to “train” the parameters of the grid. When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the instance) and an evaluation set to compute performance metrics. This can be done by using the train_test_split utility function. 3.2.3.5. Parallelism  GridSearchCV and RandomizedSearchCV evaluate each parameter setting independently. Computations can be run in parallel if your OS supports it, by using the keyword . See function signature for more details. 3.2.3.6. Robustness to failure  Some parameter settings may result in a failure to one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. Setting (or ) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or ), but completing the search. 