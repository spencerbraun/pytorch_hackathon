sklearn_3_2_tuning_the_hyper-parameters_of_an_estimator
3.2. Tuning the hyper-parameters of an estimator
modules/grid_search.html
 3.2.2. Randomized Parameter Optimization  While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search: A budget can be chosen independent of the number of parameters and possible values. Adding parameters that do not influence the performance does not decrease efficiency. Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV . Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified: This example uses the module, which contains many useful distributions for sampling parameters, such as , , or . In principle, any function can be passed that provides a (random variate sample) method to sample a value. A call to the function should provide independent random samples from possible parameter values on consecutive calls. Warning The distributions in prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via or set using . However, beginning scikit-learn 0.18, the sklearn.model_selection module sets the random state provided by the user if scipy > 0.16 is also available. For continuous parameters, such as above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing will always lead to a finer search. A continuous log-uniform random variable is available through loguniform . This is a continuous version of log-spaced parameters. For example to specify above, can be used instead of or . This is an alias to SciPyâ€™s stats.reciprocal . Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between and : Examples: Comparing randomized search and grid search for hyperparameter estimation compares the usage and efficiency of randomized search and grid search. References: Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012) 