sklearn_2_1_gaussian_mixture_models
2.1. Gaussian mixture models
modules/mixture.html
 2.1.1. Gaussian Mixture  The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A GaussianMixture.fit method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the GaussianMixture.predict method. The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance. Examples: See GMM covariances for an example of using the Gaussian mixture as clustering on the iris dataset. See Density Estimation for a Gaussian mixture for an example on plotting the density estimation. 2.1.1.1. Pros and cons of class GaussianMixture  2.1.1.1.1. Pros  Speed It is the fastest algorithm for learning mixture models Agnostic As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply. 2.1.1.1.2. Cons  Singularities When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially. Number of components This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues. 2.1.1.2. Selecting the number of components in a classical Gaussian Mixture Model  The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a Variational Bayesian Gaussian mixture avoids the specification of the number of components for a Gaussian mixture model. Examples: See Gaussian Mixture Model Selection for an example of model selection performed with classical Gaussian mixture. 2.1.1.3. Estimation algorithm Expectation-maximization  The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesnâ€™t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). Expectation-maximization is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum. 