sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems
2.5. Decomposing signals in components (matrix factorization problems)
modules/decomposition.html
 2.5.7. Latent Dirichlet Allocation (LDA)  Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents. The graphical model of LDA is a three-level generative model: Note on notations presented in the graphical model above, which can be found in Hoffman et al. (2013): The corpus is a collection of documents. A document is a sequence of words. There are topics in the corpus. The boxes represent repeated sampling. In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure. When modeling text corpora, the model assumes the following generative process for a corpus with documents and topics, with corresponding to n_components in the API: For each topic , draw . This provides a distribution over the words, i.e. the probability of a word appearing in topic . corresponds to topic_word_prior . For each document , draw the topic proportions . corresponds to doc_topic_prior . For each word in document : Draw the topic assignment Draw the observed word For parameter estimation, the posterior distribution is: \[p(z, \theta, \beta |w, \alpha, \eta)  \frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}\] Since the posterior is intractable, variational Bayesian method uses a simpler distribution to approximate it, and those variational parameters , , are optimized to maximize the Evidence Lower Bound (ELBO): \[\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{} E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]\] Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between and the true posterior . LatentDirichletAllocation implements the online variational Bayes algorithm and supports both online and batch update methods. While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points. Note Although the online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting. When LatentDirichletAllocation is applied on a “document-term” matrix, the matrix will be decomposed into a “topic-term” matrix and a “document-topic” matrix. While “topic-term” matrix is stored as components_ in the model, “document-topic” matrix can be calculated from method. LatentDirichletAllocation also implements method. This is used when data can be fetched sequentially. Examples: Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation References: “Latent Dirichlet Allocation” D. Blei, A. Ng, M. Jordan, 2003 “Online Learning for Latent Dirichlet Allocation” M. Hoffman, D. Blei, F. Bach, 2010 “Stochastic Variational Inference” M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013 See also Dimensionality reduction for dimensionality reduction with Neighborhood Components Analysis. 