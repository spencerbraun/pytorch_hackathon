sklearn_1_7_gaussian_processes
1.7. Gaussian Processes
modules/gaussian_process.html
 1.7.5. Kernels for Gaussian Processes  Kernels (also called “covariance functions” in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the “similarity” of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of [RW2006] . For guidance on how to best combine different kernels, we refer to [Duv2014] . 1.7.5.1. Gaussian Process Kernel API  The main usage of a Kernel is to compute the GP’s covariance between datapoints. For this, the method of the kernel can be called. This method can either be used to compute the “auto-covariance” of all pairs of datapoints in a 2d array X, or the “cross-covariance” of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the WhiteKernel ): If only the diagonal of the auto-covariance is being used, the method of a kernel can be called, which is more computationally efficient than the equivalent call to : Kernels are parameterized by a vector of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel’s auto-covariance with respect to via setting in the method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of , which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of can be get and set via the property of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of Hyperparameter in the respective kernel. Note that a kernel using a hyperparameter with name “x” must have the attributes self.x and self.x_bounds. The abstract base class for all kernels is Kernel . Kernel implements a similar interface as Estimator , providing the methods , , and . This allows setting kernel values also via meta-estimators such as Pipeline or GridSearch . Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with and parameters of the right operand with . An additional convenience method is , which returns a cloned version of the kernel but with the hyperparameters set to . An illustrative example: All Gaussian process kernels are interoperable with sklearn.metrics.pairwise and vice versa: instances of subclasses of Kernel can be passed as to from sklearn.metrics.pairwise . Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class PairwiseKernel . The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed. 1.7.5.2. Basic kernels  The ConstantKernel kernel can be used as part of a Product kernel where it scales the magnitude of the other factor (kernel) or as part of a Sum kernel, where it modifies the mean of the Gaussian process. It depends on a parameter . It is defined as: \[k(x_i, x_j)  constant\_value \;\forall\; x_1, x_2\] The main use-case of the WhiteKernel kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level. It is defined as: \[k(x_i, x_j)  noise\_level \text{ if } x_i  x_j \text{ else } 0\] 1.7.5.3. Kernel operators  Kernel operators take one or two base kernels and combine them into a new kernel. The Sum kernel takes two kernels and and combines them via . The Product kernel takes two kernels and and combines them via . The Exponentiation kernel takes one base kernel and a scalar parameter and combines them via . Note that magic methods , and are overridden on the Kernel objects, so one can use e.g. as a shortcut for . 1.7.5.4. Radial-basis function (RBF) kernel  The RBF kernel is a stationary kernel. It is also known as the “squared exponential” kernel. It is parameterized by a length-scale parameter , which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs (anisotropic variant of the kernel). The kernel is given by: \[k(x_i, x_j)  \text{exp}\left(- \frac{d(x_i, x_j)^2}{2l^2} \right)\] where is the Euclidean distance. This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure: 1.7.5.5. Matérn kernel  The Matern kernel is a stationary kernel and a generalization of the RBF kernel. It has an additional parameter which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter , which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs (anisotropic variant of the kernel). The kernel is given by: \[k(x_i, x_j)  \frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg)^\nu K_\nu\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg),\] where is the Euclidean distance, is a modified Bessel function and is the gamma function. As , the Matérn kernel converges to the RBF kernel. When , the Matérn kernel becomes identical to the absolute exponential kernel, i.e., \[k(x_i, x_j)  \exp \Bigg(- \frac{1}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu \tfrac{1}{2}\] In particular, : \[k(x_i, x_j)  \Bigg(1 + \frac{\sqrt{3}}{l} d(x_i , x_j )\Bigg) \exp \Bigg(-\frac{\sqrt{3}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu \tfrac{3}{2}\] and : \[k(x_i, x_j)  \Bigg(1 + \frac{\sqrt{5}}{l} d(x_i , x_j ) +\frac{5}{3l} d(x_i , x_j )^2 \Bigg) \exp \Bigg(-\frac{\sqrt{5}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu \tfrac{5}{2}\] are popular choices for learning functions that are not infinitely differentiable (as assumed by the RBF kernel) but at least once ( ) or twice differentiable ( ). The flexibility of controlling the smoothness of the learned function via allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Matérn kernel are shown in the following figure: See [RW2006] , pp84 for further details regarding the different variants of the Matérn kernel. 1.7.5.6. Rational quadratic kernel  The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter and a scale mixture parameter Only the isotropic variant where is a scalar is supported at the moment. The kernel is given by: \[k(x_i, x_j)  \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^{-\alpha}\] The prior and posterior of a GP resulting from a RationalQuadratic kernel are shown in the following figure: 1.7.5.7. Exp-Sine-Squared kernel  The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter and a periodicity parameter . Only the isotropic variant where is a scalar is supported at the moment. The kernel is given by: \[k(x_i, x_j)  \text{exp}\left(- \frac{ 2\sin^2(\pi d(x_i, x_j) / p) }{ l^ 2} \right)\] The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure: 1.7.5.8. Dot-Product kernel  The DotProduct kernel is non-stationary and can be obtained from linear regression by putting priors on the coefficients of and a prior of on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter . For , the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by \[k(x_i, x_j)  \sigma_0 ^ 2 + x_i \cdot x_j\] The DotProduct kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure: 1.7.5.9. References  RW2006 ( 1 , 2 , 3 , 4 , 5 , 6 ) Carl Eduard Rasmussen and Christopher K.I. Williams, “Gaussian Processes for Machine Learning”, MIT Press 2006, Link to an official complete PDF version of the book here . Duv2014 David Duvenaud, “The Kernel Cookbook: Advice on Covariance functions”, 2014, Link . 