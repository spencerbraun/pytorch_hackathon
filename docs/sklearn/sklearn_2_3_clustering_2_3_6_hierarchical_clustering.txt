sklearn_2_3_clustering
2.3. Clustering
modules/clustering.html
 2.3.6. Hierarchical clustering  Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the Wikipedia page for more details. The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy: Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach. Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters. Average linkage minimizes the average of the distances between all observations of pairs of clusters. Single linkage minimizes the distance between the closest observations of pairs of clusters. AgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges. FeatureAgglomeration The FeatureAgglomeration uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see Unsupervised dimensionality reduction . 2.3.6.1. Different linkage type: Ward, complete, average, and single linkage  AgglomerativeClustering supports Ward, single, average, and complete linkage strategies. Agglomerative cluster has a “rich get richer” behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data. Examples: Various Agglomerative Clustering on a 2D embedding of digits : exploration of the different linkage strategies in a real dataset. 2.3.6.2. Visualization of cluster hierarchy  It’s possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes. 2.3.6.3. Adding connectivity constraints  An interesting aspect of AgglomerativeClustering is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll. These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high. The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using sklearn.neighbors.kneighbors_graph to restrict merging to nearest neighbors as in this example , or using sklearn.feature_extraction.image.grid_to_graph to enable only merging of neighboring pixels on an image, as in the coin example. Examples: A demo of structured Ward hierarchical clustering on an image of coins : Ward clustering to split the image of coins in regions. Hierarchical clustering: structured vs unstructured ward : Example of Ward algorithm on a swiss-roll, comparison of structured approaches versus unstructured approaches. Feature agglomeration vs. univariate selection : Example of dimensionality reduction with feature agglomeration based on Ward hierarchical clustering. Agglomerative clustering with and without structure Warning Connectivity constraints with single, average and complete linkage Connectivity constraints and single, complete or average linkage can enhance the ‘rich getting richer’ aspect of agglomerative clustering, particularly so if they are built with sklearn.neighbors.kneighbors_graph . In the limit of a small number of clusters, they tend to give a few macroscopically occupied clusters and almost empty ones. (see the discussion in Agglomerative clustering with and without structure ). Single linkage is the most brittle linkage option with regard to this issue. 2.3.6.4. Varying the metric  Single, average and complete linkage can be used with a variety of distances (or affinities), in particular Euclidean distance ( l2 ), Manhattan distance (or Cityblock, or l1 ), cosine distance, or any precomputed affinity matrix. l1 distance is often good for sparse features, or sparse noise: i.e. many of the features are zero, as in text mining using occurrences of rare words. cosine distance is interesting because it is invariant to global scalings of the signal. The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class. Examples: Agglomerative clustering with different metrics 