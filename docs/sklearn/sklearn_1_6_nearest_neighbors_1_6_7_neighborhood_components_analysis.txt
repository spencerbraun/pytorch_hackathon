sklearn_1_6_nearest_neighbors
1.6. Nearest Neighbors
modules/neighbors.html
 1.6.7. Neighborhood Components Analysis  Neighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis ) is a distance metric learning algorithm which aims to improve the accuracy of nearest neighbors classification compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It can also learn a low-dimensional linear projection of data that can be used for data visualization and fast classification. In the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the mathematical formulation for more details. 1.6.7.1. Classification  Combined with a nearest neighbors classifier ( KNeighborsClassifier ), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user. NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries. To use this model for classification, one needs to combine a NeighborhoodComponentsAnalysis instance that learns the optimal transformation with a KNeighborsClassifier instance that performs the classification in the projected space. Here is an example using the two classes: The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes. 1.6.7.2. Dimensionality reduction  NCA can be used to perform supervised dimensionality reduction. The input data are projected onto a linear subspace consisting of the directions which minimize the NCA objective. The desired dimensionality can be set using the parameter . For instance, the following figure shows a comparison of dimensionality reduction with Principal Component Analysis ( sklearn.decomposition.PCA ), Linear Discriminant Analysis ( sklearn.discriminant_analysis.LinearDiscriminantAnalysis ) and Neighborhood Component Analysis ( NeighborhoodComponentsAnalysis ) on the Digits dataset, a dataset with size and . The data set is split into a training and a test set of equal size, then standardized. For evaluation the 3-nearest neighbor classification accuracy is computed on the 2-dimensional projected points found by each method. Each data sample belongs to one of 10 classes. Examples: Comparing Nearest Neighbors with and without Neighborhood Components Analysis Dimensionality Reduction with Neighborhood Components Analysis Manifold learning on handwritten digits: Locally Linear Embedding, Isomap… 1.6.7.3. Mathematical formulation  The goal of NCA is to learn an optimal linear transformation matrix of size , which maximises the sum over all samples of the probability that is correctly classified, i.e.: \[\underset{L}{\arg\max} \sum\limits_{i0}^{N - 1} p_{i}\] with  and the probability of sample being correctly classified according to a stochastic nearest neighbors rule in the learned embedded space: \[p_{i}\sum\limits_{j \in C_i}{p_{i j}}\] where is the set of points in the same class as sample , and is the softmax over Euclidean distances in the embedded space: \[p_{i j}  \frac{\exp(-||L x_i - L x_j||^2)}{\sum\limits_{k \ne i} {\exp{-(||L x_i - L x_k||^2)}}} , \quad p_{i i}  0\] 1.6.7.3.1. Mahalanobis distance  NCA can be seen as learning a (squared) Mahalanobis distance metric: \[|| L(x_i - x_j)||^2  (x_i - x_j)^TM(x_i - x_j),\] where is a symmetric positive semi-definite matrix of size . 1.6.7.4. Implementation  This implementation follows what is explained in the original paper 1 . For the optimisation method, it currently uses scipy’s L-BFGS-B with a full gradient computation at each iteration, to avoid to tune the learning rate and provide stable learning. See the examples below and the docstring of NeighborhoodComponentsAnalysis.fit for further information. 1.6.7.5. Complexity  1.6.7.5.1. Training  NCA stores a matrix of pairwise distances, taking memory. Time complexity depends on the number of iterations done by the optimisation algorithm. However, one can set the maximum number of iterations with the argument . For each iteration, time complexity is . 1.6.7.5.2. Transform  Here the operation returns , therefore its time complexity equals . There is no added space complexity in the operation. References: 1 “Neighbourhood Components Analysis” , J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520. Wikipedia entry on Neighborhood Components Analysis 