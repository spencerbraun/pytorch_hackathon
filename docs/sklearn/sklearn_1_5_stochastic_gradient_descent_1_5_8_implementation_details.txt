sklearn_1_5_stochastic_gradient_descent
1.5. Stochastic Gradient Descent
modules/sgd.html
 1.5.8. Implementation details  The implementation of SGD is influenced by the of 7 . Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input , the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from 8 . For multi-class classification, a “one versus all” approach is used. We use the truncated gradient algorithm proposed in 9 for L1 regularization (and the Elastic Net). The code is written in Cython. References: 7 “Stochastic Gradient Descent” L. Bottou - Website, 2010. 8 “Pegasos: Primal estimated sub-gradient solver for svm” S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07. 9 “Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty” Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL ‘09. 10 ( 1 , 2 ) “Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent” Xu, Wei 11 “Regularization and variable selection via the elastic net” H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320. 12 “Solving large scale linear prediction problems using stochastic gradient descent algorithms” T. Zhang - In Proceedings of ICML ‘04. 