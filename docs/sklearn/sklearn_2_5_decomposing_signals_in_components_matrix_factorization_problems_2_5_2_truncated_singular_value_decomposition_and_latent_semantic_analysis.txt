sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems
2.5. Decomposing signals in components (matrix factorization problems)
modules/decomposition.html
 2.5.2. Truncated singular value decomposition and latent semantic analysis  TruncatedSVD implements a variant of singular value decomposition (SVD) that only computes the largest singular values, where is a user-specified parameter. When truncated SVD is applied to term-document matrices (as returned by CountVectorizer or TfidfVectorizer ), this transformation is known as latent semantic analysis (LSA), because it transforms such matrices to a “semantic” space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity. Note LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes. Mathematically, truncated SVD applied to training samples produces a low-rank approximation : \[X \approx X_k  U_k \Sigma_k V_k^\top\] After this operation, is the transformed training set with features (called in the API). To also transform a test set , we multiply it with : \[X'  X V_k\] Note Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix so that it has shape × . We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same. TruncatedSVD is very similar to PCA , but differs in that the matrix does not need to be centered. When the columnwise (per-feature) means of are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA. In practical terms, this means that the TruncatedSVD transformer accepts matrices without the need to densify them, as densifying may fill up memory even for medium-sized document collections. While the TruncatedSVD transformer works with any feature matrix, using it on tf–idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on ( ) to bring the feature values closer to a Gaussian distribution, compensating for LSA’s erroneous assumptions about textual data. Examples: Clustering text documents using k-means References: Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008), Introduction to Information Retrieval , Cambridge University Press, chapter 18: Matrix decompositions & latent semantic indexing 