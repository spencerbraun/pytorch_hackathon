sklearn_1_9_naive_bayes
1.9. Naive Bayes
modules/naive_bayes.html
 1.9.4. Bernoulli Naive Bayes  BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a instance may binarize its input (depending on the parameter). The decision rule for Bernoulli naive Bayes is based on \[P(x_i \mid y)  P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)\] which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature that is an indicator for class , where the multinomial variant would simply ignore a non-occurring feature. In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits. References: C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. A. McCallum and K. Nigam (1998). A comparison of event models for Naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48. V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with Naive Bayes – Which Naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS). 