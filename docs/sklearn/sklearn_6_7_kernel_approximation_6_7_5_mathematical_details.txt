sklearn_6_7_kernel_approximation
6.7. Kernel Approximation
modules/kernel_approximation.html
 6.7.5. Mathematical Details  Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function (a so called Mercer kernel), it is guaranteed that there exists a mapping into a Hilbert space , such that \[k(x,y)  \langle \phi(x), \phi(y) \rangle\] Where denotes the inner product in the Hilbert space. If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points , one may use the value of , which corresponds to applying the algorithm to the mapped data points . The advantage of using is that the mapping never has to be calculated explicitly, allowing for arbitrary large features (even infinite). One drawback of kernel methods is, that it might be necessary to store many kernel values during optimization. If a kernelized classifier is applied to new data , needs to be computed to make predictions, possibly for many different in the training set. The classes in this submodule allow to approximate the embedding , thereby working explicitly with the representations , which obviates the need to apply the kernel or store training examples. References: RR2007 ( 1 , 2 ) “Random features for large-scale kernel machines” Rahimi, A. and Recht, B. - Advances in neural information processing 2007, LS2010 “Random Fourier approximations for skewed multiplicative histogram kernels” Random Fourier approximations for skewed multiplicative histogram kernels - Lecture Notes for Computer Sciencd (DAGM) VZ2010 ( 1 , 2 ) “Efficient additive kernels via explicit feature maps” Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010 VVZ2010 “Generalized RBF feature maps for Efficient Detection” Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010 