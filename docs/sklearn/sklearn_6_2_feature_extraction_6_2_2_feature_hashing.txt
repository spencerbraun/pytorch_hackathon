sklearn_6_2_feature_extraction
6.2. Feature extraction
modules/feature_extraction.html
 6.2.2. Feature hashing  The class FeatureHasher is a high-speed, low-memory vectorizer that uses a technique known as feature hashing , or the “hashing trick”. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of FeatureHasher apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no method. Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature’s value is zero. This mechanism is enabled by default with and is particularly useful for small hash table sizes ( ). For large hash table sizes, it can be disabled, to allow the output to be passed to estimators like sklearn.naive_bayes.MultinomialNB or sklearn.feature_selection.chi2 feature selectors that expect non-negative inputs. FeatureHasher accepts either mappings (like Python’s and its variants in the module), pairs, or strings, depending on the constructor parameter . Mapping are treated as lists of pairs, while single strings have an implicit value of 1, so is interpreted as . If a single feature occurs multiple times in a sample, the associated values will be summed (so and become ). The output from FeatureHasher is always a matrix in the CSR format. Feature hashing can be employed in document classification, but unlike text.CountVectorizer , FeatureHasher does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see Vectorizing a large text corpus with the hashing trick , below, for a combined tokenizer/hasher. As an example, consider a word-level natural language processing task that needs features extracted from pairs. One could use a Python generator function to extract features: Then, the to be fed to can be constructed using: and fed to a hasher with: to get a matrix . Note the use of a generator comprehension, which introduces laziness into the feature extraction: tokens are only processed on demand from the hasher. 6.2.2.1. Implementation details  FeatureHasher uses the signed 32-bit variant of MurmurHash3. As a result (and because of limitations in ), the maximum number of features supported is currently . The original formulation of the hashing trick by Weinberger et al. used two separate hash functions and to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits. Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the parameter; otherwise the features will not be mapped evenly to the columns. References: Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). Feature hashing for large scale multitask learning . Proc. ICML. MurmurHash3 . 