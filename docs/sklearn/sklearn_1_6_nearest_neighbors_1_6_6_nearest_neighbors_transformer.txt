sklearn_1_6_nearest_neighbors
1.6. Nearest Neighbors
modules/neighbors.html
 1.6.6. Nearest Neighbors Transformer  Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as KNeighborsClassifier and KNeighborsRegressor , but also some clustering methods such as DBSCAN and SpectralClustering , and some manifold embeddings such as TSNE and Isomap . All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors sparse graph , as given by kneighbors_graph and radius_neighbors_graph . With mode , these functions return a binary adjacency sparse graph as required, for instance, in SpectralClustering . Whereas with , they return a distance sparse graph as required, for instance, in DBSCAN . To include these functions in a scikit-learn pipeline, one can also use the corresponding classes KNeighborsTransformer and RadiusNeighborsTransformer . The benefits of this sparse graph API are multiple. First, the precomputed graph can be re-used multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline: Second, precomputing the graph can give finer control on the nearest neighbors estimation, for instance enabling multiprocessing though the parameter , which might not be available in all estimators. Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors sparse graph needs to be formatted as in radius_neighbors_graph output: a CSR matrix (although COO, CSC or LIL will be accepted). only explicitly store nearest neighborhoods of each sample with respect to the training data. This should include those at 0 distance from a query point, including the matrix diagonal when computing the nearest neighborhoods between the training data and itself. each rowâ€™s should store the distance in increasing order (optional. Unsorted data will be stable-sorted, adding a computational overhead). all values in data should be non-negative. there should be no duplicate in any row (see https://github.com/scipy/scipy/issues/5807 ). if the algorithm being passed the precomputed matrix uses k nearest neighbors (as opposed to radius neighborhood), at least k neighbors must be stored in each row (or k+1, as explained in the following note). Note When a specific number of neighbors is queried (using KNeighborsTransformer ), the definition of is ambiguous since it can either include each training point as its own neighbor, or exclude them. Neither choice is perfect, since including them leads to a different number of non-self neighbors during training and testing, while excluding them leads to a difference between and , which is against scikit-learn API. In KNeighborsTransformer we use the definition which includes each training point as its own neighbor in the count of . However, for compatibility reasons with other estimators which use the other definition, one extra neighbor will be computed when . To maximise compatibility with all estimators, a safe choice is to always include one extra neighbor in a custom nearest neighbors estimator, since unnecessary neighbors will be filtered by following estimators. Examples: Approximate nearest neighbors in TSNE : an example of pipelining KNeighborsTransformer and TSNE . Also proposes two custom nearest neighbors estimators based on external packages. Caching nearest neighbors : an example of pipelining KNeighborsTransformer and KNeighborsClassifier to enable caching of the neighbors graph during a hyper-parameter grid-search. 