sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems
2.5. Decomposing signals in components (matrix factorization problems)
modules/decomposition.html
 2.5.6. Non-negative matrix factorization (NMF or NNMF)  2.5.6.1. NMF with the Frobenius norm  NMF 1 is an alternative approach to decomposition that assumes that the data and the components are non-negative. NMF can be plugged in instead of PCA or its variants, in the cases where the data matrix does not contain negative values. It finds a decomposition of samples into two matrices and of non-negative elements, by optimizing the distance between and the matrix product . The most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices: \[d_{\mathrm{Fro}}(X, Y)  \frac{1}{2} ||X - Y||_{\mathrm{Fro}}^2  \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\] Unlike PCA , the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text. It has been observed in [Hoyer, 2004] 2 that, when carefully constrained, NMF can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by NMF from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces. The init attribute determines the initialization method applied, which has a great impact on the performance of the method. NMF implements the method Nonnegative Double Singular Value Decomposition. NNDSVD 4 is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case. Note that the Multiplicative Update (‘mu’) solver cannot update zeros present in the initialization, so it leads to poorer results when used jointly with the basic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or NNDSVDar should be preferred. NMF can also be initialized with correctly scaled random non-negative matrices by setting init"random" . An integer seed or a can also be passed to random_state to control reproducibility. In NMF , L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in ElasticNet , we control the combination of L1 and L2 with the l1_ratio ( ) parameter, and the intensity of the regularization with the alpha ( ) parameter. Then the priors terms are: \[\alpha \rho ||W||_1 + \alpha \rho ||H||_1 + \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2 + \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2\] and the regularized objective function is: \[d_{\mathrm{Fro}}(X, WH) + \alpha \rho ||W||_1 + \alpha \rho ||H||_1 + \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2 + \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2\] NMF regularizes both W and H. The public function non_negative_factorization allows a finer control through the regularization attribute, and may regularize only W, only H, or both. 2.5.6.2. NMF with a beta-divergence  As described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices: \[d_{\mathrm{Fro}}(X, Y)  \frac{1}{2} ||X - Y||_{Fro}^2  \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\] Other distance functions can be used in NMF as, for example, the (generalized) Kullback-Leibler (KL) divergence, also referred as I-divergence: \[d_{KL}(X, Y)  \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\] Or, the Itakura-Saito (IS) divergence: \[d_{IS}(X, Y)  \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)\] These three distances are special cases of the beta-divergence family, with respectively 6 . The beta-divergence are defined by : \[d_{\beta}(X, Y)  \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}^\beta + (\beta-1)Y_{ij}^\beta - \beta X_{ij} Y_{ij}^{\beta - 1})\] Note that this definition is not valid if , yet it can be continuously extended to the definitions of and respectively. NMF implements two solvers, using Coordinate Descent (‘cd’) 5 , and Multiplicative Update (‘mu’) 6 . The ‘mu’ solver can optimize every beta-divergence, including of course the Frobenius norm ( ), the (generalized) Kullback-Leibler divergence ( ) and the Itakura-Saito divergence ( ). Note that for , the ‘mu’ solver is significantly faster than for other values of . Note also that with a negative (or 0, i.e. ‘itakura-saito’) , the input matrix cannot contain zero values. The ‘cd’ solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function. NMF is best used with the method, which returns the matrix W. The matrix H is stored into the fitted model in the attribute; the method will decompose a new matrix X_new based on these stored components: Examples: Faces dataset decompositions Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation Beta-divergence loss functions References: 1 “Learning the parts of objects by non-negative matrix factorization” D. Lee, S. Seung, 1999 2 “Non-negative Matrix Factorization with Sparseness Constraints” P. Hoyer, 2004 4 “SVD based initialization: A head start for nonnegative matrix factorization” C. Boutsidis, E. Gallopoulos, 2008 5 “Fast local algorithms for large scale nonnegative matrix and tensor factorizations.” A. Cichocki, A. Phan, 2009 6 ( 1 , 2 ) “Algorithms for nonnegative matrix factorization with the beta-divergence” C. Fevotte, J. Idier, 2011 