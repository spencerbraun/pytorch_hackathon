sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions
3.3. Metrics and scoring: quantifying the quality of predictions
modules/model_evaluation.html
 3.3.2. Classification metrics  The sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the parameter. Some of these are restricted to the binary classification case: precision_recall_curve (y_true, probas_pred, *) Compute precision-recall pairs for different probability thresholds roc_curve (y_true, y_score, *[, pos_label, …]) Compute Receiver operating characteristic (ROC) Others also work in the multiclass case: balanced_accuracy_score (y_true, y_pred, *[, …]) Compute the balanced accuracy cohen_kappa_score (y1, y2, *[, labels, …]) Cohen’s kappa: a statistic that measures inter-annotator agreement. confusion_matrix (y_true, y_pred, *[, …]) Compute confusion matrix to evaluate the accuracy of a classification. hinge_loss (y_true, pred_decision, *[, …]) Average hinge loss (non-regularized) matthews_corrcoef (y_true, y_pred, *[, …]) Compute the Matthews correlation coefficient (MCC) roc_auc_score (y_true, y_score, *[, average, …]) Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. Some also work in the multilabel case: accuracy_score (y_true, y_pred, *[, …]) Accuracy classification score. classification_report (y_true, y_pred, *[, …]) Build a text report showing the main classification metrics. f1_score (y_true, y_pred, *[, labels, …]) Compute the F1 score, also known as balanced F-score or F-measure fbeta_score (y_true, y_pred, *, beta[, …]) Compute the F-beta score hamming_loss (y_true, y_pred, *[, sample_weight]) Compute the average Hamming loss. jaccard_score (y_true, y_pred, *[, labels, …]) Jaccard similarity coefficient score log_loss (y_true, y_pred, *[, eps, …]) Log loss, aka logistic loss or cross-entropy loss. multilabel_confusion_matrix (y_true, y_pred, *) Compute a confusion matrix for each class or sample precision_recall_fscore_support (y_true, …) Compute precision, recall, F-measure and support for each class precision_score (y_true, y_pred, *[, labels, …]) Compute the precision recall_score (y_true, y_pred, *[, labels, …]) Compute the recall roc_auc_score (y_true, y_score, *[, average, …]) Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. zero_one_loss (y_true, y_pred, *[, …]) Zero-one classification loss. And some work with binary and multilabel (but not multiclass) problems: average_precision_score (y_true, y_score, *) Compute average precision (AP) from prediction scores In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition. 3.3.2.1. From binary to multiclass and multilabel  Some metrics are essentially defined for binary classification tasks (e.g. f1_score , roc_auc_score ). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled (though this may be configurable through the parameter). In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the parameter. simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class. accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample. gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored. applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes for each sample in the evaluation data, and returning their ( -weighted) average. Selecting will return an array with the score for each class. While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell has value 1 if sample has label and value 0 otherwise. 3.3.2.2. Accuracy score  The accuracy_score function computes the accuracy , either the fraction (default) or the count (normalizeFalse) of correct predictions. In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0. If is the predicted value of the -th sample and is the corresponding true value, then the fraction of correct predictions over is defined as \[\texttt{accuracy}(y, \hat{y})  \frac{1}{n_\text{samples}} \sum_{i0}^{n_\text{samples}-1} 1(\hat{y}_i  y_i)\] where is the indicator function . In the multilabel case with binary label indicators: Example: See Test with permutations the significance of a classification score for an example of accuracy score usage using permutations of the dataset. 3.3.2.3. Balanced accuracy score  The balanced_accuracy_score function computes the balanced accuracy , which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy. In the binary case, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate), or the area under the ROC curve with binary predictions rather than scores: \[\texttt{balanced-accuracy}  \frac{1}{2}\left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP}\right )\] If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions). In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to . The score ranges from 0 to 1, or when is used, it rescaled to the range to 1, inclusive, with performance at random scoring 0. If is the true value of the -th sample, and is the corresponding sample weight, then we adjust the sample weight to: \[\hat{w}_i  \frac{w_i}{\sum_j{1(y_j  y_i) w_j}}\] where is the indicator function . Given predicted for sample , balanced accuracy is defined as: \[\texttt{balanced-accuracy}(y, \hat{y}, w)  \frac{1}{\sum{\hat{w}_i}} \sum_i 1(\hat{y}_i  y_i) \hat{w}_i\] With , balanced accuracy reports the relative increase from . In the binary case, this is also known as *Youden’s J statistic* , or informedness . Note The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature: Our definition: [Mosley2013] , [Kelleher2015] and [Guyon2015] , where [Guyon2015] adopt the adjusted version to ensure that random predictions have a score of and perfect predictions have a score of .. Class balanced accuracy as described in [Mosley2013] : the minimum between the precision and the recall for each class is computed. Those values are then averaged over the total number of classes to get the balanced accuracy. Balanced Accuracy as described in [Urbanowicz2015] : the average of sensitivity and specificity is computed for each class and then averaged over total number of classes. References: Guyon2015 ( 1 , 2 ) I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, Design of the 2015 ChaLearn AutoML Challenge , IJCNN 2015. Mosley2013 ( 1 , 2 ) L. Mosley, A balanced approach to the multi-class imbalance problem , IJCV 2010. Kelleher2015 John. D. Kelleher, Brian Mac Namee, Aoife D’Arcy, Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies , 2015. Urbanowicz2015 Urbanowicz R.J., Moore, J.H. ExSTraCS 2.0: description and evaluation of a scalable learning classifier system , Evol. Intel. (2015) 8: 89. 3.3.2.4. Cohen’s kappa  The function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth. The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels). Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators. 3.3.2.5. Confusion matrix  The confusion_matrix function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class (Wikipedia and other references may use different convention for axes). By definition, entry in a confusion matrix is the number of observations actually in group , but predicted to be in group . Here is an example: plot_confusion_matrix can be used to visually represent a confusion matrix as shown in the Confusion matrix example, which creates the following figure: The parameter allows to report ratios instead of counts. The confusion matrix can be normalized in 3 different ways: , , and which will divide the counts by the sum of each columns, rows, or the entire matrix, respectively. For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows: Example: See Confusion matrix for an example of using a confusion matrix to evaluate classifier output quality. See Recognizing hand-written digits for an example of using a confusion matrix to classify hand-written digits. See Classification of text documents using sparse features for an example of using a confusion matrix to classify text documents. 3.3.2.6. Classification report  The classification_report function builds a text report showing the main classification metrics. Here is a small example with custom and inferred labels: Example: See Recognizing hand-written digits for an example of classification report usage for hand-written digits. See Classification of text documents using sparse features for an example of classification report usage for text documents. See Parameter estimation using grid search with cross-validation for an example of classification report usage for grid search with nested cross-validation. 3.3.2.7. Hamming loss  The hamming_loss computes the average Hamming loss or Hamming distance between two sets of samples. If is the predicted value for the -th label of a given sample, is the corresponding true value, and is the number of classes or labels, then the Hamming loss between two samples is defined as: \[L_{Hamming}(y, \hat{y})  \frac{1}{n_\text{labels}} \sum_{j0}^{n_\text{labels} - 1} 1(\hat{y}_j \not y_j)\] where is the indicator function . In the multilabel case with binary label indicators: Note In multiclass classification, the Hamming loss corresponds to the Hamming distance between and which is similar to the Zero one loss function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive. 3.3.2.8. Precision, recall and F-measures  Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples. The F-measure ( and measures) can be interpreted as a weighted harmonic mean of the precision and recall. A measure reaches its best value at 1 and its worst score at 0. With , and are equivalent, and the recall and the precision are equally important. The precision_recall_curve computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold. The average_precision_score function computes the average precision (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as \[\text{AP}  \sum_n (R_n - R_{n-1}) P_n\] where and are the precision and recall at the nth threshold. With random predictions, the AP is the fraction of positive samples. References [Manning2008] and [Everingham2010] present alternative variants of AP that interpolate the precision-recall curve. Currently, average_precision_score does not implement any interpolated variant. References [Davis2006] and [Flach2015] describe why a linear interpolation of points on the precision-recall curve provides an overly-optimistic measure of classifier performance. This linear interpolation is used when computing area under the curve with the trapezoidal rule in auc . Several functions allow you to analyze the precision, recall and F-measures score: average_precision_score (y_true, y_score, *) Compute average precision (AP) from prediction scores f1_score (y_true, y_pred, *[, labels, …]) Compute the F1 score, also known as balanced F-score or F-measure fbeta_score (y_true, y_pred, *, beta[, …]) Compute the F-beta score precision_recall_curve (y_true, probas_pred, *) Compute precision-recall pairs for different probability thresholds precision_recall_fscore_support (y_true, …) Compute precision, recall, F-measure and support for each class precision_score (y_true, y_pred, *[, labels, …]) Compute the precision recall_score (y_true, y_pred, *[, labels, …]) Compute the recall Note that the precision_recall_curve function is restricted to the binary case. The average_precision_score function works only in binary classification and multilabel indicator format. The plot_precision_recall_curve function plots the precision recall as follows. Examples: See Classification of text documents using sparse features for an example of f1_score usage to classify text documents. See Parameter estimation using grid search with cross-validation for an example of precision_score and recall_score usage to estimate parameters using grid search with nested cross-validation. See Precision-Recall for an example of precision_recall_curve usage to evaluate classifier output quality. References: Manning2008 C.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval , 2008. Everingham2010 M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, The Pascal Visual Object Classes (VOC) Challenge , IJCV 2010. Davis2006 J. Davis, M. Goadrich, The Relationship Between Precision-Recall and ROC Curves , ICML 2006. Flach2015 P.A. Flach, M. Kull, Precision-Recall-Gain Curves: PR Analysis Done Right , NIPS 2015. 3.3.2.8.1. Binary classification  In a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer to the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to whether that prediction corresponds to the external judgment (sometimes known as the ‘’observation’’). Given these definitions, we can formulate the following table: Actual class (observation) Predicted class (expectation) tp (true positive) Correct result fp (false positive) Unexpected result fn (false negative) Missing result tn (true negative) Correct absence of result In this context, we can define the notions of precision, recall and F-measure: \[\text{precision}  \frac{tp}{tp + fp},\] \[\text{recall}  \frac{tp}{tp + fn},\] \[F_\beta  (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}.\] Here are some small examples in binary classification: 3.3.2.8.2. Multiclass and multilabel classification  In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the argument to the average_precision_score (multilabel only), f1_score , fbeta_score , precision_recall_fscore_support , precision_score and recall_score functions, as described above . Note that if all labels are included, “micro”-averaging in a multiclass setting will produce precision, recall and that are all identical to accuracy. Also note that “weighted” averaging may produce an F-score that is not between precision and recall. To make this more explicit, consider the following notation: the set of predicted pairs the set of true pairs the set of labels the set of samples the subset of with sample , i.e. the subset of with label similarly, and are subsets of for some sets and (Conventions vary on handling ; this implementation uses , and similar for .) Then the metrics are defined as: Precision Recall F_beta For multiclass classification with a “negative class”, it is possible to exclude some labels: Similarly, labels not present in the data sample may be accounted for in macro-averaging. 3.3.2.9. Jaccard similarity coefficient score  The jaccard_score function computes the average of Jaccard similarity coefficients , also called the Jaccard index, between pairs of label sets. The Jaccard similarity coefficient of the -th samples, with a ground truth label set and predicted label set , is defined as \[J(y_i, \hat{y}_i)  \frac{|y_i \cap \hat{y}_i|}{|y_i \cup \hat{y}_i|}.\] jaccard_score works like precision_recall_fscore_support as a naively set-wise measure applying natively to binary targets, and extended to apply to multilabel and multiclass through the use of (see above ). In the binary case: In the multilabel case with binary label indicators: Multiclass problems are binarized and treated like the corresponding multilabel problem: 3.3.2.10. Hinge loss  The hinge_loss function computes the average distance between the model and the data using hinge loss , a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.) If the labels are encoded with +1 and -1, : is the true value, and is the predicted decisions as output by , then the hinge loss is defined as: \[L_\text{Hinge}(y, w)  \max\left\{1 - wy, 0\right\}  \left|1 - wy\right|_+\] If there are more than two labels, hinge_loss uses a multiclass variant due to Crammer & Singer. Here is the paper describing it. If is the predicted decision for true label and is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by: \[L_\text{Hinge}(y_w, y_t)  \max\left\{1 + y_t - y_w, 0\right\}\] Here a small example demonstrating the use of the hinge_loss function with a svm classifier in a binary class problem: Here is an example demonstrating the use of the hinge_loss function with a svm classifier in a multiclass problem: 3.3.2.11. Log loss  Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs ( ) of a classifier instead of its discrete predictions. For binary classification with a true label and a probability estimate , the log loss per sample is the negative log-likelihood of the classifier given the true label: \[L_{\log}(y, p)  -\log \operatorname{Pr}(y|p)  -(y \log (p) + (1 - y) \log (1 - p))\] This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix , i.e., if sample has label taken from a set of labels. Let be a matrix of probability estimates, with . Then the log loss of the whole set is \[L_{\log}(Y, P)  -\log \operatorname{Pr}(Y|P)  - \frac{1}{N} \sum_{i0}^{N-1} \sum_{k0}^{K-1} y_{i,k} \log p_{i,k}\] To see how this generalizes the binary log loss given above, note that in the binary case, and , so expanding the inner sum over gives the binary log loss. The log_loss function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator’s method. The first in denotes 90% probability that the first sample has label 0. The log loss is non-negative. 3.3.2.12. Matthews correlation coefficient  The matthews_corrcoef function computes the Matthew’s correlation coefficient (MCC) for binary classes. Quoting Wikipedia: “The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.” In the binary (two-class) case, , , and are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as \[MCC  \frac{tp \times tn - fp \times fn}{\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\] In the multiclass case, the Matthews correlation coefficient can be defined in terms of a confusion_matrix for classes. To simplify the definition consider the following intermediate variables: the number of times class truly occurred, the number of times class was predicted, the total number of samples correctly predicted, the total number of samples. Then the multiclass MCC is defined as: \[MCC  \frac{ c \times s - \sum_{k}^{K} p_k \times t_k }{\sqrt{ (s^2 - \sum_{k}^{K} p_k^2) \times (s^2 - \sum_{k}^{K} t_k^2) }}\] When there are more than two labels, the value of the MCC will no longer range between -1 and +1. Instead the minimum value will be somewhere between -1 and 0 depending on the number and distribution of ground true labels. The maximum value is always +1. Here is a small example illustrating the usage of the matthews_corrcoef function: 3.3.2.13. Multi-label confusion matrix  The multilabel_confusion_matrix function computes class-wise (default) or sample-wise (samplewiseTrue) multilabel confusion matrix to evaluate the accuracy of a classification. multilabel_confusion_matrix also treats multiclass data as if it were multilabel, as this is a transformation commonly applied to evaluate multiclass problems with binary classification metrics (such as precision, recall, etc.). When calculating class-wise multilabel confusion matrix , the count of true negatives for class is , false negatives is , true positives is and false positives is . Here is an example demonstrating the use of the multilabel_confusion_matrix function with multilabel indicator matrix input: Or a confusion matrix can be constructed for each sample’s labels: Here is an example demonstrating the use of the multilabel_confusion_matrix function with multiclass input: Here are some examples demonstrating the use of the multilabel_confusion_matrix function to calculate recall (or sensitivity), specificity, fall out and miss rate for each class in a problem with multilabel indicator matrix input. Calculating recall (also called the true positive rate or the sensitivity) for each class: Calculating specificity (also called the true negative rate) for each class: Calculating fall out (also called the false positive rate) for each class: Calculating miss rate (also called the false negative rate) for each class: 3.3.2.14. Receiver operating characteristic (ROC)  The function roc_curve computes the receiver operating characteristic curve, or ROC curve . Quoting Wikipedia : “A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR  true positive rate) vs. the fraction of false positives out of the negatives (FPR  false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.” This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the roc_curve function: This figure shows an example of such an ROC curve: The roc_auc_score function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. For more information see the Wikipedia article on AUC . In multi-label classification, the roc_auc_score function is extended by averaging over the labels as above . Compared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesn’t require optimizing a threshold for each label. The roc_auc_score function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the predicted labels are provided in an array with values from 0 to , and the scores correspond to the probability estimates that a sample belongs to a particular class. The OvO and OvR algorithms support weighting uniformly ( ) and by prevalence ( ). One-vs-one Algorithm : Computes the average AUC of all possible pairwise combinations of classes. [HT2001] defines a multiclass AUC metric weighted uniformly: \[\frac{2}{c(c-1)}\sum_{j1}^{c}\sum_{k > j}^c (\text{AUC}(j | k) + \text{AUC}(k | j))\] where is the number of classes and is the AUC with class as the positive class and class as the negative class. In general, in the multiclass case. This algorithm is used by setting the keyword argument to and to . The [HT2001] multiclass AUC metric can be extended to be weighted by the prevalence: \[\frac{2}{c(c-1)}\sum_{j1}^{c}\sum_{k > j}^c p(j \cup k)( \text{AUC}(j | k) + \text{AUC}(k | j))\] where is the number of classes. This algorithm is used by setting the keyword argument to and to . The option returns a prevalence-weighted average as described in [FC2009] . One-vs-rest Algorithm : Computes the AUC of each class against the rest [PD2000] . The algorithm is functionally the same as the multilabel case. To enable this algorithm set the keyword argument to . Like OvO, OvR supports two types of averaging: [F2006] and [F2001] . In applications where a high false positive rate is not tolerable the parameter of roc_auc_score can be used to summarize the ROC curve up to the given limit. Examples: See Receiver Operating Characteristic (ROC) for an example of using ROC to evaluate the quality of the output of a classifier. See Receiver Operating Characteristic (ROC) with cross validation for an example of using ROC to evaluate classifier output quality, using cross-validation. See Species distribution modeling for an example of using ROC to model species distribution. References: HT2001 ( 1 , 2 ) Hand, D.J. and Till, R.J., (2001). A simple generalisation of the area under the ROC curve for multiple class classification problems. Machine learning, 45(2), pp.171-186. FC2009 Ferri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009). An Experimental Comparison of Performance Measures for Classification. Pattern Recognition Letters. 30. 27-38. PD2000 Provost, F., Domingos, P. (2000). Well-trained PETs: Improving probability estimation trees (Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business, New York University. F2006 Fawcett, T., 2006. An introduction to ROC analysis. Pattern Recognition Letters, 27(8), pp. 861-874. F2001 Fawcett, T., 2001. Using rule sets to maximize ROC performance In Data Mining, 2001. Proceedings IEEE International Conference, pp. 131-138. 3.3.2.15. Zero one loss  The zero_one_loss function computes the sum or the average of the 0-1 classification loss ( ) over . By default, the function normalizes over the sample. To get the sum of the , set to . In multilabel classification, the zero_one_loss scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set to If is the predicted value of the -th sample and is the corresponding true value, then the 0-1 loss is defined as: \[L_{0-1}(y_i, \hat{y}_i)  1(\hat{y}_i \not y_i)\] where is the indicator function . In the multilabel case with binary label indicators, where the first label set [0,1] has an error: Example: See Recursive feature elimination with cross-validation for an example of zero one loss usage to perform recursive feature elimination with cross-validation. 3.3.2.16. Brier score loss  The brier_score_loss function computes the Brier score for binary classes. Quoting Wikipedia: “The Brier score is a proper score function that measures the accuracy of probabilistic predictions. It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes.” This function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1. The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the “calibration” of a set of probabilistic predictions. \[BS  \frac{1}{N} \sum_{t1}^{N}(f_t - o_t)^2\] where : is the total number of predictions, is the predicted probability of the actual outcome . Here is a small example of usage of this function:: Example: See Probability calibration of classifiers for an example of Brier score loss usage to perform probability calibration of classifiers. References: G. Brier, Verification of forecasts expressed in terms of probability , Monthly weather review 78.1 (1950) 