sklearn_1_1_linear_models
1.1. Linear Models
modules/linear_model.html
 1.1.1. Ordinary Least Squares  LinearRegression fits a linear model with coefficients to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form: \[\min_{w} || X w - y||_2^2\] LinearRegression will take in its method arrays X, y and will store the coefficients of the linear model in its member: The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design. Examples: Linear Regression Example 1.1.1.1. Ordinary Least Squares Complexity  The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape this method has a cost of , assuming that . 