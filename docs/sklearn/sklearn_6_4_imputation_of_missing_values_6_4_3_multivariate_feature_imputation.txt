sklearn_6_4_imputation_of_missing_values
6.4. Imputation of missing values
modules/impute.html
 6.4.3. Multivariate feature imputation  A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output and the other feature columns are treated as inputs . A regressor is fit on for known . Then, the regressor is used to predict the missing values of . This is done for each feature in an iterative fashion, and then is repeated for imputation rounds. The results of the final imputation round are returned. Note This estimator is still experimental for now: default parameters or details of behaviour might change without any deprecation cycle. Resolving the following issues would help stabilize IterativeImputer : convergence criteria ( #14338 ), default estimators ( #13286 ), and use of random state ( #15611 ). To use it, you need to explicitly import . Both SimpleImputer and IterativeImputer can be used in a Pipeline as a way to build a composite estimator that supports imputation. See Imputing missing values before building an estimator . 6.4.3.1. Flexibility of IterativeImputer  There are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with IterativeImputer by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See Imputing missing values with variants of IterativeImputer . 6.4.3.2. Multiple vs. Single Imputation  In the statistics community, it is common practice to perform multiple imputations, generating, for example, separate imputations for a single feature matrix. Each of these imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation. Our implementation of IterativeImputer was inspired by the R MICE package (Multivariate Imputation by Chained Equations) 1 , but differs from it by returning a single imputation instead of multiple imputations. However, IterativeImputer can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when . See 2 , chapter 4 for more discussion on multiple vs. single imputations. It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values. Note that a call to the method of IterativeImputer is not allowed to change the number of samples. Therefore multiple imputations cannot be achieved by a single call to . 