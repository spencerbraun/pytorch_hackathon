sklearn_6_3_preprocessing_data
6.3. Preprocessing data
modules/preprocessing.html
 6.3.1. Standardization, or mean removal and variance scaling  Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance . In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. The function scale provides a quick and easy way to perform this operation on a single array-like dataset: Scaled data has zero mean and unit variance: The module further provides a utility class StandardScaler that implements the API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline : The scaler instance can then be used on new data to transform it the same way it did on the training set: It is possible to disable either centering or scaling by either passing or to the constructor of StandardScaler . 6.3.1.1. Scaling features to a range  An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler , respectively. The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data. Here is an example to scale a toy data matrix to the range: The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data: It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data: If MinMaxScaler is given an explicit the full formula is: MaxAbsScaler works in a very similar fashion, but scales in a way that the training data lies within the range by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data. Here is how to use the toy data from the previous example with this scaler: As with scale , the module further provides convenience functions minmax_scale and maxabs_scale if you donâ€™t want to create an object. 6.3.1.2. Scaling sparse data  Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales. MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go about this. However, scale and StandardScaler can accept matrices as input, as long as is explicitly passed to the constructor. Otherwise a will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. RobustScaler cannot be fitted to sparse inputs, but you can use the method on sparse inputs. Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see and ). Any other sparse input will be converted to the Compressed Sparse Rows representation . To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream. Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the method of sparse matrices is another option. 6.3.1.3. Scaling data with outliers  If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use robust_scale and RobustScaler as drop-in replacements instead. They use more robust estimates for the center and range of your data. References: Further discussion on the importance of centering and scaling data is available on this FAQ: Should I normalize/standardize/rescale the data? Scaling vs Whitening It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use sklearn.decomposition.PCA with to further remove the linear correlation across features. Scaling a 1D array All above functions (i.e. scale , minmax_scale , maxabs_scale , and robust_scale ) accept 1D array which can be useful in some specific case. 6.3.1.4. Centering kernel matrices  If you have a kernel matrix of a kernel that computes a dot product in a feature space defined by function , a KernelCenterer can transform the kernel matrix so that it contains inner products in the feature space defined by followed by removal of the mean in that space. 