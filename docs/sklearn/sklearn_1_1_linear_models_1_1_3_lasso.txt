sklearn_1_1_linear_models
1.1. Linear Models
modules/linear_model.html
 1.1.3. Lasso  The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients (see Compressive sensing: tomography reconstruction with L1 prior (Lasso) ). Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is: \[\min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}\] The lasso estimate thus solves the minimization of the least-squares penalty with added, where is a constant and is the -norm of the coefficient vector. The implementation in the class Lasso uses coordinate descent as the algorithm to fit the coefficients. See Least Angle Regression for another implementation: The function lasso_path is useful for lower-level tasks, as it computes the coefficients along the full path of possible values. Examples: Lasso and Elastic Net for Sparse Signals Compressive sensing: tomography reconstruction with L1 prior (Lasso) Common pitfalls in interpretation of coefficients of linear models Note Feature selection with Lasso As the Lasso regression yields sparse models, it can thus be used to perform feature selection, as detailed in L1-based feature selection . The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control. References “Regularization Path For Generalized linear Models by Coordinate Descent”, Friedman, Hastie & Tibshirani, J Stat Softw, 2010 ( Paper ). “An Interior-Point Method for Large-Scale L1-Regularized Least Squares,” S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007 ( Paper ) 1.1.3.1. Setting regularization parameter  The parameter controls the degree of sparsity of the estimated coefficients. 1.1.3.1.1. Using cross-validation  scikit-learn exposes objects that set the Lasso parameter by cross-validation: LassoCV and LassoLarsCV . LassoLarsCV is based on the Least Angle Regression algorithm explained below. For high-dimensional datasets with many collinear features, LassoCV is most often preferable. However, LassoLarsCV has the advantage of exploring more relevant values of parameter, and if the number of samples is very small compared to the number of features, it is often faster than LassoCV . 1.1.3.1.2. Information-criteria based model selection  Alternatively, the estimator LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples). Examples: Lasso model selection: Cross-Validation / AIC / BIC 1.1.3.1.3. Comparison with the regularization parameter of SVM  The equivalence between and the regularization parameter of SVM, is given by or , depending on the estimator and the exact objective function optimized by the model. 