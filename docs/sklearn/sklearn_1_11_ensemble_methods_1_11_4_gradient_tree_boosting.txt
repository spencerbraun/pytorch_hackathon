sklearn_1_11_ensemble_methods
1.11. Ensemble methods
modules/ensemble.html
 1.11.4. Gradient Tree Boosting  Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems in a variety of areas including Web search ranking and ecology. The module sklearn.ensemble provides methods for both classification and regression via gradient boosted decision trees. Note Scikit-learn 0.21 introduces two new experimental implementations of gradient boosting trees, namely HistGradientBoostingClassifier and HistGradientBoostingRegressor , inspired by LightGBM (See [LightGBM] ). These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples. They also have built-in support for missing values, which avoids the need for an imputer. These estimators are described in more detail below in Histogram-Based Gradient Boosting . The following guide focuses on GradientBoostingClassifier and GradientBoostingRegressor , which might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting. The usage and the parameters of GradientBoostingClassifier and GradientBoostingRegressor are described below. The 2 most important parameters of these estimators are and . 1.11.4.1. Classification  GradientBoostingClassifier supports both binary and multi-class classification. The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners: The number of weak learners (i.e. regression trees) is controlled by the parameter ; The size of each tree can be controlled either by setting the tree depth via or by setting the number of leaf nodes via . The is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via shrinkage . Note Classification with more than 2 classes requires the induction of regression trees at each iteration, thus, the total number of induced trees equals . For datasets with a large number of classes we strongly recommend to use HistGradientBoostingClassifier as an alternative to GradientBoostingClassifier . 1.11.4.2. Regression  GradientBoostingRegressor supports a number of different loss functions for regression which can be specified via the argument ; the default loss function for regression is least squares ( ). The figure below shows the results of applying GradientBoostingRegressor with least squares loss and 500 base learners to the Boston house price dataset ( sklearn.datasets.load_boston ). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the train_score_ attribute of the gradient boosting model. The test error at each iterations can be obtained via the staged_predict method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. ) by early stopping. The plot on the right shows the impurity-based feature importances which can be obtained via the property. Examples: Gradient Boosting regression Gradient Boosting Out-of-Bag estimates 1.11.4.3. Fitting additional weak-learners  Both GradientBoostingRegressor and GradientBoostingClassifier support which allows you to add more estimators to an already fitted model. 1.11.4.4. Controlling the tree size  The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth can capture interactions of order . There are two ways in which the size of the individual regression trees can be controlled. If you specify then complete binary trees of depth will be grown. Such trees will have (at most) leaf nodes and split nodes. Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter . In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with has split nodes and thus can model interactions of up to order . We found that gives comparable results to but is significantly faster to train at the expense of a slightly higher training error. The parameter corresponds to the variable in the chapter on gradient boosting in [F2001] and is related to the parameter in Râ€™s gbm package where . 1.11.4.5. Mathematical formulation  We first present GBRT for regression, and then detail the classification case. 1.11.4.5.1. Regression  GBRT regressors are additive models whose prediction for a given input is of the following form: \[\hat{y_i}  F_M(x_i)  \sum_{m1}^{M} h_m(x_i)\] where the are estimators called weak learners in the context of boosting. Gradient Tree Boosting uses decision tree regressors of fixed size as weak learners. The constant M corresponds to the parameter. Similar to other boosting algorithms, a GBRT is built in a greedy fashion: \[F_m(x)  F_{m-1}(x) + h_m(x),\] where the newly added tree is fitted in order to minimize a sum of losses , given the previous ensemble : \[h_m  \arg\min_{h} L_m  \arg\min_{h} \sum_{i1}^{n} l(y_i, F_{m-1}(x_i) + h(x_i)),\] where is defined by the parameter, detailed in the next section. By default, the initial model is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values. The initial model can also be specified via the argument. Using a first-order Taylor approximation, the value of can be approximated as follows: \[l(y_i, F_{m-1}(x_i) + h_m(x_i)) \approx l(y_i, F_{m-1}(x_i)) + h_m(x_i) \left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{FF_{m - 1}}.\] Note Briefly, a first-order Taylor approximation says that . Here, corresponds to , and corresponds to The quantity is the derivative of the loss with respect to its second parameter, evaluated at . It is easy to compute for any given in a closed form since the loss is differentiable. We will denote it by . Removing the constant terms, we have: \[h_m \approx \arg\min_{h} \sum_{i1}^{n} h(x_i) g_i\] This is minimized if is fitted to predict a value that is proportional to the negative gradient . Therefore, at each iteration, the estimator is fitted to predict the negative gradients of the samples . The gradients are updated at each iteration. This can be considered as some kind of gradient descent in a functional space. Note For some losses, e.g. the least absolute deviation (LAD) where the gradients are , the values predicted by a fitted are not accurate enough: the tree can only output integer values. As a result, the leaves values of the tree are modified once the tree is fitted, such that the leaves values minimize the loss . The update is loss-dependent: for the LAD loss, the value of a leaf is updated to the median of the samples in that leaf. 1.11.4.5.2. Classification  Gradient boosting for classification is very similar to the regression case. However, the sum of the trees is not homogeneous to a prediction: it cannot be a class, since the trees predict continuous values. The mapping from the value to a class or a probability is loss-dependent. For the deviance (or log-loss), the probability that belongs to the positive class is modeled as where is the sigmoid function. For multiclass classification, K trees (for K classes) are built at each of the iterations. The probability that belongs to class k is modeled as a softmax of the values. Note that even for a classification task, the sub-estimator is still a regressor, not a classifier. This is because the sub-estimators are trained to predict (negative) gradients , which are always continuous quantities. 1.11.4.6. Loss Functions  The following loss functions are supported and can be specified using the parameter : Regression Least squares ( ): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values. Least absolute deviation ( ): A robust loss function for regression. The initial model is given by the median of the target values. Huber ( ): Another robust loss function that combines least squares and least absolute deviation; use to control the sensitivity with regards to outliers (see [F2001] for more details). Quantile ( ): A loss function for quantile regression. Use to specify the quantile. This loss function can be used to create prediction intervals (see Prediction Intervals for Gradient Boosting Regression ). Classification Binomial deviance ( ): The negative binomial log-likelihood loss function for binary classification (provides probability estimates). The initial model is given by the log odds-ratio. Multinomial deviance ( ): The negative multinomial log-likelihood loss function for multi-class classification with mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes. Exponential loss ( ): The same loss function as AdaBoostClassifier . Less robust to mislabeled examples than ; can only be used for binary classification. 1.11.4.7. Shrinkage via learning rate  [F2001] proposed a simple regularization strategy that scales the contribution of each weak learner by a constant factor : \[F_m(x)  F_{m-1}(x) + \nu h_m(x)\] The parameter is also called the learning rate because it scales the step length the gradient descent procedure; it can be set via the parameter. The parameter strongly interacts with the parameter , the number of weak learners to fit. Smaller values of require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of favor better test error. [HTF] recommend to set the learning rate to a small constant (e.g. ) and choose by early stopping. For a more detailed discussion of the interaction between and see [R2007] . 1.11.4.8. Subsampling  [F1999] proposed stochastic gradient boosting, which combines gradient boosting with bootstrap averaging (bagging). At each iteration the base classifier is trained on a fraction of the available training data. The subsample is drawn without replacement. A typical value of is 0.5. The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in RandomForestClassifier . The number of subsampled features can be controlled via the parameter. Note Using a small value can significantly decrease the runtime. Stochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improvement in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The improvements are stored in the attribute oob_improvement_ . holds the improvement in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time consuming. Examples: Gradient Boosting regularization Gradient Boosting Out-of-Bag estimates OOB Errors for Random Forests 1.11.4.9. Interpretation with feature importance  Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models. Often features do not contribute equally to predict the target response; in many situations the majority of the features are in fact irrelevant. When interpreting a model, the first question usually is: what are those important features and how do they contributing in predicting the target response? Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the impurity-based feature importance of each tree (see Feature importance evaluation for more details). The feature importance scores of a fit gradient boosting model can be accessed via the property: Note that this computation of feature importance is based on entropy, and it is distinct from sklearn.inspection.permutation_importance which is based on permutation of the features. Examples: Gradient Boosting regression 