sklearn_1_11_ensemble_methods
1.11. Ensemble methods
modules/ensemble.html
 1.11.6. Voting Classifier  The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. 1.11.6.1. Majority Class Labels (Majority/Hard Voting)  In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. E.g., if the prediction for a given sample is classifier 1 -> class 1 classifier 2 -> class 1 classifier 3 -> class 2 the VotingClassifier (with ) would classify the sample as “class 1” based on the majority class label. In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E.g., in the following scenario classifier 1 -> class 2 classifier 2 -> class 1 the class label 1 will be assigned to the sample. 1.11.6.2. Usage  The following example shows how to fit the majority rule classifier: 1.11.6.3. Weighted Average Probabilities (Soft Voting)  In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. Specific weights can be assigned to each classifier via the parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. To illustrate this with a simple example, let’s assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w11, w21, w31. The weighted average probabilities for a sample would then be calculated as follows: classifier class 1 class 2 class 3 classifier 1 w1 * 0.2 w1 * 0.5 w1 * 0.3 classifier 2 w2 * 0.6 w2 * 0.3 w2 * 0.1 classifier 3 w3 * 0.3 w3 * 0.4 w3 * 0.3 weighted average 0.37 0.4 0.23 Here, the predicted class label is 2, since it has the highest average probability. The following example illustrates how the decision regions may change when a soft VotingClassifier is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier: 1.11.6.4. Using the with  The VotingClassifier can also be used together with GridSearchCV in order to tune the hyperparameters of the individual estimators: 1.11.6.5. Usage  In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support method): Optionally, weights can be provided for the individual classifiers: 