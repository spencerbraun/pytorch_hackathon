sklearn_1_2_linear_and_quadratic_discriminant_analysis
1.2. Linear and Quadratic Discriminant Analysis
modules/lda_qda.html
 1.2.3. Mathematical formulation of LDA dimensionality reduction  First note that the K means are vectors in , and they lie in an affine subspace of dimension at least (2 points lie on a line, 3 points lie on a plane, etc). As mentioned above, we can interpret LDA as assigning to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first sphering the data so that the covariance matrix is the identity, and then assigning to the closest mean in terms of Euclidean distance (still accounting for the class priors). Computing Euclidean distances in this d-dimensional space is equivalent to first projecting the data points into , and computing the distances there (since the other dimensions will contribute equally to each class in terms of distance). In other words, if is closest to in the original space, it will also be the case in . This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a dimensional space. We can reduce the dimension even more, to a chosen , by projecting onto the linear subspace which maximizes the variance of the after projection (in effect, we are doing a form of PCA for the transformed class means ). This corresponds to the parameter used in the transform method. See 1 for more details. 