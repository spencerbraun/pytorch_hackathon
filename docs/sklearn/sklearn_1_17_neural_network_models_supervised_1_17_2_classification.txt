sklearn_1_17_neural_network_models_supervised
1.17. Neural network models (supervised)
modules/neural_networks_supervised.html
 1.17.2. Classification  Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation . MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples: After fitting (training), the model can predict labels for new samples: MLP can fit a non-linear model to the training data. contains the weight matrices that constitute the model parameters: Currently, MLPClassifier supports only the Cross-Entropy loss function, which allows probability estimates by running the method. MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates per sample : MLPClassifier supports multi-class classification by applying Softmax as the output function. Further, the model supports multi-label classification in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to are rounded to , otherwise to . For a predicted output of a sample, the indices where the value is represents the assigned classes of that sample: See the examples below and the docstring of MLPClassifier.fit for further information. Examples: Compare Stochastic learning strategies for MLPClassifier Visualization of MLP weights on MNIST 