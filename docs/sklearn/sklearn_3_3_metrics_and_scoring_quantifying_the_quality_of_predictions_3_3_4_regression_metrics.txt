sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions
3.3. Metrics and scoring: quantifying the quality of predictions
modules/model_evaluation.html
 3.3.4. Regression metrics  The sklearn.metrics module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: mean_squared_error , mean_absolute_error , explained_variance_score and r2_score . These functions have an keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is , which specifies a uniformly weighted mean over outputs. If an of shape is passed, then its entries are interpreted as weights and an according weighted average is returned. If is is specified, then all unaltered individual scores or losses will be returned in an array of shape . The r2_score and explained_variance_score accept an additional value for the parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. is the default value for r2_score for backward compatibility. This will be changed to in the future. 3.3.4.1. Explained variance score  The explained_variance_score computes the explained variance regression score . If is the estimated target output, the corresponding (correct) target output, and is Variance , the square of the standard deviation, then the explained variance is estimated as follow: \[explained\_{}variance(y, \hat{y})  1 - \frac{Var\{ y - \hat{y}\}}{Var\{y\}}\] The best possible score is 1.0, lower values are worse. Here is a small example of usage of the explained_variance_score function: 3.3.4.2. Max error  The max_error function computes the maximum residual error , a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, would be on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted. If is the predicted value of the -th sample, and is the corresponding true value, then the max error is defined as \[\text{Max Error}(y, \hat{y})  max(| y_i - \hat{y}_i |)\] Here is a small example of usage of the max_error function: The max_error does not support multioutput. 3.3.4.3. Mean absolute error  The mean_absolute_error function computes mean absolute error , a risk metric corresponding to the expected value of the absolute error loss or -norm loss. If is the predicted value of the -th sample, and is the corresponding true value, then the mean absolute error (MAE) estimated over is defined as \[\text{MAE}(y, \hat{y})  \frac{1}{n_{\text{samples}}} \sum_{i0}^{n_{\text{samples}}-1} \left| y_i - \hat{y}_i \right|.\] Here is a small example of usage of the mean_absolute_error function: 3.3.4.4. Mean squared error  The mean_squared_error function computes mean square error , a risk metric corresponding to the expected value of the squared (quadratic) error or loss. If is the predicted value of the -th sample, and is the corresponding true value, then the mean squared error (MSE) estimated over is defined as \[\text{MSE}(y, \hat{y})  \frac{1}{n_\text{samples}} \sum_{i0}^{n_\text{samples} - 1} (y_i - \hat{y}_i)^2.\] Here is a small example of usage of the mean_squared_error function: Examples: See Gradient Boosting regression for an example of mean squared error usage to evaluate gradient boosting regression. 3.3.4.5. Mean squared logarithmic error  The mean_squared_log_error function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss. If is the predicted value of the -th sample, and is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over is defined as \[\text{MSLE}(y, \hat{y})  \frac{1}{n_\text{samples}} \sum_{i0}^{n_\text{samples} - 1} (\log_e (1 + y_i) - \log_e (1 + \hat{y}_i) )^2.\] Where means the natural logarithm of . This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate. Here is a small example of usage of the mean_squared_log_error function: 3.3.4.6. Median absolute error  The median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction. If is the predicted value of the -th sample and is the corresponding true value, then the median absolute error (MedAE) estimated over is defined as \[\text{MedAE}(y, \hat{y})  \text{median}(\mid y_1 - \hat{y}_1 \mid, \ldots, \mid y_n - \hat{y}_n \mid).\] The median_absolute_error does not support multioutput. Here is a small example of usage of the median_absolute_error function: 3.3.4.7. R² score, the coefficient of determination  The r2_score function computes the coefficient of determination , usually denoted as R². It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. As such variance is dataset dependent, R² may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R² score of 0.0. If is the predicted value of the -th sample and is the corresponding true value for total samples, the estimated R² is defined as: \[R^2(y, \hat{y})  1 - \frac{\sum_{i1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i1}^{n} (y_i - \bar{y})^2}\] where and . Note that r2_score calculates unadjusted R² without correcting for bias in sample variance of y. Here is a small example of usage of the r2_score function: Example: See Lasso and Elastic Net for Sparse Signals for an example of R² score usage to evaluate Lasso and Elastic Net on sparse signals. 3.3.4.8. Mean Poisson, Gamma, and Tweedie deviances  The mean_tweedie_deviance function computes the mean Tweedie deviance error with a parameter ( ). This is a metric that elicits predicted expectation values of regression targets. Following special cases exist, when it is equivalent to mean_squared_error . when it is equivalent to mean_poisson_deviance . when it is equivalent to mean_gamma_deviance . If is the predicted value of the -th sample, and is the corresponding true value, then the mean Tweedie deviance error (D) for power , estimated over is defined as \[\begin{split}\text{D}(y, \hat{y})  \frac{1}{n_\text{samples}} \sum_{i0}^{n_\text{samples} - 1} \begin{cases} (y_i-\hat{y}_i)^2, & \text{for }p0\text{ (Normal)}\\ 2(y_i \log(y/\hat{y}_i) + \hat{y}_i - y_i), & \text{for}p1\text{ (Poisson)}\\ 2(\log(\hat{y}_i/y_i) + y_i/\hat{y}_i - 1), & \text{for}p2\text{ (Gamma)}\\ 2\left(\frac{\max(y_i,0)^{2-p}}{(1-p)(2-p)}- \frac{y\,\hat{y}^{1-p}_i}{1-p}+\frac{\hat{y}^{2-p}_i}{2-p}\right), & \text{otherwise} \end{cases}\end{split}\] Tweedie deviance is a homogeneous function of degree . Thus, Gamma distribution with means that simultaneously scaling and has no effect on the deviance. For Poisson distribution the deviance scales linearly, and for Normal distribution ( ), quadratically. In general, the higher the less weight is given to extreme deviations between true and predicted targets. For instance, let’s compare the two predictions 1.0 and 100 that are both 50% of their corresponding true value. The mean squared error ( ) is very sensitive to the prediction difference of the second point,: If we increase to 1,: the difference in errors decreases. Finally, by setting, : we would get identical errors. The deviance when is thus only sensitive to relative errors. 