sklearn_1_13_feature_selection
1.13. Feature selection
modules/feature_selection.html
 1.13.2. Univariate feature selection  Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the method: SelectKBest removes all but the highest scoring features SelectPercentile removes all but a user-specified highest scoring percentage of features using common univariate statistical tests for each feature: false positive rate SelectFpr , false discovery rate SelectFdr , or family wise error SelectFwe . GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator. For instance, we can perform a test to the samples to retrieve only the two best features as follows: These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile ): For regression: f_regression , mutual_info_regression For classification: chi2 , f_classif , mutual_info_classif The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation. Feature selection with sparse data If you use sparse data (i.e. data represented as sparse matrices), chi2 , mutual_info_regression , mutual_info_classif will deal with the data without making it dense. Warning Beware not to use a regression scoring function with a classification problem, you will get useless results. Examples: Univariate Feature Selection Comparison of F-test and mutual information 