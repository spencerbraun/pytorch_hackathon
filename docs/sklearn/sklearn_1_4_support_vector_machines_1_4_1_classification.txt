sklearn_1_4_support_vector_machines
1.4. Support Vector Machines
modules/svm.html
 1.4.1. Classification  SVC , NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset. SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation ). On the other hand, LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept parameter , as this is assumed to be linear. It also lacks some of the attributes of SVC and NuSVC , like . As other classifiers, SVC , NuSVC and LinearSVC take as input two arrays: an array of shape holding the training samples, and an array of class labels (strings or integers), of shape : After being fitted, the model can then be used to predict new values: SVMs decision function (detailed in the Mathematical formulation ) depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in attributes , and : Examples: SVM: Maximum margin separating hyperplane , Non-linear SVM SVM-Anova: SVM with univariate feature selection , 1.4.1.1. Multi-class classification  SVC and NuSVC implement the “one-versus-one” approach for multi-class classification. In total, classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the option allows to monotonically transform the results of the “one-versus-one” classifiers to a “one-vs-rest” decision function of shape . On the other hand, LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training models. See Mathematical formulation for a complete description of the decision function. Note that the LinearSVC also implements an alternative multi-class strategy, the so-called multi-class SVM formulated by Crammer and Singer 16 , by using the option . In practice, one-vs-rest classification is usually preferred, since the results are mostly similar, but the runtime is significantly less. For “one-vs-rest” LinearSVC the attributes and have the shape and respectively. Each row of the coefficients corresponds to one of the “one-vs-rest” classifiers and similar for the intercepts, in the order of the “one” class. In the case of “one-vs-one” SVC and NuSVC , the layout of the attributes is a little more involved. In the case of a linear kernel, the attributes and have the shape and respectively. This is similar to the layout for LinearSVC described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . . . “n-1 vs n”. The shape of is with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the “one-vs-one” classifiers. Each of the support vectors is used in classifiers. The entries in each row correspond to the dual coefficients for these classifiers. This might be clearer with an example: consider a three class problem with class 0 having three support vectors and class 1 and 2 having two support vectors and respectively. For each support vector , there are two dual coefficients. Let’s call the coefficient of support vector in the classifier between classes and . Then looks like this: Coefficients for SVs of class 0 Coefficients for SVs of class 1 Coefficients for SVs of class 2 Examples: Plot different SVM classifiers in the iris dataset , 1.4.1.2. Scores and probabilities  The method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option is set to , class membership probability estimates (from the methods and ) are enabled. In the binary case, the probabilities are calibrated using Platt scaling 9 : logistic regression on the SVM’s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per 10 . Note The same probability calibration procedure is available for all estimators via the CalibratedClassifierCV (see Probability calibration ). In the case of SVC and NuSVC , this procedure is builtin in libsvm which is used under the hood, so it does not rely on scikit-learn’s CalibratedClassifierCV . The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores: the “argmax” of the scores may not be the argmax of the probabilities in binary classification, a sample may be labeled by as belonging to the positive class even if the output of is less than 0.5; and similarly, it could be labeled as negative even if the output of is more than 0.5. Platt’s method is also known to have theoretical issues. If confidence scores are required, but these do not have to be probabilities, then it is advisable to set and use instead of . Please note that when and , unlike , the method does not try to break ties by default. You can set for the output of to be the same as , otherwise the first class among the tied classes will always be returned; but have in mind that it comes with a computational cost. See SVM Tie Breaking Example for an example on tie breaking. 1.4.1.3. Unbalanced problems  In problems where it is desired to give more importance to certain classes or certain individual samples, the parameters and can be used. SVC (but not NuSVC ) implements the parameter in the method. It’s a dictionary of the form , where value is a floating point number > 0 that sets the parameter of class to . The figure below illustrates the decision boundary of an unbalanced problem, with and without weight correction. SVC , NuSVC , SVR , NuSVR , LinearSVC , LinearSVR and OneClassSVM implement also weights for individual samples in the method through the parameter. Similar to , this sets the parameter for the i-th example to , which will encourage the classifier to get these samples right. The figure below illustrates the effect of sample weighting on the decision boundary. The size of the circles is proportional to the sample weights: Examples: SVM: Separating hyperplane for unbalanced classes SVM: Weighted samples , 