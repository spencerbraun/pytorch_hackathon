sklearn_4_1_partial_dependence_plots
4.1. Partial dependence plots
modules/partial_dependence.html
 4.1.2. Computation methods  There are two main methods to approximate the integral above, namely the ‘brute’ and ‘recursion’ methods. The parameter controls which method to use. The ‘brute’ method is a generic method that works with any estimator. It approximates the above integral by computing an average over the data : \[pd_{X_S}(x_S) \approx \frac{1}{n_\text{samples}} \sum_{i1}^n f(x_S, x_C^{(i)}),\] where is the value of the i-th sample for the features in . For each value of , this method requires a full pass over the dataset which is computationally intensive. The ‘recursion’ method is faster than the ‘brute’ method, but it is only supported by some tree-based estimators. It is computed as follows. For a given point , a weighted tree traversal is performed: if a split node involves a ‘target’ feature, the corresponding left or right branch is followed; otherwise both branches are followed, each branch being weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all the visited leaves values. With the ‘brute’ method, the parameter is used both for generating the grid of values and the complement feature values . However with the ‘recursion’ method, is only used for the grid values: implicitly, the values are those of the training data. By default, the ‘recursion’ method is used on tree-based estimators that support it, and ‘brute’ is used for the rest. Note While both methods should be close in general, they might differ in some specific settings. The ‘brute’ method assumes the existence of the data points . When the features are correlated, such artificial samples may have a very low probability mass. The ‘brute’ and ‘recursion’ methods will likely disagree regarding the value of the partial dependence, because they will treat these unlikely samples differently. Remember, however, that the primary assumption for interpreting PDPs is that the features should be independent. Footnotes 1 For classification, the target response may be the probability of a class (the positive class for binary classification), or the decision function. Examples: Partial Dependence Plots References T. Hastie, R. Tibshirani and J. Friedman, The Elements of Statistical Learning , Second Edition, Section 10.13.2, Springer, 2009. C. Molnar, Interpretable Machine Learning , Section 5.1, 2019. 