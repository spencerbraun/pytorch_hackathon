sklearn_1_7_gaussian_processes
1.7. Gaussian Processes
modules/gaussian_process.html
 1.7.4. GPC examples  1.7.4.1. Probabilistic predictions with GPC  This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML). While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC. The second figure shows the log-marginal-likelihood for different choices of the kernelâ€™s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots. 1.7.4.2. Illustration of GPC on the XOR dataset  This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel ( RBF ) and a non-stationary kernel ( DotProduct ). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as RBF often obtain better results. 1.7.4.3. Gaussian process classification (GPC) on iris dataset  This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions. 