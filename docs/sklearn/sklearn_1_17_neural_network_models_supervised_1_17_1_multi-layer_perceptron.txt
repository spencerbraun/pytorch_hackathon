sklearn_1_17_neural_network_models_supervised
1.17. Neural network models (supervised)
modules/neural_networks_supervised.html
 1.17.1. Multi-layer Perceptron  Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset, where is the number of dimensions for input and is the number of dimensions for output. Given a set of features and a target , it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output. Figure 1 : One hidden layer MLP.  The leftmost layer, known as the input layer, consists of a set of neurons representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation , followed by a non-linear activation function - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values. The module contains the public attributes and . is a list of weight matrices, where weight matrix at index represents the weights between layer and layer . is a list of bias vectors, where the vector at index represents the bias values added to layer . The advantages of Multi-layer Perceptron are: Capability to learn non-linear models. Capability to learn models in real-time (on-line learning) using . The disadvantages of Multi-layer Perceptron (MLP) include: MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy. MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations. MLP is sensitive to feature scaling. Please see Tips on Practical Use section that addresses some of these disadvantages. 