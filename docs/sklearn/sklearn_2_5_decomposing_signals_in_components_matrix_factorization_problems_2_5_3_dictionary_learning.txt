sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems
2.5. Decomposing signals in components (matrix factorization problems)
modules/decomposition.html
 2.5.3. Dictionary Learning  2.5.3.1. Sparse coding with a precomputed dictionary  The SparseCoder object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the initialization parameter: Orthogonal matching pursuit ( Orthogonal Matching Pursuit (OMP) ) Least-angle regression ( Least Angle Regression ) Lasso computed by least-angle regression Lasso using coordinate descent ( Lasso ) Thresholding Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction. The dictionary learning objects offer, via the parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading. The split code for a single sample has length and is constructed using the following rule: First, the regular code of length is computed. Then, the first entries of the are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative. Examples: Sparse coding with a precomputed dictionary 2.5.3.2. Generic dictionary learning  Dictionary learning ( DictionaryLearning ) is a matrix factorization problem that amounts to finding a (usually overcomplete) dictionary that will perform well at sparsely encoding the fitted data. Representing data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the mammalian primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for supervised recognition tasks. Dictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to multiple Lasso problems, considering the dictionary fixed, and then updating the dictionary to best fit the sparse code. \[\begin{split}(U^*, V^*)  \underset{U, V}{\operatorname{arg\,min\,}} & \frac{1}{2} ||X-UV||_2^2+\alpha||U||_1 \\ \text{subject to } & ||V_k||_2  1 \text{ for all } 0 \leq k < n_{\mathrm{atoms}}\end{split}\] After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see Sparse coding with a precomputed dictionary ). It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros. The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like. Examples: Image denoising using dictionary learning References: “Online dictionary learning for sparse coding” J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009 2.5.3.3. Mini-batch dictionary learning  MiniBatchDictionaryLearning implements a faster, but less accurate version of the dictionary learning algorithm that is better suited for large datasets. By default, MiniBatchDictionaryLearning divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition. The estimator also implements , which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory. Clustering for dictionary learning Note that when using dictionary learning to extract a representation (e.g. for sparse coding) clustering can be a good proxy to learn the dictionary. For instance the MiniBatchKMeans estimator is computationally efficient and implements on-line learning with a method. Example: Online learning of a dictionary of parts of faces 