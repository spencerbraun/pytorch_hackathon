sklearn_1_2_linear_and_quadratic_discriminant_analysis
1.2. Linear and Quadratic Discriminant Analysis
modules/lda_qda.html
 1.2.2. Mathematical formulation of the LDA and QDA classifiers  Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data for each class . Predictions can then be obtained by using Bayesâ€™ rule, for each training sample : \[P(yk | x)  \frac{P(x | yk) P(yk)}{P(x)}  \frac{P(x | yk) P(y  k)}{ \sum_{l} P(x | yl) \cdot P(yl)}\] and we select the class which maximizes this posterior probability. More specifically, for linear and quadratic discriminant analysis, is modeled as a multivariate Gaussian distribution with density: \[P(x | yk)  \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k)\right)\] where is the number of features. 1.2.2.1. QDA  According to the model above, the log of the posterior is: \[\begin{split}\log P(yk | x) & \log P(x | yk) + \log P(y  k) + Cst \\ & -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y  k) + Cst,\end{split}\] where the constant term corresponds to the denominator , in addition to other constant terms from the Gaussian. The predicted class is the one that maximises this log-posterior. Note Relation with Gaussian Naive Bayes If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier naive_bayes.GaussianNB . 1.2.2.2. LDA  LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: for all . This reduces the log posterior to: \[\log P(yk | x)  -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y  k) + Cst.\] The term corresponds to the Mahalanobis Distance between the sample and the mean . The Mahalanobis distance tells how close is from , while also accounting for the variance of each feature. We can thus interpret LDA as assigning to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. The log-posterior of LDA can also be written 3 as: \[\log P(yk | x)  \omega_k^t x + \omega_{k0} + Cst.\] where and . These quantities correspond to the and attributes, respectively. From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices of the Gaussians, leading to quadratic decision surfaces. See 1 for more details. 