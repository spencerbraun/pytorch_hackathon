caret_20_recursive_feature_elimination
20 Recursive Feature Elimination
recursive-feature-elimination.html
 20.4 An Example To test the algorithm, the “Friedman 1” benchmark (Friedman, 1991) was used. There are five informative variables generated by the equation In the simulation used here: Of the 50 predictors, there are 45 pure noise variables: 5 are uniform on \[0, 1\] and 40 are random univariate standard normals. The predictors are centered and scaled: The simulation will fit models with subset sizes of 25, 20, 15, 10, 5, 4, 3, 2, 1. As previously mentioned, to fit linear models, the lmFuncs set of functions can be used. To do this, a control object is created with the rfeControl function. We also specify that repeated 10-fold cross-validation should be used in line 2.1 of Algorithm 2. The number of folds can be changed via the number argument to rfeControl (defaults to 10). The verbose option prevents copious amounts of output from being produced. The output shows that the best subset size was estimated to be 4 predictors. This set includes informative variables but did not include them all. The predictors function can be used to get a text string of variable names that were picked in the final model. The lmProfile is a list of class "rfe" that contains an object fit that is the final linear model with the remaining terms. The model can be used to get predictions for future or test samples. There are also several plot methods to visualize the results. plot(lmProfile) produces the performance profile across different subset sizes, as shown in the figure below. Also the resampling results are stored in the sub-object lmProfile$resample and can be used with several lattice functions. Univariate lattice functions ( densityplot , histogram ) can be used to plot the resampling distribution while bivariate functions ( xyplot , stripplot ) can be used to plot the distributions for different subset sizes. In the latter case, the option returnResamp``  "all" in rfeControl can be used to save all the resampling results. Example images are shown below for the random forest model. 