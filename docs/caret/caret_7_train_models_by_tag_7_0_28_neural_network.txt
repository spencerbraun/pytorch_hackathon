caret_7_train_models_by_tag
7 train Models By Tag
train-models-by-tag.html
 7.0.28 Neural Network (back to contents ) Bayesian Regularized Neural Networks Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Extreme Learning Machine Type: Classification, Regression Tuning parameters: nhid (#Hidden Units) actfun (Activation Function) Required packages: elmNN Notes: The package is no longer on CRAN but can be installed from the archive at https://cran.r-project.org/src/contrib/Archive/elmNN/ Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Monotone Multi-Layer Perceptron Neural Network Type: Classification, Regression Tuning parameters: hidden1 (#Hidden Units) n.ensemble (#Models) Required packages: monmlp Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) Required packages: RSNNS Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, with multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) Required packages: RSNNS Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Dropout Type: Regression, Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) learning.rate (Learning Rate) momentum (Momentum) dropout (Dropout Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) dropout (Dropout Rate) beta1 (beta1) beta2 (beta2) learningrate (Learning Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Users are strongly advised to define num.round themselves. Neural Network Type: Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) Required packages: neuralnet Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Stacked AutoEncoder Deep Neural Network Type: Classification, Regression Tuning parameters: layer1 (Hidden Layer 1) layer2 (Hidden Layer 2) layer3 (Hidden Layer 3) hidden_dropout (Hidden Dropouts) visible_dropout (Visible Dropout) Required packages: deepnet 