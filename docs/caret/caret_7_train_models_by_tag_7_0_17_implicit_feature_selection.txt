caret_7_train_models_by_tag
7 train Models By Tag
train-models-by-tag.html
 7.0.17 Implicit Feature Selection (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Elasticnet Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Least Angle Regression Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression Type: Regression Tuning parameters: step (#Steps) Required packages: lars Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Spike and Slab Regression Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab , plyr Notes: Unlike other packages used by train , the spikeslab package is fully loaded when this model is used. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. The lasso Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf 