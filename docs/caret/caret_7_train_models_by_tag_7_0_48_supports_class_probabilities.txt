caret_7_train_models_by_tag
7 train Models By Tag
train-models-by-tag.html
 7.0.48 Supports Class Probabilities (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Adaptive Mixture Discriminant Analysis Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Model Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Generalized Additive Model using LOESS Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim k-Nearest Neighbors Type: Regression, Classification Tuning parameters: kmax (Max. #Neighbors) distance (Distance) kernel (Kernel) Required packages: kknn k-Nearest Neighbors Type: Classification, Regression Tuning parameters: k (#Neighbors) Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Localized Linear Discriminant Analysis Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Mixture Discriminant Analysis Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Model Averaged Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Monotone Multi-Layer Perceptron Neural Network Type: Classification, Regression Tuning parameters: hidden1 (#Hidden Units) n.ensemble (#Models) Required packages: monmlp Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) Required packages: RSNNS Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, with multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) Required packages: RSNNS Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Dropout Type: Regression, Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Naive Bayes Type: Classification Tuning parameters: laplace (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: naivebayes Naive Bayes Type: Classification Tuning parameters: fL (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: klaR Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) learning.rate (Learning Rate) momentum (Momentum) dropout (Dropout Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) dropout (Dropout Rate) beta1 (beta1) beta2 (beta2) learningrate (Learning Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Users are strongly advised to define num.round themselves. Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Non-Informative Model Type: Classification, Regression No tuning parameters for this model Notes: Since this model always predicts the same value, R-squared values will always be estimated to be NA. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Generalized Linear Models Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Notes: Unlike other packages used by train , the plsRglm package is fully loaded when this model is used. Patient Rule Induction Method Type: Classification Tuning parameters: peel.alpha (peeling quantile) paste.alpha (pasting quantile) mass.min (minimum mass) Required packages: supervisedPRIM Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Robust Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Self-Organizing Maps Type: Classification, Regression Tuning parameters: xdim (Rows) ydim (Columns) user.weights (Layer Weight) topo (Topology) Required packages: kohonen Notes: As of version 3.0.0 of the kohonen package, the argument user.weights replaces the old alpha parameter. user.weights is usually a vector of relative weights such as c(1, 3) but is parameterized here as a proportion such as c(1-.75, .75) where the .75 is the value of the tuning parameter passed to train and indicates that the outcome layer has 3 times the weight as the predictor layer. Semi-Naive Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Stabilized Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: ipred Stacked AutoEncoder Deep Neural Network Type: Classification, Regression Tuning parameters: layer1 (Hidden Layer 1) layer2 (Hidden Layer 2) layer3 (Hidden Layer 3) hidden_dropout (Hidden Dropouts) visible_dropout (Visible Dropout) Required packages: deepnet Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Tree Augmented Naive Bayes Classifier Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf 