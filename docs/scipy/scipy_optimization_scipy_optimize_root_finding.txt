scipy_optimization_scipy_optimize
Optimization (scipy.optimize)
optimize.html
 Root finding  Scalar functions  If one has a single-variable equation, there are multiple different root finding algorithms that can be tried. Most of these algorithms require the endpoints of an interval in which a root is expected (because the function changes signs). In general, brentq is the best choice, but the other methods may be useful in certain circumstances or for academic purposes. When a bracket is not available, but one or more derivatives are available, then newton (or , ) may be applicable. This is especially the case if the function is defined on a subset of the complex plane, and the bracketing methods cannot be used. Fixed-point solving  A problem closely related to finding the zeros of a function is the problem of finding a fixed point of a function. A fixed point of a function is the point at which evaluation of the function returns the point: Clearly, the fixed point of is the root of Equivalently, the root of is the fixed point of The routine fixed_point provides a simple iterative method using Aitkens sequence acceleration to estimate the fixed point of given a starting point. Sets of equations  Finding a root of a set of non-linear equations can be achieved using the root function. Several methods are available, amongst which (the default) and , which, respectively, use the hybrid method of Powell and the Levenberg-Marquardt method from MINPACK. The following example considers the single-variable transcendental equation \[x+2\cos\left(x\right)0,\] a root of which can be found as follows: Consider now a set of non-linear equations \begin{eqnarray*} x_{0}\cos\left(x_{1}\right) &  & 4,\\ x_{0}x_{1}-x_{1} &  & 5. \end{eqnarray*} We define the objective function so that it also returns the Jacobian and indicate this by setting the parameter to . Also, the Levenberg-Marquardt solver is used here. Root finding for large problems  Methods and in root cannot deal with a very large number of variables ( N ), as they need to calculate and invert a dense N x N Jacobian matrix on every Newton step. This becomes rather inefficient when N grows. Consider, for instance, the following problem: we need to solve the following integrodifferential equation on the square : \[(\partial_x^2 + \partial_y^2) P + 5 \left(\int_0^1\int_0^1\cosh(P)\,dx\,dy\right)^2  0\] with the boundary condition on the upper edge and elsewhere on the boundary of the square. This can be done by approximating the continuous function P by its values on a grid, , with a small grid spacing h . The derivatives and integrals can then be approximated; for instance . The problem is then equivalent to finding the root of some function , where is a vector of length . Now, because can be large, methods or in root will take a long time to solve this problem. The solution can, however, be found using one of the large-scale solvers, for example , , or . These use what is known as the inexact Newton method, which instead of computing the Jacobian matrix exactly, forms an approximation for it. The problem we have can now be solved as follows: Still too slow? Preconditioning.  When looking for the zero of the functions , i  1, 2, …, N , the solver spends most of the time inverting the Jacobian matrix, \[J_{ij}  \frac{\partial f_i}{\partial x_j} .\] If you have an approximation for the inverse matrix , you can use it for preconditioning the linear-inversion problem. The idea is that instead of solving one solves : since matrix is “closer” to the identity matrix than is, the equation should be easier for the Krylov method to deal with. The matrix M can be passed to root with method as an option . It can be a (sparse) matrix or a scipy.sparse.linalg.LinearOperator instance. For the problem in the previous section, we note that the function to solve consists of two parts: the first one is the application of the Laplace operator, , and the second is the integral. We can actually easily compute the Jacobian corresponding to the Laplace operator part: we know that in 1-D \[\begin{split}\partial_x^2 \approx \frac{1}{h_x^2} \begin{pmatrix} -2 & 1 & 0 & 0 \cdots \\ 1 & -2 & 1 & 0 \cdots \\ 0 & 1 & -2 & 1 \cdots \\ \ldots \end{pmatrix}  h_x^{-2} L\end{split}\] so that the whole 2-D operator is represented by \[J_1  \partial_x^2 + \partial_y^2 \simeq h_x^{-2} L \otimes I + h_y^{-2} I \otimes L\] The matrix of the Jacobian corresponding to the integral is more difficult to calculate, and since all of it entries are nonzero, it will be difficult to invert. on the other hand is a relatively simple matrix, and can be inverted by scipy.sparse.linalg.splu (or the inverse can be approximated by scipy.sparse.linalg.spilu ). So we are content to take and hope for the best. In the example below, we use the preconditioner . Resulting run, first without preconditioning: and then with preconditioning: Using a preconditioner reduced the number of evaluations of the function by a factor of 4 . For problems where the residual is expensive to compute, good preconditioning can be crucial — it can even decide whether the problem is solvable in practice or not. Preconditioning is an art, science, and industry. Here, we were lucky in making a simple choice that worked reasonably well, but there is a lot more depth to this topic than is shown here. 