scipy_linear_algebra_scipy_linalg
Linear Algebra (scipy.linalg)
linalg.html
 Basic routines  Finding the inverse  The inverse of a matrix is the matrix , such that , where is the identity matrix consisting of ones down the main diagonal. Usually, is denoted . In SciPy, the matrix inverse of the NumPy array, A, is obtained using linalg.inv , or using if is a Matrix. For example, let \[\begin{split}\mathbf{A}  \left[\begin{array}{ccc} 1 & 3 & 5\\ 2 & 5 & 1\\ 2 & 3 & 8\end{array}\right],\end{split}\] then \[\begin{split}\mathbf{A^{-1}}  \frac{1}{25} \left[\begin{array}{ccc} -37 & 9 & 22 \\ 14 & 2 & -9 \\ 4 & -3 & 1 \end{array}\right]  % \left[\begin{array}{ccc} -1.48 & 0.36 & 0.88 \\ 0.56 & 0.08 & -0.36 \\ 0.16 & -0.12 & 0.04 \end{array}\right].\end{split}\] The following example demonstrates this computation in SciPy Solving a linear system  Solving linear systems of equations is straightforward using the scipy command linalg.solve . This command expects an input matrix and a right-hand side vector. The solution vector is then computed. An option for entering a symmetric matrix is offered, which can speed up the processing when applicable. As an example, suppose it is desired to solve the following simultaneous equations: \begin{eqnarray*} x + 3y + 5z &  & 10 \\ 2x + 5y + z &  & 8 \\ 2x + 3y + 8z &  & 3 \end{eqnarray*} We could find the solution vector using a matrix inverse: \[\begin{split}\left[\begin{array}{c} x\\ y\\ z\end{array}\right]\left[\begin{array}{ccc} 1 & 3 & 5\\ 2 & 5 & 1\\ 2 & 3 & 8\end{array}\right]^{-1}\left[\begin{array}{c} 10\\ 8\\ 3\end{array}\right]\frac{1}{25}\left[\begin{array}{c} -232\\ 129\\ 19\end{array}\right]\left[\begin{array}{c} -9.28\\ 5.16\\ 0.76\end{array}\right].\end{split}\] However, it is better to use the linalg.solve command, which can be faster and more numerically stable. In this case, it, however, gives the same answer as shown in the following example: Finding the determinant  The determinant of a square matrix is often denoted and is a quantity often used in linear algebra. Suppose are the elements of the matrix and let be the determinant of the matrix left by removing the row and column from . Then, for any row \[\left|\mathbf{A}\right|\sum_{j}\left(-1\right)^{i+j}a_{ij}M_{ij}.\] This is a recursive way to define the determinant, where the base case is defined by accepting that the determinant of a matrix is the only matrix element. In SciPy the determinant can be calculated with linalg.det . For example, the determinant of \[\begin{split}\mathbf{A}\left[\begin{array}{ccc} 1 & 3 & 5\\ 2 & 5 & 1\\ 2 & 3 & 8\end{array}\right]\end{split}\] is \begin{eqnarray*} \left|\mathbf{A}\right| &  & 1\left|\begin{array}{cc} 5 & 1\\ 3 & 8\end{array}\right|-3\left|\begin{array}{cc} 2 & 1\\ 2 & 8\end{array}\right|+5\left|\begin{array}{cc} 2 & 5\\ 2 & 3\end{array}\right|\\ &  & 1\left(5\cdot8-3\cdot1\right)-3\left(2\cdot8-2\cdot1\right)+5\left(2\cdot3-2\cdot5\right)-25.\end{eqnarray*}. In SciPy, this is computed as shown in this example: Computing norms  Matrix and vector norms can also be computed with SciPy. A wide range of norm definitions are available using different parameters to the order argument of linalg.norm . This function takes a rank-1 (vectors) or a rank-2 (matrices) array and an optional order argument (default is 2). Based on these inputs, a vector or matrix norm of the requested order is computed. For vector x , the order parameter can be any real number including or . The computed norm is \[\begin{split}\left\Vert \mathbf{x}\right\Vert \left\{ \begin{array}{cc} \max\left|x_{i}\right| & \textrm{ord}\textrm{inf}\\ \min\left|x_{i}\right| & \textrm{ord}-\textrm{inf}\\ \left(\sum_{i}\left|x_{i}\right|^{\textrm{ord}}\right)^{1/\textrm{ord}} & \left|\textrm{ord}\right|<\infty.\end{array}\right.\end{split}\] For matrix , the only valid values for norm are inf, and ‘fro’ (or ‘f’) Thus, \[\begin{split}\left\Vert \mathbf{A}\right\Vert \left\{ \begin{array}{cc} \max_{i}\sum_{j}\left|a_{ij}\right| & \textrm{ord}\textrm{inf}\\ \min_{i}\sum_{j}\left|a_{ij}\right| & \textrm{ord}-\textrm{inf}\\ \max_{j}\sum_{i}\left|a_{ij}\right| & \textrm{ord}1\\ \min_{j}\sum_{i}\left|a_{ij}\right| & \textrm{ord}-1\\ \max\sigma_{i} & \textrm{ord}2\\ \min\sigma_{i} & \textrm{ord}-2\\ \sqrt{\textrm{trace}\left(\mathbf{A}^{H}\mathbf{A}\right)} & \textrm{ord}\textrm{'fro'}\end{array}\right.\end{split}\] where are the singular values of . Examples: Solving linear least-squares problems and pseudo-inverses  Linear least-squares problems occur in many branches of applied mathematics. In this problem, a set of linear scaling coefficients is sought that allows a model to fit the data. In particular, it is assumed that data is related to data through a set of coefficients and model functions via the model \[y_{i}\sum_{j}c_{j}f_{j}\left(\mathbf{x}_{i}\right)+\epsilon_{i},\] where represents uncertainty in the data. The strategy of least squares is to pick the coefficients to minimize \[J\left(\mathbf{c}\right)\sum_{i}\left|y_{i}-\sum_{j}c_{j}f_{j}\left(x_{i}\right)\right|^{2}.\] Theoretically, a global minimum will occur when \[\frac{\partial J}{\partial c_{n}^{*}}0\sum_{i}\left(y_{i}-\sum_{j}c_{j}f_{j}\left(x_{i}\right)\right)\left(-f_{n}^{*}\left(x_{i}\right)\right)\] or \begin{eqnarray*} \sum_{j}c_{j}\sum_{i}f_{j}\left(x_{i}\right)f_{n}^{*}\left(x_{i}\right) &  & \sum_{i}y_{i}f_{n}^{*}\left(x_{i}\right)\\ \mathbf{A}^{H}\mathbf{Ac} &  & \mathbf{A}^{H}\mathbf{y}\end{eqnarray*}, where \[\left\{ \mathbf{A}\right\} _{ij}f_{j}\left(x_{i}\right).\] When is invertible, then \[\mathbf{c}\left(\mathbf{A}^{H}\mathbf{A}\right)^{-1}\mathbf{A}^{H}\mathbf{y}\mathbf{A}^{\dagger}\mathbf{y},\] where is called the pseudo-inverse of Notice that using this definition of the model can be written \[\mathbf{y}\mathbf{Ac}+\boldsymbol{\epsilon}.\] The command linalg.lstsq will solve the linear least-squares problem for given and . In addition, linalg.pinv or linalg.pinv2 (uses a different method based on singular value decomposition) will find given The following example and figure demonstrate the use of linalg.lstsq and linalg.pinv for solving a data-fitting problem. The data shown below were generated using the model: \[y_{i}c_{1}e^{-x_{i}}+c_{2}x_{i},\] where for , , and Noise is added to and the coefficients and are estimated using linear least squares. Generalized inverse  The generalized inverse is calculated using the command linalg.pinv or linalg.pinv2 . These two commands differ in how they compute the generalized inverse. The first uses the linalg.lstsq algorithm, while the second uses singular value decomposition. Let be an matrix, then if , the generalized inverse is \[\mathbf{A}^{\dagger}\left(\mathbf{A}^{H}\mathbf{A}\right)^{-1}\mathbf{A}^{H},\] while if matrix, the generalized inverse is \[\mathbf{A}^{\#}\mathbf{A}^{H}\left(\mathbf{A}\mathbf{A}^{H}\right)^{-1}.\] In the case that , then \[\mathbf{A}^{\dagger}\mathbf{A}^{\#}\mathbf{A}^{-1},\] as long as is invertible. 