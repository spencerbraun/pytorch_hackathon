scipy_optimization_scipy_optimize
Optimization (scipy.optimize)
optimize.html
 Constrained minimization of multivariate scalar functions ( minimize )  The minimize function provides algorithms for constrained minimization, namely , and . They require the constraints to be defined using slightly different structures. The method requires the constraints to be defined as a sequence of objects LinearConstraint and NonlinearConstraint . Methods and , on the other hand, require constraints to be defined as a sequence of dictionaries, with keys , and . As an example let us consider the constrained minimization of the Rosenbrock function: \begin{eqnarray*} \min_{x_0, x_1} & ~~100\left(x_{1}-x_{0}^{2}\right)^{2}+\left(1-x_{0}\right)^{2} &\\ \text{subject to: } & x_0 + 2 x_1 \leq 1 & \\ & x_0^2 + x_1 \leq 1 & \\ & x_0^2 - x_1 \leq 1 & \\ & 2 x_0 + x_1  1 & \\ & 0 \leq x_0 \leq 1 & \\ & -0.5 \leq x_1 \leq 2.0. & \end{eqnarray*} This optimization problem has the unique solution , for which only the first and fourth constraints are active. Trust-Region Constrained Algorithm ( )  The trust-region constrained method deals with constrained minimization problems of the form: \begin{eqnarray*} \min_x & f(x) & \\ \text{subject to: } & ~~~ c^l \leq c(x) \leq c^u, &\\ & x^l \leq x \leq x^u. & \end{eqnarray*} When the method reads the -th constraint as an equality constraint and deals with it accordingly. Besides that, one-sided constraint can be specified by setting the upper or lower bound to with the appropriate sign. The implementation is based on [EQSQP] for equality-constraint problems and on [TRIP] for problems with inequality constraints. Both are trust-region type algorithms suitable for large-scale problems. Defining Bounds Constraints:  The bound constraints and are defined using a Bounds object. Defining Linear Constraints:  The constraints and can be written in the linear constraint standard format: \begin{equation*} \begin{bmatrix}-\infty \\1\end{bmatrix} \leq \begin{bmatrix} 1& 2 \\ 2& 1\end{bmatrix} \begin{bmatrix} x_0 \\x_1\end{bmatrix} \leq \begin{bmatrix} 1 \\ 1\end{bmatrix},\end{equation*} and defined using a LinearConstraint object. Defining Nonlinear Constraints:  The nonlinear constraint: \begin{equation*} c(x)  \begin{bmatrix} x_0^2 + x_1 \\ x_0^2 - x_1\end{bmatrix} \leq \begin{bmatrix} 1 \\ 1\end{bmatrix}, \end{equation*} with Jacobian matrix: \begin{equation*} J(x)  \begin{bmatrix} 2x_0 & 1 \\ 2x_0 & -1\end{bmatrix},\end{equation*} and linear combination of the Hessians: \begin{equation*} H(x, v)  \sum_{i0}^1 v_i \nabla^2 c_i(x)  v_0\begin{bmatrix} 2 & 0 \\ 0 & 0\end{bmatrix} + v_1\begin{bmatrix} 2 & 0 \\ 0 & 0\end{bmatrix}, \end{equation*} is defined using a NonlinearConstraint object. Alternatively, it is also possible to define the Hessian as a sparse matrix, or as a LinearOperator object. When the evaluation of the Hessian is difficult to implement or computationally infeasible, one may use HessianUpdateStrategy . Currently available strategies are BFGS and SR1 . Alternatively, the Hessian may be approximated using finite differences. The Jacobian of the constraints can be approximated by finite differences as well. In this case, however, the Hessian cannot be computed with finite differences and needs to be provided by the user or defined using HessianUpdateStrategy . Solving the Optimization Problem:  The optimization problem is solved using: When needed, the objective function Hessian can be defined using a LinearOperator object, or a Hessian-vector product through the parameter . Alternatively, the first and second derivatives of the objective function can be approximated. For instance, the Hessian can be approximated with SR1 quasi-Newton approximation and the gradient with finite differences. TRIP Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999. An interior point algorithm for large-scale nonlinear programming. SIAM Journal on Optimization 9.4: 877-900. EQSQP Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the implementation of an algorithm for large-scale equality constrained optimization. SIAM Journal on Optimization 8.3: 682-706. Sequential Least SQuares Programming (SLSQP) Algorithm ( )  The SLSQP method deals with constrained minimization problems of the form: \begin{eqnarray*} \min_x & f(x) \\ \text{subject to: } & c_j(x)  0 , &j \in \mathcal{E}\\ & c_j(x) \geq 0 , &j \in \mathcal{I}\\ & \text{lb}_i \leq x_i \leq \text{ub}_i , &i  1,...,N. \end{eqnarray*} Where or are sets of indices containing equality and inequality constraints. Both linear and nonlinear constraints are defined as dictionaries with keys , and . And the optimization problem is solved with: Most of the options available for the method are not available for . 