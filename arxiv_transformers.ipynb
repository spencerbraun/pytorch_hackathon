{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    with open('arxiv/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cats = {\n",
    "'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "'cs.AI': 'Artificial Intelligence',\n",
    "'cs.CC': 'Computational Complexity',\n",
    "'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "'cs.CL': 'Computation and Language',\n",
    "'cs.CR': 'Cryptography and Security',\n",
    "'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "'cs.DB': 'Databases',\n",
    "'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "'cs.DL': 'Digital Libraries',\n",
    "'cs.DM': 'Discrete Mathematics',\n",
    "'cs.DS': 'Data Structures and Algorithms',\n",
    "'cs.ET': 'Emerging Technologies',\n",
    "'cs.FL': 'Formal Languages and Automata Theory',\n",
    "'cs.GL': 'General Literature',\n",
    "'cs.GR': 'Graphics',\n",
    "'cs.GT': 'Computer Science and Game Theory',\n",
    "'cs.IR': 'Information Retrieval',\n",
    "'cs.IT': 'Information Theory',\n",
    "'cs.LG': 'Machine Learning',\n",
    "'cs.LO': 'Logic in Computer Science',\n",
    "'cs.MA': 'Multiagent Systems',\n",
    "'cs.MM': 'Multimedia',\n",
    "'cs.MS': 'Mathematical Software',\n",
    "'cs.NA': 'Numerical Analysis',\n",
    "'cs.NE': 'Neural and Evolutionary Computing',\n",
    "'cs.NI': 'Networking and Internet Architecture',\n",
    "'cs.OH': 'Other Computer Science',\n",
    "'cs.OS': 'Operating Systems',\n",
    "'cs.PF': 'Performance',\n",
    "'cs.PL': 'Programming Languages',\n",
    "'cs.RO': 'Robotics',\n",
    "'cs.SC': 'Symbolic Computation',\n",
    "'cs.SD': 'Sound',\n",
    "'cs.SE': 'Software Engineering',\n",
    "'cs.SI': 'Social and Information Networks',\n",
    "'cs.SY': 'Systems and Control',\n",
    "'econ.EM': 'Econometrics',\n",
    "'eess.AS': 'Audio and Speech Processing',\n",
    "'eess.IV': 'Image and Video Processing',\n",
    "'eess.SP': 'Signal Processing',\n",
    "'math.AC': 'Commutative Algebra',\n",
    "'math.AG': 'Algebraic Geometry',\n",
    "'math.AP': 'Analysis of PDEs',\n",
    "'math.AT': 'Algebraic Topology',\n",
    "'math.CA': 'Classical Analysis and ODEs',\n",
    "'math.CO': 'Combinatorics',\n",
    "'math.CT': 'Category Theory',\n",
    "'math.CV': 'Complex Variables',\n",
    "'math.DG': 'Differential Geometry',\n",
    "'math.DS': 'Dynamical Systems',\n",
    "'math.FA': 'Functional Analysis',\n",
    "'math.GM': 'General Mathematics',\n",
    "'math.GN': 'General Topology',\n",
    "'math.GR': 'Group Theory',\n",
    "'math.GT': 'Geometric Topology',\n",
    "'math.HO': 'History and Overview',\n",
    "'math.IT': 'Information Theory',\n",
    "'math.KT': 'K-Theory and Homology',\n",
    "'math.LO': 'Logic',\n",
    "'math.MG': 'Metric Geometry',\n",
    "'math.MP': 'Mathematical Physics',\n",
    "'math.NA': 'Numerical Analysis',\n",
    "'math.NT': 'Number Theory',\n",
    "'math.OA': 'Operator Algebras',\n",
    "'math.OC': 'Optimization and Control',\n",
    "'math.PR': 'Probability',\n",
    "'math.QA': 'Quantum Algebra',\n",
    "'math.RA': 'Rings and Algebras',\n",
    "'math.RT': 'Representation Theory',\n",
    "'math.SG': 'Symplectic Geometry',\n",
    "'math.SP': 'Spectral Theory',\n",
    "'math.ST': 'Statistics Theory',\n",
    "'math-ph': 'Mathematical Physics',\n",
    "'q-fin.CP': 'Computational Finance',\n",
    "'q-fin.MF': 'Mathematical Finance',\n",
    "'q-fin.RM': 'Risk Management',\n",
    "'q-fin.ST': 'Statistical Finance',\n",
    "'q-fin.TR': 'Trading and Market Microstructure',\n",
    "'stat.AP': 'Applications',\n",
    "'stat.CO': 'Computation',\n",
    "'stat.ME': 'Methodology',\n",
    "'stat.ML': 'Machine Learning',\n",
    "'stat.OT': 'Other Statistics',\n",
    "'stat.TH': 'Statistics Theory'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0704.0001', 'submitter': 'Pavel Nadolsky', 'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\", 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies', 'comments': '37 pages, 15 figures; published version', 'journal-ref': 'Phys.Rev.D76:013009,2007', 'doi': '10.1103/PhysRevD.76.013009', 'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n', 'report-no': 'ANL-HEP-PR-07-12', 'categories': ['hep-ph'], 'versions': ['v1', 'v2']}\n"
     ]
    }
   ],
   "source": [
    "metadata = get_metadata()\n",
    "i = 0\n",
    "with open('arxiv/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for line in f:\n",
    "            json_line = json.loads(line)\n",
    "            print(json_line)\n",
    "            break\n",
    "# for paper in metadata:\n",
    "#     if i % 343 == 0:\n",
    "    \n",
    "#         for k, v in json.loads(paper).items():\n",
    "#             print(f'{k}: {v}')\n",
    "#         break\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['math.NT', 'math.AG']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "j = 0\n",
    "first_thousand = []\n",
    "relevant_list = list(relevant_cats.keys())\n",
    "with open('arxiv/ml_papers_snapshot_2.json', 'w') as write_file:\n",
    "    with open('arxiv/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for line in f:\n",
    "            json_paper = json.loads(line)\n",
    "            categories = json_paper['categories'][0].split(\" \")\n",
    "\n",
    "            if not any([x in relevant_list for x in categories]):\n",
    "                continue\n",
    "            try:\n",
    "                date = re.search(\"^(\\d+)\\..*\", json_paper['id']).group(1)\n",
    "            except:\n",
    "                date = None\n",
    "\n",
    "            if int(date) < 1500:\n",
    "                continue\n",
    "\n",
    "            json_paper['categories'] = categories\n",
    "            json_paper['date'] = int(date)\n",
    "            json_paper['year'] = int(f'20{date[:2]}')\n",
    "\n",
    "            json.dump(json_paper, write_file)\n",
    "            write_file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "            i += 1\n",
    "            if i == 100000:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "papers = []\n",
    "header = '\"id\"|year\"|\"tags\"|\"text\"\\n'\n",
    "with open('arxiv/arxiv_data.csv', 'w') as writefile:\n",
    "    writefile.write(header)\n",
    "    \n",
    "    with open('arxiv/ml_papers_snapshot_2.json') as f:\n",
    "        for line in f:\n",
    "            json_paper = json.loads(line)\n",
    "            id_ = json_paper['id']\n",
    "            categories = json_paper['categories']\n",
    "            year = json_paper['year']\n",
    "            abstract = json_paper['abstract']\n",
    "            \n",
    "            write_line = f'\"{id_}\"|\"{year}\"|\"{categories}\"|\"{abstract}\"'\n",
    "            writefile.write(write_line)\n",
    "            writefile.write(\"\\n\")\n",
    "            \n",
    "\n",
    "            i += 1\n",
    "            if i % 10000 == 0:\n",
    "                print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1611.04432',\n",
       " 'submitter': '- Departement Mathematiques Orsay',\n",
       " 'authors': 'Jean-Pierre Kahane (LMO)',\n",
       " 'title': \"Conditions for Beurling's integers to have a density\",\n",
       " 'comments': 'in French',\n",
       " 'journal-ref': None,\n",
       " 'doi': None,\n",
       " 'abstract': \"  In 1997 H.G.Diamond gave a condition on Beurling's generalized prime numbers\\nin order that the corresponding generalized integers have a density. We give a\\nnew proof of this condition (Theorem 1) and a proof that it is not necessary\\n(Theorem 2 and Examples). However, it is very near to be necessary (Theorem 3).\\nBoth proofs of Theorems 1 and 2 rely on Fourier analysis, mainly the Wiener\\nalgebra, and partly on probability methods.\\n\",\n",
       " 'report-no': None,\n",
       " 'categories': ['math.NT'],\n",
       " 'versions': ['v1'],\n",
       " 'date': 1611,\n",
       " 'year': 2016}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'year\"', 'tags', 'text']\n",
      "['1501.00001', '2015', \"['cs.IT', 'cs.NI', 'math.IT']\", '  Distinction of OFDM signals from single carrier signals is highly important\\nfor adaptive receiver algorithms and signal identification applications. OFDM\\nsignals exhibit Gaussian characteristics in time domain and fourth order\\ncumulants of Gaussian distributed signals vanish in contrary to the cumulants\\nof other signals. Thus fourth order cumulants can be utilized for OFDM signal\\nidentification. In this paper, first, formulations of the estimates of the\\nfourth order cumulants for OFDM signals are provided. Then it is shown these\\nestimates are affected significantly from the wireless channel impairments,\\nfrequency offset, phase offset and sampling mismatch. To overcome these\\nproblems, a general chi-square constant false alarm rate Gaussianity test which\\nemploys estimates of cumulants and their covariances is adapted to the specific\\ncase of wireless OFDM signals. Estimation of the covariance matrix of the\\nfourth order cumulants are greatly simplified peculiar to the OFDM signals. A\\nmeasurement setup is developed to analyze the performance of the identification\\nmethod and for comparison purposes. A parametric measurement analysis is\\nprovided depending on modulation order, signal to noise ratio, number of\\nsymbols, and degree of freedom of the underlying test. The proposed method\\noutperforms statistical tests which are based on fixed thresholds or empirical\\nvalues, while a priori information requirement and complexity of the proposed\\nmethod are lower than the coherent identification techniques.\\n']\n",
      "['1501.00008', '2015', \"['quant-ph', 'cs.DS']\", '  This article reviews the 2008 quantum algorithm for linear systems of\\nequations due to Harrow, Hassidim and Lloyd, as well as some of the followup\\nand related work. It was submitted to the Springer Encyclopedia of Algorithms.\\n']\n",
      "['1501.00010', '2015', \"['math.DS']\", '  We consider random perturbations of non-uniformly expanding maps, possibly\\nhaving a non-degenerate critical set. We prove that, if the Lebesgue measure of\\nthe set of points failing the non-uniform expansion or the slow recurrence to\\nthe critical set at a certain time, for almost all random orbits, decays in a\\n(stretched) exponential fashion, then the decay of correlations along random\\norbits is stretched exponential, up to some waiting time. As applications, we\\nobtain almost sure stretched exponential decay of random correlations for Viana\\nmaps, as for a class of non-uniformly expanding local diffeomorphisms and a\\nquadratic family of interval maps.\\n']\n",
      "['1501.00011', '2015', \"['quant-ph', 'cs.CC']\", \"  Quantum computing is a good way to justify difficult physics experiments. But\\nuntil quantum computers are built, do computer scientists need to know anything\\nabout quantum information? In fact, quantum computing is not merely a recipe\\nfor new computing devices, but a new way of looking at the world that has been\\nastonishingly intellectually productive. In this article, I'll talk about where\\nquantum computing came from, what it is, and what we can learn from it.\\n\"]\n",
      "['1501.00014', '2015', \"['cs.DS', 'math.OC']\", '  Given real numbers whose sum is an integer, we study the problem of finding\\nintegers which match these real numbers as closely as possible, in the sense of\\nL^p norm, while preserving the sum. We describe the structure of solutions for\\nthis integer optimization problem and propose an algorithm with complexity O(N\\nlog N) for solving it. In contrast to fractional rounding and randomized\\nrounding, which yield biased estimators of the solution when applied to this\\nproblem, our method yields an exact solution which minimizes the relative\\nrounding error across the set of all solutions for any value of p greater than\\n1, while avoiding the complexity of exhaustive search. The proposed algorithm\\nalso solves a class of integer optimization problems with integer constraints\\nand may be used as the rounding step of relaxed integer programming problems,\\nfor rounding real-valued solutions. We give several examples of applications\\nfor the proposed algorithm.\\n']\n",
      "['1501.00019', '2015', \"['q-bio.PE', 'q-bio.QM', 'stat.AP']\", '  This article shows how 3D geometric morphometric data can be analyzed using\\nnewly developed distance-based evolutionary tree inference methods, with\\nextensions to planar graphs. Application of these methods to 3D representations\\nof the skullcap (calvaria) of 13 diverse skulls in the genus Homo, ranging from\\nHomo erectus (ergaster) at about 1.6 mya, all the way forward to modern humans,\\nyields a remarkably clear phylogenetic tree. Various evolutionary hypotheses\\nare tested. Results of these tests include rejection of the monophyly of Homo\\nheidelbergensis, the Multi-Regional hypothesis, and the hypothesis that the\\nunusual 12,000 year old (12kya) Iwo Eleru skull represents a modern human.\\nRather, by quantitative phylogenetic analyses the latter is seen to be an old\\n(200-400kya) lineage that probably represents a novel African species, Homo\\niwoelerueensis. It diverged after the lineage leading to Neanderthals, and may\\nhave been driven to extinction in the last 10kya by modern humans, Homo\\nsapiens, another African species of Homo that appeared about 100kya. Another\\nenigmatic skull, Qafzeh 6 from the Middle East about 90kya, appears to be a\\nhybrid of two thirds near, but not, anatomically modern human and one third of\\nan archaic lineage diverging close to classic European Neanderthals. Overall,\\nthe tree clearly implies an accelerating rate of skullcap shape change, and by\\nextension, change of the underlying brain, over the last 400kya in Africa. This\\nacceleration may have extended right up to the origin of modern humans. Methods\\nof distance-based evolutionary tree inference are refined and extended, with\\nparticular attention to diagnosing the model and achieving a better fit. This\\nincludes power transformations of the input data which favor root Procrustes\\ndistances.\\n']\n",
      "['1501.00023', '2015', \"['math.RA']\", '  We introduce graded gamma rings from a more general point of view via methods\\ndeveloped by Krasner and Halberstadt for graded rings. We propose three\\nequivalent aspects of studying graded gamma rings, nonhomogeneous,\\nsemihomogeneous and homogeneous. The graded Jacobson radical of a graded gamma\\nring is introduced and its elementwise description is given. Also, a relation\\nbetween the graded Jacobson radical and the Jacobson radical of a graded gamma\\nring is examined.\\n']\n",
      "['1501.00026', '2015', \"['q-fin.PM', 'math.OC']\", '  We investigate the impact of capital gains taxes on optimal investment\\ndecisions in a quite simple model. Namely, we consider a risk neutral investor\\nwho owns one risky stock from which she assumes that it has a lower expected\\nreturn than the riskless bank account and determine the optimal stopping time\\nat which she sells the stock to invest the proceeds in the bank account up to\\nthe maturity date. In the case of linear taxes and a positive riskless interest\\nrate, the problem is nontrivial because at the selling time the investor has to\\nrealize book profits which triggers tax payments. We derive a boundary that is\\ncontinuous and increasing in time and decreasing in the volatility of the stock\\nsuch that the investor sells the stock at the first time its price is smaller\\nor equal to this boundary.\\n']\n",
      "['1501.00027', '2015', \"['cs.OH', 'quant-ph']\", '  The most powerful form of quantum learning system possible would somehow\\nlearn the parameters W of a quantum system f(X, W), for f representing the\\nlargest, most powerful set of possible input-output relations. This paper\\naddresses the issue of how to enlarge the set represented by f, by using a new\\nformulation of time-symmetric physics to model analog quantum computers based\\non spin and by exploring possible sources of backwards-time free energy so as\\nto address problems of decoherence and dissipation.\\n']\n",
      "['1501.00028', '2015', \"['math.GT']\", '  We define an elementary relatively $\\\\mathbb Z/4$ graded Lagrangian-Floer\\nchain complex for restricted immersions of compact 1-manifolds into the\\npillowcase, and apply it to the intersection diagram obtained by taking\\ntraceless $SU(2)$ character varieties of 2-tangle decompositions of knots.\\nCalculations for torus knots are explained in terms of pictures in the\\npunctured plane. The relation to the reduced instanton homology of knots is\\nexplored.\\n']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('arxiv/arxiv_data.csv', newline='') as f:\n",
    "    reader = csv.reader(f, delimiter='|', quoting=csv.QUOTE_ALL)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x133d2af40>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 100, 100, 102]\n",
      "['[CLS]', '[UNK]', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "['introduction to algorithms', 'the elements of statistical learning']\n",
    "test_token = tokenizer.encode(['introduction to algorithms', 'the elements of statistical learning'])\n",
    "\n",
    "print(test_token)\n",
    "print(tokenizer.convert_ids_to_tokens(test_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  4955,  2000, 13792,   102])\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.LongTensor(test_token)\n",
    "print(test_tensor)\n",
    "\n",
    "\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = model.to(device)\n",
    "test_tensor = test_tensor.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([1, 5])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'tuple'>\n",
      "3\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(test_tensor.size())\n",
    "test_tensor = test_tensor.unsqueeze(0)\n",
    "print(test_tensor.size())\n",
    "\n",
    "print(type(test_tensor))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=test_tensor)\n",
    "\n",
    "\n",
    "print(type(out))\n",
    "print(len(out))\n",
    "\n",
    "hidden_states = out[2]\n",
    "print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.6710e-01,  2.6857e-02, -7.1127e-01, -1.6682e-01,  7.5849e-02,\n",
      "         1.0340e-01,  1.4030e-01,  1.9636e-01, -2.6274e-02, -6.5242e-02,\n",
      "         6.3598e-02, -2.1960e-01, -3.4613e-01, -3.5831e-01, -3.7734e-01,\n",
      "         5.8638e-03, -4.2269e-02,  3.9169e-02, -2.6189e-01,  2.9446e-01,\n",
      "         2.4032e-01,  5.2831e-01, -1.1056e-01,  4.6630e-01,  1.5466e-01,\n",
      "         3.0498e-02,  3.6937e-02,  3.4433e-01, -3.4441e-01,  1.5501e-01,\n",
      "         6.6546e-02,  4.1077e-01, -2.1893e-01, -9.6625e-02, -2.3164e-01,\n",
      "         2.1648e-01,  3.1286e-01,  6.0638e-02, -5.5950e-01,  5.1150e-01,\n",
      "        -5.9491e-01,  3.4276e-01,  4.0101e-01,  2.5867e-01, -1.4939e-02,\n",
      "        -7.0332e-01, -3.8815e-01,  5.0209e-02, -1.6225e-01, -2.3893e-01,\n",
      "        -2.4715e-01,  1.1856e-01,  5.7324e-01,  6.2366e-02,  2.1231e-01,\n",
      "         6.6461e-01,  1.0568e-01, -6.1718e-01,  1.7590e-01, -2.6281e-01,\n",
      "         9.9906e-02,  2.4703e-01,  9.7846e-02, -2.5034e-01,  6.3563e-01,\n",
      "         4.3127e-01, -1.2051e-01,  5.2409e-01, -4.8432e-01, -8.6533e-02,\n",
      "        -2.6314e-01, -4.3678e-01, -2.5229e-02,  3.7458e-01, -1.3921e-01,\n",
      "        -3.0722e-01,  4.0942e-01,  4.3337e-01, -2.3970e-01, -4.4732e-01,\n",
      "        -2.4108e-01,  6.4383e-01, -1.0837e-01,  5.6350e-01,  3.1378e-01,\n",
      "         3.9528e-01, -4.2919e-01,  4.1737e-01, -5.0726e-01, -8.1680e-02,\n",
      "        -1.3553e-01, -8.3050e-02, -1.2449e-01,  3.3090e-01,  6.3283e-01,\n",
      "         1.0989e-01,  2.6903e-02,  1.8133e-01, -1.1469e-01,  3.6181e-01,\n",
      "        -1.5015e-01, -1.3531e-02,  5.6845e-02,  8.6671e-02, -1.3635e-03,\n",
      "        -3.6063e-01, -1.0663e-01,  3.3299e-02,  5.0733e-01, -5.0139e-01,\n",
      "         5.7634e-02,  6.1643e-02,  5.3341e-01, -5.4542e-01, -1.6321e-01,\n",
      "         5.9277e-01,  2.8701e-01, -7.8639e-03, -1.2927e-01, -1.2228e-01,\n",
      "        -2.6964e-01, -1.4616e-01, -5.9818e-02,  1.0683e+00, -5.4881e-01,\n",
      "         1.1298e-01, -1.3960e-01,  1.9392e-01,  3.2579e-01, -7.6844e-02,\n",
      "        -3.3530e-01,  9.7257e-01,  4.1887e-01, -1.0263e-01, -4.4621e-01,\n",
      "         4.5991e-01, -2.4025e-01, -7.6025e-02, -8.6094e-01,  1.4811e-01,\n",
      "        -1.2633e-01,  1.4222e-01, -1.2049e-01, -2.2278e-01,  3.6273e-01,\n",
      "        -1.9946e-01,  9.0398e-02, -2.6241e-01,  1.0478e-01, -7.1563e-02,\n",
      "         4.3928e-01, -1.5005e-01, -8.1426e-01,  8.5820e-02, -2.4355e-01,\n",
      "         1.4176e-01,  1.6391e-01,  1.9638e-01, -8.2770e-02,  4.7697e-01,\n",
      "         1.5855e-01, -4.4609e-01, -6.4368e-02,  4.3365e-01, -8.4577e-02,\n",
      "        -2.7646e-01,  3.2169e-01,  1.6662e-01,  4.3703e-01,  4.1551e-01,\n",
      "        -4.5755e-01, -5.1508e-01,  5.4699e-01, -2.8152e-01,  3.2731e-01,\n",
      "         1.1368e-02,  3.1295e-01,  1.5037e-01,  3.8471e-01,  2.8665e-01,\n",
      "        -2.2609e+00,  2.1108e-01,  3.2467e-01, -2.9613e-01,  2.0513e-01,\n",
      "        -3.8575e-02,  3.4304e-01, -4.8628e-01,  1.0742e-01, -2.0891e-01,\n",
      "        -4.4045e-01,  6.1258e-02, -5.6603e-01,  1.3679e-01,  7.8850e-01,\n",
      "         3.1366e-01,  7.1830e-02, -1.7131e-01, -3.8048e-01, -1.2563e-01,\n",
      "        -2.1053e-01, -3.1072e-01,  1.8692e-01,  2.3812e-01, -3.7925e-01,\n",
      "         4.7907e-01, -3.4102e-01, -2.5282e-01, -1.4678e-01,  5.1067e-01,\n",
      "        -5.2860e-01,  4.6526e-01,  1.4277e-01, -3.2917e-01,  3.9287e-01,\n",
      "         5.7453e-02, -1.2824e-01, -6.4874e-02, -8.5275e-03, -4.4673e-01,\n",
      "         5.3379e-01,  3.6186e-02, -6.8390e-01,  4.9985e-01,  3.2558e-01,\n",
      "         3.8788e-01,  1.0094e+00,  2.2659e-01,  4.1005e-01, -2.0237e-02,\n",
      "         1.4103e-01, -5.0491e-01,  6.2917e-01,  3.8810e-03, -2.2138e-01,\n",
      "         1.0976e-01, -4.5478e-01, -1.8490e-01, -5.4460e-02, -4.9893e-01,\n",
      "         9.4258e-03, -1.7916e-01,  2.4021e-01,  3.2836e-01,  1.3853e-01,\n",
      "        -3.9565e-01, -2.9594e-01,  3.3735e-01,  9.2996e-03, -4.8266e-01,\n",
      "        -3.3225e-01, -3.2071e-01,  4.5711e-01, -6.1959e-01, -1.4579e-01,\n",
      "         7.4131e-02, -2.8845e-01,  1.3344e-01,  6.2045e-01,  5.9225e-01,\n",
      "         5.6229e-02,  1.3310e-02,  4.0563e-01, -2.0933e-01, -1.4288e-01,\n",
      "        -4.0086e-01, -2.4540e-01, -4.0071e-01,  1.8685e-01, -1.1200e-01,\n",
      "        -2.5337e-01, -4.0437e-01, -5.4879e-02, -6.2546e-01, -4.5475e-01,\n",
      "        -1.0318e+00,  4.2085e-01, -9.2459e-02, -3.8899e-01,  3.7598e-01,\n",
      "        -1.2934e-01,  5.9222e-01, -6.0684e-01,  2.5983e-01, -3.0893e-01,\n",
      "        -1.4986e-01, -3.8027e-01, -1.4489e-01,  3.5355e-01, -1.2646e-01,\n",
      "        -4.0283e-01,  4.6941e-02, -7.0173e-01, -1.9382e-01,  3.4839e-01,\n",
      "         2.1868e-01,  4.1372e-01, -1.1681e-01, -2.3398e-01, -4.0253e-01,\n",
      "        -3.1835e-01, -7.8106e-02, -3.0308e-01,  5.6470e-01, -4.0697e-01,\n",
      "         1.0325e-01,  1.3936e-02,  1.1815e-01, -1.9811e+00,  3.0746e-03,\n",
      "         1.5891e-03, -7.4578e-02, -6.5033e-02,  8.8748e-02,  2.6980e-01,\n",
      "        -4.2394e-01, -7.8262e-01, -2.2717e-01, -3.1943e-01, -2.0856e-01,\n",
      "        -4.3775e-01,  7.3006e-01,  2.0289e-01, -1.3320e-02, -4.1034e-02,\n",
      "        -7.3308e-01, -1.1557e-01,  5.7122e-01,  2.3351e-01, -2.7583e-01,\n",
      "        -3.5087e-01, -2.3247e-01,  2.3277e-01,  5.4181e-01, -2.5864e-01,\n",
      "         2.4144e-01, -3.1469e-01, -2.9384e-02,  8.7050e-02, -5.9166e-01,\n",
      "        -1.9101e-01, -3.8043e-02, -8.5697e-02,  3.9380e-01,  1.2906e-01,\n",
      "         1.0031e-01,  3.8304e-01,  2.5332e-01, -1.2016e-01,  7.0213e-01,\n",
      "         5.0267e-01,  1.6442e-01,  7.6638e-01,  1.2727e-01,  1.1729e-01,\n",
      "        -3.0898e-01, -2.3525e-01,  2.9141e-01,  1.7783e-01,  7.6971e-02,\n",
      "         4.6565e-01, -6.5111e-02, -8.9822e-03, -7.2651e-01,  3.1031e-01,\n",
      "        -2.8626e-02, -2.8145e-01, -1.7386e-01,  4.2387e-01, -5.1398e-01,\n",
      "         4.5433e-01, -7.2328e-02, -4.8299e-01, -1.1646e-01, -1.0034e+00,\n",
      "        -8.3277e-01, -1.1065e-01, -1.8320e-01, -3.8849e-01,  3.3266e-01,\n",
      "        -4.5537e-01, -7.4574e-01, -4.7791e-01, -2.4591e-01, -1.6215e-01,\n",
      "        -5.3358e-01,  5.2557e-01, -1.6401e-01,  5.1721e-01, -4.5122e-01,\n",
      "        -5.7557e-01,  3.9971e-02,  8.7705e-02, -7.3504e-02, -3.5651e-01,\n",
      "        -3.8253e-01, -5.4519e-01, -2.4452e-01,  9.4109e-02,  5.2880e-01,\n",
      "         1.7196e-01,  3.9031e-01,  3.3722e-01, -2.3013e-01,  3.9096e-01,\n",
      "         1.4946e-01,  1.6549e-01, -1.8786e-01, -4.4573e-01, -8.8582e-01,\n",
      "         5.1708e-01, -7.2068e-01,  5.7785e-01,  2.3072e-01, -6.1522e-01,\n",
      "         7.0449e-02,  6.6967e-02,  6.2146e-02,  1.0790e-01, -5.0417e-01,\n",
      "         2.2688e-01, -1.9449e-02,  2.1392e-01,  7.7943e-01, -3.3447e-01,\n",
      "         6.3792e-01,  2.1459e-01,  2.7445e-01, -2.2532e-02,  3.2730e-01,\n",
      "         8.9217e-02, -3.3894e-01,  5.4817e-01, -1.8289e-01,  1.7503e-01,\n",
      "         6.5170e-02, -3.9385e-01, -5.5775e-01, -1.0016e-01,  1.1761e-01,\n",
      "        -2.2797e-01,  4.9795e-02, -9.5518e-02, -2.3093e-01, -2.0154e-01,\n",
      "         8.3142e-02,  1.1367e-01,  2.8845e-01,  5.2153e-01, -1.3153e-01,\n",
      "        -2.7596e-03, -4.1365e-01,  2.4360e-01,  3.1758e-01, -2.9822e-01,\n",
      "         1.3217e-01,  3.3069e-01,  4.1420e-01, -3.3257e-01, -4.7019e-01,\n",
      "         1.6929e-01, -3.2210e-01,  6.9402e-01, -2.7695e-01,  1.0848e-01,\n",
      "        -2.6173e-01,  2.3344e-01, -1.9354e-01,  9.5955e-02,  5.2928e-01,\n",
      "         1.0880e-01,  2.5093e-02, -1.5361e-02,  1.3651e-01, -3.3534e-01,\n",
      "        -5.3783e-01, -3.7155e-02,  2.9069e-01,  1.2852e-01, -2.3337e-01,\n",
      "         6.3895e-02, -8.2033e-01, -1.3769e-01, -2.6390e-01,  2.4331e-01,\n",
      "         4.7589e-02, -5.3228e-02,  2.3085e-01,  5.2086e-01, -4.5761e-01,\n",
      "        -3.7694e-01,  3.6904e-01,  4.3751e-01, -2.0810e-01, -4.4905e-01,\n",
      "        -2.6543e-01, -1.5769e-01,  1.4597e-01, -3.9180e-02,  2.9534e-01,\n",
      "         4.7532e-02, -3.1809e-01, -6.5509e-01,  4.0308e-02,  4.1274e-01,\n",
      "        -4.3273e-01,  1.5866e-01, -1.5331e-01,  1.4048e-01,  1.7875e-02,\n",
      "         8.3010e-02,  2.3237e-01,  5.0295e-01, -5.5582e-01, -1.5211e-02,\n",
      "        -2.9419e-01, -5.8041e-01,  4.1439e-01, -4.7606e-01, -4.9771e-01,\n",
      "        -2.9379e-01, -5.2424e-01, -1.1855e-01,  4.4434e-01,  1.3044e-01,\n",
      "         4.3504e-02, -1.8931e-01, -2.3105e-01, -5.8877e-01,  9.6759e-02,\n",
      "        -8.1975e-03,  2.9122e-01, -1.5131e-01, -1.3207e-02, -5.3893e-01,\n",
      "         4.7040e-02, -4.8159e-01,  1.3543e-02, -6.5694e-01, -2.2763e-01,\n",
      "        -5.4273e-02,  1.1691e-01,  5.9473e-01, -1.3950e-01, -5.5501e-01,\n",
      "        -7.8895e-02,  9.1782e-02, -3.5295e-01, -1.3894e-01, -6.9312e-02,\n",
      "        -2.2128e-01, -1.9973e-01,  4.6714e-01, -1.6052e-01,  2.0793e-01,\n",
      "         5.9294e-01, -7.0080e-02,  1.5902e-01, -1.9441e-01,  7.3197e-02,\n",
      "        -1.3443e-01, -3.0783e-01, -1.6760e-01, -2.5995e-02, -4.2349e-01,\n",
      "        -1.1366e-01,  4.1846e-01, -1.5382e-01, -5.6186e-01,  1.7727e-01,\n",
      "        -6.0901e-02,  1.0619e-01,  4.4316e-01,  4.4507e-01,  1.6958e-01,\n",
      "         3.9349e-01,  9.3351e-01,  3.5746e-01, -1.3149e-01, -7.1409e-01,\n",
      "        -8.4992e-02,  1.9763e-01, -1.7546e-01,  4.2209e-01,  8.6106e-01,\n",
      "        -1.6933e-01, -2.6450e-01, -2.9788e-02, -1.1933e-01,  4.6427e-01,\n",
      "        -1.3903e-03, -6.3351e-01,  2.4409e-01, -2.4454e-01, -1.8539e-01,\n",
      "         4.2322e-01, -6.6048e-01,  2.0602e-01,  1.4603e-01, -1.5672e-01,\n",
      "         2.2505e-02,  8.4800e-03,  2.1362e-02,  2.6677e-01,  2.9052e-01,\n",
      "        -4.3831e-02, -2.0162e-02,  3.0902e-01, -4.9332e-01,  5.0562e-01,\n",
      "        -1.0628e-01,  2.8812e-01,  1.3949e-01,  4.1089e-01, -3.1651e-03,\n",
      "         9.8455e-02, -3.5239e-02, -8.3416e-02, -2.3620e-01,  1.0702e-01,\n",
      "         6.6153e-02,  8.9650e-01, -7.1956e-01,  3.5872e-01,  2.4344e-01,\n",
      "        -6.5015e-01,  3.2466e-01,  3.7425e-01, -3.9389e-01, -3.0229e-01,\n",
      "         5.0358e-01, -1.0503e-01, -7.5329e-02,  5.8379e-02, -8.2531e-01,\n",
      "        -3.1833e-01, -2.7260e-01,  1.2713e-01,  3.4735e-01,  7.2228e-01,\n",
      "         1.0866e-01,  5.6928e-01, -2.0946e-01, -1.0454e-01,  5.6707e-01,\n",
      "        -3.8061e-01,  1.2225e-01, -4.3018e-01,  4.6131e-01,  3.6690e-01,\n",
      "         2.0032e-01,  8.0939e-01, -1.0257e-02, -2.7526e-02, -1.0249e-02,\n",
      "         2.7362e-01,  1.8403e-01,  3.8643e-01,  1.4958e-01,  1.8184e-01,\n",
      "         1.8643e-01, -3.4549e-01,  1.0837e-01, -1.8612e-02,  2.7900e-01,\n",
      "         1.2994e-01,  6.5828e-01, -2.5847e-01,  1.8242e-01, -2.0227e-01,\n",
      "         2.2644e-01, -8.1550e-01,  2.4784e-01,  7.4752e-01,  1.4240e-01,\n",
      "        -1.1813e-01, -5.1301e-01, -8.0669e-02, -2.3236e-01,  1.1883e-02,\n",
      "         1.1329e-01,  1.3073e-01,  3.0818e-01,  3.4482e-01,  6.9083e-02,\n",
      "        -4.1692e-03, -1.1786e-01, -1.5733e-01,  3.5697e-02, -4.1204e-02,\n",
      "        -2.6989e-01, -5.5211e-01,  2.8128e-01, -3.6879e-01, -4.8513e-01,\n",
      "        -7.4647e-02,  3.1394e-02, -3.4188e-01,  1.0187e-01,  3.2299e-02,\n",
      "         2.5230e-01, -7.2089e-02,  5.7537e-02,  4.6531e-01, -2.1976e-01,\n",
      "         6.0485e-01,  9.7260e-01, -1.4656e-01,  5.6433e-01, -3.6145e-01,\n",
      "        -2.9142e-01, -7.3250e-03, -2.2783e-01, -5.3007e-03,  1.4522e-01,\n",
      "         3.9973e-01, -2.0406e-01, -2.2772e-01, -2.5895e-01,  3.3894e-01,\n",
      "        -8.9740e-01, -1.1989e-01, -3.9609e-01,  1.1777e-01, -4.9371e-02,\n",
      "        -1.1907e-01, -3.1297e-01,  4.3025e-02,  1.2152e-01, -1.9948e-01,\n",
      "         2.7339e-01,  2.9747e-01, -3.5646e-01,  1.0700e-01, -1.5277e-01,\n",
      "         2.0691e-01, -7.1307e-02, -1.6452e-01,  4.3577e-02,  5.2974e-01,\n",
      "        -3.8472e-01,  6.3195e-01,  4.4621e-01,  1.1569e-01,  8.6775e-02,\n",
      "         3.1453e-01, -4.7055e-01, -4.4358e-01, -1.3695e-01,  3.8840e-01,\n",
      "         9.0945e-02,  2.8569e-01, -2.2589e+00,  9.0485e-02, -3.8329e-01,\n",
      "        -3.3406e-01,  3.6097e-01, -2.6446e-01,  3.1436e-01, -2.5452e-01,\n",
      "         5.0507e-02, -4.1229e-01,  2.9733e-01, -2.4189e-01,  5.9263e-02,\n",
      "         4.2343e-02, -4.6795e-01,  3.6074e-01])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 3072])\n",
      "tensor([-0.2671,  0.0269, -0.7113,  ..., -0.0484, -0.1982,  0.3941])\n",
      "torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "# get last four layers\n",
    "last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "# cast layers to a tuple and concatenate over the last dimension\n",
    "cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "print(cat_hidden_states.size())\n",
    "\n",
    "# take the mean of the concatenated vector over the token dimension\n",
    "cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "print(cat_sentence_embedding)\n",
    "print(cat_sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len -= tokenizer.num_special_tokens_to_add()\n",
    "\n",
    "with open(dataset, \"rt\") as f_p:\n",
    "    for line in f_p:\n",
    "        line = line.rstrip()\n",
    "\n",
    "        if not line:\n",
    "            print(line)\n",
    "            subword_len_counter = 0\n",
    "            continue\n",
    "\n",
    "        token = line.split()[0]\n",
    "\n",
    "        current_subwords_len = len(tokenizer.tokenize(token))\n",
    "\n",
    "        # Token contains strange control characters like \\x96 or \\x95\n",
    "        # Just filter out the complete line\n",
    "        if current_subwords_len == 0:\n",
    "            continue\n",
    "\n",
    "        if (subword_len_counter + current_subwords_len) > max_len:\n",
    "            print(\"\")\n",
    "            print(line)\n",
    "            subword_len_counter = current_subwords_len\n",
    "            continue\n",
    "\n",
    "        subword_len_counter += current_subwords_len\n",
    "\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3",
   "language": "python",
   "name": ".venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
