{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lines = []\n",
    "with open('model_data/data/all_data.csv') as f:\n",
    "    for line in f:\n",
    "        input_lines.append(line)\n",
    "del input_lines[404]                \n",
    "len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.95_loss.0.755801.csv',\n",
       " 'caret.csv',\n",
       " 'numpy.csv',\n",
       " 'sklearn.csv',\n",
       " 'all_data.csv',\n",
       " 'scipy.csv',\n",
       " 'all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.99_loss.0.6853602.csv',\n",
       " 'all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.750086.csv']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"model_data/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lines = []\n",
    "with open('model_data/data/all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.99_loss.0.6853602.csv') as f:\n",
    "    for line in f:\n",
    "        output_lines.append(line)\n",
    "del output_lines[404]        \n",
    "len(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458, 100)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched = []\n",
    "for line_idx in range(len(output_lines)):\n",
    "    matched.append((input_lines[line_idx], output_lines[line_idx]))\n",
    "\n",
    "matched_clean_idxs = [re.match(\".*[A-Z].*\", x[1]) is None for x in matched]\n",
    "matched_clean = [x for x in matched if not re.match(\".*[A-Z].*\", x[1])]\n",
    "matched_clean_vectors = [x[1].split(\",\") for x in matched_clean[1:]]\n",
    "\n",
    "vec_matrix = np.array(matched_clean_vectors)\n",
    "vec_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_clean_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "distances = distance.cdist([vec_matrix[5]], vec_matrix, \"cosine\")[0]\n",
    "# min_index = np.argmin(distances)\n",
    "ind = np.argpartition(distances, 6)[:6]\n",
    "sorted_ind = ind[np.argsort(distances[ind])]\n",
    "min_distances = distances[sorted_ind]\n",
    "max_similarity = [1 - x for x in min_distances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.651529930521255,\n",
       " 0.6213898333778319,\n",
       " 0.619677542475636,\n",
       " 0.617218577679545,\n",
       " 0.6120399246897927]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topNMatches(idx, matrix, n=5):\n",
    "    \n",
    "    distances = distance.cdist([matrix[idx]], matrix, \"cosine\")[0]\n",
    "\n",
    "    ind = np.argpartition(distances, n+1)[:(n+1)]\n",
    "    sorted_ind = ind[np.argsort(distances[ind])][1:]\n",
    "    min_distances = distances[sorted_ind]\n",
    "    max_similarity = [1 - x for x in min_distances]\n",
    "    \n",
    "    return (max_similarity, sorted_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity, idxs = topNMatches(5, vec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([179,  93, 124, 323, 302])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = matched[5]\n",
    "matches = [matched[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = defaultdict(list)\n",
    "for idx in range(vec_matrix.shape[0]):\n",
    "    rankings[idx] = list(topNMatches(idx, vec_matrix, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector_recommendations.pkl', 'wb') as f:\n",
    "    pickle.dump(rankings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "for x in zip([1,2,3],[4,5,6]):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-85-ad41dcf565ed>:11: UserWarning:\n",
      "\n",
      "Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x972 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_pca = PCA(n_components=5).fit_transform(vec_matrix)\n",
    "tsne = TSNE(n_components=2, perplexity=5).fit_transform(doc_pca)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsne[:,0], tsne[:,1],s=3)\n",
    "# for x, y, token in zip(tsne[:,0],tsne[:,1],mft):\n",
    "#     ax.annotate(token, xy=(x,y), size=10)\n",
    "\n",
    "fig.set_size_inches(15,13.5)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(tsne, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig2 = px.scatter(tsne_df, x=\"x\", y=\"y\",\n",
    "#                  size=\"population\", color=\"continent\", hover_name=\"country\",\n",
    "                  size_max=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          25.728511810302734,
          23.12775993347168,
          25.073015213012695,
          26.519821166992188,
          36.965057373046875,
          4.652074813842773,
          -16.646141052246094,
          24.443010330200195,
          41.34932327270508,
          17.603681564331055,
          24.721708297729492,
          17.566526412963867,
          45.675804138183594,
          31.98655128479004,
          -21.61494255065918,
          24.25528335571289,
          24.172426223754883,
          -0.29479989409446716,
          9.541126251220703,
          17.49117660522461,
          32.72306823730469,
          33.234519958496094,
          24.04085350036621,
          25.09331703186035,
          6.071749687194824,
          9.09376049041748,
          30.066251754760742,
          10.688270568847656,
          37.29462432861328,
          35.16575241088867,
          12.090653419494629,
          34.07964324951172,
          31.174291610717773,
          37.214717864990234,
          40.16612243652344,
          12.630170822143555,
          25.464641571044922,
          37.716068267822266,
          37.460601806640625,
          34.350242614746094,
          36.08296203613281,
          25.294973373413086,
          33.79408645629883,
          -2.130748987197876,
          36.54226303100586,
          32.90909194946289,
          36.081199645996094,
          40.56191635131836,
          7.299603462219238,
          -0.5082921385765076,
          4.471426010131836,
          -2.719817876815796,
          4.379337787628174,
          7.410096168518066,
          23.142507553100586,
          60.99042510986328,
          6.039086818695068,
          1.7059441804885864,
          0.3685000538825989,
          24.724075317382812,
          21.228713989257812,
          22.219755172729492,
          10.606437683105469,
          -35.310691833496094,
          15.702829360961914,
          61.0112190246582,
          22.93539810180664,
          25.670024871826172,
          -3.444366455078125,
          23.21393585205078,
          25.870685577392578,
          21.614110946655273,
          31.021440505981445,
          36.45248031616211,
          41.72429275512695,
          25.556434631347656,
          41.968868255615234,
          -34.31364440917969,
          -9.06624984741211,
          -9.526147842407227,
          -7.624096393585205,
          -27.133886337280273,
          -26.38546371459961,
          -23.21247673034668,
          -35.703651428222656,
          -14.550199508666992,
          -24.639291763305664,
          -33.55467987060547,
          -28.367328643798828,
          -32.62550735473633,
          -29.795032501220703,
          -19.11219024658203,
          -17.3728084564209,
          -9.41239070892334,
          -10.692902565002441,
          -14.713973045349121,
          -27.284086227416992,
          -6.26499605178833,
          -23.63250160217285,
          -15.946684837341309,
          -14.887167930603027,
          -12.681434631347656,
          -10.562728881835938,
          -16.947181701660156,
          -11.380829811096191,
          -10.939366340637207,
          -14.989171981811523,
          -14.842076301574707,
          -8.396607398986816,
          -24.6910343170166,
          -7.819914817810059,
          -13.709071159362793,
          -10.340784072875977,
          -14.25219440460205,
          -16.25470733642578,
          -16.835397720336914,
          -11.164347648620605,
          -11.742416381835938,
          -36.27059555053711,
          -11.900513648986816,
          -19.020315170288086,
          -29.32530975341797,
          -33.62580490112305,
          -24.03146743774414,
          -11.031439781188965,
          -18.878681182861328,
          -10.58154582977295,
          -10.857561111450195,
          -24.001771926879883,
          -26.71570587158203,
          -31.434391021728516,
          -31.415119171142578,
          13.255953788757324,
          -32.96961212158203,
          -33.226497650146484,
          -21.2641544342041,
          -30.9901123046875,
          21.591655731201172,
          -38.41066360473633,
          -28.55240249633789,
          -34.77922439575195,
          -29.0958251953125,
          -37.9051628112793,
          -34.87437438964844,
          -35.576778411865234,
          -30.410308837890625,
          -34.06093215942383,
          -28.413433074951172,
          -15.666694641113281,
          -9.165767669677734,
          18.13813018798828,
          12.700855255126953,
          -0.3640453815460205,
          -10.428766250610352,
          6.537873268127441,
          19.291305541992188,
          -9.183677673339844,
          -35.23310852050781,
          12.99011516571045,
          -54.18962097167969,
          -32.7601318359375,
          17.996736526489258,
          53.59926223754883,
          34.412567138671875,
          15.162907600402832,
          10.649892807006836,
          5.553590297698975,
          -2.533629894256592,
          8.555134773254395,
          15.825797080993652,
          44.68238830566406,
          38.80559539794922,
          12.08427619934082,
          22.594341278076172,
          42.54088592529297,
          -13.125054359436035,
          26.91744613647461,
          4.979323387145996,
          -16.432981491088867,
          40.34959411621094,
          -0.6282615065574646,
          22.51806640625,
          6.745402812957764,
          30.397520065307617,
          3.6981072425842285,
          2.926746129989624,
          36.68170166015625,
          -8.653544425964355,
          25.73995590209961,
          -6.012798309326172,
          3.265824317932129,
          -28.886608123779297,
          -17.167007446289062,
          7.405720233917236,
          2.40409779548645,
          6.482633113861084,
          -34.02708053588867,
          6.503764629364014,
          39.55952835083008,
          -11.830486297607422,
          -0.6526691317558289,
          41.47591018676758,
          7.3489484786987305,
          41.4511604309082,
          17.99444007873535,
          -10.333157539367676,
          10.37672233581543,
          37.253997802734375,
          6.030693531036377,
          29.14464569091797,
          20.111682891845703,
          5.473392486572266,
          -30.08064842224121,
          -9.439059257507324,
          54.440040588378906,
          18.665241241455078,
          -16.636428833007812,
          -13.930367469787598,
          -26.473264694213867,
          37.14579772949219,
          -28.049129486083984,
          -7.096373081207275,
          2.4445769786834717,
          8.826881408691406,
          45.496150970458984,
          6.184957027435303,
          -4.424113750457764,
          5.225194454193115,
          22.16119384765625,
          -12.882640838623047,
          20.408742904663086,
          -43.58170700073242,
          13.801234245300293,
          -29.384315490722656,
          -33.3873291015625,
          9.582551002502441,
          -25.914878845214844,
          -7.594740867614746,
          39.916500091552734,
          -35.42583084106445,
          9.340567588806152,
          6.55902624130249,
          26.8749942779541,
          -4.553877353668213,
          8.319865226745605,
          10.75838851928711,
          6.769481182098389,
          -33.73951721191406,
          -18.403656005859375,
          21.337749481201172,
          25.933422088623047,
          11.295851707458496,
          21.897668838500977,
          21.51054573059082,
          20.404253005981445,
          6.238799095153809,
          15.670947074890137,
          -40.79636764526367,
          -55.27554702758789,
          30.151941299438477,
          -43.13465118408203,
          22.260459899902344,
          13.492218017578125,
          -1.8610360622406006,
          -50.38933181762695,
          -46.53719711303711,
          -34.63258361816406,
          11.969736099243164,
          18.469703674316406,
          27.815820693969727,
          30.586288452148438,
          -37.952598571777344,
          -8.835708618164062,
          -2.963498115539551,
          10.31978988647461,
          16.960744857788086,
          -9.52208137512207,
          47.304779052734375,
          -17.6257266998291,
          12.581007957458496,
          32.02803039550781,
          46.25836181640625,
          -6.3202805519104,
          -38.22138595581055,
          46.39560317993164,
          8.39880084991455,
          11.60952091217041,
          -34.099422454833984,
          -10.58495044708252,
          -46.289939880371094,
          27.166767120361328,
          61.84012985229492,
          39.257850646972656,
          -15.2736234664917,
          -13.12070083618164,
          37.73273468017578,
          -38.38186264038086,
          -62.323158264160156,
          -38.1537971496582,
          -38.35684585571289,
          59.823116302490234,
          39.5343132019043,
          45.641910552978516,
          36.235843658447266,
          34.824310302734375,
          36.702632904052734,
          45.73464584350586,
          -9.192887306213379,
          41.6663703918457,
          37.57456588745117,
          45.68684387207031,
          2.1666738986968994,
          -16.470046997070312,
          48.52772903442383,
          27.665882110595703,
          -16.44303321838379,
          38.43828582763672,
          -9.924503326416016,
          -30.591732025146484,
          -57.15346908569336,
          21.01600456237793,
          -42.346221923828125,
          -16.79857635498047,
          20.382225036621094,
          -12.146331787109375,
          -17.181554794311523,
          61.848358154296875,
          47.85709762573242,
          60.874088287353516,
          46.767845153808594,
          30.185901641845703,
          -11.927072525024414,
          56.250972747802734,
          20.380168914794922,
          32.830108642578125,
          39.782142639160156,
          54.25266647338867,
          62.149169921875,
          32.95960998535156,
          37.376773834228516,
          30.143644332885742,
          36.02823257446289,
          38.21668243408203,
          12.827595710754395,
          36.68952178955078,
          18.750322341918945,
          53.426658630371094,
          56.8702507019043,
          57.665313720703125,
          54.131996154785156,
          58.1964225769043,
          58.50611877441406,
          18.75621795654297,
          9.679972648620605,
          45.086158752441406,
          28.804269790649414,
          -60.09021759033203,
          -52.60442352294922,
          -2.5450336933135986,
          -45.21306228637695,
          31.482358932495117,
          -61.01461410522461,
          -52.0395622253418,
          -3.4877774715423584,
          -44.234046936035156,
          31.133394241333008,
          -60.77088165283203,
          -51.49874496459961,
          -2.137449264526367,
          -41.91303634643555,
          30.156150817871094,
          -59.13352584838867,
          -53.0892333984375,
          -2.0863752365112305,
          -42.48878860473633,
          29.43959617614746,
          -59.845272064208984,
          -51.052486419677734,
          -0.4226716458797455,
          -45.41254806518555,
          -37.108802795410156,
          -46.93587112426758,
          -48.30006790161133,
          -36.9613151550293,
          -47.994441986083984,
          -49.53517532348633,
          -36.04179382324219,
          -47.31013107299805,
          -49.305789947509766,
          -26.21601676940918,
          -7.155320644378662,
          -10.097306251525879,
          -12.776802062988281,
          -20.869874954223633,
          -6.233047962188721,
          -9.440268516540527,
          -18.081890106201172,
          -19.095911026000977,
          -13.165608406066895,
          36.264835357666016,
          -4.979550838470459,
          -9.624624252319336,
          -13.706341743469238,
          -10.73940372467041,
          25.52048683166504,
          -30.376176834106445,
          -7.913294792175293,
          -12.163625717163086,
          -2.993481159210205,
          -26.234865188598633,
          -5.2164626121521,
          -26.865827560424805,
          -24.30002212524414,
          -26.139633178710938,
          -21.964473724365234,
          28.457841873168945,
          -11.840287208557129,
          -21.5106258392334,
          -25.25076675415039,
          -12.250483512878418,
          -45.07001495361328,
          -21.25748062133789,
          -20.287229537963867,
          -19.93340492248535,
          -7.54385232925415,
          -15.385982513427734,
          -21.457286834716797,
          -11.243695259094238,
          -18.657127380371094,
          -17.952377319335938,
          -14.508299827575684,
          -9.3287935256958,
          -17.64494514465332,
          -27.688732147216797,
          -30.2112979888916,
          25.935871124267578,
          -18.65396499633789,
          -6.902006149291992,
          -58.16628646850586,
          -10.427685737609863,
          -17.65066909790039,
          -25.388416290283203,
          5.030549049377441,
          3.842339038848877,
          -8.50777530670166,
          -13.80052375793457,
          27.82887077331543,
          -6.160133361816406,
          -10.775651931762695,
          -5.448819160461426,
          -13.181327819824219,
          -27.85083770751953,
          -26.852779388427734,
          23.340627670288086,
          -3.5244853496551514,
          -1.7070516347885132,
          -25.791696548461914,
          -29.269794464111328
         ],
         "xaxis": "x",
         "y": [
          -29.300447463989258,
          -37.66606140136719,
          -22.705596923828125,
          1.1633021831512451,
          -22.138896942138672,
          -34.21895980834961,
          -66.95245361328125,
          -24.817323684692383,
          -22.24664878845215,
          -43.0740852355957,
          -23.8746395111084,
          -41.19247817993164,
          -4.058839321136475,
          -22.30252456665039,
          -32.97835922241211,
          -30.923093795776367,
          -17.311471939086914,
          -27.492626190185547,
          -31.690872192382812,
          -41.84550857543945,
          -37.058074951171875,
          -20.989957809448242,
          -36.32468795776367,
          -30.41859245300293,
          -28.54756736755371,
          -31.15927505493164,
          -35.91358947753906,
          -32.49033737182617,
          -38.08615493774414,
          -20.844406127929688,
          -33.31229019165039,
          -16.013118743896484,
          -36.1880989074707,
          -15.315483093261719,
          -23.15856170654297,
          -40.6583366394043,
          -43.85410690307617,
          -37.96900939941406,
          -16.843486785888672,
          -20.94906234741211,
          -39.06679916381836,
          -44.40129470825195,
          -37.80011749267578,
          -26.52369499206543,
          -42.760231018066406,
          -19.697938919067383,
          -40.08091354370117,
          -23.390748977661133,
          -32.66434097290039,
          -31.132171630859375,
          -33.228553771972656,
          -31.282665252685547,
          -35.30033874511719,
          -29.43145751953125,
          -32.330841064453125,
          3.089627981185913,
          -38.07514190673828,
          -35.219520568847656,
          -28.809492111206055,
          -37.906211853027344,
          -34.45244216918945,
          -43.09071731567383,
          -30.162160873413086,
          -29.426864624023438,
          -41.15928268432617,
          2.519174337387085,
          -45.35867691040039,
          -38.9265022277832,
          -4.492448806762695,
          -38.860355377197266,
          -35.12275314331055,
          -43.533206939697266,
          -23.51407814025879,
          -43.29518508911133,
          -18.68532943725586,
          -22.24394989013672,
          -19.23561668395996,
          -61.30362319946289,
          -83.27880096435547,
          -78.32123565673828,
          -79.08253479003906,
          -71.418212890625,
          -71.31890869140625,
          -70.45825958251953,
          -59.384342193603516,
          -68.61122131347656,
          -68.61624908447266,
          -61.01251983642578,
          -64.52960968017578,
          -62.28246307373047,
          -64.0935287475586,
          -86.02267456054688,
          -73.78112030029297,
          -86.8204116821289,
          -85.19628143310547,
          -75.91486358642578,
          -65.26624298095703,
          -79.1063461303711,
          -71.79124450683594,
          -74.6466064453125,
          -81.02933502197266,
          -86.88194274902344,
          -82.45816040039062,
          -81.39927673339844,
          -77.70046997070312,
          -68.40447235107422,
          -78.2236328125,
          -79.09212493896484,
          -81.35453033447266,
          -72.78632354736328,
          -77.14263153076172,
          -88.6226806640625,
          -77.73673248291016,
          -81.28425598144531,
          -68.24717712402344,
          -68.77123260498047,
          -87.01976013183594,
          -88.83412170410156,
          -58.760501861572266,
          -89.86034393310547,
          -86.40892791748047,
          -63.704994201660156,
          -62.356201171875,
          -69.66155242919922,
          -75.60587310791016,
          -80.62625122070312,
          -68.27749633789062,
          -84.68341827392578,
          -24.861352920532227,
          -24.14852523803711,
          -32.356807708740234,
          -19.61467933654785,
          -41.086219787597656,
          -32.061893463134766,
          -20.177932739257812,
          -25.572160720825195,
          -19.704391479492188,
          -38.14389419555664,
          -29.702299118041992,
          -31.836538314819336,
          -22.3461971282959,
          -33.332054138183594,
          -29.630374908447266,
          -29.826927185058594,
          -22.031808853149414,
          -31.811357498168945,
          -23.04168128967285,
          -32.95549774169922,
          10.73642635345459,
          24.130674362182617,
          51.198020935058594,
          18.354345321655273,
          51.5376091003418,
          45.298526763916016,
          54.07265090942383,
          33.34840393066406,
          42.81769561767578,
          29.7099552154541,
          46.87889099121094,
          9.167702674865723,
          31.018903732299805,
          50.55867385864258,
          15.918238639831543,
          -3.2039363384246826,
          17.87316131591797,
          15.10116958618164,
          33.77370071411133,
          45.318721771240234,
          11.459728240966797,
          18.24444580078125,
          27.972999572753906,
          24.379159927368164,
          18.927589416503906,
          27.844066619873047,
          26.9783878326416,
          58.51835250854492,
          30.12915802001953,
          46.30939865112305,
          12.543087005615234,
          26.77190589904785,
          43.24396514892578,
          21.47687339782715,
          28.569856643676758,
          19.961719512939453,
          9.329434394836426,
          23.95391273498535,
          22.25922393798828,
          26.68947410583496,
          19.56247329711914,
          -22.445356369018555,
          50.492576599121094,
          34.671180725097656,
          11.432631492614746,
          11.500593185424805,
          10.525657653808594,
          12.1783447265625,
          31.89738655090332,
          52.464786529541016,
          33.19847869873047,
          26.64030647277832,
          53.70474624633789,
          36.79719543457031,
          24.627717971801758,
          36.38535690307617,
          49.155677795410156,
          26.216073989868164,
          20.991365432739258,
          32.66643524169922,
          32.770179748535156,
          9.008140563964844,
          54.5799674987793,
          46.0300407409668,
          34.248817443847656,
          25.42317008972168,
          17.388978958129883,
          47.65264892578125,
          11.303849220275879,
          27.441743850708008,
          37.12199020385742,
          23.273880004882812,
          34.960166931152344,
          27.342710494995117,
          23.84387969970703,
          20.495012283325195,
          4.0037760734558105,
          31.349287033081055,
          30.91061019897461,
          31.109697341918945,
          26.643569946289062,
          27.04947853088379,
          41.287906646728516,
          40.13111877441406,
          41.83953094482422,
          46.996673583984375,
          33.886531829833984,
          50.3236083984375,
          37.81815719604492,
          56.19008255004883,
          32.518165588378906,
          30.334335327148438,
          14.191386222839355,
          26.202051162719727,
          18.757511138916016,
          30.23358917236328,
          19.354755401611328,
          21.61887550354004,
          51.86518859863281,
          34.51470184326172,
          55.023887634277344,
          42.932289123535156,
          29.707317352294922,
          15.389262199401855,
          23.718408584594727,
          24.697261810302734,
          24.431774139404297,
          30.024362564086914,
          41.28273391723633,
          33.79710006713867,
          8.313033103942871,
          4.728938579559326,
          40.87312698364258,
          22.51972198486328,
          41.03862380981445,
          44.10566711425781,
          38.523616790771484,
          35.51008605957031,
          45.69207763671875,
          47.019073486328125,
          40.828643798828125,
          7.897162437438965,
          6.244964599609375,
          32.43263626098633,
          44.0751953125,
          52.8675651550293,
          51.29793167114258,
          19.052902221679688,
          42.080780029296875,
          27.421010971069336,
          54.04692077636719,
          44.439327239990234,
          -3.298250436782837,
          24.858983993530273,
          27.994930267333984,
          49.45186233520508,
          2.227067708969116,
          30.522445678710938,
          42.58604431152344,
          35.897640228271484,
          57.96043395996094,
          16.772884368896484,
          2.5726659297943115,
          10.95430850982666,
          3.896320343017578,
          48.77302932739258,
          59.59614944458008,
          -12.899252891540527,
          51.208736419677734,
          3.502286911010742,
          46.92921447753906,
          50.260284423828125,
          13.438692092895508,
          3.1264095306396484,
          -3.0349557399749756,
          3.954162359237671,
          -3.6066019535064697,
          3.1222119331359863,
          -4.914016246795654,
          57.38172912597656,
          27.052038192749023,
          -13.799471855163574,
          2.608077049255371,
          7.855990409851074,
          49.33207702636719,
          25.462730407714844,
          3.4397573471069336,
          50.312896728515625,
          21.004226684570312,
          44.8272590637207,
          46.67046356201172,
          8.696236610412598,
          41.888031005859375,
          42.00023651123047,
          51.76930618286133,
          32.246856689453125,
          58.597084045410156,
          53.08708190917969,
          13.237981796264648,
          2.0550758838653564,
          4.690878391265869,
          26.052221298217773,
          8.18258285522461,
          46.47780227661133,
          16.715173721313477,
          54.99842071533203,
          -3.0822198390960693,
          23.496389389038086,
          18.71199607849121,
          10.742523193359375,
          10.088075637817383,
          33.66520309448242,
          20.44636344909668,
          35.32662582397461,
          3.4896352291107178,
          42.838321685791016,
          31.77246856689453,
          52.60795974731445,
          11.035283088684082,
          12.155433654785156,
          10.839808464050293,
          11.161282539367676,
          12.528388023376465,
          11.806791305541992,
          33.94681930541992,
          10.703742027282715,
          27.47298240661621,
          11.312217712402344,
          3.95550537109375,
          5.386488437652588,
          41.9141845703125,
          36.251487731933594,
          9.570757865905762,
          4.326012134552002,
          6.554783344268799,
          43.36684036254883,
          36.07466125488281,
          10.735213279724121,
          5.341842174530029,
          5.24688196182251,
          52.758331298828125,
          34.0711784362793,
          12.26016616821289,
          4.60930061340332,
          6.497317314147949,
          43.21540069580078,
          34.59625244140625,
          15.098489761352539,
          5.568709373474121,
          6.264615535736084,
          52.20314025878906,
          36.978546142578125,
          45.5894775390625,
          14.703099250793457,
          36.99802780151367,
          46.355224609375,
          14.401333808898926,
          37.235565185546875,
          45.600223541259766,
          13.944236755371094,
          36.481117248535156,
          -2.541541576385498,
          -35.96639633178711,
          -34.73751449584961,
          -8.929483413696289,
          -29.789548873901367,
          -35.76153564453125,
          -34.53734588623047,
          0.5221949219703674,
          -24.027271270751953,
          -9.944391250610352,
          21.03378677368164,
          -4.699133396148682,
          -20.955869674682617,
          -7.520905017852783,
          -16.674924850463867,
          0.36685481667518616,
          3.374504804611206,
          -36.238014221191406,
          -12.425552368164062,
          -25.667726516723633,
          1.016772747039795,
          -27.226211547851562,
          -24.645198822021484,
          1.2224639654159546,
          0.05057502165436745,
          -24.769363403320312,
          16.556560516357422,
          -18.949525833129883,
          -32.3853645324707,
          -19.637670516967773,
          -7.876437664031982,
          13.418975830078125,
          -0.6257396340370178,
          -25.216670989990234,
          -1.0198264122009277,
          -4.796323299407959,
          -25.718666076660156,
          -29.249534606933594,
          -7.898834228515625,
          -3.0473716259002686,
          -24.362159729003906,
          -26.08243751525879,
          -14.111565589904785,
          2.047898054122925,
          2.7268314361572266,
          2.592970848083496,
          -35.95702362060547,
          -0.3302527368068695,
          -28.315288543701172,
          9.488492965698242,
          -15.569083213806152,
          -22.947900772094727,
          -19.06147003173828,
          26.170316696166992,
          8.141048431396484,
          -23.638208389282227,
          -19.59011459350586,
          30.492265701293945,
          -4.801666259765625,
          -14.976167678833008,
          -22.86225700378418,
          -6.635542392730713,
          -30.078657150268555,
          -1.6241544485092163,
          -16.778987884521484,
          -24.833065032958984,
          -31.600116729736328,
          -3.5686588287353516,
          3.123525857925415
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"1391aede-3d49-443c-9da2-8fc971270e75\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"1391aede-3d49-443c-9da2-8fc971270e75\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '1391aede-3d49-443c-9da2-8fc971270e75',\n",
       "                        [{\"hovertemplate\": \"x=%{x}<br>y=%{y}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [25.728511810302734, 23.12775993347168, 25.073015213012695, 26.519821166992188, 36.965057373046875, 4.652074813842773, -16.646141052246094, 24.443010330200195, 41.34932327270508, 17.603681564331055, 24.721708297729492, 17.566526412963867, 45.675804138183594, 31.98655128479004, -21.61494255065918, 24.25528335571289, 24.172426223754883, -0.29479989409446716, 9.541126251220703, 17.49117660522461, 32.72306823730469, 33.234519958496094, 24.04085350036621, 25.09331703186035, 6.071749687194824, 9.09376049041748, 30.066251754760742, 10.688270568847656, 37.29462432861328, 35.16575241088867, 12.090653419494629, 34.07964324951172, 31.174291610717773, 37.214717864990234, 40.16612243652344, 12.630170822143555, 25.464641571044922, 37.716068267822266, 37.460601806640625, 34.350242614746094, 36.08296203613281, 25.294973373413086, 33.79408645629883, -2.130748987197876, 36.54226303100586, 32.90909194946289, 36.081199645996094, 40.56191635131836, 7.299603462219238, -0.5082921385765076, 4.471426010131836, -2.719817876815796, 4.379337787628174, 7.410096168518066, 23.142507553100586, 60.99042510986328, 6.039086818695068, 1.7059441804885864, 0.3685000538825989, 24.724075317382812, 21.228713989257812, 22.219755172729492, 10.606437683105469, -35.310691833496094, 15.702829360961914, 61.0112190246582, 22.93539810180664, 25.670024871826172, -3.444366455078125, 23.21393585205078, 25.870685577392578, 21.614110946655273, 31.021440505981445, 36.45248031616211, 41.72429275512695, 25.556434631347656, 41.968868255615234, -34.31364440917969, -9.06624984741211, -9.526147842407227, -7.624096393585205, -27.133886337280273, -26.38546371459961, -23.21247673034668, -35.703651428222656, -14.550199508666992, -24.639291763305664, -33.55467987060547, -28.367328643798828, -32.62550735473633, -29.795032501220703, -19.11219024658203, -17.3728084564209, -9.41239070892334, -10.692902565002441, -14.713973045349121, -27.284086227416992, -6.26499605178833, -23.63250160217285, -15.946684837341309, -14.887167930603027, -12.681434631347656, -10.562728881835938, -16.947181701660156, -11.380829811096191, -10.939366340637207, -14.989171981811523, -14.842076301574707, -8.396607398986816, -24.6910343170166, -7.819914817810059, -13.709071159362793, -10.340784072875977, -14.25219440460205, -16.25470733642578, -16.835397720336914, -11.164347648620605, -11.742416381835938, -36.27059555053711, -11.900513648986816, -19.020315170288086, -29.32530975341797, -33.62580490112305, -24.03146743774414, -11.031439781188965, -18.878681182861328, -10.58154582977295, -10.857561111450195, -24.001771926879883, -26.71570587158203, -31.434391021728516, -31.415119171142578, 13.255953788757324, -32.96961212158203, -33.226497650146484, -21.2641544342041, -30.9901123046875, 21.591655731201172, -38.41066360473633, -28.55240249633789, -34.77922439575195, -29.0958251953125, -37.9051628112793, -34.87437438964844, -35.576778411865234, -30.410308837890625, -34.06093215942383, -28.413433074951172, -15.666694641113281, -9.165767669677734, 18.13813018798828, 12.700855255126953, -0.3640453815460205, -10.428766250610352, 6.537873268127441, 19.291305541992188, -9.183677673339844, -35.23310852050781, 12.99011516571045, -54.18962097167969, -32.7601318359375, 17.996736526489258, 53.59926223754883, 34.412567138671875, 15.162907600402832, 10.649892807006836, 5.553590297698975, -2.533629894256592, 8.555134773254395, 15.825797080993652, 44.68238830566406, 38.80559539794922, 12.08427619934082, 22.594341278076172, 42.54088592529297, -13.125054359436035, 26.91744613647461, 4.979323387145996, -16.432981491088867, 40.34959411621094, -0.6282615065574646, 22.51806640625, 6.745402812957764, 30.397520065307617, 3.6981072425842285, 2.926746129989624, 36.68170166015625, -8.653544425964355, 25.73995590209961, -6.012798309326172, 3.265824317932129, -28.886608123779297, -17.167007446289062, 7.405720233917236, 2.40409779548645, 6.482633113861084, -34.02708053588867, 6.503764629364014, 39.55952835083008, -11.830486297607422, -0.6526691317558289, 41.47591018676758, 7.3489484786987305, 41.4511604309082, 17.99444007873535, -10.333157539367676, 10.37672233581543, 37.253997802734375, 6.030693531036377, 29.14464569091797, 20.111682891845703, 5.473392486572266, -30.08064842224121, -9.439059257507324, 54.440040588378906, 18.665241241455078, -16.636428833007812, -13.930367469787598, -26.473264694213867, 37.14579772949219, -28.049129486083984, -7.096373081207275, 2.4445769786834717, 8.826881408691406, 45.496150970458984, 6.184957027435303, -4.424113750457764, 5.225194454193115, 22.16119384765625, -12.882640838623047, 20.408742904663086, -43.58170700073242, 13.801234245300293, -29.384315490722656, -33.3873291015625, 9.582551002502441, -25.914878845214844, -7.594740867614746, 39.916500091552734, -35.42583084106445, 9.340567588806152, 6.55902624130249, 26.8749942779541, -4.553877353668213, 8.319865226745605, 10.75838851928711, 6.769481182098389, -33.73951721191406, -18.403656005859375, 21.337749481201172, 25.933422088623047, 11.295851707458496, 21.897668838500977, 21.51054573059082, 20.404253005981445, 6.238799095153809, 15.670947074890137, -40.79636764526367, -55.27554702758789, 30.151941299438477, -43.13465118408203, 22.260459899902344, 13.492218017578125, -1.8610360622406006, -50.38933181762695, -46.53719711303711, -34.63258361816406, 11.969736099243164, 18.469703674316406, 27.815820693969727, 30.586288452148438, -37.952598571777344, -8.835708618164062, -2.963498115539551, 10.31978988647461, 16.960744857788086, -9.52208137512207, 47.304779052734375, -17.6257266998291, 12.581007957458496, 32.02803039550781, 46.25836181640625, -6.3202805519104, -38.22138595581055, 46.39560317993164, 8.39880084991455, 11.60952091217041, -34.099422454833984, -10.58495044708252, -46.289939880371094, 27.166767120361328, 61.84012985229492, 39.257850646972656, -15.2736234664917, -13.12070083618164, 37.73273468017578, -38.38186264038086, -62.323158264160156, -38.1537971496582, -38.35684585571289, 59.823116302490234, 39.5343132019043, 45.641910552978516, 36.235843658447266, 34.824310302734375, 36.702632904052734, 45.73464584350586, -9.192887306213379, 41.6663703918457, 37.57456588745117, 45.68684387207031, 2.1666738986968994, -16.470046997070312, 48.52772903442383, 27.665882110595703, -16.44303321838379, 38.43828582763672, -9.924503326416016, -30.591732025146484, -57.15346908569336, 21.01600456237793, -42.346221923828125, -16.79857635498047, 20.382225036621094, -12.146331787109375, -17.181554794311523, 61.848358154296875, 47.85709762573242, 60.874088287353516, 46.767845153808594, 30.185901641845703, -11.927072525024414, 56.250972747802734, 20.380168914794922, 32.830108642578125, 39.782142639160156, 54.25266647338867, 62.149169921875, 32.95960998535156, 37.376773834228516, 30.143644332885742, 36.02823257446289, 38.21668243408203, 12.827595710754395, 36.68952178955078, 18.750322341918945, 53.426658630371094, 56.8702507019043, 57.665313720703125, 54.131996154785156, 58.1964225769043, 58.50611877441406, 18.75621795654297, 9.679972648620605, 45.086158752441406, 28.804269790649414, -60.09021759033203, -52.60442352294922, -2.5450336933135986, -45.21306228637695, 31.482358932495117, -61.01461410522461, -52.0395622253418, -3.4877774715423584, -44.234046936035156, 31.133394241333008, -60.77088165283203, -51.49874496459961, -2.137449264526367, -41.91303634643555, 30.156150817871094, -59.13352584838867, -53.0892333984375, -2.0863752365112305, -42.48878860473633, 29.43959617614746, -59.845272064208984, -51.052486419677734, -0.4226716458797455, -45.41254806518555, -37.108802795410156, -46.93587112426758, -48.30006790161133, -36.9613151550293, -47.994441986083984, -49.53517532348633, -36.04179382324219, -47.31013107299805, -49.305789947509766, -26.21601676940918, -7.155320644378662, -10.097306251525879, -12.776802062988281, -20.869874954223633, -6.233047962188721, -9.440268516540527, -18.081890106201172, -19.095911026000977, -13.165608406066895, 36.264835357666016, -4.979550838470459, -9.624624252319336, -13.706341743469238, -10.73940372467041, 25.52048683166504, -30.376176834106445, -7.913294792175293, -12.163625717163086, -2.993481159210205, -26.234865188598633, -5.2164626121521, -26.865827560424805, -24.30002212524414, -26.139633178710938, -21.964473724365234, 28.457841873168945, -11.840287208557129, -21.5106258392334, -25.25076675415039, -12.250483512878418, -45.07001495361328, -21.25748062133789, -20.287229537963867, -19.93340492248535, -7.54385232925415, -15.385982513427734, -21.457286834716797, -11.243695259094238, -18.657127380371094, -17.952377319335938, -14.508299827575684, -9.3287935256958, -17.64494514465332, -27.688732147216797, -30.2112979888916, 25.935871124267578, -18.65396499633789, -6.902006149291992, -58.16628646850586, -10.427685737609863, -17.65066909790039, -25.388416290283203, 5.030549049377441, 3.842339038848877, -8.50777530670166, -13.80052375793457, 27.82887077331543, -6.160133361816406, -10.775651931762695, -5.448819160461426, -13.181327819824219, -27.85083770751953, -26.852779388427734, 23.340627670288086, -3.5244853496551514, -1.7070516347885132, -25.791696548461914, -29.269794464111328], \"xaxis\": \"x\", \"y\": [-29.300447463989258, -37.66606140136719, -22.705596923828125, 1.1633021831512451, -22.138896942138672, -34.21895980834961, -66.95245361328125, -24.817323684692383, -22.24664878845215, -43.0740852355957, -23.8746395111084, -41.19247817993164, -4.058839321136475, -22.30252456665039, -32.97835922241211, -30.923093795776367, -17.311471939086914, -27.492626190185547, -31.690872192382812, -41.84550857543945, -37.058074951171875, -20.989957809448242, -36.32468795776367, -30.41859245300293, -28.54756736755371, -31.15927505493164, -35.91358947753906, -32.49033737182617, -38.08615493774414, -20.844406127929688, -33.31229019165039, -16.013118743896484, -36.1880989074707, -15.315483093261719, -23.15856170654297, -40.6583366394043, -43.85410690307617, -37.96900939941406, -16.843486785888672, -20.94906234741211, -39.06679916381836, -44.40129470825195, -37.80011749267578, -26.52369499206543, -42.760231018066406, -19.697938919067383, -40.08091354370117, -23.390748977661133, -32.66434097290039, -31.132171630859375, -33.228553771972656, -31.282665252685547, -35.30033874511719, -29.43145751953125, -32.330841064453125, 3.089627981185913, -38.07514190673828, -35.219520568847656, -28.809492111206055, -37.906211853027344, -34.45244216918945, -43.09071731567383, -30.162160873413086, -29.426864624023438, -41.15928268432617, 2.519174337387085, -45.35867691040039, -38.9265022277832, -4.492448806762695, -38.860355377197266, -35.12275314331055, -43.533206939697266, -23.51407814025879, -43.29518508911133, -18.68532943725586, -22.24394989013672, -19.23561668395996, -61.30362319946289, -83.27880096435547, -78.32123565673828, -79.08253479003906, -71.418212890625, -71.31890869140625, -70.45825958251953, -59.384342193603516, -68.61122131347656, -68.61624908447266, -61.01251983642578, -64.52960968017578, -62.28246307373047, -64.0935287475586, -86.02267456054688, -73.78112030029297, -86.8204116821289, -85.19628143310547, -75.91486358642578, -65.26624298095703, -79.1063461303711, -71.79124450683594, -74.6466064453125, -81.02933502197266, -86.88194274902344, -82.45816040039062, -81.39927673339844, -77.70046997070312, -68.40447235107422, -78.2236328125, -79.09212493896484, -81.35453033447266, -72.78632354736328, -77.14263153076172, -88.6226806640625, -77.73673248291016, -81.28425598144531, -68.24717712402344, -68.77123260498047, -87.01976013183594, -88.83412170410156, -58.760501861572266, -89.86034393310547, -86.40892791748047, -63.704994201660156, -62.356201171875, -69.66155242919922, -75.60587310791016, -80.62625122070312, -68.27749633789062, -84.68341827392578, -24.861352920532227, -24.14852523803711, -32.356807708740234, -19.61467933654785, -41.086219787597656, -32.061893463134766, -20.177932739257812, -25.572160720825195, -19.704391479492188, -38.14389419555664, -29.702299118041992, -31.836538314819336, -22.3461971282959, -33.332054138183594, -29.630374908447266, -29.826927185058594, -22.031808853149414, -31.811357498168945, -23.04168128967285, -32.95549774169922, 10.73642635345459, 24.130674362182617, 51.198020935058594, 18.354345321655273, 51.5376091003418, 45.298526763916016, 54.07265090942383, 33.34840393066406, 42.81769561767578, 29.7099552154541, 46.87889099121094, 9.167702674865723, 31.018903732299805, 50.55867385864258, 15.918238639831543, -3.2039363384246826, 17.87316131591797, 15.10116958618164, 33.77370071411133, 45.318721771240234, 11.459728240966797, 18.24444580078125, 27.972999572753906, 24.379159927368164, 18.927589416503906, 27.844066619873047, 26.9783878326416, 58.51835250854492, 30.12915802001953, 46.30939865112305, 12.543087005615234, 26.77190589904785, 43.24396514892578, 21.47687339782715, 28.569856643676758, 19.961719512939453, 9.329434394836426, 23.95391273498535, 22.25922393798828, 26.68947410583496, 19.56247329711914, -22.445356369018555, 50.492576599121094, 34.671180725097656, 11.432631492614746, 11.500593185424805, 10.525657653808594, 12.1783447265625, 31.89738655090332, 52.464786529541016, 33.19847869873047, 26.64030647277832, 53.70474624633789, 36.79719543457031, 24.627717971801758, 36.38535690307617, 49.155677795410156, 26.216073989868164, 20.991365432739258, 32.66643524169922, 32.770179748535156, 9.008140563964844, 54.5799674987793, 46.0300407409668, 34.248817443847656, 25.42317008972168, 17.388978958129883, 47.65264892578125, 11.303849220275879, 27.441743850708008, 37.12199020385742, 23.273880004882812, 34.960166931152344, 27.342710494995117, 23.84387969970703, 20.495012283325195, 4.0037760734558105, 31.349287033081055, 30.91061019897461, 31.109697341918945, 26.643569946289062, 27.04947853088379, 41.287906646728516, 40.13111877441406, 41.83953094482422, 46.996673583984375, 33.886531829833984, 50.3236083984375, 37.81815719604492, 56.19008255004883, 32.518165588378906, 30.334335327148438, 14.191386222839355, 26.202051162719727, 18.757511138916016, 30.23358917236328, 19.354755401611328, 21.61887550354004, 51.86518859863281, 34.51470184326172, 55.023887634277344, 42.932289123535156, 29.707317352294922, 15.389262199401855, 23.718408584594727, 24.697261810302734, 24.431774139404297, 30.024362564086914, 41.28273391723633, 33.79710006713867, 8.313033103942871, 4.728938579559326, 40.87312698364258, 22.51972198486328, 41.03862380981445, 44.10566711425781, 38.523616790771484, 35.51008605957031, 45.69207763671875, 47.019073486328125, 40.828643798828125, 7.897162437438965, 6.244964599609375, 32.43263626098633, 44.0751953125, 52.8675651550293, 51.29793167114258, 19.052902221679688, 42.080780029296875, 27.421010971069336, 54.04692077636719, 44.439327239990234, -3.298250436782837, 24.858983993530273, 27.994930267333984, 49.45186233520508, 2.227067708969116, 30.522445678710938, 42.58604431152344, 35.897640228271484, 57.96043395996094, 16.772884368896484, 2.5726659297943115, 10.95430850982666, 3.896320343017578, 48.77302932739258, 59.59614944458008, -12.899252891540527, 51.208736419677734, 3.502286911010742, 46.92921447753906, 50.260284423828125, 13.438692092895508, 3.1264095306396484, -3.0349557399749756, 3.954162359237671, -3.6066019535064697, 3.1222119331359863, -4.914016246795654, 57.38172912597656, 27.052038192749023, -13.799471855163574, 2.608077049255371, 7.855990409851074, 49.33207702636719, 25.462730407714844, 3.4397573471069336, 50.312896728515625, 21.004226684570312, 44.8272590637207, 46.67046356201172, 8.696236610412598, 41.888031005859375, 42.00023651123047, 51.76930618286133, 32.246856689453125, 58.597084045410156, 53.08708190917969, 13.237981796264648, 2.0550758838653564, 4.690878391265869, 26.052221298217773, 8.18258285522461, 46.47780227661133, 16.715173721313477, 54.99842071533203, -3.0822198390960693, 23.496389389038086, 18.71199607849121, 10.742523193359375, 10.088075637817383, 33.66520309448242, 20.44636344909668, 35.32662582397461, 3.4896352291107178, 42.838321685791016, 31.77246856689453, 52.60795974731445, 11.035283088684082, 12.155433654785156, 10.839808464050293, 11.161282539367676, 12.528388023376465, 11.806791305541992, 33.94681930541992, 10.703742027282715, 27.47298240661621, 11.312217712402344, 3.95550537109375, 5.386488437652588, 41.9141845703125, 36.251487731933594, 9.570757865905762, 4.326012134552002, 6.554783344268799, 43.36684036254883, 36.07466125488281, 10.735213279724121, 5.341842174530029, 5.24688196182251, 52.758331298828125, 34.0711784362793, 12.26016616821289, 4.60930061340332, 6.497317314147949, 43.21540069580078, 34.59625244140625, 15.098489761352539, 5.568709373474121, 6.264615535736084, 52.20314025878906, 36.978546142578125, 45.5894775390625, 14.703099250793457, 36.99802780151367, 46.355224609375, 14.401333808898926, 37.235565185546875, 45.600223541259766, 13.944236755371094, 36.481117248535156, -2.541541576385498, -35.96639633178711, -34.73751449584961, -8.929483413696289, -29.789548873901367, -35.76153564453125, -34.53734588623047, 0.5221949219703674, -24.027271270751953, -9.944391250610352, 21.03378677368164, -4.699133396148682, -20.955869674682617, -7.520905017852783, -16.674924850463867, 0.36685481667518616, 3.374504804611206, -36.238014221191406, -12.425552368164062, -25.667726516723633, 1.016772747039795, -27.226211547851562, -24.645198822021484, 1.2224639654159546, 0.05057502165436745, -24.769363403320312, 16.556560516357422, -18.949525833129883, -32.3853645324707, -19.637670516967773, -7.876437664031982, 13.418975830078125, -0.6257396340370178, -25.216670989990234, -1.0198264122009277, -4.796323299407959, -25.718666076660156, -29.249534606933594, -7.898834228515625, -3.0473716259002686, -24.362159729003906, -26.08243751525879, -14.111565589904785, 2.047898054122925, 2.7268314361572266, 2.592970848083496, -35.95702362060547, -0.3302527368068695, -28.315288543701172, 9.488492965698242, -15.569083213806152, -22.947900772094727, -19.06147003173828, 26.170316696166992, 8.141048431396484, -23.638208389282227, -19.59011459350586, 30.492265701293945, -4.801666259765625, -14.976167678833008, -22.86225700378418, -6.635542392730713, -30.078657150268555, -1.6241544485092163, -16.778987884521484, -24.833065032958984, -31.600116729736328, -3.5686588287353516, 3.123525857925415], \"yaxis\": \"y\"}],\n",
       "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('1391aede-3d49-443c-9da2-8fc971270e75');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {'caret': 'R', 'numpy': 'Python', 'scipy': 'Python', 'sklearn': 'Python'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lookup = [(x[0].strip('\"').split(\"_\")[0], languages.get(x[0].strip('\"').split(\"_\")[0])) for x in matched_clean[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"caret_11_subsampling_for_class_imbalances 11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html  11.1 Subsampling Techniques To illustrate these methods, lets simulate some data with a class imbalance using this method. We will simulate a training and test set where each contains 10000 samples and a minority class rate of about 5.9%: Lets create different versions of the training set prior to model tuning: For these data, well use a bagged classification and estimate the area under the ROC curve using five repeats of 10-fold CV. We will collate the resampling results and create a wrapper to estimate the test set performance: The training and test set estimates for the area under the ROC curve do not appear to correlate. Based on the resampling results, one would infer that up-sampling is nearly perfect and that ROSE does relatively poorly. The reason that up-sampling appears to perform so well is that the samples in the majority class are replicated and have a large potential to be in both the model building and hold-out sets. In essence, the hold-outs here are not truly independent samples. In reality, all of the sampling methods do about the same (based on the test set). The statistics for the basic model fit with no sampling are fairly in-line with one another (0.939 via resampling and 0.925 for the test set). \"\\n',\n",
       " '\"caret_11_subsampling_for_class_imbalances 11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html  11.3 Complications The user should be aware that there are a few things that can happening when subsampling that can cause issues in their code. As previously mentioned, when sampling occurs in relation to pre-processing is one such issue. Others are: Sparsely represented categories in factor variables may turn into zero-variance predictors or may be completely sampled out of the model. The underlying functions that do the sampling (e.g. SMOTE , downSample , etc) operate in very different ways and this can affect your results. For example, SMOTE and ROSE will convert your predictor input argument into a data frame (even if you start with a matrix). Currently, sample weights are not supported with sub-sampling. If you use tuneLength to specify the search grid, understand that the data that is used to determine the grid has not been sampled. In most cases, this will not matter but if the grid creation process is affected by the sample size, you may end up using a sub-optimal tuning grid. For some models that require more samples than parameters, a reduction in the sample size may prevent you from being able to fit the model. \"\\n',\n",
       " '\"caret_11_subsampling_for_class_imbalances 11 Subsampling For Class Imbalances subsampling-for-class-imbalances.html  11.4 Using Custom Subsampling Techniques Users have the ability to create their own type of subsampling procedure. To do this, alternative syntax is used with the sampling argument of the trainControl . Previously, we used a simple string as the value of this argument. Another way to specify the argument is to use a list with three (named) elements: The name value is a character string used when the train object is printed. It can be any string. The func element is a function that does the subsampling. It should have arguments called x and y that will contain the predictors and outcome data, respectively. The function should return a list with elements of the same name. The first element is a single logical value that indicates whether the subsampling should occur first relative to pre-process. A value of FALSE means that the subsampling function will receive the sampled versions of x and y . For example, here is what the list version of the sampling argument looks like when simple down-sampling is used: As another example, suppose we want to use SMOTE but use 10 nearest neighbors instead of the default of 5. To do this, we can create a simple wrapper around the SMOTE function and call this instead: The control object would then be: \"\\n',\n",
       " '\"caret_12_using_recipes_with_train 12 Using Recipes with train using-recipes-with-train.html  12.1 Why Should you learn this? Here are two reasons: 12.1.1 More versatile tools for preprocessing data caret s preprocessing tools have a lot of options but the list is not exhaustive and they will only be called in a specific order. If you would like a broader set of options, the ability to write your own preprocessing tools, or to call them in the order that you desire then you can use a recipe to do that. 12.1.2 Using additional data to measure performance In most modeling functions, including train , most variables are consigned to be either predictors or outcomes. For recipes, there are more options. For example, you might want to have specific columns of your data set be available when you compute how well the model is performing, such as: if different stratification variables (e.g.\\xa0patients, ZIP codes, etc) are required to do correct summaries or ancillary data might be need to compute the expected profit or loss based on the model results. To get these data properly, they need to be made available and handled the same way as all of the other data. This means they should be sub- or resampled as all of the other data. Recipes let you do that. \"\\n',\n",
       " '\"caret_12_using_recipes_with_train 12 Using Recipes with train using-recipes-with-train.html  12.2 An Example The QSARdata package contains several chemistry data sets. These data sets have rows for different potential drugs (called compounds here). For each compound, some important characteristic is measured. This illustration will use the AquaticTox data. The outcome is called Activity is a measure of how harmful the compound might be to people. We want to predict this during the drug discovery phase in R&D To do this, a set of molecular descriptors are computed based on the compounds formula. There are a lot of different types of these and we will use the 2-dimensional MOE descriptor set. First, lets load the package and get the data together: We will build a model on these data to predict the activity. Some notes: A common aspect to chemical descriptors is that they are highly correlated . Many descriptors often measure some variation of the same thing. For example, in these data, there are 56 potential predictors that measure different flavors of surface area. It might be a good idea to reduce the dimensionality of these data by pre-filtering the predictors and/or using a dimension reduction technique. Other descriptors are counts of certain types of aspects of the molecule. For example, one predictor is the number of Bromine atoms. The vast majority of compounds lack Bromine and this leads to a near-zero variance situation discussed previously. It might be a good idea to pre-filter these. Also, to demonstrate the utility of recipes, suppose that we could score potential drugs on the basis of how manufacturable they might be. We might want to build a model on the entire data set but only evaluate it on compounds that could be reasonably manufactured. For illustration, well assume that, as a compounds molecule weight increases, its manufacturability decreases . For this purpose, we create a new variable ( manufacturability ) that is neither an outcome or predictor but will be needed to compute performance. For this analysis, we will compute the RMSE using weights based on the manufacturability column such that a difficult compound has less impact on the RMSE. There is no way to include this extra variable using the default train method or using train.formula . Now, lets create a recipe incrementally. First, we will use the formula methods to declare the outcome and predictors but change the analysis role of the manufacturability variable so that it will only be available when summarizing the model fit. Using this new role, the manufacturability column will be available when the summary function is executed and the appropriate rows of the data set will be exposed during resampling. For example, if one were to debug the model_stats function during execution of a model, the data object might look like this: More than one variable can have this role so that multiple columns can be made available. Now lets add some steps to the recipe First, we remove sparse and unbalanced predictors: Note that we have only specified what will happen once the recipe is executed. This is only a specification that uses a generic declaration of all_predictors . As mentioned above, there are a lot of different surface area predictors and they tend to have very high correlations with one another. Well add one or more predictors to the model in place of these predictors using principal component analysis. The step will retain the number of components required to capture 95% of the information contained in these 56 predictors. Well name these new predictors surf_area_1 , surf_area_2 etc. Now, lets specific that the third step in the recipe is to reduce the number of predictors so that no pair has an absolute correlation greater than 0.90. However, we might want to keep the surface area principal components so we exclude these from the filter (using the minus sign) Finally, we can center and scale all of the predictors that are available at the end of the recipe: Lets use this recipe to fit a SVM model and pick the tuning parameters that minimize the weighted RMSE value: What variables were generated by the recipe? The trained recipe is available in the train object and now shows specific variables involved in each step: \"\\n',\n",
       " '\"caret_12_using_recipes_with_train 12 Using Recipes with train using-recipes-with-train.html  12.3 Case Weights For models that accept them , case weights can be passed to the model fitting routines using a role of \"case weight\" . \"\\n',\n",
       " '\"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.2 Illustrative Example 1: SVMs with Laplacian Kernels The package currently contains support vector machine (SVM) models using linear, polynomial and radial basis function kernels. The kernlab package has other functions, including the Laplacian kernel. We will illustrate the model components for this model, which has two parameters: the standard cost parameter for SVMs and one kernel parameter ( sigma ) \"\\n',\n",
       " '\"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.4 The sort Element This is an optional function that sorts the tuning parameters from the simplest model to the most complex. There are times where this ordering is not obvious. This information is used when the performance values are tied across multiple parameters. We would probably want to choose the least complex model in those cases. Here, we will sort by the cost value. Smaller values of C produce smoother class boundaries than larger values: 13.4.1 The levels Element train ensures that classification models always predict factors with the same levels. To do this at prediction time, the package needs to know the levels from the model object (specifically, the finalModels slot of the train object). For model functions using S3 methods, train automatically attaches a character vector called obsLevels to the object and the package code uses this value. However, this strategy does not work for S4 methods. In these cases, the package will use the code found in the levels slot of the model list. For example, the ksvm function uses S4 methods but, unlike most model functions, has a builtin function called lev that will extract the class levels (if any). In this case, our levels code would be: In most other cases, the levels will beed to be extracted from data contained in the fitted model object. As another example, objects created using the ctree function in the party package would need to use: Again, this slot is only used for classification models using S4 methods. We should now be ready to fit our model. A plot of the data shows that the model doesnt change when the cost value is above 16. \"\\n',\n",
       " '\"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.5 Illustrative Example 2: Something More Complicated - LogitBoost ###The loop Element This function can be used to create custom loops for models to tune over. In most cases, the function can just return the existing tuning grid. For example, a LogitBoost model can be trained over the number of boosting iterations. In the caTools package, the LogitBoost function can be used to fit this model. For example: If we were to tune the model evaluating models where the number of iterations was 11, 21, 31, 41 and 51, the grid could be During resampling, train could loop over all five rows in lbGrid and fit five models. However, the predict.LogitBoost function has an argument called nIter that can produce, in this case, predictions from mod for all five models. Instead of train fitting five models, we could fit a single model with nIter  classhl num>51 and derive predictions for all five models using only mod`. The terminology used here is that nIter is a sequential tuning parameter (and the other parameters would be considered fixed ). The loop argument for models is used to produce two objects: loop : this is the actual loop that is used by train . submodels is a list that has as many elements as there are rows in loop . The list has all the extra parameter settings that can be derived for each model. Going back to the LogitBoost example, we could have: For this case, train first fits the nIter  51 model. When the model is predicted, that code has a for loop that iterates over the elements of submodel[[1]] to get the predictions for the other 4 models. In the end, predictions for all five models (for nIter  seq(11, 51, by  10) ) with a single model fit. There are other models built-in to caret that are used this way. There are a number of models that have multiple sequential tuning parameters. If the loop argument is left NULL the results of tuneGrid are used as the simple loop and is recommended for most situations. Note that the machinery that is used to derive the extra predictions is up to the user to create, typically in the predict and prob elements of the custom model object. For the LogitBoost model, some simple code to create these objects would be: For the LogitBoost custom model object, we could use this code in the predict slot: A few more notes: The code in the fit element does not have to change. The prob slot works in the same way. The only difference is that the values saved in the outgoing lists are matrices or data frames of probabilities for each class. After model training (i.e.\\xa0predicting new samples), the value of submodels is set to NULL and the code produces a single set of predictions. If the model had one sequential parameter and one fixed parameter, the loop data frame would have two columns (one for each parameter). If the model is tuned over more than one value of the fixed parameter, the submodels list would have more than one element. If loop had 10 rows, then length(submodels) would be 10 and loop[i,] would be linked to submodels[[i]] . In this case, the prediction function was called by namespace too (i.e. caTools::predict.LogitBoost ). This may not seem necessary but what functions are available can vary depending on what parallel processing technology is being used. For example, the nature of forking used by doMC and doParallel tends to have easier access to functions while PSOCK methods in doParallel do not. It may be easier to take the safe path of using the namespace operator wherever possible to avoid errors that are difficult to track down. Here is a slimmed down version of the logitBoost code already in the package: Should you care about this? Lets tune the model over the same data set used for the SVM model above and see how long it takes: On a data set with 157 instances and 60 predictors and a model that is tuned over only 3 parameter values, there is a 1.57-fold speed-up. If the model were more computationally taxing or the data set were larger or the number of tune parameters that were evaluated was larger, the speed-up would increase. Here is a plot of the speed-up for a few more values of tuneLength : The speed-ups show a significant decrease in training time using this method. Note: The previous examples were run using parallel processing. The remainder in this chapter are run sequentially and, for simplicity, the namespace operator is not used in the custom code modules below. \"\\n',\n",
       " '\"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.6 Illustrative Example 3: Nonstandard Formulas (Note: the previous third illustration (SMOTE During Resampling) is no longer needed due to the inclusion of subsampling via train .) One limitation of train is that it requires the use of basic model formulas. There are several functions that use special formulas or operators on predictors that wont (and perhaps should not) work in the top level call to train . However, we can still fit these models. Here is an example using the mboost function in the mboost package from the help page. We can create a custom model that mimics this code so that we can obtain resampling estimates for this specific model: \"\\n',\n",
       " '\"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.7 Illustrative Example 4: PLS Feature Extraction Pre-Processing PCA is a common tool for feature extraction prior to modeling but is unsupervised . Partial Least Squares (PLS) is essentially a supervised version of PCA. For some data sets, there may be some benefit to using PLS to generate new features from the original data (the PLS scores) then use those as an input into a different predictive model. PLS requires parameter tuning. In the example below, we use PLS on a data set with highly correlated predictors then use the PLS scores in a random forest model. The trick here is to save the PLS loadings along with the random forest model fit so that the loadings can be used on future samples for prediction. Also, the PLS and random forest models are jointly tuned instead of an initial modeling process that finalizes the PLS model, then builds the random forest model separately. In this was we optimize both at once. Another important point is that the resampling results reflect the variability in the random forest and PLS models. If we did PLS up-front then resampled the random forest model, we would under-estimate the noise in the modeling process. The tecator spectroscopy data are used: Here is the model code: We fit the models and look at the resampling results for the joint model: The test set results indicate that these data like the linear model more than anything: \"\\n',\n",
       " '\"caret_13_using_your_own_model_in_train 13 Using Your Own Model in train using-your-own-model-in-train.html  13.9 Illustrative Example 6: Offsets in Generalized Linear Models Like the mboost example above , a custom method is required since a formula element is used to set the offset variable. Here is an example from ?glm : We can write a small custom method to duplicate this model. Two details of note: If we have factors in the data and do not want train to convert them to dummy variables, the formula method for train should be avoided. We can let glm do that inside the custom method. This would help glm understand that the dummy variable columns came from the same original factor. This will avoid errors in other functions used with glm (e.g. anova ). The slot for x should include any variables that are on the right-hand side of the model formula, including the offset column. Here is the custom model: \"\\n',\n",
       " '\"caret_15_variable_importance 15 Variable Importance variable-importance.html  15.2 Model Independent Metrics If there is no model-specific way to estimate importance (or the argument useModel  FALSE is used in varImp ) the importance of each predictor is evaluated individually using a filter approach. For classification, ROC curve analysis is conducted on each predictor. For two class problems, a series of cutoffs is applied to the predictor data to predict the class. The sensitivity and specificity are computed for each cutoff and the ROC curve is computed. The trapezoidal rule is used to compute the area under the ROC curve. This area is used as the measure of variable importance. For multi-class outcomes, the problem is decomposed into all pair-wise problems and the area under the curve is calculated for each class pair (i.e.\\xa0class 1 vs.\\xa0class 2, class 2 vs.\\xa0class 3 etc.). For a specific class, the maximum area under the curve across the relevant pair-wise AUCs is used as the variable importance measure. For regression, the relationship between each predictor and the outcome is evaluated. An argument, nonpara , is used to pick the model fitting technique. When nonpara  FALSE , a linear model is fit and the absolute value of the t -value for the slope of the predictor is used. Otherwise, a loess smoother is fit between the outcome and the predictor. The R 2 statistic is calculated for this model against the intercept only null model. This number is returned as a relative measure of variable importance. \"\\n',\n",
       " '\"caret_15_variable_importance 15 Variable Importance variable-importance.html  15.3 An Example On the model training web, several models were fit to the example data. The boosted tree model has a built-in variable importance score but neither the support vector machine or the regularized discriminant analysis model do. The function automatically scales the importance scores to be between 0 and 100. Using scale  FALSE avoids this normalization step. To get the area under the ROC curve for each predictor, the filterVarImp function can be used. The area under the ROC curve is computed for each class. Alternatively, for models where no built-in importance score is implemented (or exists), the varImp can still be used to get scores. For SVM classification models, the default behavior is to compute the area under the ROC curve. For importance scores generated from varImp.train , a plot method can be used to visualize the results. In the plot below, the top option is used to make the image more readable. \"\\n',\n",
       " '\"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.1 Yet Another k -Nearest Neighbor Function knn3 is a function for k -nearest neighbor classification. This particular implementation is a modification of the knn C code and returns the vote information for all of the classes ( knn only returns the probability for the winning class). There is a formula interface via There are also print and predict methods. For the Sonar data in the mlbench package, we can fit an 11-nearest neighbor model: Similarly, caret contains a k -nearest neighbor regression function, knnreg . It returns the average outcome for the neighbor. \"\\n',\n",
       " '\"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.3 Bagged MARS and FDA Multivariate adaptive regression splines (MARS) models, like classification/regression trees, are unstable predictors (Breiman, 1996). This means that small perturbations in the training data might lead to significantly different models. Bagged trees and random forests are effective ways of improving tree models by exploiting these instabilities. caret contains a function, bagEarth , that fits MARS models via the earth function. There are formula and non-formula interfaces. Also, flexible discriminant analysis is a generalization of linear discriminant analysis that can use non-linear features as inputs. One way of doing this is the use MARS-type features to classify samples. The function bagFDA fits FDA models of a set of bootstrap samples and aggregates the predictions to reduce noise. This function is deprecated in favor of the bag function. \"\\n',\n",
       " '\"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.4 Bagging The bag function offers a general platform for bagging classification and regression models. Like rfe and sbf , it is open and models are specified by declaring functions for the model fitting and prediction code (and several built-in sets of functions exist in the package). The function bagControl has options to specify the functions (more details below). The function also has a few non-standard features: The argument var can enable random sampling of the predictors at each bagging iteration. This is to de-correlate the bagged models in the same spirit of random forests (although here the sampling is done once for the whole model). The default is to use all the predictors for each model. The bagControl function has a logical argument called downSample that is useful for classification models with severe class imbalance. The bootstrapped data set is reduced so that the sample sizes for the classes with larger frequencies are the same as the sample size for the minority class. If a parallel backend for the foreach package has been loaded and registered, the bagged models can be trained in parallel. The functions control function requires the following arguments: 16.4.1 The fit Function Inputs: x : a data frame of the training set predictor data. y : the training set outcomes. ... arguments passed from train to this function The output is the object corresponding to the trained model and any other objects required for prediction. A simple example for a linear discriminant analysis model from the MASS package is: 16.4.2 The pred Function This should be a function that produces predictors for new samples. Inputs: object : the object generated by the fit module. x : a matrix or data frame of predictor data. The output is either a number vector (for regression), a factor (or character) vector for classification or a matrix/data frame of class probabilities. For classification, it is probably better to average class probabilities instead of using the votes of the class predictions. Using the lda example again: 16.4.3 The aggregate Function This should be a function that takes the predictions from the constituent models and converts them to a single prediction per sample. Inputs: x : a list of objects returned by the pred module. type : an optional string that describes the type of output (e.g. class, prob etc.). The output is either a number vector (for regression), a factor (or character) vector for classification or a matrix/data frame of class probabilities. For the linear discriminant model above, we saved the matrix of class probabilities. To average them and generate a class prediction, we could use: For example, to bag a conditional inference tree (from the party package): \"\\n',\n",
       " '\"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.5 Model Averaged Neural Networks The avNNet fits multiple neural network models to the same data set and predicts using the average of the predictions coming from each constituent model. The models can be different either due to different random number seeds to initialize the network or by fitting the models on bootstrap samples of the original training set (i.e.\\xa0bagging the neural network). For classification models, the class probabilities are averaged to produce the final class prediction (as opposed to voting from the individual class predictions. As an example, the model can be fit via train : \"\\n',\n",
       " '\"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.6 Neural Networks with a Principal Component Step Neural networks can be affected by severe amounts of multicollinearity in the predictors. The function pcaNNet is a wrapper around the preProcess and nnet functions that will run principal component analysis on the predictors before using them as inputs into a neural network. The function will keep enough components that will capture some pre-defined threshold on the cumulative proportion of variance (see the thresh argument). For new samples, the same transformation is applied to the new predictor values (based on the loadings from the training set). The function is available for both regression and classification. This function is deprecated in favor of the train function using method  \"nnet\" and preProc  \"pca\" . \"\\n',\n",
       " '\"caret_16_miscellaneous_model_functions 16 Miscellaneous Model Functions miscellaneous-model-functions.html  16.7 Independent Component Regression The icr function can be used to fit a model analogous to principal component regression (PCR), but using independent component analysis (ICA). The predictor data are centered and projected to the ICA components. These components are then regressed against the outcome. The user needed to specify the number of components to keep. The model uses the preProcess function to compute the latent variables using the fastICA package. Like PCR, there is no guarantee that there will be a correlation between the new latent variable and the outcomes. \"\\n',\n",
       " '\"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.1 Measures for Regression The function postResample can be used to estimate the root mean squared error (RMSE), simple R 2 , and the mean absolute error (MAE) for numeric outcomes. For example: A note about how R 2 is calculated by caret : it takes the straightforward approach of computing the correlation between the observed and predicted values (i.e.\\xa0R) and squaring the value. When the model is poor, this can lead to differences between this estimator and the more widely known estimate derived form linear regression models. Mostly notably, the correlation approach will not generate negative values of R 2 (which are theoretically invalid). A comparison of these and other estimators can be found in Kvalseth 1985 . \"\\n',\n",
       " '\"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.2 Measures for Predicted Classes Before proceeding, lets make up some test set data: We would expect that this model will do well on these data: Generating the predicted classes based on the typical 50% cutoff for the probabilities, we can compute the confusion matrix , which shows a cross-tabulation of the observed and predicted classes. The confusionMatrix function can be used to generate these results: For two classes, this function assumes that the class corresponding to an event is the first class level (but this can be changed using the positive argument. Note that there are a number of statistics shown here. The no-information rate is the largest proportion of the observed classes (there were more class 2 data than class 1 in this test set). A hypothesis test is also computed to evaluate whether the overall accuracy rate is greater than the rate of the largest class. Also, the prevalence of the positive event is computed from the data (unless passed in as an argument), the detection rate (the rate of true events also predicted to be events) and the detection prevalence (the prevalence of predicted events). If the prevalence of the event is different than those seen in the test set, the prevalence option can be used to adjust this. Suppose a 2x2 table: When there are three or more classes, confusionMatrix will show the confusion matrix and a set of one-versus-all results. For example, in a three class problem, the sensitivity of the first class is calculated against all the samples in the second and third classes (and so on). The confusionMatrix matrix frames the errors in terms of sensitivity and specificity. In the case of information retrieval, the precision and recall might be more appropriate. In this case, the option mode can be used to get those statistics: Again, the positive argument can be used to control which factor level is associated with a found or important document or sample. There are individual functions called sensitivity , specificity , posPredValue , negPredValue , precision , recall , and F_meas . Also, a resampled estimate of the training set can also be obtained using confusionMatrix.train . For each resampling iteration, a confusion matrix is created from the hold-out samples and these values can be aggregated to diagnose issues with the model fit. These values are the percentages that hold-out samples landed in the confusion matrix during resampling. There are several methods for normalizing these values. See ?confusionMatrix.train for details. The default performance function used by train is postResample , which generates the accuracy and Kappa statistics: As shown below, another function called twoClassSummary can be used to get the sensitivity and specificity using the default probability cutoff. Another function, multiClassSummary , can do similar calculations when there are three or more classes but both require class probabilities for each class. \"\\n',\n",
       " '\"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.3 Measures for Class Probabilities For data with two classes, there are specialized functions for measuring model performance. First, the twoClassSummary function computes the area under the ROC curve and the specificity and sensitivity under the 50% cutoff. Note that: this function uses the first class level to define the event of interest. To change this, use the lev option to the function there must be columns in the data for each of the class probabilities (named the same as the outcomes class levels) A similar function can be used to get the analugous precision-recall values and the area under the precision-recall curve: This function requires that the MLmetrics package is installed. For multi-class problems, there are additional functions that can be used to calculate performance. One, mnLogLoss computes the negative of the multinomial log-likelihood (smaller is better) based on the class probabilities. This can be used to optimize tuning parameters but can lead to results that are inconsistent with other measures (e.g.\\xa0accuracy or the area under the ROC curve), especially when the other measures are near their best possible values. The function has similar arguments to the other functions described above. Here is the two-class data from above: Additionally, the function multiClassSummary computes a number of relevant metrics: the overall accuracy and Kappa statistics using the predicted classes the negative of the multinomial log loss (if class probabilities are available) averages of the one versus all statistics such as sensitivity, specificity, the area under the ROC curve, etc. \"\\n',\n",
       " '\"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.4 Lift Curves The lift function can be used to evaluate probabilities thresholds that can capture a certain percentage of hits. The function requires a set of sample probability predictions (not from the training set) and the true class labels. For example, we can simulate two-class samples using the twoClassSim function and fit a set of models to the training set: The lift function does the calculations and the corresponding plot function is used to plot the lift curve (although some call this the gain curve ). The value argument creates reference lines: There is also a ggplot method for lift objects: From this we can see that, to find 60 percent of the hits, a little more than 30 percent of the data can be sampled (when ordered by the probability predictions). The LDA model does somewhat worse than the other two models. \"\\n',\n",
       " '\"caret_17_measuring_performance 17 Measuring Performance measuring-performance.html  17.5 Calibration Curves Calibration curves can be used to characterisze how consistent the predicted class probabilities are with the observed event rates. Other functions in the gbm package, the rms package (and others) can also produce calibrartion curves. The format for the function is very similar to the lift function: There is also a ggplot method that shows the confidence intervals for the proportions inside of the subsets: \"\\n',\n",
       " '\"caret_18_feature_selection_overview 18 Feature Selection Overview feature-selection-overview.html  18.1 Models with Built-In Feature Selection Many models that can be accessed using caret s train function produce prediction equations that do not necessarily use all the predictors. These models are thought to have built-in feature selection: ada , AdaBag , AdaBoost.M1 , adaboost , bagEarth , bagEarthGCV , bagFDA , bagFDAGCV , bartMachine , blasso , BstLm , bstSm , C5.0 , C5.0Cost , C5.0Rules , C5.0Tree , cforest , chaid , ctree , ctree2 , cubist , deepboost , earth , enet , evtree , extraTrees , fda , gamboost , gbm_h2o , gbm , gcvEarth , glmnet_h2o , glmnet , glmStepAIC , J48 , JRip , lars , lars2 , lasso , LMT , LogitBoost , M5 , M5Rules , msaenet , nodeHarvest , OneR , ordinalNet , ordinalRF , ORFlog , ORFpls , ORFridge , ORFsvm , pam , parRF , PART , penalized , PenalizedLDA , qrf , ranger , Rborist , relaxo , rf , rFerns , rfRules , rotationForest , rotationForestCp , rpart , rpart1SE , rpart2 , rpartCost , rpartScore , rqlasso , rqnc , RRF , RRFglobal , sdwd , smda , sparseLDA , spikeslab , wsrf , xgbDART , xgbLinear , xgbTree . Many of the functions have an ancillary method called predictors that returns a vector indicating which predictors were used in the final model. In many cases, using these models with built-in feature selection will be more efficient than algorithms where the search routine for the right predictors is external to the model. Built-in feature selection typically couples the predictor search algorithm with the parameter estimation and are usually optimized with a single objective function (e.g.\\xa0error rates or likelihood). \"\\n',\n",
       " '\"caret_18_feature_selection_overview 18 Feature Selection Overview feature-selection-overview.html  18.2 Feature Selection Methods Apart from models with built-in feature selection, most approaches for reducing the number of predictors can be placed into two main categories. Using the terminology of John, Kohavi, and Pfleger (1994) : Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. caret has wrapper methods based on recursive feature elimination , genetic algorithms , and simulated annealing . Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model. Saeys, Inza, and Larranaga (2007) surveys filter methods. caret has a general framework for using univariate filters . Both approaches have advantages and drawbacks. Filter methods are usually more computationally efficient than wrapper methods, but the selection criterion is not directly related to the effectiveness of the model. Also, most filter methods evaluate each predictor separately and, consequently, redundant (i.e.\\xa0highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified. The downside of the wrapper method is that many models are evaluated (which may also require parameter tuning) and thus an increase in computation time. There is also an increased risk of over-fitting with wrappers. \"\\n',\n",
       " '\"caret_18_feature_selection_overview 18 Feature Selection Overview feature-selection-overview.html  18.3 External Validation It is important to realize that feature selection is part of the model building process and, as such, should be externally validated. Just as parameter tuning can result in over-fitting, feature selection can over-fit to the predictors (especially when search wrappers are used). In each of the caret functions for feature selection, the selection process is included in any resampling loops. See See Ambroise and McLachlan (2002) for a demonstration of this issue. \"\\n',\n",
       " '\"caret_19_feature_selection_using_univariate_filters 19 Feature Selection using Univariate Filters feature-selection-using-univariate-filters.html  19.1 Univariate Filters Another approach to feature selection is to pre-screen the predictors using simple univariate statistical methods then only use those that pass some criterion in the subsequent model steps. Similar to recursive selection, cross-validation of the subsequent models will be biased as the remaining predictors have already been evaluate on the data set. Proper performance estimates via resampling should include the feature selection step. As an example, it has been suggested for classification models, that predictors can be filtered by conducting some sort of k -sample test (where k is the number of classes) to see if the mean of the predictor is different between the classes. Wilcoxon tests, t -tests and ANOVA models are sometimes used. Predictors that have statistically significant differences between the classes are then used for modeling. The caret function sbf (for selection by filter) can be used to cross-validate such feature selection schemes. Similar to rfe , functions can be passed into sbf for the computational components: univariate filtering, model fitting, prediction and performance summaries (details are given below). The function is applied to the entire training set and also to different resampled versions of the data set. From this, generalizable estimates of performance can be computed that properly take into account the feature selection step. Also, the results of the predictor filters can be tracked over resamples to understand the uncertainty in the filtering. \"\\n',\n",
       " '\"caret_19_feature_selection_using_univariate_filters 19 Feature Selection using Univariate Filters feature-selection-using-univariate-filters.html  19.2 Basic Syntax Similar to the rfe function, the syntax for sbf is: In this case, the details are specificed using the sbfControl function. Here, the argument functions dictates what the different components should do. This argument should have elements called filter , fit , pred and summary . 19.2.1 The score Function This function takes as inputs the predictors and the outcome in objects called x and y , respectively. By default, each predictor in x is passed to the score function individually. In this case, the function should return a single score. Alternatively, all the predictors can be exposed to the function using the multivariate argument to sbfControl . In this case, the output should be a named vector of scores where the names correspond to the column names of x . There are two built-in functions called anovaScores and gamScores . anovaScores treats the outcome as the independent variable and the predictor as the outcome. In this way, the null hypothesis is that the mean predictor values are equal across the different classes. For regression, gamScores fits a smoothing spline in the predictor to the outcome using a generalized additive model and tests to see if there is any functional relationship between the two. In each function the p-value is used as the score. 19.2.2 The filter Function This function takes as inputs the scores coming out of the score function (in an argument called score ). The function also has the training set data as inputs (arguments are called x and y ). The output should be a named logical vector where the names correspond to the column names of x . Columns with values of TRUE will be used in the subsequent model. 19.2.3 The fit Function The component is very similar to the rfe -specific function described above. For sbf , there are no first or last arguments. The function should have arguments x , y and ... . The data within x have been filtered using the filter function described above. The output of the fit function should be a fitted model. With some data sets, no predictors will survive the filter. In these cases, a model with predictors cannot be computed, but the lack of viable predictors should not be ignored in the final results. To account for this issue, caret contains a model function called nullModel that fits a simple model that is independent of any of the predictors. For problems where the outcome is numeric, the function predicts every sample using the simple mean of the training set outcomes. For classification, the model predicts all samples using the most prevalent class in the training data. This function can be used in the fit component function to error-trap cases where no predictors are selected. For example, there are several built-in functions for some models. The object rfSBF is a set of functions that may be useful for fitting random forest models with filtering. The fit function here uses nullModel to check for cases with no predictors: 19.2.4 The summary and pred Functions The summary function is used to calculate model performance on held-out samples. The pred function is used to predict new samples using the current predictor set. The arguments and outputs for these two functions are identical to the previously discussed summary and pred functions in previously described sections. \"\\n',\n",
       " '\"caret_19_feature_selection_using_univariate_filters 19 Feature Selection using Univariate Filters feature-selection-using-univariate-filters.html  19.3 The Example Returning to the example from (Friedman, 1991), we can fit another random forest model with the predictors pre-filtered using the generalized additive model approach described previously. In this case, the training set indicated that 6 should be used in the random forest model, but the resampling results indicate that there is some variation in this number. Some of the informative predictors are used, but a few others are erroneous retained. Similar to rfe , there are methods for predictors , densityplot , histogram and varImp . \"\\n',\n",
       " '\"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.1 Backwards Selection First, the algorithm fits the model to all predictors. Each predictor is ranked using its importance to the model. Let S be a sequence of ordered numbers which are candidate values for the number of predictors to retain ( S 1 > S 2 , ). At each iteration of feature selection, the S i top ranked predictors are retained, the model is refit and performance is assessed. The value of S i with the best performance is determined and the top S i predictors are used to fit the final model. Algorithm 1 has a more complete definition. The algorithm has an optional step (line 1.9) where the predictor rankings are recomputed on the model on the reduced feature set. Svetnik et al (2004) showed that, for random forest models, there was a decrease in performance when the rankings were re-computed at every step. However, in other cases when the initial rankings are not good (e.g.\\xa0linear models with highly collinear predictors), re-calculation can slightly improve performance. One potential issue over-fitting to the predictor set such that the wrapper procedure could focus on nuances of the training data that are not found in future samples (i.e.\\xa0over-fitting to predictors and samples). For example, suppose a very large number of uninformative predictors were collected and one such predictor randomly correlated with the outcome. The RFE algorithm would give a good rank to this variable and the prediction error (on the same data set) would be lowered. It would take a different test/validation to find out that this predictor was uninformative. The was referred to as selection bias by Ambroise and McLachlan (2002) . In the current RFE algorithm, the training data is being used for at least three purposes: predictor selection, model fitting and performance evaluation. Unless the number of samples is large, especially in relation to the number of variables, one static training set may not be able to fulfill these needs. \"\\n',\n",
       " '\"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.2 Resampling and External Validation Since feature selection is part of the model building process, resampling methods (e.g.\\xa0cross-validation, the bootstrap) should factor in the variability caused by feature selection when calculating performance. For example, the RFE procedure in Algorithm 1 can estimate the model performance on line 1.7, which during the selection process. Ambroise and McLachlan (2002) and Svetnik et al (2004) showed that improper use of resampling to measure performance will result in models that perform poorly on new samples. To get performance estimates that incorporate the variation due to feature selection, it is suggested that the steps in Algorithm 1 be encapsulated inside an outer layer of resampling (e.g.\\xa010-fold cross-validation). Algorithm 2 shows a version of the algorithm that uses resampling. While this will provide better estimates of performance, it is more computationally burdensome. For users with access to machines with multiple processors, the first For loop in Algorithm 2 (line 2.1) can be easily parallelized. Another complication to using resampling is that multiple lists of the best predictors are generated at each iteration. At first this may seem like a disadvantage, but it does provide a more probabilistic assessment of predictor importance than a ranking based on a single fixed data set. At the end of the algorithm, a consensus ranking can be used to determine the best predictors to retain. \"\\n',\n",
       " '\"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.3 Recursive Feature Elimination via caret In caret , Algorithm 1 is implemented by the function rfeIter . The resampling-based Algorithm 2 is in the rfe function. Given the potential selection bias issues, this document focuses on rfe . There are several arguments: x , a matrix or data frame of predictor variables y , a vector (numeric or factor) of outcomes sizes , a integer vector for the specific subset sizes that should be tested (which need not to include ncol(x) ) rfeControl , a list of options that can be used to specify the model and the methods for prediction, ranking etc. For a specific model, a set of functions must be specified in rfeControl$functions . Sections below has descriptions of these sub-functions. There are a number of pre-defined sets of functions for several models, including: linear regression (in the object lmFuncs ), random forests ( rfFuncs ), naive Bayes ( nbFuncs ), bagged trees ( treebagFuncs ) and functions that can be used with caret s train function ( caretFuncs ). The latter is useful if the model has tuning parameters that must be determined at each iteration. \"\\n',\n",
       " '\"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.5 Helper Functions To use feature elimination for an arbitrary model, a set of functions must be passed to rfe for each of the steps in Algorithm 2. This section defines those functions and uses the existing random forest functions as an illustrative example. caret contains a list called rfFuncs , but this document will use a more simple version that will be better for illustrating the ideas. A set of simplified functions used here and called rfRFE . 20.5.1 The summary Function The summary function takes the observed and predicted values and computes one or more performance metrics (see line 2.14). The input is a data frame with columns obs and pred . The output should be a named vector of numeric variables. Note that the metric argument of the rfe function should reference one of the names of the output of summary . The example function is: Two functions in caret that can be used as the summary funciton are defaultSummary and twoClassSummary (for classification problems with two classes). 20.5.2 The fit Function This function builds the model based on the current data set (lines 2.3, 2.9 and 2.17). The arguments for the function must be: x : the current training set of predictor data with the appropriate subset of variables y : the current outcome data (either a numeric or factor vector) first : a single logical value for whether the current predictor set has all possible variables (e.g.\\xa0line 2.3) last : similar to first , but TRUE when the last model is fit with the final subset size and predictors. (line 2.17) ... : optional arguments to pass to the fit function in the call to rfe The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: For feature selection without re-ranking at each iteration, the random forest variable importances only need to be computed on the first iterations when all of the predictors are in the model. This can be accomplished using importance``  first . 20.5.3 The pred Function This function returns a vector of predictions (numeric or factors) from the current model (lines 2.4 and 2.10). The input arguments must be object : the model generated by the fit function x : the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 20.5.4 The rank Function This function is used to return the predictors in the order of the most important to the least important (lines 2.5 and 2.11). Inputs are: object : the model generated by the fit function x : the current set of predictor set for the training samples y : the current training outcomes The function should return a data frame with a column called var that has the current variable names. The first row should be the most important predictor etc. Other columns can be included in the output and will be returned in the final rfe object. For random forests, the function below uses caret s varImp function to extract the random forest importances and orders them. For classification, randomForest will produce a column of importances for each class. In this case, the default ranking function orders the predictors by the averages importance across the classes. 20.5.5 The selectSize Function This function determines the optimal number of predictors based on the resampling output (line 2.15). Inputs for the function are: x : a matrix with columns for the performance metrics and the number of variables, called Variables metric : a character string of the performance measure to optimize (e.g.\\xa0RMSE, Accuracy) maximize : a single logical for whether the metric should be maximized This function should return an integer corresponding to the optimal subset size. caret comes with two examples functions for this purpose: pickSizeBest and pickSizeTolerance . The former simply selects the subset size that has the best value. The latter takes into account the whole profile and tries to pick a subset size that is small without sacrificing too much performance. For example, suppose we have computed the RMSE over a series of variables sizes: These are depicted in the figure below. The solid circle identifies the subset size with the absolute smallest RMSE. However, there are many smaller subsets that produce approximately the same performance but with fewer predictors. In this case, we might be able to accept a slightly larger error for less predictors. The pickSizeTolerance determines the absolute best value then the percent difference of the other points to this value. In the case of RMSE, this would be where RMSE {opt} is the absolute best error rate. These tolerance values are plotted in the bottom panel. The solid triangle is the smallest subset size that is within 10% of the optimal value. This approach can produce good results for many of the tree based models, such as random forest, where there is a plateau of good performance for larger subset sizes. For trees, this is usually because unimportant variables are infrequently used in splits and do not significantly affect performance. 20.5.6 The selectVar Function After the optimal subset size is determined, this function will be used to calculate the best rankings for each variable across all the resampling iterations (line 2.16). Inputs for the function are: y : a list of variables importance for each resampling iteration and each subset size (generated by the user-defined rank function). In the example, each each of the cross-validation groups the output of the rank function is saved for each of the 10 subset sizes (including the original subset). If the rankings are not recomputed at each iteration, the values will be the same within each cross-validation iteration. size : the integer returned by the selectSize function This function should return a character string of predictor names (of length size ) in the order of most important to least important For random forests, only the first importance calculation (line 2.5) is used since these are the rankings on the full set of predictors. These importances are averaged and the top predictors are returned. Note that if the predictor rankings are recomputed at each iteration (line 2.11) the user will need to write their own selection function to use the other ranks. \"\\n',\n",
       " '\"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.6 The Example For random forest, we fit the same series of model sizes as the linear model. The option to save all the resampling results across subset sizes was changed for this model and are used to show the lattice plot function capabilities in the figures below. The resampling profile can be visualized along with plots of the individual resampling results: \"\\n',\n",
       " '\"caret_20_recursive_feature_elimination 20 Recursive Feature Elimination recursive-feature-elimination.html  20.7 Using a Recipe A recipe can be used to specify the model terms and any preprocessing that may be needed. Instead of using an existing recipe can be used along with a data frame containing the predictors and outcome: The recipe is prepped within each resample in the same manner that train executes the preProc option. However, since a recipe can do a variety of different operations, there are some potentially complicating factors. The main pitfall is that the recipe can involve the creation and deletion of predictors. There are a number of steps that can reduce the number of predictors, such as the ones for pooling factors into an other category, PCA signal extraction, as well as filters for near-zero variance predictors and highly correlated predictors. For this reason, it may be difficult to know how many predictors are available for the full model. Also, this number will likely vary between iterations of resampling. To illustrate, lets use the blood-brain barrier data where there is a high degree of correlation between the predictors. A simple recipe could be Originally, there are 134 predictors and, for the entire data set, the processed version has: When calling rfe , lets start the maximum subset size at 28: What was the distribution of the maximum number of terms: So 28ish. Suppose that we used sizes  2:ncol(bbbDescr) when calling rfe . A warning is issued that: \"\\n',\n",
       " '\"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.1 Genetic Algorithms Genetic algorithms (GAs) mimic Darwinian forces of natural selection to find optimal values of some function ( Mitchell, 1998 ). An initial set of candidate solutions are created and their corresponding fitness values are calculated (where larger values are better). This set of solutions is referred to as a population and each solution as an individual . The individuals with the best fitness values are combined randomly to produce offsprings which make up the next population. To do so, individual are selected and undergo cross-over (mimicking genetic reproduction) and also are subject to random mutations. This process is repeated again and again and many generations are produced (i.e.\\xa0iterations of the search procedure) that should create better and better solutions. For feature selection, the individuals are subsets of predictors that are encoded as binary; a feature is either included or not in the subset. The fitness values are some measure of model performance, such as the RMSE or classification accuracy. One issue with using GAs for feature selection is that the optimization process can be very aggressive and their is potential for the GA to overfit to the predictors (much like the previous discussion for RFE). \"\\n',\n",
       " '\"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.2 Internal and External Performance Estimates The genetic algorithm code in caret conducts the search of the feature space repeatedly within resampling iterations. First, the training data are split be whatever resampling method was specified in the control function. For example, if 10-fold cross-validation is selected, the entire genetic algorithm is conducted 10 separate times. For the first fold, nine tenths of the data are used in the search while the remaining tenth is used to estimate the external performance since these data points were not used in the search. During the genetic algorithm, a measure of fitness is needed to guide the search. This is the internal measure of performance. During the search, the data that are available are the instances selected by the top-level resampling (e.g.\\xa0the nine tenths mentioned above). A common approach is to conduct another resampling procedure. Another option is to use a holdout set of samples to determine the internal estimate of performance (see the holdout argument of the control function). While this is faster, it is more likely to cause overfitting of the features and should only be used when a large amount of training data are available. Yet another idea is to use a penalized metric (such as the AIC statistic) but this may not exist for some metrics (e.g.\\xa0the area under the ROC curve). The internal estimates of performance will eventually overfit the subsets to the data. However, since the external estimate is not used by the search, it is able to make better assessments of overfitting. After resampling, this function determines the optimal number of generations for the GA. Finally, the entire data set is used in the last execution of the genetic algorithm search and the final model is built on the predictor subset that is associated with the optimal number of generations determined by resampling (although the update function can be used to manually set the number of generations). \"\\n',\n",
       " '\"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.3 Basic Syntax The most basic usage of the function is: where x : a data frame or matrix of predictor values y : a factor or numeric vector of outcomes iters : the number of generations for the GA This isnt very specific. All of the action is in the control function. That can be used to specify the model to be fit, how predictions are made and summarized as well as the genetic operations. Suppose that we want to fit a linear regression model. To do this, we can use train as an interface and pass arguments to that function through gafs : Other options, such as preProcess , can be passed in as well. Some important options to gafsControl are: method , number , repeats , index , indexOut , etc: options similar to those for train top control resampling. metric : this is similar to train s option but, in this case, the value should be a named vector with values for the internal and external metrics. If none are specified, the first value returned by the summary functions (see details below) are used and a warning is issued. A similar two-element vector for the option maximize is also required. See the last example here for an illustration. holdout : this is a number between [0, 1) that can be used to hold out samples for computing the internal fitness value. Note that this is independent of the external resampling step. Suppose 10-fold CV is being used. Within a resampling iteration, holdout can be used to sample an additional proportion of the 90% resampled data to use for estimating fitness. This may not be a good idea unless you have a very large training set and want to avoid an internal resampling procedure to estimate fitness. allowParallel and genParallel : these are logicals to control where parallel processing should be used (if at all). The former will parallelize the external resampling while the latter parallelizes the fitness calculations within a generation. allowParallel will almost always be more advantageous. There are a few built-in sets of functions to use with gafs : caretGA , rfGA , and treebagGA . The first is a simple interface to train . When using this, as shown above, arguments can be passed to train using the ... structure and the resampling estimates of performance can be used as the internal fitness value. The functions provided by rfGA and treebagGA avoid using train and their internal estimates of fitness come from using the out-of-bag estimates generated from the model. The GA implementation in caret uses the underlying code from the GA package ( Scrucca, 2013 ). \"\\n',\n",
       " '\"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.4 Genetic Algorithm Example Using the example from the previous page where there are five real predictors and 40 noise predictors: Well fit a random forest model and use the out-of-bag RMSE estimate as the internal performance metric and use the same repeated 10-fold cross-validation process used with the search. To do this, well use the built-in rfGA object for this purpose. The default GA operators will be used and conduct 200 generations of the algorithm. With 5 repeats of 10-fold cross-validation, the GA was executed 50 times. The average external performance is calculated across resamples and these results are used to determine the optimal number of iterations for the final GA to avoid over-fitting. Across the resamples, an average of 9.3 predictors were selected at the end of each of the algorithms. The plot function is used to monitor the average of the internal out-of-bag RMSE estimates as well as the average of the external performance estimates calculated from the 50 out-of-sample predictions. By default, this function uses ggplot2 package. A black and white theme can be added to the output object: Based on these results, the generation associated with the best external RMSE estimate was 2.81. Using the entire training set, the final GA is conducted and, at generation 195, there were 12 that were selected: real1, real2, real3, real4, real5, bogus3, bogus5, bogus7, bogus8, bogus14, bogus17, bogus29. The random forest model with these predictors is created using the entire training set is trained and this is the model that is used when predict.gafs is executed. Note: the correlation between the internal and external fitness values is somewhat atypical for most real-world problems. This is a function of the nature of the simulations (a small number of uncorrelated informative predictors) and that the OOB error estimate from random forest is a product of hundreds of trees. Your mileage may vary. \"\\n',\n",
       " '\"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.6 The Example Revisited The previous GA included some of the non-informative predictors. We can cheat a little and try to bias the search to get the right solution. We can try to encourage the algorithm to choose fewer predictors, we can penalize the the RMSE estimate. Normally, a metric like the Akaike information criterion (AIC) statistic would be used. However, with a random forest model, there is no real notion of model degrees of freedom. As an alternative, we can use desirability functions to penalize the RMSE. To do this, two functions are created that translate the number of predictors and the RMSE values to a measure of desirability. For the number of predictors, the most desirable property would be a single predictor and the worst situation would be if the model required all 50 predictors. That desirability function is visualized as: For the RMSE, the best case would be zero. Many poor models have values around four. To give the RMSE value more weight in the overall desirability calculation, we use a scale parameter value of 2. This desirability function is: To use the overall desirability to drive the feature selection, the internal function requires replacement. We make a copy of rfGA and add code using the desirability package and the function returns the estimated RMSE and the overall desirability. The gafsControl function also need changes. The metric argument needs to reflect that the overall desirability score should be maximized internally but the RMSE estimate should be minimized externally. Here are the RMSE values for this search: The final GA found 6 that were selected: real1, real2, real3, real4, real5, bogus43. During resampling, the average number of predictors selected was 5.2, indicating that the penalty on the number of predictors was effective. \"\\n',\n",
       " '\"caret_21_feature_selection_using_genetic_algorithms 21 Feature Selection using Genetic Algorithms feature-selection-using-genetic-algorithms.html  21.7 Using Recipes Like the other feature selection routines, gafs can take a data recipe as an input. This is advantageous when your data needs preprocessing before the model, such as: creation of dummy variables from factors specification of interactions missing data imputation more complex feature engineering methods Like train , the recipes preprocessing steps are calculated within each resample. This makes sure that the resampling statistics capture the variation and effect that the preprocessing has on the model. As an example, the Ames housing data is used. These data contain a number of categorical predictors that require conversion to indicators as well as other variables that require processing. To load (and split) the data: Here is a recipe that does differetn types of preprocssing on the predictor set: If this were executed on the training set, it would produce 280 predictor columns out of the original 79. Lets tune some linear models with gafs and, for the sake of computational time, only use 10 generations of the algorithm: \"\\n',\n",
       " '\"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.1 Simulated Annealing Simulated annealing (SA) is a global search method that makes small random changes (i.e.\\xa0perturbations) to an initial candidate solution. If the performance value for the perturbed value is better than the previous solution, the new solution is accepted. If not, an acceptance probability is determined based on the difference between the two performance values and the current iteration of the search. From this, a sub-optimal solution can be accepted on the off-change that it may eventually produce a better solution in subsequent iterations. See Kirkpatrick (1984) or Rutenbar (1989) for better descriptions. In the context of feature selection, a solution is a binary vector that describes the current subset. The subset is perturbed by randomly changing a small number of members in the subset. \"\\n',\n",
       " '\"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.2 Internal and External Performance Estimates Much of the discussion on this subject in the genetic algorithm page is relevant here, although SA search is less aggressive than GA search. In any case, the implementation here conducts the SA search inside the resampling loops and uses an external performance estimate to choose how many iterations of the search are appropriate. \"\\n',\n",
       " '\"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.3 Basic Syntax The syntax of this function is very similar to the previous information for genetic algorithm searches. The most basic usage of the function is: where x : a data frame or matrix of predictor values y : a factor or numeric vector of outcomes iters : the number of iterations for the SA This isnt very specific. All of the action is in the control function. That can be used to specify the model to be fit, how predictions are made and summarized as well as the genetic operations. Suppose that we want to fit a linear regression model. To do this, we can use train as an interface and pass arguments to that function through safs : Other options, such as preProcess , can be passed in as well. Some important options to safsControl are: method , number , repeats , index , indexOut , etc: options similar to those for train top control resampling. metric : this is similar to train s option but, in this case, the value should be a named vector with values for the internal and external metrics. If none are specified, the first value returned by the summary functions (see details below) are used and a warning is issued. A similar two-element vector for the option maximize is also required. See the last example here for an illustration. holdout : this is a number between [0, 1) that can be used to hold out samples for computing the internal fitness value. Note that this is independent of the external resampling step. Suppose 10-fold CV is being used. Within a resampling iteration, holdout can be used to sample an additional proportion of the 90% resampled data to use for estimating fitness. This may not be a good idea unless you have a very large training set and want to avoid an internal resampling procedure to estimate fitness. improve : an integer (or infinity) defining how many iterations should pass without an improvement in fitness before the current subset is reset to the last known improvement. allowParallel : should the external resampling loop be run in parallel?. There are a few built-in sets of functions to use with safs : caretSA , rfSA , and treebagSA . The first is a simple interface to train . When using this, as shown above, arguments can be passed to train using the ... structure and the resampling estimates of performance can be used as the internal fitness value. The functions provided by rfSA and treebagSA avoid using train and their internal estimates of fitness come from using the out-of-bag estimates generated from the model. \"\\n',\n",
       " '\"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.4 Simulated Annealing Example Using the example from the previous page where there are five real predictors and 40 noise predictors. Well fit a random forest model and use the out-of-bag RMSE estimate as the internal performance metric and use the same repeated 10-fold cross-validation process used with the search. To do this, well use the built-in rfSA object for this purpose. The default SA operators will be used with 1000 iterations of the algorithm. As with the GA, we can plot the internal and external performance over iterations. The performance here isnt as good as the previous GA or RFE solutions. Based on these results, the iteration associated with the best external RMSE estimate was 212 with a corresponding RMSE estimate of 3.31. Using the entire training set, the final SA is conducted and, at iteration 212, there were 21 selected: real1, real2, real5, bogus1, bogus3, bogus9, bogus10, bogus13, bogus14, bogus15, bogus19, bogus20, bogus23, bogus24, bogus25, bogus26, bogus28, bogus31, bogus33, bogus38, bogus44. The random forest model with these predictors is created using the entire training set is trained and this is the model that is used when predict.safs is executed. \"\\n',\n",
       " '\"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.5 Customizing the Search 22.5.1 The fit Function This function builds the model based on a proposed current subset. The arguments for the function must be: x : the current training set of predictor data with the appropriate subset of variables y : the current outcome data (either a numeric or factor vector) lev : a character vector with the class levels (or NULL for regression problems) last : a logical that is TRUE when the final SA search is conducted on the entire data set ... : optional arguments to pass to the fit function in the call to safs The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: 22.5.2 The pred Function This function returns a vector of predictions (numeric or factors) from the current model. The input arguments must be object : the model generated by the fit function x : the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 22.5.3 The fitness_intern Function The fitness_intern function takes the fitted model and computes one or more performance metrics. The inputs to this function are: object : the model generated by the fit function x : the current set of predictor set. If the option safsControl$holdout is zero, these values will be from the current resample (i.e.\\xa0the same data used to fit the model). Otherwise, the predictor values are from the hold-out set created by safsControl$holdout . y : outcome values. See the note for the x argument to understand which data are presented to the function. maximize : a logical from safsControl that indicates whether the metric should be maximized or minimized p : the total number of possible predictors The output should be a named numeric vector of performance values. In many cases, some resampled measure of performance is used. In the example above using random forest, the OOB error was used. In other cases, the resampled performance from train can be used and, if safsControl$holdout is not zero, a static hold-out set can be used. This depends on the data and problem at hand. If left The example function for random forest is: 22.5.4 The fitness_extern Function The fitness_extern function takes the observed and predicted values form the external resampling process and computes one or more performance metrics. The input arguments are: data : a data frame or predictions generated by the fit function. For regression, the predicted values in a column called pred . For classification, pred is a factor vector. Class probabilities are usually attached as columns whose names are the class levels (see the random forest example for the fit function above) lev : a character vector with the class levels (or NULL for regression problems) The output should be a named numeric vector of performance values. The example function for random forest is: Two functions in caret that can be used as the summary function are defaultSummary and twoClassSummary (for classification problems with two classes). 22.5.5 The initial Function This function creates an initial subset. Inputs are: vars : the number of possible predictors prob : the probability that a feature is in the subset ... : not currently used The output should be a vector of integers indicating which predictors are in the initial subset. Alternatively, instead of a function, a vector of integers can be used in this slot. 22.5.6 The perturb Function This function perturbs the subset. Inputs are: x : the integers defining the current subset vars : the number of possible predictors number : the number of predictors to randomly change ... : not currently used The output should be a vector of integers indicating which predictors are in the new subset. 22.5.7 The prob Function This function computes the acceptance probability. Inputs are: old : the fitness value for the current subset new : the fitness value for the new subset iteration : the current iteration number or, if the improve argument of safsControl is used, the number of iterations since the last restart ... : not currently used The output should be a numeric value between zero and one. One of the biggest difficulties in using simulated annealing is the specification of the acceptance probability calculation. There are many references on different methods for doing this but the general consensus is that 1) the probability should decrease as the difference between the current and new solution increases and 2) the probability should decrease over iterations. One issue is that the difference in fitness values can be scale-dependent. In this package, the default probability calculations uses the percent difference, i.e. (current - new)/current to normalize the difference. The basic form of the probability simply takes the difference, multiplies by the iteration number and exponentiates this product: To demonstrate this, the plot below shows the probability profile for different fitness values of the current subset and different (absolute) differences. For the example data that were simulated, the RMSE values ranged between values greater than 4 to just under 3. In the plot below, the red curve in the right-hand panel shows how the probability changes over time when comparing a current value of 4 with a new values of 4.5 (smaller values being better). While this difference would likely be accepted in the first few iterations, it is unlikely to be accepted after 30 or 40. Also, larger differences are uniformly disfavored relative to smaller differences. While this is the default, any user-written function can be used to assign probabilities. \"\\n',\n",
       " '\"caret_22_feature_selection_using_simulated_annealing 22 Feature Selection using Simulated Annealing feature-selection-using-simulated-annealing.html  22.6 Using Recipes Similar to the previous section on genetic algorothms, recipes can be used with safs . Using the same data as before: Lets again use linear models with the function: \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.10 Animal Scat Data Reid (2105) collected data on animal feses in coastal California. The data consist of DNA verified species designations as well as fields related to the time and place of the collection and the scat itself. The data frame scat_orig contains while scat contains data on the three main species. \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.1 Blood-Brain Barrier Data Mente and Lombardo (2005) developed models to predict the log of the ratio of the concentration of a compound in the brain and the concentration in blood. For each compound, they computed three sets of molecular descriptors: MOE 2D, rule-of-five and Charge Polar Surface Area (CPSA). In all, 134 descriptors were calculated. Included in this package are 208 non-proprietary literature compounds. The vector logBBB contains the log concentration ratio and the data fame bbbDescr contains the descriptor values. \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.2 COX-2 Activity Data From Sutherland, OBrien, and Weaver (2003) : A set of 467 cyclooxygenase-2 (COX-2) inhibitors has been assembled from the published work of a single research group, with in vitro activities against human recombinant enzyme expressed as IC50 values ranging from 1 nM to >100 uM (53 compounds have indeterminate IC50 values). A set of 255 descriptors (MOE2D and QikProp) were generated. To classify the data, we used a cutoff of 2^{2.5} to determine activity. Using data(cox2) exposes three R objects: cox2Descr is a data frame with the descriptor data, cox2IC50 is a numeric vector of IC50 assay values and cox2Class is a factor vector with the activity results. \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.3 DHFR Inhibition Sutherland and Weaver (2004) discuss QSAR models for dihydrofolate reductase (DHFR) inhibition. This data set contains values for 325 compounds. For each compound, 228 molecular descriptors have been calculated. Additionally, each samples is designated as active or inactive. The data frame dhfr contains a column called Y with the outcome classification. The remainder of the columns are molecular descriptor values. \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.4 Tecator NIR Data These data can be found in the datasets section of StatLib. The data consist of 100 near infrared absorbance spectra used to predict the moisture, fat and protein values of chopped meat. From StatLib : These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents. If results from these data are used in a publication we want you to mention the instrument and company name (Tecator) in the publication. In addition, please send a preprint of your article to: Karin Thente, Tecator AB, Box 70, S-263 21 Hoganas, Sweden. One reference for these data is Borggaard and Thodberg (1992). Using data(tecator) loads a 215 x 100 matrix of absorbance spectra and a 215 x 3 matrix of outcomes. \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.5 Fatty Acid Composition Data Brodnjak-Voncina et al. (2005) describe a set of data where seven fatty acid compositions were used to classify commercial oils as either pumpkin (labeled A ), sunflower ( B ), peanut ( C ), olive ( D ), soybean ( E ), rapeseed ( F ) and corn ( G ). There were 96 data points contained in their Table 1 with known results. The breakdown of the classes is given in below: As a note, the paper states on page 32 that there are 37 unknown samples while the table on pages 33 and 34 shows that there are 34 unknowns. \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.6 German Credit Data Data from Dr.\\xa0Hans Hofmann of the University of Hamburg and stored at the UC Irvine Machine Learning Repository . These data have two classes for the credit worthiness: good or bad. There are predictors related to attributes, such as: checking account status, duration, credit history, purpose of the loan, amount of the loan, savings accounts or bonds, employment duration, Installment rate in percentage of disposable income, personal information, other debtors/guarantors, residence duration, property, age, other installment plans, housing, number of existing credits, job information, Number of people being liable to provide maintenance for, telephone, and foreign worker status. Many of these predictors are discrete and have been expanded into several 0/1 indicator variables \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.7 Kelly Blue Book Resale data for 2005 model year GM cars Kuiper (2008) collected data on Kelly Blue Book resale data for 804 GM cars (2005 model year). cars is data frame of the suggested retail price (column Price ) and various characteristics of each car (columns Mileage , Cylinder , Doors , Cruise , Sound , Leather , Buick , Cadillac , Chevy , Pontiac , Saab , Saturn , convertible , coupe , hatchback , sedan and wagon ) \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.8 Cell Body Segmentation Data Hill, LaPan, Li and Haney (2007) develop models to predict which cells in a high content screen were well segmented. The data consists of 119 imaging measurements on 2019. The original analysis used 1009 for training and 1010 as a test set (see the column called Case ). The outcome class is contained in a factor variable called Class with levels PS for poorly segmented and WS for well segmented. \"\\n',\n",
       " '\"caret_23_data_sets 23 Data Sets data-sets.html  23.9 Sacramento House Price Data This data frame contains house and sale price data for 932 homes in Sacramento CA. The original data were obtained from the website for the SpatialKey software . From their website: The Sacramento real estate transactions file is a list of 985 real estate transactions in the Sacramento area reported over a five-day period, as reported by the Sacramento Bee. Google was used to fill in missing/incorrect data. \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.10 Class Distance Calculations caret contains functions to generate new predictors variables based on distances to class centroids (similar to how linear discriminant analysis works). For each level of a factor variable, the class centroid and covariance matrix is calculated. For new samples, the Mahalanobis distance to each of the class centroids is computed and can be used as an additional predictor. This can be helpful for non-linear models when the true decision boundary is actually linear. In cases where there are more predictors within a class than samples, the classDist function has arguments called pca and keep arguments that allow for principal components analysis within each class to be used to avoid issues with singular covariance matrices. predict.classDist is then used to generate the class distances. By default, the distances are logged, but this can be changed via the trans argument to predict.classDist . As an example, we can used the MDRR data. This image shows a scatterplot matrix of the class distances for the held-out samples: \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.1 Creating Dummy Variables The function dummyVars can be used to generate a complete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method. For example, the etitanic data set in the earth package includes two factors: pclass (passenger class, with levels 1st, 2nd, 3rd) and sex (with levels female, male). The base R function model.matrix would generate the following variables: Using dummyVars : Note there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as lm . \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.2 Zero- and Near Zero-Variance Predictors In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e.\\xa0a zero-variance predictor). For many models (excluding tree-based models), this may cause the model to crash or the fit to be unstable. Similarly, predictors might have only a handful of unique values that occur with very low frequencies. For example, in the drug resistance data, the nR11 descriptor (number of 11-membered rings) data have a few unique numeric values that are highly unbalanced: The concern here that these predictors may become zero-variance predictors when the data are split into cross-validation/bootstrap sub-samples or that a few samples may have an undue influence on the model. These near-zero-variance predictors may need to be identified and eliminated prior to modeling. To identify these types of predictors, the following two metrics can be calculated: the frequency of the most prevalent value over the second most frequent value (called the frequency ratio), which would be near one for well-behaved predictors and very large for highly-unbalanced data and the percent of unique values is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases If the frequency ratio is greater than a pre-specified threshold and the unique value percentage is less than a threshold, we might consider a predictor to be near zero-variance. We would not want to falsely identify data that have low granularity but are evenly distributed, such as data from a discrete uniform distribution. Using both criteria should not falsely detect such predictors. Looking at the MDRR data, the nearZeroVar function can be used to identify near zero-variance variables (the saveMetrics argument can be used to show the details and usually defaults to FALSE ): By default, nearZeroVar will return the positions of the variables that are flagged to be problematic. \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.3 Identifying Correlated Predictors While there are some models that thrive on correlated predictors (such as pls ), other models may benefit from reducing the level of correlation between the predictors. Given a correlation matrix, the findCorrelation function uses the following algorithm to flag predictors for removal: For the previous MDRR data, there are 65 descriptors that are almost perfectly correlated (|correlation| > 0.999), such as the total information index of atomic composition ( IAC ) and the total information content index (neighborhood symmetry of 0-order) ( TIC0 ) (correlation  1). The code chunk below shows the effect of removing descriptors with absolute correlations above 0.75. \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.4 Linear Dependencies The function findLinearCombos uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). For example, consider the following matrix that is could have been produced by a less-than-full-rank parameterizations of a two-way experimental layout: Note that columns two and three add up to the first column. Similarly, columns four, five and six add up the first column. findLinearCombos will return a list that enumerates these dependencies. For each linear combination, it will incrementally remove columns from the matrix and test to see if the dependencies have been resolved. findLinearCombos will also return a vector of column positions can be removed to eliminate the linear dependencies: These types of dependencies can arise when large numbers of binary chemical fingerprints are used to describe the structure of a molecule. \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.5 The preProcess Function The preProcess class can be used for many operations on predictors, including centering and scaling. The function preProcess estimates the required parameters for each operation and predict.preProcess is used to apply them to specific data sets. This function can also be interfaces when calling the train function. Several types of techniques are described in the next few sections and then another example is used to demonstrate how multiple methods can be used. Note that, in all cases, the preProcess function estimates whatever it requires from a specific data set (e.g.\\xa0the training set) and then applies these transformations to any data set without recomputing the values \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.6 Centering and Scaling In the example below, the half of the MDRR data are used to estimate the location and scale of the predictors. The function preProcess doesnt actually pre-process the data. predict.preProcess is used to pre-process this and other data sets. The preProcess option \"range\" scales the data to the interval between zero and one. \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.7 Imputation preProcess can be used to impute data sets based only on information in the training set. One method of doing this is with K-nearest neighbors. For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predictor is imputed using these values (e.g.\\xa0using the mean). Using this approach will automatically trigger preProcess to center and scale the data, regardless of what is in the method argument. Alternatively, bagged trees can also be used to impute. For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is used to predict the value. While, in theory, this is a more powerful method of imputing, the computational costs are much higher than the nearest neighbor technique. \"\\n',\n",
       " '\"caret_3_preprocessing 3 Pre-Processing pre-processing.html  3.9 Putting It All Together In Applied Predictive Modeling there is a case study where the execution times of jobs in a high performance computing environment are being predicted. The data are: The data are a mix of categorical and numeric predictors. Suppose we want to use the Yeo-Johnson transformation on the continuous predictors then center and scale them. Lets also suppose that we will be running a tree-based models so we might want to keep the factors as factors (as opposed to creating dummy variables). We run the function on all the columns except the last, which is the outcome. The two predictors labeled as ignored in the output are the two factor predictors. These are not altered but the numeric predictors are transformed. However, the predictor for the number of pending jobs, has a very sparse and unbalanced distribution: For some other models, this might be an issue (especially if we resample or down-sample the data). We can add a filter to check for zero- or near zero-variance predictors prior to running the pre-processing calculations: Note that one predictor is labeled as removed and the processed data lack the sparse predictor. \"\\n',\n",
       " '\"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.1 Simple Splitting Based on the Outcome The function createDataPartition can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. For example, to create a single 80/20% split of the iris data: The list  FALSE avoids returning the data as a list. This function also has an argument, times , that can create multiple splits at once; the data indices are returned in a list of integer vectors. Similarly, createResample can be used to make simple bootstrap samples and createFolds can be used to generate balanced crossvalidation groupings from a set of data. \"\\n',\n",
       " '\"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.2 Splitting Based on the Predictors Also, the function maxDissim can be used to create subsamples using a maximum dissimilarity approach ( Willett, 1999 ). Suppose there is a data set A with m samples and a larger data set B with n samples. We may want to create a subsample from B that is diverse when compared to A . To do this, for each sample in B , the function calculates the m dissimilarities between each point in A . The most dissimilar point in B is added to A and the process continues. There are many methods in R to calculate dissimilarity. caret uses the proxy package. See the manual for that package for a list of available measures. Also, there are many ways to calculate which sample is most dissimilar. The argument obj can be used to specify any function that returns a scalar measure. caret includes two functions, minDiss and sumDiss , that can be used to maximize the minimum and total dissimilarities, respectfully. As an example, the figure below shows a scatter plot of two chemical descriptors for the Cox2 data. Using an initial random sample of 5 compounds, we can select 20 more compounds from the data so that the new compounds are most dissimilar from the initial 5 that were specified. The panels in the figure show the results using several combinations of distance metrics and scoring functions. For these data, the distance measure has less of an impact than the scoring method for determining which compounds are most dissimilar. The visualization below shows the data set (small points), the starting samples (larger blue points) and the order in which the other 20 samples are added. \"\\n',\n",
       " '\"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.3 Data Splitting for Time Series Simple random sampling of time series is probably not the best way to resample times series data. Hyndman and Athanasopoulos (2013) discuss rolling forecasting origin techniques that move the training and test sets in time. caret contains a function called createTimeSlices that can create the indices for this type of splitting. The three parameters for this type of splitting are: initialWindow : the initial number of consecutive values in each training set sample horizon : The number of consecutive values in test set sample fixedWindow : A logical: if FALSE , the training set always start at the first sample and the training set size will vary over data splits. As an example, suppose we have a time series with 20 data points. We can fix initialWindow  5 and look at different settings of the other two arguments. In the plot below, rows in each panel correspond to different data splits (i.e.\\xa0resamples) and the columns correspond to different data points. Also, red indicates samples that are in included in the training set and the blue indicates samples in the test set. \"\\n',\n",
       " '\"caret_4_data_splitting 4 Data Splitting data-splitting.html  4.4 Simple Splitting with Important Groups In some cases there is an important qualitative factor in the data that should be considered during (re)sampling. For example: in clinical trials, there may be hospital-to-hospital differences with longitudinal or repeated measures data, subjects (or general independent experimental unit) may have multiple rows in the data set, etc. There may be an interest in making sure that these groups are not contained in the training and testing set since this may bias the test set performance to be more optimistic. Also, when one or more specific groups are held out, the resampling might capture the ruggedness of the model. In the example where clinical data is recorded over multiple sites, the resampling performance estimates partly measure how extensible the model is across sites. To split the data based on groups, groupKFold can be used: The results in folds can be used as inputs into the index argument of the trainControl function. This plot shows how each subject is partitioned between the modeling and holdout sets. Note that since k was less than 20 when folds was created, there are some holdouts with model than one subject. \"\\n',\n",
       " '\"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.1 Model Training and Parameter Tuning The caret package has several functions that attempt to streamline the model building and evaluation process. The train function can be used to evaluate, using resampling, the effect of model tuning parameters on performance choose the optimal model across these parameters estimate model performance from a training set First, a specific model must be chosen. Currently, 238 are available using caret ; see train Model List or train Models By Tag for details. On these pages, there are lists of tuning parameters that can potentially be optimized. User-defined models can also be created. The first step in tuning the model (line 1 in the algorithm below) is to choose a set of parameters to evaluate. For example, if fitting a Partial Least Squares (PLS) model, the number of PLS components to evaluate must be specified. Once the model and tuning parameter values have been defined, the type of resampling should be also be specified. Currently, k -fold cross-validation (once or repeated), leave-one-out cross-validation and bootstrap (simple estimation or the 632 rule) resampling methods can be used by train . After resampling, the process produces a profile of performance measures is available to guide the user as to which tuning parameter values should be chosen. By default, the function automatically chooses the tuning parameters associated with the best value, although different algorithms can be used (see details below). \"\\n',\n",
       " '\"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.2 An Example The Sonar data are available in the mlbench package. Here, we load the data: The function createDataPartition can be used to create a stratified random sample of the data into training and test sets: We will use these data illustrate functionality on this (and other) pages. \"\\n',\n",
       " '\"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.3 Basic Parameter Tuning By default, simple bootstrap resampling is used for line 3 in the algorithm above. Others are available, such as repeated K -fold cross-validation, leave-one-out etc. The function trainControl can be used to specifiy the type of resampling: More information about trainControl is given in a section below . The first two arguments to train are the predictor and outcome data objects, respectively. The third argument, method , specifies the type of model (see train Model List or train Models By Tag ). To illustrate, we will fit a boosted tree model via the gbm package. The basic syntax for fitting this model using repeated cross-validation is shown below: For a gradient boosting machine (GBM) model, there are three main tuning parameters: number of iterations, i.e.\\xa0trees, (called n.trees in the gbm function) complexity of the tree, called interaction.depth learning rate: how quickly the algorithm adapts, called shrinkage the minimum number of training set samples in a node to commence splitting ( n.minobsinnode ) The default values tested for this model are shown in the first two columns ( shrinkage and n.minobsinnode are not shown beause the grid set of candidate models all use a single value for these tuning parameters). The column labeled  Accuracy  is the overall agreement rate averaged over cross-validation iterations. The agreement standard deviation is also calculated from the cross-validation results. The column  Kappa  is Cohens (unweighted) Kappa statistic averaged across the resampling results. train works with specific models (see train Model List or train Models By Tag ). For these models, train can automatically create a grid of tuning parameters. By default, if p is the number of tuning parameters, the grid size is 3^p . As another example, regularized discriminant analysis (RDA) models have two parameters ( gamma and lambda ), both of which lie between zero and one. The default training grid would produce nine combinations in this two-dimensional space. There is additional functionality in train that is described in the next section. \"\\n',\n",
       " '\"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.4 Notes on Reproducibility Many models utilize random numbers during the phase where parameters are estimated. Also, the resampling indices are chosen using random numbers. There are two main ways to control the randomness in order to assure reproducible results. There are two approaches to ensuring that the same resamples are used between calls to train . The first is to use set.seed just prior to calling train . The first use of random numbers is to create the resampling information. Alternatively, if you would like to use specific splits of the data, the index argument of the trainControl function can be used. This is briefly discussed below. When the models are created inside of resampling , the seeds can also be set. While setting the seed prior to calling train may guarantee that the same random numbers are used, this is unlikely to be the case when parallel processing is used (depending which technology is utilized). To set the model fitting seeds, trainControl has an additional argument called seeds that can be used. The value for this argument is a list of integer vectors that are used as seeds. The help page for trainControl describes the appropriate format for this option. How random numbers are used is highly dependent on the package author. There are rare cases where the underlying model function does not control the random number seed, especially if the computations are conducted in C code. Also, please note that some packages load random numbers when loaded (directly or via namespace) and this may affect reproducibility. \"\\n',\n",
       " '\"caret_5_model_training_and_tuning 5 Model Training and Tuning model-training-and-tuning.html  5.6 Choosing the Final Model Another method for customizing the tuning process is to modify the algorithm that is used to select the best parameter values, given the performance numbers. By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models). Other schemes for selecting model can be used. Breiman et al (1984) suggested the one standard error rule for simple tree-based models. In this case, the model with the best performance value is identified and, using resampling, we can estimate the standard error of performance. The final model used was the simplest model within one standard error of the (empirically) best model. With simple trees this makes sense, since these models will start to over-fit as they become more and more specific to the training data. train allows the user to specify alternate rules for selecting the final model. The argument selectionFunction can be used to supply a function to algorithmically determine the final model. There are three existing functions in the package: best is chooses the largest/smallest value, oneSE attempts to capture the spirit of Breiman et al (1984) and tolerance selects the least complex model within some percent tolerance of the best value. See ?best for more details. User-defined functions can be used, as long as they have the following arguments: x is a data frame containing the tune parameters and their associated performance metrics. Each row corresponds to a different tuning parameter combination. metric a character string indicating which performance metric should be optimized (this is passed in directly from the metric argument of train . maximize is a single logical value indicating whether larger values of the performance metric are better (this is also directly passed from the call to train ). The function should output a single integer indicating which row in x is chosen. As an example, if we chose the previous boosted tree model on the basis of overall accuracy, we would choose: n.trees  1450, interaction.depth  5, shrinkage  0.1, n.minobsinnode  20. However, the scale in this plots is fairly tight, with accuracy values ranging from 0.863 to 0.922. A less complex model (e.g.\\xa0fewer, more shallow trees) might also yield acceptable accuracy. The tolerance function could be used to find a less complex model based on ( x - x best )/ x best x 100, which is the percent difference. For example, to select parameter values based on a 2% loss of performance: This indicates that we can get a less complex model with an area under the ROC curve of 0.914 (compared to the pick the best value of 0.922). The main issue with these functions is related to ordering the models from simplest to complex. In some cases, this is easy (e.g.\\xa0simple trees, partial least squares), but in cases such as this model, the ordering of models is subjective. For example, is a boosted tree model using 100 iterations and a tree depth of 2 more complex than one with 50 iterations and a depth of 8? The package makes some choices regarding the orderings. In the case of boosted trees, the package assumes that increasing the number of iterations adds complexity at a faster rate than increasing the tree depth, so models are ordered on the number of iterations then ordered with depth. See ?best for more examples for specific models. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.10 Ensemble Model (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Model Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit Boosted Tree Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst , plyr C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.11 Feature Extraction (back to contents ) Independent Component Regression Type: Regression Tuning parameters: n.comp (#Components) Required packages: fastICA Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Principal Component Analysis Type: Regression Tuning parameters: ncomp (#Components) Required packages: pls Projection Pursuit Regression Type: Regression Tuning parameters: nterms (# Terms) Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Supervised Principal Component Analysis Type: Regression Tuning parameters: threshold (Threshold) n.components (#Components) Required packages: superpc \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.12 Feature Selection Wrapper (back to contents ) Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Regression with Backwards Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Forward Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.13 Gaussian Process (back to contents ) Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.14 Generalized Additive Model (back to contents ) Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Generalized Additive Model using LOESS Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.15 Generalized Linear Model (back to contents ) Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. Generalized Additive Model using LOESS Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Negative Binomial Generalized Linear Model Type: Regression Tuning parameters: link (Link Function) Required packages: MASS A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.16 Handle Missing Predictor Data (back to contents ) AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.17 Implicit Feature Selection (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Elasticnet Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Least Angle Regression Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression Type: Regression Tuning parameters: step (#Steps) Required packages: lars Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Spike and Slab Regression Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab , plyr Notes: Unlike other packages used by train , the spikeslab package is fully loaded when this model is used. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. The lasso Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.18 Kernel Method (back to contents ) Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Polynomial Kernel Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Polynomial Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Radial Basis Function Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS , kernlab Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.19 L1 Regularization (back to contents ) Bayesian Ridge Regression (Model Averaged) Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Elasticnet Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Least Angle Regression Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression Type: Regression Tuning parameters: step (#Steps) Required packages: lars Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. The lasso Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.1 Accepts Case Weights (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Linear Regression Type: Regression Tuning parameters: intercept (intercept) A model-specific variable importance metric is available. Linear Regression with Stepwise Selection Type: Regression No tuning parameters for this model Required packages: MASS Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Negative Binomial Generalized Linear Model Type: Regression Tuning parameters: link (Link Function) Required packages: MASS A model-specific variable importance metric is available. Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Projection Pursuit Regression Type: Regression Tuning parameters: nterms (# Terms) Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Robust Linear Model Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS A model-specific variable importance metric is available. Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.20 L2 Regularization (back to contents ) Bayesian Ridge Regression Type: Regression No tuning parameters for this model Required packages: monomvn Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Polynomial Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Radial Basis Function Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS , kernlab Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Ridge Regression Type: Regression Tuning parameters: lambda (Weight Decay) Required packages: elasticnet Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.21 Linear Classifier (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. Factor-Based Linear Discriminant Analysis Type: Classification Tuning parameters: q (# Factors) Required packages: HiDimDA Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Localized Linear Discriminant Analysis Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Maximum Uncertainty Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: HiDimDA Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Robust SIMCA Type: Classification No tuning parameters for this model Required packages: rrcovHD Notes: Unlike other packages used by train , the rrcovHD package is fully loaded when this model is used. Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Stabilized Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: ipred Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.22 Linear Regression (back to contents ) Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Ridge Regression Type: Regression No tuning parameters for this model Required packages: monomvn Bayesian Ridge Regression (Model Averaged) Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Elasticnet Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Independent Component Regression Type: Regression Tuning parameters: n.comp (#Components) Required packages: fastICA L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Angle Regression Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression Type: Regression Tuning parameters: step (#Steps) Required packages: lars Linear Regression Type: Regression Tuning parameters: intercept (intercept) A model-specific variable importance metric is available. Linear Regression with Backwards Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Forward Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection Type: Regression No tuning parameters for this model Required packages: MASS Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Non-Negative Least Squares Type: Regression No tuning parameters for this model Required packages: nnls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Penalized Linear Regression Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Principal Component Analysis Type: Regression Tuning parameters: ncomp (#Components) Required packages: pls Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Relaxed Lasso Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo , plyr Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Ridge Regression Type: Regression Tuning parameters: lambda (Weight Decay) Required packages: elasticnet Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba Robust Linear Model Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS A model-specific variable importance metric is available. Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Spike and Slab Regression Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab , plyr Notes: Unlike other packages used by train , the spikeslab package is fully loaded when this model is used. Supervised Principal Component Analysis Type: Regression Tuning parameters: threshold (Threshold) n.components (#Components) Required packages: superpc Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. The lasso Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.23 Logic Regression (back to contents ) Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.24 Logistic Regression (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.25 Mixture Model (back to contents ) Adaptive Mixture Discriminant Analysis Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Mixture Discriminant Analysis Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.26 Model Tree (back to contents ) Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.27 Multivariate Adaptive Regression Splines (back to contents ) Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.28 Neural Network (back to contents ) Bayesian Regularized Neural Networks Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Extreme Learning Machine Type: Classification, Regression Tuning parameters: nhid (#Hidden Units) actfun (Activation Function) Required packages: elmNN Notes: The package is no longer on CRAN but can be installed from the archive at https://cran.r-project.org/src/contrib/Archive/elmNN/ Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Monotone Multi-Layer Perceptron Neural Network Type: Classification, Regression Tuning parameters: hidden1 (#Hidden Units) n.ensemble (#Models) Required packages: monmlp Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) Required packages: RSNNS Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, with multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) Required packages: RSNNS Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Dropout Type: Regression, Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) learning.rate (Learning Rate) momentum (Momentum) dropout (Dropout Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) dropout (Dropout Rate) beta1 (beta1) beta2 (beta2) learningrate (Learning Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Users are strongly advised to define num.round themselves. Neural Network Type: Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) Required packages: neuralnet Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Stacked AutoEncoder Deep Neural Network Type: Classification, Regression Tuning parameters: layer1 (Hidden Layer 1) layer2 (Hidden Layer 2) layer3 (Hidden Layer 3) hidden_dropout (Hidden Dropouts) visible_dropout (Visible Dropout) Required packages: deepnet \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.29 Oblique Tree (back to contents ) Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.2 Bagging (back to contents ) Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Model Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.30 Ordinal Outcomes (back to contents ) Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.31 Partial Least Squares (back to contents ) Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Generalized Linear Models Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Notes: Unlike other packages used by train , the plsRglm package is fully loaded when this model is used. Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.32 Patient Rule Induction Method (back to contents ) Patient Rule Induction Method Type: Classification Tuning parameters: peel.alpha (peeling quantile) paste.alpha (pasting quantile) mass.min (minimum mass) Required packages: supervisedPRIM \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.33 Polynomial Model (back to contents ) Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Least Squares Support Vector Machine with Polynomial Kernel Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Polynomial Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Robust Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.34 Prototype Models (back to contents ) Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Greedy Prototype Selection Type: Classification Tuning parameters: eps (Ball Size) Minkowski (Distance Order) Required packages: proxy , protoclass k-Nearest Neighbors Type: Regression, Classification Tuning parameters: kmax (Max. #Neighbors) distance (Distance) kernel (Kernel) Required packages: kknn k-Nearest Neighbors Type: Classification, Regression Tuning parameters: k (#Neighbors) Learning Vector Quantization Type: Classification Tuning parameters: size (Codebook Size) k (#Prototypes) Required packages: class Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Optimal Weighted Nearest Neighbor Classifier Type: Classification Tuning parameters: K (#Neighbors) Required packages: snn Stabilized Nearest Neighbor Classifier Type: Classification Tuning parameters: lambda (Stabilization Parameter) Required packages: snn \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.35 Quantile Regression (back to contents ) Non-Convex Penalized Quantile Regression Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Quantile Regression with LASSO penalty Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.36 Radial Basis Function (back to contents ) Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Radial Basis Function Kernel Regularized Least Squares Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS , kernlab Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.37 Random Forest (back to contents ) Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Random Ferns Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.38 Regularization (back to contents ) Bayesian Regularized Neural Networks Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.39 Relevance Vector Machines (back to contents ) Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.3 Bayesian Model (back to contents ) Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Bayesian Regularized Neural Networks Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Bayesian Ridge Regression Type: Regression No tuning parameters for this model Required packages: monomvn Bayesian Ridge Regression (Model Averaged) Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. Model Averaged Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Naive Bayes Type: Classification Tuning parameters: laplace (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: naivebayes Naive Bayes Type: Classification Tuning parameters: fL (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: klaR Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Semi-Naive Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Spike and Slab Regression Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab , plyr Notes: Unlike other packages used by train , the spikeslab package is fully loaded when this model is used. The Bayesian lasso Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity . For example, when sparsity  .5 , only coefficients where at least half the posterior estimates are nonzero are used. Tree Augmented Naive Bayes Classifier Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.40 Ridge Regression (back to contents ) Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Ridge Regression with Variable Selection Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.41 Robust Methods (back to contents ) L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Relevance Vector Machines with Linear Kernel Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.42 Robust Model (back to contents ) Quantile Random Forest Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Linear Model Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS A model-specific variable importance metric is available. Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Robust SIMCA Type: Classification No tuning parameters for this model Required packages: rrcovHD Notes: Unlike other packages used by train , the rrcovHD package is fully loaded when this model is used. SIMCA Type: Classification No tuning parameters for this model Required packages: rrcov , rrcovHD \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.43 ROC Curves (back to contents ) ROC-Based Classifier Type: Classification Tuning parameters: xgenes (#Variables Retained) Required packages: rocc \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.44 Rule-Based Model (back to contents ) Adaptive-Network-Based Fuzzy Inference System Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Dynamic Evolving Neural-Fuzzy Inference System Type: Regression Tuning parameters: Dthr (Threshold) max.iter (Max. Iterations) Required packages: frbs Fuzzy Inference Rules by Descent Method Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Fuzzy Rules Using Chis Method Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs Fuzzy Rules Using Genetic Cooperative-Competitive Learning and Pittsburgh Type: Classification Tuning parameters: max.num.rule (Max. #Rules) popu.size (Population Size) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules Using the Structural Learning Algorithm on Vague Environment Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules via MOGUL Type: Regression Tuning parameters: max.gen (Max. Generations) max.iter (Max. Iterations) max.tune (Max. Tuning Iterations) Required packages: frbs Fuzzy Rules via Thrift Type: Regression Tuning parameters: popu.size (Population Size) num.labels (# Fuzzy Labels) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules with Weight Factor Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs Genetic Lateral Tuning and Rule Selection of Linguistic Fuzzy Systems Type: Regression Tuning parameters: popu.size (Population Size) num.labels (# Fuzzy Labels) max.gen (Max. Generations) Required packages: frbs Hybrid Neural Fuzzy Inference System Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Model Rules Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Patient Rule Induction Method Type: Classification Tuning parameters: peel.alpha (peeling quantile) paste.alpha (pasting quantile) mass.min (minimum mass) Required packages: supervisedPRIM Random Forest Rule-Based Model Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest , inTrees , plyr A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Simplified TSK Fuzzy Rules Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Subtractive Clustering and Fuzzy c-Means Rules Type: Regression Tuning parameters: r.a (Radius) eps.high (Upper Threshold) eps.low (Lower Threshold) Required packages: frbs Wang and Mendel Fuzzy Rules Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.45 Self-Organising Maps (back to contents ) Self-Organizing Maps Type: Classification, Regression Tuning parameters: xdim (Rows) ydim (Columns) user.weights (Layer Weight) topo (Topology) Required packages: kohonen Notes: As of version 3.0.0 of the kohonen package, the argument user.weights replaces the old alpha parameter. user.weights is usually a vector of relative weights such as c(1, 3) but is parameterized here as a proportion such as c(1-.75, .75) where the .75 is the value of the tuning parameter passed to train and indicates that the outcome layer has 3 times the weight as the predictor layer. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.46 String Kernel (back to contents ) Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.47 Support Vector Machines (back to contents ) L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Polynomial Kernel Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.48 Supports Class Probabilities (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Adaptive Mixture Discriminant Analysis Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Adjacent Categories Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bagged MARS Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged MARS using gCV Pruning Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Bagged Model Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Bayesian Generalized Linear Model Type: Regression, Classification No tuning parameters for this model Required packages: arm Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Continuation Ratio Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Ensembles of Generalized Linear Models Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Notes: Unlike other packages used by train , the randomGLM package is fully loaded when this model is used. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth , mda A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Gaussian Process Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Generalized Additive Model using LOESS Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the mgcv package is fully loaded when this model is used. Generalized Additive Model using Splines Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train , the gam package is fully loaded when this model is used. Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS Generalized Partial Least Squares Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet , Matrix A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim k-Nearest Neighbors Type: Regression, Classification Tuning parameters: kmax (Max. #Neighbors) distance (Distance) kernel (Kernel) Required packages: kknn k-Nearest Neighbors Type: Classification, Regression Tuning parameters: k (#Neighbors) Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Localized Linear Discriminant Analysis Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Mixture Discriminant Analysis Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Model Averaged Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Model Averaged Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Monotone Multi-Layer Perceptron Neural Network Type: Classification, Regression Tuning parameters: hidden1 (#Hidden Units) n.ensemble (#Models) Required packages: monmlp Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) Required packages: RSNNS Multi-Layer Perceptron Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, with multiple layers Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) Required packages: RSNNS Multi-Step Adaptive MCP-Net Type: Regression, Classification Tuning parameters: alphas (Alpha) nsteps (#Adaptive Estimation Steps) scale (Adaptive Weight Scaling Factor) Required packages: msaenet A model-specific variable importance metric is available. Multilayer Perceptron Network by Stochastic Gradient Descent Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Learning Rate Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R , plyr A model-specific variable importance metric is available. Multilayer Perceptron Network with Dropout Type: Regression, Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Regression, Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multivariate Adaptive Regression Spline Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Multivariate Adaptive Regression Splines Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Notes: Unlike other packages used by train , the earth package is fully loaded when this model is used. Naive Bayes Type: Classification Tuning parameters: laplace (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: naivebayes Naive Bayes Type: Classification Tuning parameters: fL (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: klaR Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Nearest Shrunken Centroids Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) learning.rate (Learning Rate) momentum (Momentum) dropout (Dropout Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Neural Network Type: Classification, Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) dropout (Dropout Rate) beta1 (beta1) beta2 (beta2) learningrate (Learning Rate) activation (Activation Function) Required packages: mxnet Notes: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Users are strongly advised to define num.round themselves. Neural Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Non-Informative Model Type: Classification, Regression No tuning parameters for this model Notes: Since this model always predicts the same value, R-squared values will always be estimated to be NA. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Ordered Logistic or Probit Regression Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Parallel Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071 , randomForest , foreach , import A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Generalized Linear Models Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Notes: Unlike other packages used by train , the plsRglm package is fully loaded when this model is used. Patient Rule Induction Method Type: Classification Tuning parameters: peel.alpha (peeling quantile) paste.alpha (pasting quantile) mass.min (minimum mass) Required packages: supervisedPRIM Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Logistic Regression Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression Type: Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet , plyr A model-specific variable importance metric is available. Notes: Requires ordinalNet package version > 2.0 Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Radial Basis Function Network Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Random Forest Type: Classification Tuning parameters: nsets (# score sets tried prior to the approximation) ntreeperdiv (# of trees (small RFs)) ntreefinal (# of trees (final RF)) Required packages: e1071 , ranger , dplyr , ordinalForest A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) splitrule (Splitting Rule) min.node.size (Minimal Node Size) Required packages: e1071 , ranger , dplyr A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) minNode (Minimal Node Size) Required packages: Rborist A model-specific variable importance metric is available. Random Forest Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Logistic Regression Type: Classification Tuning parameters: cost (Cost) loss (Loss Function) epsilon (Tolerance) Required packages: LiblineaR Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest , RRF A model-specific variable importance metric is available. Regularized Random Forest Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Robust Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Self-Organizing Maps Type: Classification, Regression Tuning parameters: xdim (Rows) ydim (Columns) user.weights (Layer Weight) topo (Topology) Required packages: kohonen Notes: As of version 3.0.0 of the kohonen package, the argument user.weights replaces the old alpha parameter. user.weights is usually a vector of relative weights such as c(1, 3) but is parameterized here as a proportion such as c(1-.75, .75) where the .75 is the value of the tuning parameter passed to train and indicates that the outcome layer has 3 times the weight as the predictor layer. Semi-Naive Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Single C5.0 Ruleset Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification Type: Classification No tuning parameters for this model Required packages: RWeka Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Partial Least Squares Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Stabilized Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: ipred Stacked AutoEncoder Deep Neural Network Type: Classification, Regression Tuning parameters: layer1 (Hidden Layer 1) layer2 (Hidden Layer 2) layer3 (Hidden Layer 3) hidden_dropout (Hidden Dropouts) visible_dropout (Visible Dropout) Required packages: deepnet Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Tree Augmented Naive Bayes Classifier Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Variational Bayesian Multinomial Probit Regression Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp Weighted Subspace Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.49 Text Mining (back to contents ) Support Vector Machines with Boundrange String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Spectrum String Kernel Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.4 Binary Predictors Only (back to contents ) Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.50 Tree-Based Model (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged CART Type: Regression, Classification No tuning parameters for this model Required packages: ipred , plyr , e1071 A model-specific variable importance metric is available. Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit Boosted Tree Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst , plyr C4.5-like Trees Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. CART Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore , plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Tree Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Model Tree Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Single C5.0 Tree Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. Tree Models from Genetic Algorithms Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.51 Two Class Only (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost Bagged Logic Regression Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Notes: Unlike other packages used by train , the logicFS package is fully loaded when this model is used. Bayesian Additive Regression Trees Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. CHi-squared Automated Interaction Detection Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Generalized Linear Model Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Logic Regression Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Oblique Random Forest Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Notes: Unlike other packages used by train , the obliqueRF package is fully loaded when this model is used. Partial Least Squares Generalized Linear Models Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Notes: Unlike other packages used by train , the plsRglm package is fully loaded when this model is used. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart , plyr , rotationForest A model-specific variable importance metric is available. Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Tree-Based Ensembles Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.5 Boosting (back to contents ) AdaBoost Classification Trees Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag , plyr A model-specific variable importance metric is available. Bagged AdaBoost Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag , plyr A model-specific variable importance metric is available. Boosted Classification Trees Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada , plyr Boosted Generalized Additive Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost , plyr , import Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr , mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop . If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Logistic Regression Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst , plyr Boosted Tree Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party , mboost , plyr , partykit Boosted Tree Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst , plyr C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cubist Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) subsample (Subsample Percentage) colsample_bytree (Subsample Ratio of Columns) rate_drop (Fraction of Trees Dropped) skip_drop (Prob. of Skipping Drop-out) min_child_weight (Minimum Sum of Instance Weight) Required packages: xgboost , plyr A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost , plyr A model-specific variable importance metric is available. Gradient Boosting Machines Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Stochastic Gradient Boosting Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm , plyr A model-specific variable importance metric is available. \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.6 Categorical Predictors Only (back to contents ) Model Averaged Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Naive Bayes Classifier Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Semi-Naive Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.7 Cost Sensitive Learning (back to contents ) Cost-Sensitive C5.0 Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50 , plyr A model-specific variable importance metric is available. Cost-Sensitive CART Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart , plyr L2 Regularized Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR Linear Support Vector Machines with Class Weights Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Multilayer Perceptron Network with Dropout Type: Classification Tuning parameters: size (#Hidden Units) dropout (Dropout Rate) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Multilayer Perceptron Network with Weight Decay Type: Classification Tuning parameters: size (#Hidden Units) lambda (L2 Regularization) batch_size (Batch Size) lr (Learning Rate) rho (Rho) decay (Learning Rate Decay) cost (Cost) activation (Activation Function) Required packages: keras Notes: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train , the dplyr package is fully loaded when this model is used. Support Vector Machines with Class Weights Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.8 Discriminant Analysis (back to contents ) Adaptive Mixture Discriminant Analysis Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Binary Discriminant Analysis Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Diagonal Discriminant Analysis Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Factor-Based Linear Discriminant Analysis Type: Classification Tuning parameters: q (# Factors) Required packages: HiDimDA Heteroscedastic Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Localized Linear Discriminant Analysis Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Maximum Uncertainty Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: HiDimDA Mixture Discriminant Analysis Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA , plyr Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR , MASS Regularized Discriminant Analysis Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Robust Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Mixture Discriminant Analysis Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Robust Quadratic Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Notes: Unlike other packages used by train , the rrlda package is fully loaded when this model is used. Shrinkage Discriminant Analysis Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Sparse Linear Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Stabilized Linear Discriminant Analysis Type: Classification No tuning parameters for this model Required packages: ipred \"\\n',\n",
       " '\"caret_7_train_models_by_tag 7 train Models By Tag train-models-by-tag.html  7.0.9 Distance Weighted Discrimination (back to contents ) Distance Weighted Discrimination with Polynomial Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab , kerndwd Linear Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Sparse Distance Weighted Discrimination Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. \"\"numpy_an_example Basic functions #an-example  An example  \"\\n',\n",
       " '\"numpy_array_creation Basic functions #array-creation  Array Creation  There are several ways to create arrays. For example, you can create an array from a regular Python list or tuple using the function. The type of the resulting array is deduced from the type of the elements in the sequences. A frequent error consists in calling with multiple arguments, rather than providing a single sequence as an argument. transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on. The type of the array can also be explicitly specified at creation time: Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation. The function creates an array full of zeros, the function creates an array full of ones, and the function creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is . To create sequences of numbers, NumPy provides the function which is analogous to the Python built-in , but returns an array. When is used with floating point arguments, it is generally not possible to predict the number of elements obtained, due to the finite floating point precision. For this reason, it is usually better to use the function that receives as an argument the number of elements that we want, instead of the step: See also array , zeros , zeros_like , ones , ones_like , empty , empty_like , arange , linspace , numpy.random.Generator.rand , numpy.random.Generator.randn , fromfunction , fromfile \"\\n',\n",
       " '\"numpy_basic_operations Basic functions #basic-operations  Basic Operations  Arithmetic operators on arrays apply elementwise . A new array is created and filled with the result. Unlike in many matrix languages, the product operator operates elementwise in NumPy arrays. The matrix product can be performed using the operator (in python >3.5) or the function or method: Some operations, such as and , act in place to modify an existing array rather than create a new one. When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting). Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the class. By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the parameter you can apply an operation along the specified axis of an array: \"\\n',\n",
       " '\"numpy_broadcasting_rules Basic functions #broadcasting-rules  Broadcasting rules  Broadcasting allows universal functions to deal in a meaningful way with inputs that do not have exactly the same shape. The first rule of broadcasting is that if all input arrays do not have the same number of dimensions, a 1 will be repeatedly prepended to the shapes of the smaller arrays until all the arrays have the same number of dimensions. The second rule of broadcasting ensures that arrays with a size of 1 along a particular dimension act as if they had the size of the array with the largest shape along that dimension. The value of the array element is assumed to be the same along that dimension for the broadcast array. After application of the broadcasting rules, the sizes of all arrays must match. More details can be found in Broadcasting . Advanced indexing and index tricks  NumPy offers more indexing facilities than regular Python sequences. In addition to indexing by integers and slices, as we saw before, arrays can be indexed by arrays of integers and arrays of booleans. \"\\n',\n",
       " '\"numpy_changing_the_shape_of_an_array Basic functions #changing-the-shape-of-an-array  Changing the shape of an array  An array has a shape given by the number of elements along each axis: The shape of an array can be changed with various commands. Note that the following three commands all return a modified array, but do not change the original array: The order of the elements in the array resulting from ravel() is normally C-style, that is, the rightmost index changes the fastest, so the element after a[0,0] is a[0,1]. If the array is reshaped to some other shape, again the array is treated as C-style. NumPy normally creates arrays stored in this order, so ravel() will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions ravel() and reshape() can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest. The reshape function returns its argument with a modified shape, whereas the ndarray.resize method modifies the array itself: If a dimension is given as -1 in a reshaping operation, the other dimensions are automatically calculated: See also ndarray.shape , reshape , resize , ravel \"\\n',\n",
       " '\"numpy_deep_copy Basic functions #deep-copy  Deep Copy  The method makes a complete copy of the array and its data. Sometimes should be called after slicing if the original array is not required anymore. For example, suppose is a huge intermediate result and the final result only contains a small fraction of , a deep copy should be made when constructing with slicing: If is used instead, is referenced by and will persist in memory even if is executed. \"\\n',\n",
       " '\"numpy_functions_and_methods_overview Basic functions #functions-and-methods-overview  Functions and Methods Overview  Here is a list of some useful NumPy functions and methods names ordered in categories. See Routines for the full list. Array Creation arange , array , copy , empty , empty_like , eye , fromfile , fromfunction , identity , linspace , logspace , mgrid , ogrid , ones , ones_like , r_ , zeros , zeros_like Conversions ndarray.astype , atleast_1d , atleast_2d , atleast_3d , mat Manipulations array_split , column_stack , concatenate , diagonal , dsplit , dstack , hsplit , hstack , ndarray.item , newaxis , ravel , repeat , reshape , resize , squeeze , swapaxes , take , transpose , vsplit , vstack Questions all , any , nonzero , where Ordering argmax , argmin , argsort , max , min , ptp , searchsorted , sort Operations choose , compress , cumprod , cumsum , inner , ndarray.fill , imag , prod , put , putmask , real , sum Basic Statistics cov , mean , std , var Basic Linear Algebra cross , dot , outer , linalg.svd , vdot Less Basic  \"\\n',\n",
       " '\"numpy_histograms Basic functions #histograms  Histograms  The NumPy function applied to an array returns a pair of vectors: the histogram of the array and a vector of the bin edges. Beware: also has a function to build histograms (called , as in Matlab) that differs from the one in NumPy. The main difference is that plots the histogram automatically, while only generates the data. Further reading  The Python tutorial NumPy Reference SciPy Tutorial SciPy Lecture Notes A matlab, R, IDL, NumPy/SciPy dictionary \"\\n',\n",
       " '\"numpy_indexing_slicing_and_iterating Basic functions #indexing-slicing-and-iterating  Indexing, Slicing and Iterating  One-dimensional arrays can be indexed, sliced and iterated over, much like lists and other Python sequences. Multidimensional arrays can have one index per axis. These indices are given in a tuple separated by commas: When fewer indices are provided than the number of axes, the missing indices are considered complete slices The expression within brackets in is treated as an followed by as many instances of as needed to represent the remaining axes. NumPy also allows you to write this using dots as . The dots ( ) represent as many colons as needed to produce a complete indexing tuple. For example, if is an array with 5 axes, then is equivalent to , to and to . Iterating over multidimensional arrays is done with respect to the first axis: However, if one wants to perform an operation on each element in the array, one can use the attribute which is an iterator over all the elements of the array: See also Indexing , Indexing (reference), newaxis , ndenumerate , indices Shape Manipulation  \"\\n',\n",
       " '\"numpy_indexing_with_arrays_of_indices Basic functions #indexing-with-arrays-of-indices  Indexing with Arrays of Indices  When the indexed array is multidimensional, a single array of indices refers to the first dimension of . The following example shows this behavior by converting an image of labels into a color image using a palette. We can also give indexes for more than one dimension. The arrays of indices for each dimension must have the same shape. In Python, is exactly the same as so we can put and in a and then do the indexing with that. However, we can not do this by putting and into an array, because this array will be interpreted as indexing the first dimension of a. Another common use of indexing with arrays is the search of the maximum value of time-dependent series: You can also use indexing with arrays as a target to assign to: However, when the list of indices contains repetitions, the assignment is done several times, leaving behind the last value: This is reasonable enough, but watch out if you want to use Pythons construct, as it may not do what you expect: Even though 0 occurs twice in the list of indices, the 0th element is only incremented once. This is because Python requires a+1 to be equivalent to a  a + 1. \"\\n',\n",
       " '\"numpy_indexing_with_boolean_arrays Basic functions #indexing-with-boolean-arrays  Indexing with Boolean Arrays  When we index arrays with arrays of (integer) indices we are providing the list of indices to pick. With boolean indices the approach is different; we explicitly choose which items in the array we want and which ones we dont. The most natural way one can think of for boolean indexing is to use boolean arrays that have the same shape as the original array: This property can be very useful in assignments: You can look at the following example to see how to use boolean indexing to generate an image of the Mandelbrot set : The second way of indexing with booleans is more similar to integer indexing; for each dimension of the array we give a 1D boolean array selecting the slices we want: Note that the length of the 1D boolean array must coincide with the length of the dimension (or axis) you want to slice. In the previous example, has length 3 (the number of rows in ), and (of length 4) is suitable to index the 2nd axis (columns) of . \"\\n',\n",
       " '\"numpy_indexing_with_strings Basic functions #indexing-with-strings  Indexing with strings  See Structured arrays . Linear Algebra  Work in progress. Basic linear algebra to be included here. \"\\n',\n",
       " '\"numpy_no_copy_at_all Basic functions #no-copy-at-all  No Copy at All  Simple assignments make no copy of objects or their data. Python passes mutable objects as references, so function calls make no copy. \"\\n',\n",
       " '\"numpy_printing_arrays Basic functions #printing-arrays  Printing Arrays  When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout: the last axis is printed from left to right, the second-to-last is printed from top to bottom, the rest are also printed from top to bottom, with each slice separated from the next by an empty line. One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices. See below to get more details on . If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners: To disable this behaviour and force NumPy to print the entire array, you can change the printing options using . \"\\n',\n",
       " '\"numpy_simple_array_operations Basic functions #simple-array-operations  Simple Array Operations  See linalg.py in numpy folder for more. Tricks and Tips  Here we give a list of short and useful tips. \"\\n',\n",
       " '\"numpy_splitting_one_array_into_several_smaller_ones Basic functions #splitting-one-array-into-several-smaller-ones  Splitting one array into several smaller ones  Using hsplit , you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur: vsplit splits along the vertical axis, and array_split allows one to specify along which axis to split. Copies and Views  When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases: \"\\n',\n",
       " '\"numpy_stacking_together_different_arrays Basic functions #stacking-together-different-arrays  Stacking together different arrays  Several arrays can be stacked together along different axes: The function column_stack stacks 1D arrays as columns into a 2D array. It is equivalent to hstack only for 2D arrays: On the other hand, the function row_stack is equivalent to vstack for any input arrays. In fact, row_stack is an alias for vstack : In general, for arrays with more than two dimensions, hstack stacks along their second axes, vstack stacks along their first axes, and concatenate allows for an optional arguments giving the number of the axis along which the concatenation should happen. Note In complex cases, r_ and c_ are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals (:) When used with arrays as arguments, r_ and c_ are similar to vstack and hstack in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate. See also hstack , vstack , column_stack , concatenate , c_ , r_ \"\\n',\n",
       " '\"numpy_the_ix__function Basic functions #the-ix-function  The ix_() function  The ix_ function can be used to combine different vectors so as to obtain the result for each n-uplet. For example, if you want to compute all the a+b*c for all the triplets taken from each of the vectors a, b and c: You could also implement the reduce as follows: and then use it as: The advantage of this version of reduce compared to the normal ufunc.reduce is that it makes use of the Broadcasting Rules in order to avoid creating an argument array the size of the output times the number of vectors. \"\\n',\n",
       " '\"numpy_universal_functions Basic functions #universal-functions  Universal Functions  NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called universal functions( ). Within NumPy, these functions operate elementwise on an array, producing an array as output. See also all , any , apply_along_axis , argmax , argmin , argsort , average , bincount , ceil , clip , conj , corrcoef , cov , cross , cumprod , cumsum , diff , dot , floor , inner , invert , lexsort , max , maximum , mean , median , min , minimum , nonzero , outer , prod , re , round , sort , std , sum , trace , transpose , var , vdot , vectorize , where \"\\n',\n",
       " '\"numpy_vector_stacking Basic functions #vector-stacking  Vector Stacking  How do we construct a 2D array from a list of equally-sized row vectors? In MATLAB this is quite easy: if and are two vectors of the same length you only need do . In NumPy this works via the functions , , and , depending on the dimension in which the stacking is to be done. For example: The logic behind those functions in more than two dimensions can be strange. See also NumPy for Matlab users \"\\n',\n",
       " '\"numpy_view_or_shallow_copy Basic functions #view-or-shallow-copy  View or Shallow Copy  Different array objects can share the same data. The method creates a new array object that looks at the same data. Slicing an array returns a view of it: \"\\n',\n",
       " '\"numpy_automatic_reshaping Basic functions #automatic-reshaping  Automatic Reshaping  To change the dimensions of an array, you can omit one of the sizes which will then be deduced automatically: \"\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.1. Classification  DecisionTreeClassifier is a class capable of performing multi-class classification on a dataset. As with other classifiers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of size holding the training samples, and an array Y of integer values, size , holding the class labels for the training samples: After being fitted, the model can then be used to predict the class of samples: Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf: DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, , K-1]) classification. Using the Iris dataset, we can construct a tree as follows: Once trained, you can plot the tree with the plot_tree function: We can also export the tree in Graphviz format using the export_graphviz exporter. If you use the conda package manager, the graphviz binaries and the python package can be installed with . Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with . Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file : The export_graphviz exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically: Alternatively, the tree can also be exported in textual format with the function export_text . This method doesnt require the installation of external libraries and is more compact: Examples: Plot the decision surface of a decision tree on the iris dataset Understanding the decision tree structure \"\\n',\n",
       " '\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.2. Regression  Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class. As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values: Examples: Decision Tree Regression \"\\n',\n",
       " '\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.3. Multi-output problems  A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of size . When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased. With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes: Store n output values in leaves, instead of 1; Use splitting criteria that compute the average reduction across all n outputs. This module offers support for multi-output problems by implementing this strategy in both DecisionTreeClassifier and DecisionTreeRegressor . If a decision tree is fit on an output array Y of size then the resulting estimator will: Output n_output values upon ; Output a list of n_output arrays of class probabilities upon . The use of multi-output trees for regression is demonstrated in Multi-output Decision Tree Regression . In this example, the input X is a single real value and the outputs Y are the sine and cosine of X. The use of multi-output trees for classification is demonstrated in Face completion with a multi-output estimators . In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces. Examples: Multi-output Decision Tree Regression Face completion with a multi-output estimators References: M. Dumont et al, Fast multi-class image annotation with random subwindows and multiple output randomized trees , International Conference on Computer Vision Theory and Applications 2009 \"\\n',\n",
       " '\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.4. Complexity  In general, the run time cost to construct a balanced binary tree is and query time . Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through to find the feature that offers the largest reduction in entropy. This has a cost of at each node, leading to a total cost over the entire trees (by summing the cost at each node) of . \"\\n',\n",
       " '\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.5. Tips on practical use  Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit. Consider performing dimensionality reduction ( PCA , ICA , or Feature selection ) beforehand to give your tree a better chance of finding features that are discriminative. Understanding the decision tree structure will help in gaining more insights about how the decision tree makes predictions, which is important for understanding the important features in the data. Visualise your tree as you are training by using the function. Use as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth. Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use to control the size of the tree to prevent overfitting. Use or to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While can create arbitrarily small leaves, guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, is often the best choice. Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights ( ) for each class to the same value. Also note that weight-based pre-pruning criteria, such as , will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like . If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as , which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights. All decision trees use arrays internally. If training data is not in this format, a copy of the dataset will be made. If the input matrix X is very sparse, it is recommended to convert to sparse before calling fit and sparse before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples. \"\\n',\n",
       " '\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART  What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn? ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data. C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rules precondition if the accuracy of the rule improves without it. C5.0 is Quinlans latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate. CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node. scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now. \"\\n',\n",
       " '\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.7. Mathematical formulation  Given training vectors , i1,, l and a label vector , a decision tree recursively partitions the space such that the samples with the same labels are grouped together. Let the data at node be represented by . For each candidate split consisting of a feature and threshold , partition the data into and subsets \\\\[ \\\\begin{align}\\\\begin{aligned}Q_{left}(\\\\theta)  {(x, y) | x_j < t_m}\\\\\\\\Q_{right}(\\\\theta)  Q \\\\setminus Q_{left}(\\\\theta)\\\\end{aligned}\\\\end{align} \\\\] The impurity at is computed using an impurity function , the choice of which depends on the task being solved (classification or regression) \\\\[G(Q, \\\\theta)  \\\\frac{n_{left}}{N_m} H(Q_{left}(\\\\theta)) + \\\\frac{n_{right}}{N_m} H(Q_{right}(\\\\theta))\\\\] Select the parameters that minimises the impurity \\\\[\\\\theta^*  \\\\operatorname{argmin}_\\\\theta G(Q, \\\\theta)\\\\] Recurse for subsets and until the maximum allowable depth is reached, or . 1.10.7.1. Classification criteria  If a target is a classification outcome taking on values 0,1,,K-1, for node , representing a region with observations, let \\\\[p_{mk}  1/ N_m \\\\sum_{x_i \\\\in R_m} I(y_i  k)\\\\] be the proportion of class k observations in node Common measures of impurity are Gini \\\\[H(X_m)  \\\\sum_k p_{mk} (1 - p_{mk})\\\\] Entropy \\\\[H(X_m)  - \\\\sum_k p_{mk} \\\\log(p_{mk})\\\\] and Misclassification \\\\[H(X_m)  1 - \\\\max(p_{mk})\\\\] where is the training data in node 1.10.7.2. Regression criteria  If the target is a continuous value, then for node , representing a region with observations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes. Mean Squared Error: \\\\[ \\\\begin{align}\\\\begin{aligned}\\\\bar{y}_m  \\\\frac{1}{N_m} \\\\sum_{i \\\\in N_m} y_i\\\\\\\\H(X_m)  \\\\frac{1}{N_m} \\\\sum_{i \\\\in N_m} (y_i - \\\\bar{y}_m)^2\\\\end{aligned}\\\\end{align} \\\\] Mean Absolute Error: \\\\[ \\\\begin{align}\\\\begin{aligned}median(y)_m  \\\\underset{i \\\\in N_m}{\\\\mathrm{median}}(y_i)\\\\\\\\H(X_m)  \\\\frac{1}{N_m} \\\\sum_{i \\\\in N_m} |y_i - median(y)_m|\\\\end{aligned}\\\\end{align} \\\\] where is the training data in node \"\\n',\n",
       " '\"sklearn_1_10_decision_trees 1.10. Decision Trees modules/tree.html  1.10.8. Minimal Cost-Complexity Pruning  Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of [BRE] . This algorithm is parameterized by known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, of a given tree : \\\\[R_\\\\alpha(T)  R(T) + \\\\alpha|T|\\\\] where is the number of terminal nodes in and is traditionally defined as the total misclassification rate of the terminal nodes. Alternatively, scikit-learn uses the total sample weighted impurity of the terminal nodes for . As shown above, the impurity of a node depends on the criterion. Minimal cost-complexity pruning finds the subtree of that minimizes . The cost complexity measure of a single node is . The branch, , is defined to be a tree where node is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, . However, the cost complexity measure of a node, , and its branch, , can be equal depending on . We define the effective of a node to be the value where they are equal, or . A non-terminal node with the smallest value of is the weakest link and will be pruned. This process stops when the pruned trees minimal is greater than the parameter. Examples: Post pruning decision trees with cost complexity pruning References: BRE L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984. https://en.wikipedia.org/wiki/Decision_tree_learning https://en.wikipedia.org/wiki/Predictive_analytics J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993. T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009. \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.1. Bagging meta-estimator  In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees). Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set: When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [B1999] . When samples are drawn with replacement, then the method is known as Bagging [B1996] . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [H1998] . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [LG2012] . In scikit-learn, bagging methods are offered as a unified BaggingClassifier meta-estimator (resp. BaggingRegressor ), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, and control the size of the subsets (in terms of samples and features), while and control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting . As an example, the snippet below illustrates how to instantiate a bagging ensemble of KNeighborsClassifier base estimators, each built on random subsets of 50% of the samples and 50% of the features. Examples: Single estimator versus bagging: bias-variance decomposition References B1999 L. Breiman, Pasting small votes for classification in large databases and on-line, Machine Learning, 36(1), 85-103, 1999. B1996 L. Breiman, Bagging predictors, Machine Learning, 24(2), 123-140, 1996. H1998 T. Ho, The random subspace method for constructing decision forests, Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998. LG2012 G. Louppe and P. Geurts, Ensembles on Random Patches, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012. \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.2. Forests of randomized trees  The sklearn.ensemble module includes two averaging algorithms based on randomized decision trees : the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques [B1998] specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers. As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of size holding the training samples, and an array Y of size holding the target values (class labels) for the training samples: Like decision trees , forests of trees also extend to multi-output problems (if Y is an array of size ). 1.11.2.1. Random Forests  In random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size . (See the parameter tuning guidelines for more details). The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model. In contrast to the original publication [B2001] , the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class. 1.11.2.2. Extremely Randomized Trees  In extremely randomized trees (see ExtraTreesClassifier and ExtraTreesRegressor classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias: 1.11.2.3. Parameters  The main parameters to adjust when using these methods is and . The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are (always considering all features instead of a random subset) for regression problems, and (using a random subset of size ) for classification tasks (where is the number of features in the data). Good results are often achieved when setting in combination with (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default ( ) while the default strategy for extra-trees is to use the whole dataset ( ). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting . Note The size of the model with the default parameters is , where is the number of trees and is the number of samples. In order to reduce the size of the model, you can change these parameters: , , and . 1.11.2.4. Parallelization  Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the parameter. If then computations are partitioned into jobs, and run on cores of the machine. If then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using jobs will unfortunately not be times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets). Examples: Plot the decision surfaces of ensembles of trees on the iris dataset Pixel importances with a parallel forest of trees Face completion with a multi-output estimators References B2001 Breiman, Random Forests, Machine Learning, 45(1), 5-32, 2001. B1998 Breiman, Arcing Classifiers, Annals of Statistics 1998. P. Geurts, D. Ernst., and L. Wehenkel, Extremely randomized trees, Machine Learning, 63(1), 3-42, 2006. 1.11.2.5. Feature importance evaluation  The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features . In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature. By averaging the estimates of predictive ability over several randomized trees one can reduce the variance of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to [L2014] for more information on MDI and feature importance evaluation with Random Forests. Warning The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore do not necessarily inform us on which features are most important to make good predictions on held-out dataset . Secondly, they favor high cardinality features , that is features with many unique values. Permutation feature importance is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: Permutation Importance vs Random Forest Feature Importance (MDI) . The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a ExtraTreesClassifier model. In practice those estimates are stored as an attribute named on the fitted model. This is an array with shape whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function. Examples: Pixel importances with a parallel forest of trees Feature importances with forests of trees References L2014 G. Louppe, Understanding Random Forests: From Theory to Practice, PhD Thesis, U. of Liege, 2014. 1.11.2.6. Totally Random Trees Embedding  RandomTreesEmbedding implements an unsupervised transformation of the data. Using a forest of completely random trees, RandomTreesEmbedding encodes the data by the indices of the leaves a data point ends up in. This index is then encoded in a one-of-K manner, leading to a high dimensional, sparse binary coding. This coding can be computed very efficiently and can then be used as a basis for other learning tasks. The size and sparsity of the code can be influenced by choosing the number of trees and the maximum depth per tree. For each tree in the ensemble, the coding contains one entry of one. The size of the coding is at most , the maximum number of leaves in the forest. As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation. Examples: Hashing feature transformation using Totally Random Trees Manifold learning on handwritten digits: Locally Linear Embedding, Isomap compares non-linear dimensionality reduction techniques on handwritten digits. Feature transformations with ensembles of trees compares supervised and unsupervised tree based feature transformations. See also Manifold learning techniques can also be useful to derive non-linear representations of feature space, also these approaches focus also on dimensionality reduction. \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.3. AdaBoost  The module sklearn.ensemble includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995] . The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights , , , to each of the training samples. Initially, those weights are all set to , so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence [HTF] . AdaBoost can be used both for classification and regression problems: For multi-class classification, AdaBoostClassifier implements AdaBoost-SAMME and AdaBoost-SAMME.R [ZZRH2009] . For regression, AdaBoostRegressor implements AdaBoost.R2 [D1997] . 1.11.3.1. Usage  The following example shows how to fit an AdaBoost classifier with 100 weak learners: The number of weak learners is controlled by the parameter . The parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the parameter. The main parameters to tune to obtain good results are and the complexity of the base estimators (e.g., its depth or minimum required number of samples to consider a split ). Examples: Discrete versus Real AdaBoost compares the classification error of a decision stump, decision tree, and a boosted decision stump using AdaBoost-SAMME and AdaBoost-SAMME.R. Multi-class AdaBoosted Decision Trees shows the performance of AdaBoost-SAMME and AdaBoost-SAMME.R on a multi-class problem. Two-class AdaBoost shows the decision boundary and decision function values for a non-linearly separable two-class problem using AdaBoost-SAMME. Decision Tree Regression with AdaBoost demonstrates regression with the AdaBoost.R2 algorithm. References FS1995 Y. Freund, and R. Schapire, A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, 1997. ZZRH2009 J. Zhu, H. Zou, S. Rosset, T. Hastie. Multi-class AdaBoost, 2009. D1997 Drucker. Improving Regressors using Boosting Techniques, 1997. HTF ( 1 , 2 , 3 ) T. Hastie, R. Tibshirani and J. Friedman, Elements of Statistical Learning Ed. 2, Springer, 2009. \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.4. Gradient Tree Boosting  Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems in a variety of areas including Web search ranking and ecology. The module sklearn.ensemble provides methods for both classification and regression via gradient boosted decision trees. Note Scikit-learn 0.21 introduces two new experimental implementations of gradient boosting trees, namely HistGradientBoostingClassifier and HistGradientBoostingRegressor , inspired by LightGBM (See [LightGBM] ). These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples. They also have built-in support for missing values, which avoids the need for an imputer. These estimators are described in more detail below in Histogram-Based Gradient Boosting . The following guide focuses on GradientBoostingClassifier and GradientBoostingRegressor , which might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting. The usage and the parameters of GradientBoostingClassifier and GradientBoostingRegressor are described below. The 2 most important parameters of these estimators are and . 1.11.4.1. Classification  GradientBoostingClassifier supports both binary and multi-class classification. The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners: The number of weak learners (i.e. regression trees) is controlled by the parameter ; The size of each tree can be controlled either by setting the tree depth via or by setting the number of leaf nodes via . The is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via shrinkage . Note Classification with more than 2 classes requires the induction of regression trees at each iteration, thus, the total number of induced trees equals . For datasets with a large number of classes we strongly recommend to use HistGradientBoostingClassifier as an alternative to GradientBoostingClassifier . 1.11.4.2. Regression  GradientBoostingRegressor supports a number of different loss functions for regression which can be specified via the argument ; the default loss function for regression is least squares ( ). The figure below shows the results of applying GradientBoostingRegressor with least squares loss and 500 base learners to the Boston house price dataset ( sklearn.datasets.load_boston ). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the train_score_ attribute of the gradient boosting model. The test error at each iterations can be obtained via the staged_predict method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. ) by early stopping. The plot on the right shows the impurity-based feature importances which can be obtained via the property. Examples: Gradient Boosting regression Gradient Boosting Out-of-Bag estimates 1.11.4.3. Fitting additional weak-learners  Both GradientBoostingRegressor and GradientBoostingClassifier support which allows you to add more estimators to an already fitted model. 1.11.4.4. Controlling the tree size  The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth can capture interactions of order . There are two ways in which the size of the individual regression trees can be controlled. If you specify then complete binary trees of depth will be grown. Such trees will have (at most) leaf nodes and split nodes. Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter . In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with has split nodes and thus can model interactions of up to order . We found that gives comparable results to but is significantly faster to train at the expense of a slightly higher training error. The parameter corresponds to the variable in the chapter on gradient boosting in [F2001] and is related to the parameter in Rs gbm package where . 1.11.4.5. Mathematical formulation  We first present GBRT for regression, and then detail the classification case. 1.11.4.5.1. Regression  GBRT regressors are additive models whose prediction for a given input is of the following form: \\\\[\\\\hat{y_i}  F_M(x_i)  \\\\sum_{m1}^{M} h_m(x_i)\\\\] where the are estimators called weak learners in the context of boosting. Gradient Tree Boosting uses decision tree regressors of fixed size as weak learners. The constant M corresponds to the parameter. Similar to other boosting algorithms, a GBRT is built in a greedy fashion: \\\\[F_m(x)  F_{m-1}(x) + h_m(x),\\\\] where the newly added tree is fitted in order to minimize a sum of losses , given the previous ensemble : \\\\[h_m  \\\\arg\\\\min_{h} L_m  \\\\arg\\\\min_{h} \\\\sum_{i1}^{n} l(y_i, F_{m-1}(x_i) + h(x_i)),\\\\] where is defined by the parameter, detailed in the next section. By default, the initial model is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values. The initial model can also be specified via the argument. Using a first-order Taylor approximation, the value of can be approximated as follows: \\\\[l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\\\approx l(y_i, F_{m-1}(x_i)) + h_m(x_i) \\\\left[ \\\\frac{\\\\partial l(y_i, F(x_i))}{\\\\partial F(x_i)} \\\\right]_{FF_{m - 1}}.\\\\] Note Briefly, a first-order Taylor approximation says that . Here, corresponds to , and corresponds to The quantity is the derivative of the loss with respect to its second parameter, evaluated at . It is easy to compute for any given in a closed form since the loss is differentiable. We will denote it by . Removing the constant terms, we have: \\\\[h_m \\\\approx \\\\arg\\\\min_{h} \\\\sum_{i1}^{n} h(x_i) g_i\\\\] This is minimized if is fitted to predict a value that is proportional to the negative gradient . Therefore, at each iteration, the estimator is fitted to predict the negative gradients of the samples . The gradients are updated at each iteration. This can be considered as some kind of gradient descent in a functional space. Note For some losses, e.g. the least absolute deviation (LAD) where the gradients are , the values predicted by a fitted are not accurate enough: the tree can only output integer values. As a result, the leaves values of the tree are modified once the tree is fitted, such that the leaves values minimize the loss . The update is loss-dependent: for the LAD loss, the value of a leaf is updated to the median of the samples in that leaf. 1.11.4.5.2. Classification  Gradient boosting for classification is very similar to the regression case. However, the sum of the trees is not homogeneous to a prediction: it cannot be a class, since the trees predict continuous values. The mapping from the value to a class or a probability is loss-dependent. For the deviance (or log-loss), the probability that belongs to the positive class is modeled as where is the sigmoid function. For multiclass classification, K trees (for K classes) are built at each of the iterations. The probability that belongs to class k is modeled as a softmax of the values. Note that even for a classification task, the sub-estimator is still a regressor, not a classifier. This is because the sub-estimators are trained to predict (negative) gradients , which are always continuous quantities. 1.11.4.6. Loss Functions  The following loss functions are supported and can be specified using the parameter : Regression Least squares ( ): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values. Least absolute deviation ( ): A robust loss function for regression. The initial model is given by the median of the target values. Huber ( ): Another robust loss function that combines least squares and least absolute deviation; use to control the sensitivity with regards to outliers (see [F2001] for more details). Quantile ( ): A loss function for quantile regression. Use to specify the quantile. This loss function can be used to create prediction intervals (see Prediction Intervals for Gradient Boosting Regression ). Classification Binomial deviance ( ): The negative binomial log-likelihood loss function for binary classification (provides probability estimates). The initial model is given by the log odds-ratio. Multinomial deviance ( ): The negative multinomial log-likelihood loss function for multi-class classification with mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes. Exponential loss ( ): The same loss function as AdaBoostClassifier . Less robust to mislabeled examples than ; can only be used for binary classification. 1.11.4.7. Shrinkage via learning rate  [F2001] proposed a simple regularization strategy that scales the contribution of each weak learner by a constant factor : \\\\[F_m(x)  F_{m-1}(x) + \\\\nu h_m(x)\\\\] The parameter is also called the learning rate because it scales the step length the gradient descent procedure; it can be set via the parameter. The parameter strongly interacts with the parameter , the number of weak learners to fit. Smaller values of require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of favor better test error. [HTF] recommend to set the learning rate to a small constant (e.g. ) and choose by early stopping. For a more detailed discussion of the interaction between and see [R2007] . 1.11.4.8. Subsampling  [F1999] proposed stochastic gradient boosting, which combines gradient boosting with bootstrap averaging (bagging). At each iteration the base classifier is trained on a fraction of the available training data. The subsample is drawn without replacement. A typical value of is 0.5. The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in RandomForestClassifier . The number of subsampled features can be controlled via the parameter. Note Using a small value can significantly decrease the runtime. Stochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improvement in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The improvements are stored in the attribute oob_improvement_ . holds the improvement in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time consuming. Examples: Gradient Boosting regularization Gradient Boosting Out-of-Bag estimates OOB Errors for Random Forests 1.11.4.9. Interpretation with feature importance  Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models. Often features do not contribute equally to predict the target response; in many situations the majority of the features are in fact irrelevant. When interpreting a model, the first question usually is: what are those important features and how do they contributing in predicting the target response? Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the impurity-based feature importance of each tree (see Feature importance evaluation for more details). The feature importance scores of a fit gradient boosting model can be accessed via the property: Note that this computation of feature importance is based on entropy, and it is distinct from sklearn.inspection.permutation_importance which is based on permutation of the features. Examples: Gradient Boosting regression \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.5. Histogram-Based Gradient Boosting  Scikit-learn 0.21 introduced two new experimental implementations of gradient boosting trees, namely HistGradientBoostingClassifier and HistGradientBoostingRegressor , inspired by LightGBM (See [LightGBM] ). These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples. They also have built-in support for missing values, which avoids the need for an imputer. These fast estimators first bin the input samples into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. The API of these estimators is slightly different, and some of the features from GradientBoostingClassifier and GradientBoostingRegressor are not yet supported, for instance some loss functions. These estimators are still experimental : their predictions and their API might change without any deprecation cycle. To use them, you need to explicitly import : Examples: Partial Dependence Plots 1.11.5.1. Usage  Most of the parameters are unchanged from GradientBoostingClassifier and GradientBoostingRegressor . One exception is the parameter that replaces , and controls the number of iterations of the boosting process: Available losses for regression are least_squares, least_absolute_deviation, which is less sensitive to outliers, and poisson, which is well suited to model counts and frequencies. For classification, binary_crossentropy is used for binary classification and categorical_crossentropy is used for multiclass classification. By default the loss is auto and will select the appropriate loss depending on y passed to fit . The size of the trees can be controlled through the , , and parameters. The number of bins used to bin the data is controlled with the parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible, which is the default. The parameter is a regularizer on the loss function and corresponds to in equation (2) of [XGBoost] . Note that early-stopping is enabled by default if the number of samples is larger than 10,000 . The early-stopping behaviour is controlled via the , , , , and parameters. It is possible to early-stop using an arbitrary scorer , or just the training or validation loss. Note that for technical reasons, using a scorer is significantly slower than using the loss. By default, early-stopping is performed if there are at least 10,000 samples in the training set, using the validation loss. 1.11.5.2. Missing values support  HistGradientBoostingClassifier and HistGradientBoostingRegressor have built-in support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently: When the missingness pattern is predictive, the splits can be done on whether the feature value is missing or not: If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples. 1.11.5.3. Sample weight support  HistGradientBoostingClassifier and HistGradientBoostingRegressor sample support weights during fit . The following toy example demonstrates how the model ignores the samples with zero sample weights: As you can see, the is comfortably classified as since the first two samples are ignored due to their sample weights. Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights. Note that the binning stage (specifically the quantiles computation) does not take the weights into account. 1.11.5.4. Monotonic Constraints  Depending on the problem at hand, you may have prior knowledge indicating that a given feature should in general have a positive (or negative) effect on the target value. For example, all else being equal, a higher credit score should increase the probability of getting approved for a loan. Monotonic constraints allow you to incorporate such prior knowledge into the model. A positive monotonic constraint is a constraint of the form: , where is the predictor with two features. Similarly, a negative monotonic constraint is of the form: . Note that monotonic constraints only constraint the output all else being equal. Indeed, the following relation is not enforced by a positive constraint: . You can specify a monotonic constraint on each feature using the parameter. For each feature, a value of 0 indicates no constraint, while -1 and 1 indicate a negative and positive constraint, respectively: In a binary classification context, imposing a monotonic constraint means that the feature is supposed to have a positive / negative effect on the probability to belong to the positive class. Monotonic constraints are not supported for multiclass context. Examples: Monotonic Constraints 1.11.5.5. Low-level parallelism  HistGradientBoostingClassifier and HistGradientBoostingRegressor have implementations that use OpenMP for parallelization through Cython. For more details on how to control the number of threads, please refer to our Parallelism notes. The following parts are parallelized: mapping samples from real values to integer-valued bins (finding the bin thresholds is however sequential) building histograms is parallelized over features finding the best split point at a node is parallelized over features during fit, mapping samples into the left and right children is parallelized over samples gradient and hessians computations are parallelized over samples predicting is parallelized over samples 1.11.5.6. Why its faster  The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs GradientBoostingClassifier and GradientBoostingRegressor ) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of where is the number of samples at the node. HistGradientBoostingClassifier and HistGradientBoostingRegressor , in contrast, do not require sorting the feature values and instead use a data-structure called a histogram, where the samples are implicitly ordered. Building a histogram has a complexity, so the node splitting procedure has a complexity, much smaller than the previous one. In addition, instead of considering split points, we here consider only split points, which is much smaller. In order to build histograms, the input data needs to be binned into integer-valued bins. This binning procedure does require sorting the feature values, but it only happens once at the very beginning of the boosting process (not at each node, like in GradientBoostingClassifier and GradientBoostingRegressor ). Finally, many parts of the implementation of HistGradientBoostingClassifier and HistGradientBoostingRegressor are parallelized. References F1999 Friedmann, Jerome H., 2007, Stochastic Gradient Boosting R2007 G. Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007 XGBoost Tianqi Chen, Carlos Guestrin, XGBoost: A Scalable Tree Boosting System LightGBM ( 1 , 2 ) Ke et. al. LightGBM: A Highly Efficient Gradient BoostingDecision Tree \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.6. Voting Classifier  The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. 1.11.6.1. Majority Class Labels (Majority/Hard Voting)  In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. E.g., if the prediction for a given sample is classifier 1 -> class 1 classifier 2 -> class 1 classifier 3 -> class 2 the VotingClassifier (with ) would classify the sample as class 1 based on the majority class label. In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E.g., in the following scenario classifier 1 -> class 2 classifier 2 -> class 1 the class label 1 will be assigned to the sample. 1.11.6.2. Usage  The following example shows how to fit the majority rule classifier: 1.11.6.3. Weighted Average Probabilities (Soft Voting)  In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. Specific weights can be assigned to each classifier via the parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. To illustrate this with a simple example, lets assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w11, w21, w31. The weighted average probabilities for a sample would then be calculated as follows: classifier class 1 class 2 class 3 classifier 1 w1 * 0.2 w1 * 0.5 w1 * 0.3 classifier 2 w2 * 0.6 w2 * 0.3 w2 * 0.1 classifier 3 w3 * 0.3 w3 * 0.4 w3 * 0.3 weighted average 0.37 0.4 0.23 Here, the predicted class label is 2, since it has the highest average probability. The following example illustrates how the decision regions may change when a soft VotingClassifier is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier: 1.11.6.4. Using the with  The VotingClassifier can also be used together with GridSearchCV in order to tune the hyperparameters of the individual estimators: 1.11.6.5. Usage  In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support method): Optionally, weights can be provided for the individual classifiers: \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.7. Voting Regressor  The idea behind the VotingRegressor is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses. 1.11.7.1. Usage  The following example shows how to fit the VotingRegressor: Examples: Plot individual and voting regression predictions \"\\n',\n",
       " '\"sklearn_1_11_ensemble_methods 1.11. Ensemble methods modules/ensemble.html  1.11.8. Stacked generalization  Stacked generalization is a method for combining estimators to reduce their biases [W1992] [HTF] . More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation. The StackingClassifier and StackingRegressor provide such strategies which can be applied to classification and regression problems. The parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators: The will use the predictions of the as input. It needs to be a classifier or a regressor when using StackingClassifier or StackingRegressor , respectively: To train the and , the method needs to be called on the training data: During training, the are fitted on the whole training data . They will be used when calling or . To generalize and avoid over-fitting, the is trained on out-samples using sklearn.model_selection.cross_val_predict internally. For StackingClassifier , note that the output of the is controlled by the parameter and it is called by each estimator. This parameter is either a string, being estimator method names, or which will automatically identify an available method depending on the availability, tested in the order of preference: , and . A StackingRegressor and StackingClassifier can be used as any other regressor or classifier, exposing a , , and methods, e.g.: Note that it is also possible to get the output of the stacked using the method: In practise, a stacking predictor predict as good as the best predictor of the base layer and even sometimes outputperform it by combining the different strength of the these predictors. However, training a stacking predictor is computationally expensive. Note For StackingClassifier , when using , the first column is dropped when the problem is a binary classification problem. Indeed, both probability columns predicted by each estimator are perfectly collinear. Note Multiple stacking layers can be achieved by assigning to a StackingClassifier or StackingRegressor : References W1992 Wolpert, David H. Stacked generalization. Neural networks 5.2 (1992): 241-259. \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.1. Multilabel classification format  In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements, corresponds to the subset of labels. An array such as represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample. Producing multilabel data as a list of sets of labels may be more intuitive. The MultiLabelBinarizer transformer can be used to convert between a collection of collections of labels and the indicator format. \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.2. One-Vs-The-Rest  This strategy, also known as one-vs-all , is implemented in OneVsRestClassifier . The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice. 1.12.2.1. Multiclass learning  Below is an example of multiclass learning using OvR: 1.12.2.2. Multilabel learning  OneVsRestClassifier also supports multilabel classification. To use this feature, feed the classifier an indicator matrix, in which cell [i, j] indicates the presence of label j in sample i. Examples: Multilabel classification \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.3. One-Vs-One  OneVsOneClassifier constructs one classifier per pair of classes. At prediction time, the class which received the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers. Since it requires to fit classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which dont scale well with . This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used times. The decision function is the result of a monotonic transformation of the one-versus-one classification. 1.12.3.1. Multiclass learning  Below is an example of multiclass learning using OvO: References: Pattern Recognition and Machine Learning. Springer, Christopher M. Bishop, page 183, (First Edition) \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.4. Error-Correcting Output-Codes  Output-code based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy. In this implementation, we simply use a randomly-generated code book as advocated in 3 although more elaborate methods may be added in the future. At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. In OutputCodeClassifier , the attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since is much smaller than n_classes. A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name error-correcting. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging. 1.12.4.1. Multiclass learning  Below is an example of multiclass learning using Output-Codes: References: Solving multiclass learning problems via error-correcting output codes, Dietterich T., Bakiri G., Journal of Artificial Intelligence Research 2, 1995. 3 The error coding method and PICTs, James G., Hastie T., Journal of Computational and Graphical statistics 7, 1998. The Elements of Statistical Learning, Hastie T., Tibshirani R., Friedman J., page 606 (second-edition) 2008. \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.5. Multioutput regression  Multioutput regression support can be added to any regressor with MultiOutputRegressor . This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As MultiOutputRegressor fits one regressor per target it can not take advantage of correlations between targets. Below is an example of multioutput regression: \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.6. Multioutput classification  Multioutput classification support can be added to any classifier with MultiOutputClassifier . This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3,yn). Below is an example of multioutput classification: \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.7. Classifier Chain  Classifier chains (see ClassifierChain ) are a way of combining a number of binary classifiers into a single multi-label model that is capable of exploiting correlations among targets. For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number. When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features. Clearly the order of the chain is important. The first model in the chain has no information about the other labels while the last model in the chain has features indicating the presence of all of the other labels. In general one does not know the optimal ordering of the models in the chain so typically many randomly ordered chains are fit and their predictions are averaged together. References: Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, Classifier Chains for Multi-label Classification, 2009. \"\\n',\n",
       " '\"sklearn_1_12_multiclass_and_multilabel_algorithms 1.12. Multiclass and multilabel algorithms modules/multiclass.html  1.12.8. Regressor Chain  Regressor chains (see RegressorChain ) is analogous to ClassifierChain as a way of combining a number of regressions into a single multi-target model that is capable of exploiting correlations among targets. \"\\n',\n",
       " '\"sklearn_1_13_feature_selection 1.13. Feature selection modules/feature_selection.html  1.13.1. Removing features with low variance  VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesnt meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples. As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by \\\\[\\\\mathrm{Var}[X]  p(1 - p)\\\\] so we can select using the threshold : As expected, has removed the first column, which has a probability of containing a zero. \"\\n',\n",
       " '\"sklearn_1_13_feature_selection 1.13. Feature selection modules/feature_selection.html  1.13.2. Univariate feature selection  Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the method: SelectKBest removes all but the highest scoring features SelectPercentile removes all but a user-specified highest scoring percentage of features using common univariate statistical tests for each feature: false positive rate SelectFpr , false discovery rate SelectFdr , or family wise error SelectFwe . GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator. For instance, we can perform a test to the samples to retrieve only the two best features as follows: These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile ): For regression: f_regression , mutual_info_regression For classification: chi2 , f_classif , mutual_info_classif The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation. Feature selection with sparse data If you use sparse data (i.e. data represented as sparse matrices), chi2 , mutual_info_regression , mutual_info_classif will deal with the data without making it dense. Warning Beware not to use a regression scoring function with a classification problem, you will get useless results. Examples: Univariate Feature Selection Comparison of F-test and mutual information \"\\n',\n",
       " '\"sklearn_1_13_feature_selection 1.13. Feature selection modules/feature_selection.html  1.13.3. Recursive feature elimination  Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination ( RFE ) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a attribute or through a attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. RFECV performs RFE in a cross-validation loop to find the optimal number of features. Examples: Recursive feature elimination : A recursive feature elimination example showing the relevance of pixels in a digit classification task. Recursive feature elimination with cross-validation : A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation. \"\\n',\n",
       " '\"sklearn_1_13_feature_selection 1.13. Feature selection modules/feature_selection.html  1.13.4. Feature selection using SelectFromModel  SelectFromModel is a meta-transformer that can be used along with any estimator that has a or attribute after fitting. The features are considered unimportant and removed, if the corresponding or values are below the provided parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are mean, median and float multiples of these like 0.1*mean. In combination with the criteria, one can use the parameter to set a limit on the number of features to select. For examples on how it is to be used refer to the sections below. Examples Feature selection using SelectFromModel and LassoCV : Selecting the two most important features from the diabetes dataset without knowing the threshold beforehand. 1.13.4.1. L1-based feature selection  Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with feature_selection.SelectFromModel to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the linear_model.Lasso for regression, and of linear_model.LogisticRegression and svm.LinearSVC for classification: With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected. Examples: Classification of text documents using sparse features : Comparison of different algorithms for document classification including L1-based feature selection. L1-recovery and compressive sensing For a good choice of alpha, the Lasso can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be sufficiently large, or L1 models will perform at random, where sufficiently large depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated. There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation ( LassoCV or LassoLarsCV ), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC ( LassoLarsIC ) tends, on the opposite, to set high values of alpha. Reference Richard G. Baraniuk Compressive Sensing, IEEE Signal Processing Magazine [120] July 2007 http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf 1.13.4.2. Tree-based feature selection  Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute impurity-based feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer): Examples: Feature importances with forests of trees : example on synthetic data showing the recovery of the actually meaningful features. Pixel importances with a parallel forest of trees : example on face recognition data. \"\\n',\n",
       " '\"sklearn_1_13_feature_selection 1.13. Feature selection modules/feature_selection.html  1.13.5. Feature selection as part of a pipeline  Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a sklearn.pipeline.Pipeline : In this snippet we make use of a sklearn.svm.LinearSVC coupled with sklearn.feature_selection.SelectFromModel to evaluate feature importances and select the most relevant features. Then, a sklearn.ensemble.RandomForestClassifier is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the sklearn.pipeline.Pipeline examples for more details. \"\\n',\n",
       " '\"sklearn_1_14_semi-supervised 1.14. Semi-Supervised modules/label_propagation.html  1.14.1. Label Propagation  Label propagation denotes a few variations of semi-supervised graph inference algorithms. A few features available in this model: Can be used for classification and regression tasks Kernel methods to project data into alternate dimensional spaces provides two label propagation models: LabelPropagation and LabelSpreading . Both work by constructing a similarity graph over all items in the input dataset. An illustration of label-propagation: the structure of unlabeled observations is consistent with the class structure, and thus the class label can be propagated to the unlabeled observations of the training set.  LabelPropagation and LabelSpreading differ in modifications to the similarity matrix that graph and the clamping effect on the label distributions. Clamping allows the algorithm to change the weight of the true ground labeled data to some degree. The LabelPropagation algorithm performs hard clamping of input labels, which means . This clamping factor can be relaxed, to say , which means that we will always retain 80 percent of our original label distribution, but the algorithm gets to change its confidence of the distribution within 20 percent. LabelPropagation uses the raw similarity matrix constructed from the data with no modifications. In contrast, LabelSpreading minimizes a loss function that has regularization properties, as such it is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix. This procedure is also used in Spectral clustering . Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available: rbf ( ). is specified by keyword gamma. knn ( ). is specified by keyword n_neighbors. The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times. Examples Decision boundary of label propagation versus SVM on the Iris dataset Label Propagation learning a complex structure Label Propagation digits: Demonstrating performance Label Propagation digits active learning References [1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised Learning (2006), pp. 193-216 [2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005 https://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf \"\\n',\n",
       " '\"sklearn_1_16_probability_calibration 1.16. Probability calibration modules/calibration.html  1.16.1. Calibration curves  The following plot compares how well the probabilistic predictions of different classifiers are calibrated, using calibration_curve . The x axis represents the average predicted probability in each bin. The y axis is the fraction of positives , i.e. the proportion of samples whose class is the positive class (in each bin). LogisticRegression returns well calibrated predictions by default as it directly optimizes log-loss. In contrast, the other methods return biased probabilities; with different biases per method: GaussianNB tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features. RandomForestClassifier shows the opposite behavior: the histograms show peaks at approximately 0.2 and 0.9 probability, while probabilities close to 0 or 1 are very rare. An explanation for this is given by Niculescu-Mizil and Caruana 1 : Methods such as bagging and random forests that average predictions from a base set of models can have difficulty making predictions near 0 and 1 because variance in the underlying base models will bias predictions that should be near zero or one away from these values. Because predictions are restricted to the interval [0,1], errors caused by variance tend to be one-sided near zero and one. For example, if a model should predict p  0 for a case, the only way bagging can achieve this is if all bagged trees predict zero. If we add noise to the trees that bagging is averaging over, this noise will cause some trees to predict values larger than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We observe this effect most strongly with random forests because the base-level trees trained with random forests have relatively high variance due to feature subsetting. As a result, the calibration curve also referred to as the reliability diagram (Wilks 1995 2 ) shows a characteristic sigmoid shape, indicating that the classifier could trust its intuition more and return probabilities closer to 0 or 1 typically. Linear Support Vector Classification ( LinearSVC ) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana 1 ), which focus on hard samples that are close to the decision boundary (the support vectors). \"\\n',\n",
       " '\"sklearn_1_16_probability_calibration 1.16. Probability calibration modules/calibration.html  1.16.2. Calibrating a classifier  Calibrating a classifier consists in fitting a regressor (called a calibrator ) that maps the output of the classifier (as given by predict or predict_proba ) to a calibrated probability in [0, 1]. Denoting the output of the classifier for a given sample by , the calibrator tries to predict . The samples that are used to train the calibrator should not be used to train the target classifier. \"\\n',\n",
       " '\"sklearn_1_16_probability_calibration 1.16. Probability calibration modules/calibration.html  1.16.3. Usage  The CalibratedClassifierCV class is used to calibrate a classifier. CalibratedClassifierCV uses a cross-validation approach to fit both the classifier and the regressor. For each of the k couple, a classifier is trained on the train set, and its predictions on the test set are used to fit a regressor. We end up with k couples where each regressor maps the output of its corresponding classifier into [0, 1]. Each couple is exposed in the attribute, where each entry is a calibrated classifier with a predict_proba method that outputs calibrated probabilities. The output of predict_proba for the main CalibratedClassifierCV instance corresponds to the average of the predicted probabilities of the estimators in the list. The output of predict is the class that has the highest probability. The regressor that is used for calibration depends on the parameter. corresponds to a parametric approach based on Platts logistic model 3 , i.e. is modeled as where is the logistic function, and and are real numbers to be determined when fitting the regressor via maximum likelihood. will instead fit a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function (see sklearn.isotonic ). An already fitted classifier can be calibrated by setting . In this case, the data is only used to fit the regressor. It is up to the user make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor. CalibratedClassifierCV can calibrate probabilities in a multiclass setting if the base estimator supports multiclass predictions. The classifier is calibrated first for each class separately in a one-vs-rest fashion 4 . When predicting probabilities, the calibrated probabilities for each class are predicted separately. As those probabilities do not necessarily sum to one, a postprocessing is performed to normalize them. The sklearn.metrics.brier_score_loss may be used to evaluate how well a classifier is calibrated. Examples: Probability Calibration curves Probability Calibration for 3-class classification Probability calibration of classifiers Comparison of Calibration of Classifiers References: 1 ( 1 , 2 ) Predicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil & R. Caruana, ICML 2005 2 On the combination of forecast probabilities for consecutive precipitation periods. Wea. Forecasting, 5, 640650., Wilks, D. S., 1990a 3 Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods, J. Platt, (1999) 4 Transforming Classifier Scores into Accurate Multiclass Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002) \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.1. Multi-layer Perceptron  Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset, where is the number of dimensions for input and is the number of dimensions for output. Given a set of features and a target , it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output. Figure 1 : One hidden layer MLP.  The leftmost layer, known as the input layer, consists of a set of neurons representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation , followed by a non-linear activation function - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values. The module contains the public attributes and . is a list of weight matrices, where weight matrix at index represents the weights between layer and layer . is a list of bias vectors, where the vector at index represents the bias values added to layer . The advantages of Multi-layer Perceptron are: Capability to learn non-linear models. Capability to learn models in real-time (on-line learning) using . The disadvantages of Multi-layer Perceptron (MLP) include: MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy. MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations. MLP is sensitive to feature scaling. Please see Tips on Practical Use section that addresses some of these disadvantages. \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.2. Classification  Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation . MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples: After fitting (training), the model can predict labels for new samples: MLP can fit a non-linear model to the training data. contains the weight matrices that constitute the model parameters: Currently, MLPClassifier supports only the Cross-Entropy loss function, which allows probability estimates by running the method. MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates per sample : MLPClassifier supports multi-class classification by applying Softmax as the output function. Further, the model supports multi-label classification in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to are rounded to , otherwise to . For a predicted output of a sample, the indices where the value is represents the assigned classes of that sample: See the examples below and the docstring of MLPClassifier.fit for further information. Examples: Compare Stochastic learning strategies for MLPClassifier Visualization of MLP weights on MNIST \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.3. Regression  Class MLPRegressor implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values. MLPRegressor also supports multi-output regression, in which a sample can have more than one target. \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.4. Regularization  Both MLPRegressor and MLPClassifier use parameter for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot displays varying decision function with value of alpha. See the examples below for further information. Examples: Varying regularization in Multi-layer Perceptron \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.5. Algorithms  MLP trains using Stochastic Gradient Descent , Adam , or L-BFGS . Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e. \\\\[w \\\\leftarrow w - \\\\eta (\\\\alpha \\\\frac{\\\\partial R(w)}{\\\\partial w} + \\\\frac{\\\\partial Loss}{\\\\partial w})\\\\] where is the learning rate which controls the step-size in the parameter space search. is the loss function used for the network. More details can be found in the documentation of SGD Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments. With SGD or Adam, training supports online and mini-batch learning. L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of L-BFGS . If the selected solver is L-BFGS, training does not support online nor mini-batch learning. \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.6. Complexity  Suppose there are training samples, features, hidden layers, each containing neurons - for simplicity, and output neurons. The time complexity of backpropagation is , where is the number of iterations. Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training. \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.7. Mathematical formulation  Given a set of training examples where and , a one hidden layer one hidden neuron MLP learns the function where and are model parameters. represent the weights of the input layer and hidden layer, respectively; and represent the bias added to the hidden layer and the output layer, respectively. is the activation function, set by default as the hyperbolic tan. It is given as, \\\\[g(z) \\\\frac{e^z-e^{-z}}{e^z+e^{-z}}\\\\] For binary classification, passes through the logistic function to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class. If there are more than two classes, itself would be a vector of size (n_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as, \\\\[\\\\text{softmax}(z)_i  \\\\frac{\\\\exp(z_i)}{\\\\sum_{l1}^k\\\\exp(z_l)}\\\\] where represents the th element of the input to softmax, which corresponds to class , and is the number of classes. The result is a vector containing the probabilities that sample belong to each class. The output is the class with the highest probability. In regression, the output remains as ; therefore, output activation function is just the identity function. MLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as, \\\\[Loss(\\\\hat{y},y,W)  -y \\\\ln {\\\\hat{y}} - (1-y) \\\\ln{(1-\\\\hat{y})} + \\\\alpha ||W||_2^2\\\\] where is an L2-regularization term (aka penalty) that penalizes complex models; and is a non-negative hyperparameter that controls the magnitude of the penalty. For regression, MLP uses the Square Error loss function; written as, \\\\[Loss(\\\\hat{y},y,W)  \\\\frac{1}{2}||\\\\hat{y} - y ||_2^2 + \\\\frac{\\\\alpha}{2} ||W||_2^2\\\\] Starting from initial random weights, multi-layer perceptron (MLP) minimizes the loss function by repeatedly updating these weights. After computing the loss, a backward pass propagates it from the output layer to the previous layers, providing each weight parameter with an update value meant to decrease the loss. In gradient descent, the gradient of the loss with respect to the weights is computed and deducted from . More formally, this is expressed as, \\\\[W^{i+1}  W^i - \\\\epsilon \\\\nabla {Loss}_{W}^{i}\\\\] where is the iteration step, and is the learning rate with a value larger than 0. The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number. \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.8. Tips on Practical Use  Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the same scaling to the test set for meaningful results. You can use StandardScaler for standardization. An alternative and recommended approach is to use StandardScaler in a Pipeline Finding a reasonable regularization parameter is best done using GridSearchCV , usually in the range . Empirically, we observed that converges faster and with better solutions on small datasets. For relatively large datasets, however, is very robust. It usually converges quickly and gives pretty good performance. with momentum or nesterovs momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned. \"\\n',\n",
       " '\"sklearn_1_17_neural_network_models_supervised 1.17. Neural network models (supervised) modules/neural_networks_supervised.html  1.17.9. More control with warm_start  If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using and and iterating yourself can be helpful: References: Learning representations by back-propagating errors. Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. Stochastic Gradient Descent L. Bottou - Website, 2010. Backpropagation Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011. Efficient BackProp Y. LeCun, L. Bottou, G. Orr, K. Mller - In Neural Networks: Tricks of the Trade 1998. Adam: A method for stochastic optimization. Kingma, Diederik, and Jimmy Ba. arXiv preprint arXiv:1412.6980 (2014). \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.10. Bayesian Regression  Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. This can be done by introducing uninformative priors over the hyper parameters of the model. The regularization used in Ridge regression and classification is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients with precision . Instead of setting manually, it is possible to treat it as a random variable to be estimated from the data. To obtain a fully probabilistic model, the output is assumed to be Gaussian distributed around : \\\\[p(y|X,w,\\\\alpha)  \\\\mathcal{N}(y|X w,\\\\alpha)\\\\] where is again treated as a random variable that is to be estimated from the data. The advantages of Bayesian Regression are: It adapts to the data at hand. It can be used to include regularization parameters in the estimation procedure. The disadvantages of Bayesian regression include: Inference of the model can be time consuming. References A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning Original Algorithm is detailed in the book by Radford M. Neal 1.1.10.1. Bayesian Ridge Regression  BayesianRidge estimates a probabilistic model of the regression problem as described above. The prior for the coefficient is given by a spherical Gaussian: \\\\[p(w|\\\\lambda)  \\\\mathcal{N}(w|0,\\\\lambda^{-1}\\\\mathbf{I}_{p})\\\\] The priors over and are chosen to be gamma distributions , the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression , and is similar to the classical Ridge . The parameters , and are estimated jointly during the fit of the model, the regularization parameters and being estimated by maximizing the log marginal likelihood . The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters and is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters and . There are four more hyperparameters, , , and of the gamma prior distributions over and . These are usually chosen to be non-informative . By default . Bayesian Ridge Regression is used for regression: After being fitted, the model can then be used to predict new values: The coefficients of the model can be accessed: Due to the Bayesian framework, the weights found are slightly different to the ones found by Ordinary Least Squares . However, Bayesian Ridge Regression is more robust to ill-posed problems. Examples: Bayesian Ridge Regression Curve Fitting with Bayesian Ridge Regression References: Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006 David J. C. MacKay, Bayesian Interpolation , 1992. Michael E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine , 2001. 1.1.10.2. Automatic Relevance Determination - ARD  ARDRegression is very similar to Bayesian Ridge Regression , but can lead to sparser coefficients 1 2 . ARDRegression poses a different prior over , by dropping the assumption of the Gaussian being spherical. Instead, the distribution over is assumed to be an axis-parallel, elliptical Gaussian distribution. This means each coefficient is drawn from a Gaussian distribution, centered on zero and with a precision : \\\\[p(w|\\\\lambda)  \\\\mathcal{N}(w|0,A^{-1})\\\\] with . In contrast to Bayesian Ridge Regression , each coordinate of has its own standard deviation . The prior over all is chosen to be the same gamma distribution given by hyperparameters and . ARD is also known in the literature as Sparse Bayesian Learning and Relevance Vector Machine 3 4 . Examples: Automatic Relevance Determination Regression (ARD) References: 1 Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1 2 David Wipf and Srikantan Nagarajan: A new view of automatic relevance determination 3 Michael E. Tipping: Sparse Bayesian Learning and the Relevance Vector Machine 4 Tristan Fletcher: Relevance Vector Machines explained \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.11. Logistic regression  Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function . Logistic regression is implemented in LogisticRegression . This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional , or Elastic-Net regularization. Note Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting C to a very high value. As an optimization problem, binary class penalized logistic regression minimizes the following cost function: \\\\[\\\\min_{w, c} \\\\frac{1}{2}w^T w + C \\\\sum_{i1}^n \\\\log(\\\\exp(- y_i (X_i^T w + c)) + 1) .\\\\] Similarly, regularized logistic regression solves the following optimization problem: \\\\[\\\\min_{w, c} \\\\|w\\\\|_1 + C \\\\sum_{i1}^n \\\\log(\\\\exp(- y_i (X_i^T w + c)) + 1).\\\\] Elastic-Net regularization is a combination of and , and minimizes the following cost function: \\\\[\\\\min_{w, c} \\\\frac{1 - \\\\rho}{2}w^T w + \\\\rho \\\\|w\\\\|_1 + C \\\\sum_{i1}^n \\\\log(\\\\exp(- y_i (X_i^T w + c)) + 1),\\\\] where controls the strength of regularization vs. regularization (it corresponds to the parameter). Note that, in this notation, its assumed that the target takes values in the set at trial . We can also see that Elastic-Net is equivalent to when and equivalent to when . The solvers implemented in the class LogisticRegression are liblinear, newton-cg, lbfgs, sag and saga: The solver liblinear uses a coordinate descent (CD) algorithm, and relies on the excellent C++ LIBLINEAR library , which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a one-vs-rest fashion so separate binary classifiers are trained for all classes. This happens under the hood, so LogisticRegression instances using this solver behave as multiclass classifiers. For regularization sklearn.svm.l1_min_c allows to calculate the lower bound for C in order to get a non null (all feature weights to zero) model. The lbfgs, sag and newton-cg solvers only support regularization or no regularization, and are found to converge faster for some high-dimensional data. Setting to multinomial with these solvers learns a true multinomial logistic regression model 5 , which means that its probability estimates should be better calibrated than the default one-vs-rest setting. The sag solver uses Stochastic Average Gradient descent 6 . It is faster than other solvers for large datasets, when both the number of samples and the number of features are large. The saga solver 7 is a variant of sag that also supports the non-smooth . This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports . The lbfgs is an optimization algorithm that approximates the BroydenFletcherGoldfarbShanno algorithm 8 , which belongs to quasi-Newton methods. The lbfgs solver is recommended for use for small data-sets but for larger datasets its performance suffers. 9 The following table summarizes the penalties supported by each solver: Solvers Penalties liblinear lbfgs newton-cg sag saga Multinomial + L2 penalty no yes yes yes yes OVR + L2 penalty yes yes yes yes yes Multinomial + L1 penalty no no no no yes OVR + L1 penalty yes no no no yes Elastic-Net no no no no yes No penalty (none) no yes yes yes yes Behaviors Penalize the intercept (bad) yes no no no no Faster for large datasets no no no yes yes Robust to unscaled datasets yes yes yes no no The lbfgs solver is used by default for its robustness. For large datasets the saga solver is usually faster. For large dataset, you may also consider using SGDClassifier with log loss, which might be even faster but requires more tuning. Examples: L1 Penalty and Sparsity in Logistic Regression Regularization path of L1- Logistic Regression Plot multinomial and One-vs-Rest Logistic Regression Multiclass sparse logistic regression on 20newgroups MNIST classification using multinomial logistic + L1 Differences from liblinear: There might be a difference in the scores obtained between LogisticRegression with or LinearSVC and the external liblinear library directly, when and the fit (or) the data to be predicted are zeroes. This is because for the sample(s) with zero, LogisticRegression and LinearSVC predict the negative class, while liblinear predicts the positive class. Note that a model with and having many samples with zero, is likely to be a underfit, bad model and you are advised to set and increase the intercept_scaling. Note Feature selection with sparse logistic regression A logistic regression with penalty yields sparse models, and can thus be used to perform feature selection, as detailed in L1-based feature selection . Note P-value estimation It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The natively supports this. Within sklearn, one could use bootstrapping instead as well. LogisticRegressionCV implements Logistic Regression with built-in cross-validation support, to find the optimal and parameters according to the attribute. The newton-cg, sag, saga and lbfgs solvers are found to be faster for high-dimensional dense data, due to warm-starting (see Glossary ). References: 5 Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4 6 Mark Schmidt, Nicolas Le Roux, and Francis Bach: Minimizing Finite Sums with the Stochastic Average Gradient. 7 Aaron Defazio, Francis Bach, Simon Lacoste-Julien: SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. 8 https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm 9 Performance Evaluation of Lbfgs vs other solvers \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.12. Generalized Linear Regression  Generalized Linear Models (GLM) extend linear models in two ways 10 . First, the predicted values are linked to a linear combination of the input variables via an inverse link function as \\\\[\\\\hat{y}(w, X)  h(Xw).\\\\] Secondly, the squared loss function is replaced by the unit deviance of a distribution in the exponential family (or more precisely, a reproductive exponential dispersion model (EDM) 11 ). The minimization problem becomes: \\\\[\\\\min_{w} \\\\frac{1}{2 n_{\\\\text{samples}}} \\\\sum_i d(y_i, \\\\hat{y}_i) + \\\\frac{\\\\alpha}{2} ||w||_2,\\\\] where is the L2 regularization penalty. When sample weights are provided, the average becomes a weighted average. The following table lists some specific EDMs and their unit deviance (all of these are instances of the Tweedie family): Distribution Target Domain Unit Deviance Normal Poisson Gamma Inverse Gaussian The Probability Density Functions (PDF) of these distributions are illustrated in the following figure, PDF of a random variable Y following Poisson, Tweedie (power1.5) and Gamma distributions with different mean values ( ). Observe the point mass at for the Poisson distribution and the Tweedie (power1.5) distribution, but not for the Gamma distribution which has a strictly positive target domain.  The choice of the distribution depends on the problem at hand: If the target values are counts (non-negative integer valued) or relative frequencies (non-negative), you might use a Poisson deviance with log-link. If the target values are positive valued and skewed, you might try a Gamma deviance with log-link. If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian deviance (or even higher variance powers of the Tweedie family). Examples of use cases include: Agriculture / weather modeling: number of rain events per year (Poisson), amount of rainfall per event (Gamma), total rainfall per year (Tweedie / Compound Poisson Gamma). Risk modeling / insurance policy pricing: number of claim events / policyholder per year (Poisson), cost per event (Gamma), total cost per policyholder per year (Tweedie / Compound Poisson Gamma). Predictive maintenance: number of production interruption events per year (Poisson), duration of interruption (Gamma), total interruption time per year (Tweedie / Compound Poisson Gamma). References: 10 McCullagh, Peter; Nelder, John (1989). Generalized Linear Models, Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5. 11 Jrgensen, B. (1992). The theory of exponential dispersion models and analysis of deviance. Monografias de matemtica, no. 51. See also Exponential dispersion model. 1.1.12.1. Usage  TweedieRegressor implements a generalized linear model for the Tweedie distribution, that allows to model any of the above mentioned distributions using the appropriate parameter. In particular: : Normal distribution. Specific estimators such as Ridge , ElasticNet are generally more appropriate in this case. : Poisson distribution. PoissonRegressor is exposed for convenience. However, it is strictly equivalent to . : Gamma distribution. GammaRegressor is exposed for convenience. However, it is strictly equivalent to . : Inverse Gaussian distribution. The link function is determined by the parameter. Usage example: Examples: Poisson regression and non-normal loss Tweedie regression on insurance claims 1.1.12.2. Practical considerations  The feature matrix should be standardized before fitting. This ensures that the penalty treats features equally. Since the linear predictor can be negative and Poisson, Gamma and Inverse Gaussian distributions dont support negative values, it is necessary to apply an inverse link function that guarantees the non-negativeness. For example with , the inverse link function becomes . If you want to model a relative frequency, i.e. counts per exposure (time, volume, ) you can do so by using a Poisson distribution and passing as target values together with as sample weights. For a concrete example see e.g. Tweedie regression on insurance claims . When performing cross-validation for the parameter of , it is advisable to specify an explicit function, because the default scorer TweedieRegressor.score is a function of itself. \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.13. Stochastic Gradient Descent - SGD  Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The method allows online/out-of-core learning. The classes SGDClassifier and SGDRegressor provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with , SGDClassifier fits a logistic regression model, while with it fits a linear support vector machine (SVM). References Stochastic Gradient Descent \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.14. Perceptron  The Perceptron is another simple classification algorithm suitable for large scale learning. By default: It does not require a learning rate. It is not regularized (penalized). It updates its model only on mistakes. The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser. \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.15. Passive Aggressive Algorithms  The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter . For classification, PassiveAggressiveClassifier can be used with (PA-I) or (PA-II). For regression, PassiveAggressiveRegressor can be used with (PA-I) or (PA-II). References: Online Passive-Aggressive Algorithms K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006) \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.16. Robustness regression: outliers and modeling errors  Robust regression aims to fit a regression model in the presence of corrupt data: either outliers, or error in the model. 1.1.16.1. Different scenario and useful concepts  There are different things to keep in mind when dealing with data corrupted by outliers: Outliers in X or in y ? Outliers in the y direction Outliers in the X direction Fraction of outliers versus amplitude of error The number of outlying points matters, but also how much they are outliers. Small outliers Large outliers An important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data. Note that in general, robust fitting in high-dimensional setting (large ) is very hard. The robust models here will probably not work in these settings. Trade-offs: which estimator? Scikit-learn provides 3 robust regression estimators: RANSAC , Theil Sen and HuberRegressor . HuberRegressor should be faster than RANSAC and Theil Sen unless the number of samples are very large, i.e >> . This is because RANSAC and Theil Sen fit on smaller subsets of the data. However, both Theil Sen and RANSAC are unlikely to be as robust as HuberRegressor for the default parameters. RANSAC is faster than Theil Sen and scales much better with the number of samples. RANSAC will deal better with large outliers in the y direction (most common situation). Theil Sen will cope better with medium-size outliers in the X direction, but this property will disappear in high-dimensional settings. When in doubt, use RANSAC . 1.1.16.2. RANSAC: RANdom SAmple Consensus  RANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers from the complete data set. RANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is dependent on the number of iterations (see parameter). It is typically used for linear and non-linear regression problems and is especially popular in the field of photogrammetric computer vision. The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers. 1.1.16.2.1. Details of the algorithm  Each iteration performs the following steps: Select random samples from the original data and check whether the set of data is valid (see ). Fit a model to the random subset ( ) and check whether the estimated model is valid (see ). Classify all data as inliers or outliers by calculating the residuals to the estimated model ( ) - all data samples with absolute residuals smaller than the are considered as inliers. Save fitted model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has better score. These steps are performed either a maximum number of times ( ) or until one of the special stop criteria are met (see and ). The final model is estimated using all inlier samples (consensus set) of the previously determined best model. The and functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, should be used as it is called prior to fitting the model and thus leading to better computational performance. Examples: Robust linear model estimation using RANSAC Robust linear estimator fitting References: https://en.wikipedia.org/wiki/RANSAC Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography Martin A. Fischler and Robert C. Bolles - SRI International (1981) Performance Evaluation of RANSAC Family Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009) 1.1.16.3. Theil-Sen estimator: generalized-median-based estimator  The TheilSenRegressor estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It loses its robustness properties and becomes no better than an ordinary least squares in high dimension. Examples: Theil-Sen Regression Robust linear estimator fitting References: https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator 1.1.16.3.1. Theoretical considerations  TheilSenRegressor is comparable to the Ordinary Least Squares (OLS) in terms of asymptotic efficiency and as an unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric method which means it makes no assumption about the underlying distribution of the data. Since Theil-Sen is a median-based estimator, it is more robust against corrupted data aka outliers. In univariate setting, Theil-Sen has a breakdown point of about 29.3% in case of a simple linear regression which means that it can tolerate arbitrary corrupted data of up to 29.3%. The implementation of TheilSenRegressor in scikit-learn follows a generalization to a multivariate linear regression model 12 using the spatial median which is a generalization of the median to multiple dimensions 13 . In terms of time and space complexity, Theil-Sen scales according to \\\\[\\\\binom{n_{\\\\text{samples}}}{n_{\\\\text{subsamples}}}\\\\] which makes it infeasible to be applied exhaustively to problems with a large number of samples and features. Therefore, the magnitude of a subpopulation can be chosen to limit the time and space complexity by considering only a random subset of all possible combinations. Examples: Theil-Sen Regression References: 12 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: Theil-Sen Estimators in a Multiple Linear Regression Model. 13 Krkkinen and S. yrm: On Computation of Spatial Median for Robust Data Mining. 1.1.16.4. Huber Regression  The HuberRegressor is different to Ridge because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from TheilSenRegressor and RANSACRegressor because it does not ignore the effect of the outliers but gives a lesser weight to them. The loss function that HuberRegressor minimizes is given by \\\\[\\\\min_{w, \\\\sigma} {\\\\sum_{i1}^n\\\\left(\\\\sigma + H_{\\\\epsilon}\\\\left(\\\\frac{X_{i}w - y_{i}}{\\\\sigma}\\\\right)\\\\sigma\\\\right) + \\\\alpha {||w||_2}^2}\\\\] where \\\\[\\\\begin{split}H_{\\\\epsilon}(z)  \\\\begin{cases} z^2, & \\\\text {if } |z| < \\\\epsilon, \\\\\\\\ 2\\\\epsilon|z| - \\\\epsilon^2, & \\\\text{otherwise} \\\\end{cases}\\\\end{split}\\\\] It is advised to set the parameter to 1.35 to achieve 95% statistical efficiency. 1.1.16.5. Notes  The HuberRegressor differs from using SGDRegressor with loss set to in the following ways. HuberRegressor is scaling invariant. Once is set, scaling and down or up by different values would produce the same robustness to outliers as before. as compared to SGDRegressor where has to be set again when and are scaled. HuberRegressor should be more efficient to use on data with small number of samples while SGDRegressor needs a number of passes on the training data to produce the same robustness. Examples: HuberRegressor vs Ridge on dataset with strong outliers References: Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172 Note that this estimator is different from the R implementation of Robust Regression ( http://www.ats.ucla.edu/stat/r/dae/rreg.htm ) because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold. \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.17. Polynomial regression: extending linear models with basis functions  One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data. For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data: \\\\[\\\\hat{y}(w, x)  w_0 + w_1 x_1 + w_2 x_2\\\\] If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this: \\\\[\\\\hat{y}(w, x)  w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\\\\] The (sometimes surprising) observation is that this is still a linear model : to see this, imagine creating a new set of features \\\\[z  [x_1, x_2, x_1 x_2, x_1^2, x_2^2]\\\\] With this re-labeling of the data, our problem can be written \\\\[\\\\hat{y}(w, z)  w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\\\\] We see that the resulting polynomial regression is in the same class of linear models we considered above (i.e. the model is linear in ) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data. Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees: This figure is created using the PolynomialFeatures transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows: The features of have been transformed from to , and can now be used within any linear model. This sort of preprocessing can be streamlined with the Pipeline tools. A single object representing a simple polynomial regression can be created and used as follows: The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients. In some cases its not necessary to include higher powers of any single feature, but only the so-called interaction features that multiply together at most distinct features. These can be gotten from PolynomialFeatures with the setting . For example, when dealing with boolean features, for all and is therefore useless; but represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier: And the classifier predictions are perfect: \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.1. Ordinary Least Squares  LinearRegression fits a linear model with coefficients to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form: \\\\[\\\\min_{w} || X w - y||_2^2\\\\] LinearRegression will take in its method arrays X, y and will store the coefficients of the linear model in its member: The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design. Examples: Linear Regression Example 1.1.1.1. Ordinary Least Squares Complexity  The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape this method has a cost of , assuming that . \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.2. Ridge regression and classification  1.1.2.1. Regression  Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares: \\\\[\\\\min_{w} || X w - y||_2^2 + \\\\alpha ||w||_2^2\\\\] The complexity parameter controls the amount of shrinkage: the larger the value of , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. As with other linear models, Ridge will take in its method arrays X, y and will store the coefficients of the linear model in its member: 1.1.2.2. Classification  The Ridge regressor has a classifier variant: RidgeClassifier . This classifier first converts binary targets to and then treats the problem as a regression task, optimizing the same objective as above. The predicted class corresponds to the sign of the regressors prediction. For multiclass classification, the problem is treated as multi-output regression, and the predicted class corresponds to the output with the highest value. It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However in practice all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the RidgeClassifier allows for a very different choice of the numerical solvers with distinct computational performance profiles. The RidgeClassifier can be significantly faster than e.g. LogisticRegression with a high number of classes, because it is able to compute the projection matrix only once. This classifier is sometimes referred to as a Least Squares Support Vector Machines with a linear kernel. Examples: Plot Ridge coefficients as a function of the regularization Classification of text documents using sparse features Common pitfalls in interpretation of coefficients of linear models 1.1.2.3. Ridge Complexity  This method has the same order of complexity as Ordinary Least Squares . 1.1.2.4. Setting the regularization parameter: generalized Cross-Validation  RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efficient form of leave-one-out cross-validation: Specifying the value of the cv attribute will trigger the use of cross-validation with GridSearchCV , for example for 10-fold cross-validation, rather than Generalized Cross-Validation. References Notes on Regularized Least Squares, Rifkin & Lippert ( technical report , course slides ). \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.3. Lasso  The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients (see Compressive sensing: tomography reconstruction with L1 prior (Lasso) ). Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is: \\\\[\\\\min_{w} { \\\\frac{1}{2n_{\\\\text{samples}}} ||X w - y||_2 ^ 2 + \\\\alpha ||w||_1}\\\\] The lasso estimate thus solves the minimization of the least-squares penalty with added, where is a constant and is the -norm of the coefficient vector. The implementation in the class Lasso uses coordinate descent as the algorithm to fit the coefficients. See Least Angle Regression for another implementation: The function lasso_path is useful for lower-level tasks, as it computes the coefficients along the full path of possible values. Examples: Lasso and Elastic Net for Sparse Signals Compressive sensing: tomography reconstruction with L1 prior (Lasso) Common pitfalls in interpretation of coefficients of linear models Note Feature selection with Lasso As the Lasso regression yields sparse models, it can thus be used to perform feature selection, as detailed in L1-based feature selection . The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control. References Regularization Path For Generalized linear Models by Coordinate Descent, Friedman, Hastie & Tibshirani, J Stat Softw, 2010 ( Paper ). An Interior-Point Method for Large-Scale L1-Regularized Least Squares, S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007 ( Paper ) 1.1.3.1. Setting regularization parameter  The parameter controls the degree of sparsity of the estimated coefficients. 1.1.3.1.1. Using cross-validation  scikit-learn exposes objects that set the Lasso parameter by cross-validation: LassoCV and LassoLarsCV . LassoLarsCV is based on the Least Angle Regression algorithm explained below. For high-dimensional datasets with many collinear features, LassoCV is most often preferable. However, LassoLarsCV has the advantage of exploring more relevant values of parameter, and if the number of samples is very small compared to the number of features, it is often faster than LassoCV . 1.1.3.1.2. Information-criteria based model selection  Alternatively, the estimator LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples). Examples: Lasso model selection: Cross-Validation / AIC / BIC 1.1.3.1.3. Comparison with the regularization parameter of SVM  The equivalence between and the regularization parameter of SVM, is given by or , depending on the estimator and the exact objective function optimized by the model. \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.4. Multi-task Lasso  The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: is a 2D array, of shape . The constraint is that the selected features are the same for all the regression problems, also called tasks. The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns. Fitting a time-series model, imposing that any active feature be active at all times. Examples: Joint feature selection with multi-task Lasso Mathematically, it consists of a linear model trained with a mixed -norm for regularization. The objective function to minimize is: \\\\[\\\\min_{w} { \\\\frac{1}{2n_{\\\\text{samples}}} ||X W - Y||_{\\\\text{Fro}} ^ 2 + \\\\alpha ||W||_{21}}\\\\] where indicates the Frobenius norm \\\\[||A||_{\\\\text{Fro}}  \\\\sqrt{\\\\sum_{ij} a_{ij}^2}\\\\] and reads \\\\[||A||_{2 1}  \\\\sum_i \\\\sqrt{\\\\sum_j a_{ij}^2}.\\\\] The implementation in the class MultiTaskLasso uses coordinate descent as the algorithm to fit the coefficients. \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.5. Elastic-Net  ElasticNet is a linear regression model trained with both and -norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso , while still maintaining the regularization properties of Ridge . We control the convex combination of and using the parameter. Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both. A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridges stability under rotation. The objective function to minimize is in this case \\\\[\\\\min_{w} { \\\\frac{1}{2n_{\\\\text{samples}}} ||X w - y||_2 ^ 2 + \\\\alpha \\\\rho ||w||_1 + \\\\frac{\\\\alpha(1-\\\\rho)}{2} ||w||_2 ^ 2}\\\\] The class ElasticNetCV can be used to set the parameters ( ) and ( ) by cross-validation. Examples: Lasso and Elastic Net for Sparse Signals Lasso and Elastic Net The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control. References Regularization Path For Generalized linear Models by Coordinate Descent, Friedman, Hastie & Tibshirani, J Stat Softw, 2010 ( Paper ). An Interior-Point Method for Large-Scale L1-Regularized Least Squares, S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007 ( Paper ) \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.6. Multi-task Elastic-Net  The MultiTaskElasticNet is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: is a 2D array of shape . The constraint is that the selected features are the same for all the regression problems, also called tasks. Mathematically, it consists of a linear model trained with a mixed -norm and -norm for regularization. The objective function to minimize is: \\\\[\\\\min_{W} { \\\\frac{1}{2n_{\\\\text{samples}}} ||X W - Y||_{\\\\text{Fro}}^2 + \\\\alpha \\\\rho ||W||_{2 1} + \\\\frac{\\\\alpha(1-\\\\rho)}{2} ||W||_{\\\\text{Fro}}^2}\\\\] The implementation in the class MultiTaskElasticNet uses coordinate descent as the algorithm to fit the coefficients. The class MultiTaskElasticNetCV can be used to set the parameters ( ) and ( ) by cross-validation. \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.7. Least Angle Regression  Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features. The advantages of LARS are: It is numerically efficient in contexts where the number of features is significantly greater than the number of samples. It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares. It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model. If two features are almost equally correlated with the target, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable. It is easily modified to produce solutions for other estimators, like the Lasso. The disadvantages of the LARS method include: Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article. The LARS model can be used using estimator Lars , or its low-level implementation lars_path or lars_path_gram . \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.8. LARS Lasso  LassoLars is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients. Examples: Lasso path using LARS The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions lars_path or lars_path_gram . 1.1.8.1. Mathematical formulation  The algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each ones correlations with the residual. Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the norm of the parameter vector. The full coefficients path is stored in the array , which has size (n_features, max_features+1). The first column is always zero. References: Original Algorithm is detailed in the paper Least Angle Regression by Hastie et al. \"\\n',\n",
       " '\"sklearn_1_1_linear_models 1.1. Linear Models modules/linear_model.html  1.1.9. Orthogonal Matching Pursuit (OMP)  OrthogonalMatchingPursuit and orthogonal_mp implements the OMP algorithm for approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients (ie. the pseudo-norm). Being a forward feature selection method like Least Angle Regression , orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements: \\\\[\\\\underset{w}{\\\\operatorname{arg\\\\,min\\\\,}} ||y - Xw||_2^2 \\\\text{ subject to } ||w||_0 \\\\leq n_{\\\\text{nonzero\\\\_coefs}}\\\\] Alternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as: \\\\[\\\\underset{w}{\\\\operatorname{arg\\\\,min\\\\,}} ||w||_0 \\\\text{ subject to } ||y-Xw||_2^2 \\\\leq \\\\text{tol}\\\\] OMP is based on a greedy algorithm that includes at each step the atom most highly correlated with the current residual. It is similar to the simpler matching pursuit (MP) method, but better in that at each iteration, the residual is recomputed using an orthogonal projection on the space of the previously chosen dictionary elements. Examples: Orthogonal Matching Pursuit References: https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf Matching pursuits with time-frequency dictionaries , S. G. Mallat, Z. Zhang, \"\\n',\n",
       " '\"sklearn_1_2_linear_and_quadratic_discriminant_analysis 1.2. Linear and Quadratic Discriminant Analysis modules/lda_qda.html  1.2.1. Dimensionality reduction using Linear Discriminant Analysis  LinearDiscriminantAnalysis can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the output is necessarily less than the number of classes, so this is in general a rather strong dimensionality reduction, and only makes sense in a multiclass setting. This is implemented in the method. The desired dimensionality can be set using the parameter. This parameter has no influence on the and methods. Examples: Comparison of LDA and PCA 2D projection of Iris dataset : Comparison of LDA and PCA for dimensionality reduction of the Iris dataset \"\\n',\n",
       " '\"sklearn_1_2_linear_and_quadratic_discriminant_analysis 1.2. Linear and Quadratic Discriminant Analysis modules/lda_qda.html  1.2.2. Mathematical formulation of the LDA and QDA classifiers  Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data for each class . Predictions can then be obtained by using Bayes rule, for each training sample : \\\\[P(yk | x)  \\\\frac{P(x | yk) P(yk)}{P(x)}  \\\\frac{P(x | yk) P(y  k)}{ \\\\sum_{l} P(x | yl) \\\\cdot P(yl)}\\\\] and we select the class which maximizes this posterior probability. More specifically, for linear and quadratic discriminant analysis, is modeled as a multivariate Gaussian distribution with density: \\\\[P(x | yk)  \\\\frac{1}{(2\\\\pi)^{d/2} |\\\\Sigma_k|^{1/2}}\\\\exp\\\\left(-\\\\frac{1}{2} (x-\\\\mu_k)^t \\\\Sigma_k^{-1} (x-\\\\mu_k)\\\\right)\\\\] where is the number of features. 1.2.2.1. QDA  According to the model above, the log of the posterior is: \\\\[\\\\begin{split}\\\\log P(yk | x) & \\\\log P(x | yk) + \\\\log P(y  k) + Cst \\\\\\\\ & -\\\\frac{1}{2} \\\\log |\\\\Sigma_k| -\\\\frac{1}{2} (x-\\\\mu_k)^t \\\\Sigma_k^{-1} (x-\\\\mu_k) + \\\\log P(y  k) + Cst,\\\\end{split}\\\\] where the constant term corresponds to the denominator , in addition to other constant terms from the Gaussian. The predicted class is the one that maximises this log-posterior. Note Relation with Gaussian Naive Bayes If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier naive_bayes.GaussianNB . 1.2.2.2. LDA  LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: for all . This reduces the log posterior to: \\\\[\\\\log P(yk | x)  -\\\\frac{1}{2} (x-\\\\mu_k)^t \\\\Sigma^{-1} (x-\\\\mu_k) + \\\\log P(y  k) + Cst.\\\\] The term corresponds to the Mahalanobis Distance between the sample and the mean . The Mahalanobis distance tells how close is from , while also accounting for the variance of each feature. We can thus interpret LDA as assigning to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. The log-posterior of LDA can also be written 3 as: \\\\[\\\\log P(yk | x)  \\\\omega_k^t x + \\\\omega_{k0} + Cst.\\\\] where and . These quantities correspond to the and attributes, respectively. From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices of the Gaussians, leading to quadratic decision surfaces. See 1 for more details. \"\\n',\n",
       " '\"sklearn_1_2_linear_and_quadratic_discriminant_analysis 1.2. Linear and Quadratic Discriminant Analysis modules/lda_qda.html  1.2.3. Mathematical formulation of LDA dimensionality reduction  First note that the K means are vectors in , and they lie in an affine subspace of dimension at least (2 points lie on a line, 3 points lie on a plane, etc). As mentioned above, we can interpret LDA as assigning to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first sphering the data so that the covariance matrix is the identity, and then assigning to the closest mean in terms of Euclidean distance (still accounting for the class priors). Computing Euclidean distances in this d-dimensional space is equivalent to first projecting the data points into , and computing the distances there (since the other dimensions will contribute equally to each class in terms of distance). In other words, if is closest to in the original space, it will also be the case in . This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a dimensional space. We can reduce the dimension even more, to a chosen , by projecting onto the linear subspace which maximizes the variance of the after projection (in effect, we are doing a form of PCA for the transformed class means ). This corresponds to the parameter used in the transform method. See 1 for more details. \"\\n',\n",
       " '\"sklearn_1_2_linear_and_quadratic_discriminant_analysis 1.2. Linear and Quadratic Discriminant Analysis modules/lda_qda.html  1.2.4. Shrinkage  Shrinkage is a form of regularization used to improve the estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator, and shrinkage helps improving the generalization performance of the classifier. Shrinkage LDA can be used by setting the parameter of the LinearDiscriminantAnalysis class to auto. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf 2 . Note that currently shrinkage only works when setting the parameter to lsqr or eigen. The parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix. \"\\n',\n",
       " '\"sklearn_1_2_linear_and_quadratic_discriminant_analysis 1.2. Linear and Quadratic Discriminant Analysis modules/lda_qda.html  1.2.5. Estimation algorithms  Using LDA and QDA requires computing the log-posterior which depends on the class priors , the class means , and the covariance matrices. The svd solver is the default solver used for LinearDiscriminantAnalysis , and it is the only available solver for QuadraticDiscriminantAnalysis . It can perform both classification and transform (for LDA). As it does not rely on the calculation of the covariance matrix, the svd solver may be preferable in situations where the number of features is large. The svd solver cannot be used with shrinkage. For QDA, the use of the SVD solver relies on the fact that the covariance matrix is, by definition, equal to where comes from the SVD of the (centered) matrix: . It turns out that we can compute the log-posterior above without having to explictly compute : computing and via the SVD of is enough. For LDA, two SVDs are computed: the SVD of the centered input matrix and the SVD of the class-wise mean vectors. The lsqr solver is an efficient algorithm that only works for classification. It needs to explicitly compute the covariance matrix , and supports shrinkage. This solver computes the coefficients by solving for , thus avoiding the explicit computation of the inverse . The eigen solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the eigen solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features. Examples: Normal and Shrinkage Linear Discriminant Analysis for classification : Comparison of LDA classifiers with and without shrinkage. References: 1 ( 1 , 2 ) The Elements of Statistical Learning, Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008. 2 Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004. 3 R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification (Second Edition), section 2.6.2. \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.1. Classification  SVC , NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset. SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation ). On the other hand, LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept parameter , as this is assumed to be linear. It also lacks some of the attributes of SVC and NuSVC , like . As other classifiers, SVC , NuSVC and LinearSVC take as input two arrays: an array of shape holding the training samples, and an array of class labels (strings or integers), of shape : After being fitted, the model can then be used to predict new values: SVMs decision function (detailed in the Mathematical formulation ) depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in attributes , and : Examples: SVM: Maximum margin separating hyperplane , Non-linear SVM SVM-Anova: SVM with univariate feature selection , 1.4.1.1. Multi-class classification  SVC and NuSVC implement the one-versus-one approach for multi-class classification. In total, classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the option allows to monotonically transform the results of the one-versus-one classifiers to a one-vs-rest decision function of shape . On the other hand, LinearSVC implements one-vs-the-rest multi-class strategy, thus training models. See Mathematical formulation for a complete description of the decision function. Note that the LinearSVC also implements an alternative multi-class strategy, the so-called multi-class SVM formulated by Crammer and Singer 16 , by using the option . In practice, one-vs-rest classification is usually preferred, since the results are mostly similar, but the runtime is significantly less. For one-vs-rest LinearSVC the attributes and have the shape and respectively. Each row of the coefficients corresponds to one of the one-vs-rest classifiers and similar for the intercepts, in the order of the one class. In the case of one-vs-one SVC and NuSVC , the layout of the attributes is a little more involved. In the case of a linear kernel, the attributes and have the shape and respectively. This is similar to the layout for LinearSVC described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is 0 vs 1, 0 vs 2 ,  0 vs n, 1 vs 2, 1 vs 3, 1 vs n, . . . n-1 vs n. The shape of is with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the one-vs-one classifiers. Each of the support vectors is used in classifiers. The entries in each row correspond to the dual coefficients for these classifiers. This might be clearer with an example: consider a three class problem with class 0 having three support vectors and class 1 and 2 having two support vectors and respectively. For each support vector , there are two dual coefficients. Lets call the coefficient of support vector in the classifier between classes and . Then looks like this: Coefficients for SVs of class 0 Coefficients for SVs of class 1 Coefficients for SVs of class 2 Examples: Plot different SVM classifiers in the iris dataset , 1.4.1.2. Scores and probabilities  The method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option is set to , class membership probability estimates (from the methods and ) are enabled. In the binary case, the probabilities are calibrated using Platt scaling 9 : logistic regression on the SVMs scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per 10 . Note The same probability calibration procedure is available for all estimators via the CalibratedClassifierCV (see Probability calibration ). In the case of SVC and NuSVC , this procedure is builtin in libsvm which is used under the hood, so it does not rely on scikit-learns CalibratedClassifierCV . The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores: the argmax of the scores may not be the argmax of the probabilities in binary classification, a sample may be labeled by as belonging to the positive class even if the output of is less than 0.5; and similarly, it could be labeled as negative even if the output of is more than 0.5. Platts method is also known to have theoretical issues. If confidence scores are required, but these do not have to be probabilities, then it is advisable to set and use instead of . Please note that when and , unlike , the method does not try to break ties by default. You can set for the output of to be the same as , otherwise the first class among the tied classes will always be returned; but have in mind that it comes with a computational cost. See SVM Tie Breaking Example for an example on tie breaking. 1.4.1.3. Unbalanced problems  In problems where it is desired to give more importance to certain classes or certain individual samples, the parameters and can be used. SVC (but not NuSVC ) implements the parameter in the method. Its a dictionary of the form , where value is a floating point number > 0 that sets the parameter of class to . The figure below illustrates the decision boundary of an unbalanced problem, with and without weight correction. SVC , NuSVC , SVR , NuSVR , LinearSVC , LinearSVR and OneClassSVM implement also weights for individual samples in the method through the parameter. Similar to , this sets the parameter for the i-th example to , which will encourage the classifier to get these samples right. The figure below illustrates the effect of sample weighting on the decision boundary. The size of the circles is proportional to the sample weights: Examples: SVM: Separating hyperplane for unbalanced classes SVM: Weighted samples , \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.2. Regression  The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression. The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target. There are three different implementations of Support Vector Regression: SVR , NuSVR and LinearSVR . LinearSVR provides a faster implementation than SVR but only considers the linear kernel, while NuSVR implements a slightly different formulation than SVR and LinearSVR . See Implementation details for further details. As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values: Examples: Support Vector Regression (SVR) using linear and non-linear kernels \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.3. Density estimation, novelty detection  The class OneClassSVM implements a One-Class SVM which is used in outlier detection. See Novelty and Outlier Detection for the description and usage of OneClassSVM. \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.4. Complexity  Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the libsvm -based implementation scales between and depending on how efficiently the libsvm cache is used in practice (dataset dependent). If the data is very sparse should be replaced by the average number of non-zero features in a sample vector. For the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm -based SVC counterpart and can scale almost linearly to millions of samples and/or features. \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.5. Tips on Practical Use  Avoiding data copy : For SVC , SVR , NuSVC and NuSVR , if the data passed to certain methods is not C-ordered contiguous and double precision, it will be copied before calling the underlying C implementation. You can check whether a given numpy array is C-contiguous by inspecting its attribute. For LinearSVC (and LogisticRegression ) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use the SGDClassifier class instead. The objective function can be configured to be almost the same as the LinearSVC model. Kernel cache size : For SVC , SVR , NuSVC and NuSVR , the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to set to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB). Setting C : is by default and its a reasonable default choice. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization. LinearSVC and LinearSVR are less sensitive to when it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, larger values will take more time to train, sometimes up to 10 times longer, as shown in 11 . Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data . For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a Pipeline : See section Preprocessing data for more details on scaling and normalization. Regarding the parameter, quoting 12 : We found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance), the code without using shrinking may be much faster Parameter in NuSVC / OneClassSVM / NuSVR approximates the fraction of training errors and support vectors. In SVC , if the data is unbalanced (e.g. many positive and few negative), set and/or try different penalty parameters . Randomness of the underlying implementations : The underlying implementations of SVC and NuSVC use a random number generator only to shuffle the data for probability estimation (when is set to ). This randomness can be controlled with the parameter. If is set to these estimators are not random and has no effect on the results. The underlying OneClassSVM implementation is similar to the ones of SVC and NuSVC . As no probability estimation is provided for OneClassSVM , it is not random. The underlying LinearSVC implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when is set to ). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller parameter. This randomness can also be controlled with the parameter. When is set to the underlying implementation of LinearSVC is not random and has no effect on the results. Using L1 penalization as provided by yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing yields a more complex model (more features are selected). The value that yields a null model (all weights equal to zero) can be calculated using l1_min_c . \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.6. Kernel functions  The kernel function can be any of the following: linear: . polynomial: , where is specified by parameter , by . rbf: , where is specified by parameter , must be greater than 0. sigmoid , where is specified by . Different kernels are specified by the parameter: 1.4.6.1. Parameters of the RBF Kernel  When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: and . The parameter , common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low makes the decision surface smooth, while a high aims at classifying all training examples correctly. defines how much influence a single training example has. The larger is, the closer other examples must be to be affected. Proper choice of and is critical to the SVMs performance. One is advised to use sklearn.model_selection.GridSearchCV with and spaced exponentially far apart to choose good values. Examples: RBF SVM parameters Non-linear SVM 1.4.6.2. Custom Kernels  You can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix. Classifiers with custom kernels behave the same way as any other classifiers, except that: Field is now empty, only indices of support vectors are stored in A reference (and not a copy) of the first argument in the method is stored for future reference. If that array changes between the use of and you will have unexpected results. 1.4.6.2.1. Using Python functions as kernels  You can use your own defined kernels by passing a function to the parameter. Your kernel must take as arguments two matrices of shape , and return a kernel matrix of shape . The following code defines a linear kernel and creates a classifier instance that will use that kernel: Examples: SVM with custom kernel . 1.4.6.2.2. Using the Gram matrix  You can pass pre-computed kernels by using the option. You should then pass Gram matrix instead of X to the and methods. The kernel values between all training vectors and the test vectors must be provided: \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.7. Mathematical formulation  A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called support vectors: In general, when the problem isnt linearly separable, the support vectors are the samples within the margin boundaries. We recommend 13 and 14 as good references for the theory and practicalities of SVMs. 1.4.7.1. SVC  Given training vectors , i1,, n, in two classes, and a vector , our goal is to find and such that the prediction given by is correct for most samples. SVC solves the following primal problem: \\\\[ \\\\begin{align}\\\\begin{aligned}\\\\min_ {w, b, \\\\zeta} \\\\frac{1}{2} w^T w + C \\\\sum_{i1}^{n} \\\\zeta_i\\\\\\\\\\\\begin{split}\\\\textrm {subject to } & y_i (w^T \\\\phi (x_i) + b) \\\\geq 1 - \\\\zeta_i,\\\\\\\\ & \\\\zeta_i \\\\geq 0, i1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} \\\\] Intuitively, were trying to maximize the margin (by minimizing ), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value would be for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance from their correct margin boundary. The penalty term controls the strengh of this penalty, and as a result, acts as an inverse regularization parameter (see note below). The dual problem to the primal is \\\\[ \\\\begin{align}\\\\begin{aligned}\\\\min_{\\\\alpha} \\\\frac{1}{2} \\\\alpha^T Q \\\\alpha - e^T \\\\alpha\\\\\\\\\\\\begin{split} \\\\textrm {subject to } & y^T \\\\alpha  0\\\\\\\\ & 0 \\\\leq \\\\alpha_i \\\\leq C, i1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} \\\\] where is the vector of all ones, and is an by positive semidefinite matrix, , where is the kernel. The terms are called the dual coefficients, and they are upper-bounded by . This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function : see kernel trick . Once the optimization problem is solved, the output of decision_function for a given sample becomes: \\\\[\\\\sum_{i\\\\in SV} y_i \\\\alpha_i K(x_i, x) + b,\\\\] and the predicted class correspond to its sign. We only need to sum over the support vectors (i.e. the samples that lie within the margin) because the dual coefficients are zero for the other samples. These parameters can be accessed through the attributes which holds the product , which holds the support vectors, and which holds the independent term Note While SVM models derived from libsvm and liblinear use as regularization parameter, most other estimators use . The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is sklearn.linear_model.Ridge regression, the relation between them is given as . 1.4.7.2. LinearSVC  The primal problem can be equivalently formulated as \\\\[\\\\min_ {w, b} \\\\frac{1}{2} w^T w + C \\\\sum_{i1}\\\\max(0, y_i (w^T \\\\phi(x_i) + b)),\\\\] where we make use of the hinge loss . This is the form that is directly optimized by LinearSVC , but unlike the dual form, this one does not involve inner products between samples, so the famous kernel trick cannot be applied. This is why only the linear kernel is supported by LinearSVC ( is the identity function). 1.4.7.3. NuSVC  The -SVC formulation 15 is a reparameterization of the -SVC and therefore mathematically equivalent. We introduce a new parameter (instead of ) which controls the number of support vectors and margin errors : is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin. 1.4.7.4. SVR  Given training vectors , i1,, n, and a vector -SVR solves the following primal problem: \\\\[ \\\\begin{align}\\\\begin{aligned}\\\\min_ {w, b, \\\\zeta, \\\\zeta^*} \\\\frac{1}{2} w^T w + C \\\\sum_{i1}^{n} (\\\\zeta_i + \\\\zeta_i^*)\\\\\\\\\\\\begin{split}\\\\textrm {subject to } & y_i - w^T \\\\phi (x_i) - b \\\\leq \\\\varepsilon + \\\\zeta_i,\\\\\\\\ & w^T \\\\phi (x_i) + b - y_i \\\\leq \\\\varepsilon + \\\\zeta_i^*,\\\\\\\\ & \\\\zeta_i, \\\\zeta_i^* \\\\geq 0, i1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} \\\\] Here, we are penalizing samples whose prediction is at least away from their true target. These samples penalize the objective by or , depending on whether their predictions lie above or below the tube. The dual problem is \\\\[ \\\\begin{align}\\\\begin{aligned}\\\\min_{\\\\alpha, \\\\alpha^*} \\\\frac{1}{2} (\\\\alpha - \\\\alpha^*)^T Q (\\\\alpha - \\\\alpha^*) + \\\\varepsilon e^T (\\\\alpha + \\\\alpha^*) - y^T (\\\\alpha - \\\\alpha^*)\\\\\\\\\\\\begin{split} \\\\textrm {subject to } & e^T (\\\\alpha - \\\\alpha^*)  0\\\\\\\\ & 0 \\\\leq \\\\alpha_i, \\\\alpha_i^* \\\\leq C, i1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} \\\\] where is the vector of all ones, is an by positive semidefinite matrix, is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function . The prediction is: \\\\[\\\\sum_{i \\\\in SV}(\\\\alpha_i - \\\\alpha_i^*) K(x_i, x) + b\\\\] These parameters can be accessed through the attributes which holds the difference , which holds the support vectors, and which holds the independent term 1.4.7.5. LinearSVR  The primal problem can be equivalently formulated as \\\\[\\\\min_ {w, b} \\\\frac{1}{2} w^T w + C \\\\sum_{i1}\\\\max(0, |y_i - (w^T \\\\phi(x_i) + b)| - \\\\varepsilon),\\\\] where we make use of the epsilon-insensitive loss, i.e. errors of less than are ignored. This is the form that is directly optimized by LinearSVR . \"\\n',\n",
       " '\"sklearn_1_4_support_vector_machines 1.4. Support Vector Machines modules/svm.html  1.4.8. Implementation details  Internally, we use libsvm 12 and liblinear 11 to handle all computations. These libraries are wrapped using C and Cython. For a description of the implementation and details of the algorithms used, please refer to their respective papers. References: 9 Platt Probabilistic outputs for SVMs and comparisons to regularized likelihood methods . 10 Wu, Lin and Weng, Probability estimates for multi-class classification by pairwise coupling , JMLR 5:975-1005, 2004. 11 ( 1 , 2 ) Fan, Rong-En, et al., LIBLINEAR: A library for large linear classification. , Journal of machine learning research 9.Aug (2008): 1871-1874. 12 ( 1 , 2 ) Chang and Lin, LIBSVM: A Library for Support Vector Machines . 13 Bishop, Pattern recognition and machine learning , chapter 7 Sparse Kernel Machines 14 A Tutorial on Support Vector Regression , Alex J. Smola, Bernhard Schlkopf - Statistics and Computing archive Volume 14 Issue 3, August 2004, p. 199-222. 15 Schlkopf et. al New Support Vector Algorithms 16 Crammer and Singer On the Algorithmic Implementation ofMulticlass Kernel-based Vector Machines , JMLR 2001. \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.1. Classification  The class SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a SGDClassifier trained with the hinge loss, equivalent to a linear SVM. As other classifiers, SGD has to be fitted with two arrays: an array of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values (class labels) for the training samples: After being fitted, the model can then be used to predict new values: SGD fits a linear model to the training data. The attribute holds the model parameters: The attribute holds the intercept (aka offset or bias): Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter . The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by SGDClassifier.decision_function : The concrete loss function can be set via the parameter. SGDClassifier supports the following loss functions: : (soft-margin) linear Support Vector Machine, : smoothed hinge loss, : logistic regression, and all regression losses below. In this case the target is encoded as -1 or 1, and the problem is treated as a regression problem. The predicted class then correspond to the sign of the predicted target. Please refer to the mathematical section below for formulas. The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models (i.e. with more zero coefficents), even when L2 penalty is used. Using or enables the method, which gives a vector of probability estimates per sample : The concrete penalty can be set via the parameter. SGD supports the following penalties: : L2 norm penalty on . : L1 norm penalty on . : Convex combination of L2 and L1; . The default setting is . The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net 11 solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter controls the convex combination of L1 and L2 penalty. SGDClassifier supports multi-class classification by combining multiple binary classifiers in a one versus all (OVA) scheme. For each of the classes, a binary classifier is learned that discriminates between that and all other classes. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. The Figure below illustrates the OVA approach on the iris dataset. The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers. In the case of multi-class classification is a two-dimensional array of shape (n_classes, n_features) and is a one-dimensional array of shape (n_classes,). The i-th row of holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute ). Note that, in principle, since they allow to create a probability model, and are more suitable for one-vs-all classification. SGDClassifier supports both weighted classes and weighted instances via the fit parameters and . See the examples below and the docstring of SGDClassifier.fit for further information. SGDClassifier supports averaged SGD (ASGD) 10 . Averaging can be enabled by setting . ASGD performs the same updates as the regular SGD (see Mathematical formulation ), but instead of using the last value of the coefficients as the attribute (i.e. the values of the last update), is set instead to the average value of the coefficients across all updates. The same is done for the attribute. When using ASGD the learning rate can be larger and even constant, leading on some datasets to a speed up in training time. For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in LogisticRegression . Examples: SGD: Maximum margin separating hyperplane , Plot multi-class SGD on the iris dataset SGD: Weighted samples Comparing various online solvers SVM: Separating hyperplane for unbalanced classes (See the Note in the example) \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.2. Regression  The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. SGDRegressor is well suited for regression problems with a large number of training samples (> 10.000), for other problems we recommend Ridge , Lasso , or ElasticNet . The concrete loss function can be set via the parameter. SGDRegressor supports the following loss functions: : Ordinary least squares, : Huber loss for robust regression, : linear Support Vector Regression. Please refer to the mathematical section below for formulas. The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter . This parameter depends on the scale of the target variables. The parameter determines the regularization to be used (see description above in the classification section). SGDRegressor also supports averaged SGD 10 (here again, see description above in the classification section). For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in Ridge . \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.3. Stochastic Gradient Descent for sparse data  Note The sparse implementation produces slightly different results from the dense implementation, due to a shrunk learning rate for the intercept. See Implementation details . There is built-in support for sparse data given in any matrix in a format supported by scipy.sparse . For maximum efficiency, however, use the CSR matrix format as defined in scipy.sparse.csr_matrix . Examples: Classification of text documents using sparse features \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.4. Complexity  The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of , where k is the number of iterations (epochs) and is the average number of non-zero attributes per sample. Recent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase as the training set size increases. \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.5. Stopping criterion  The classes SGDClassifier and SGDRegressor provide two criteria to stop the algorithm when a given level of convergence is reached: With , the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score (using the method) computed on the validation set. The size of the validation set can be changed with the parameter . With , the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the training data. In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve times in a row. The improvement is evaluated with absolute tolerance , and the algorithm stops in any case after a maximum number of iteration . \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.6. Tips on Practical Use  Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be easily done using StandardScaler : If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed. Finding a reasonable regularization term is best done using automatic hyper-parameter search, e.g. GridSearchCV or RandomizedSearchCV , usually in the range . Empirically, we found that SGD converges after observing approximately 10^6 training samples. Thus, a reasonable first guess for the number of iterations is , where is the size of the training set. If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant such that the average L2 norm of the training data equals one. We found that Averaged SGD works best with a larger number of features and a higher eta0 References: Efficient BackProp Y. LeCun, L. Bottou, G. Orr, K. Mller - In Neural Networks: Tricks of the Trade 1998. \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.7. Mathematical formulation  We describe here the mathematical details of the SGD procedure. A good overview with convergence rates can be found in 12 . Given a set of training examples where and ( for classification), our goal is to learn a linear scoring function with model parameters and intercept . In order to make predictions for binary classification, we simply look at the sign of . To find the model parameters, we minimize the regularized training error given by \\\\[E(w,b)  \\\\frac{1}{n}\\\\sum_{i1}^{n} L(y_i, f(x_i)) + \\\\alpha R(w)\\\\] where is a loss function that measures model (mis)fit and is a regularization term (aka penalty) that penalizes model complexity; is a non-negative hyperparameter that controls the regularization stength. Different choices for entail different classifiers or regressors: Hinge (soft-margin): equivalent to Support Vector Classification. . Perceptron: . Modified Huber: if , and otherwise. Log: equivalent to Logistic Regression. . Least-Squares: Linear regression (Ridge or Lasso depending on ). . Huber: less sensitive to outliers than least-squares. It is equivalent to least squares when , and otherwise. Epsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression. . All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below. Popular choices for the regularization term (the parameter) include: L2 norm: , L1 norm: , which leads to sparse solutions. Elastic Net: , a convex combination of L2 and L1, where is given by . The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space ( ) when . 1.5.7.1. SGD  Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of by considering a single training example at a time. The class SGDClassifier implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by \\\\[w \\\\leftarrow w - \\\\eta \\\\left[\\\\alpha \\\\frac{\\\\partial R(w)}{\\\\partial w} + \\\\frac{\\\\partial L(w^T x_i + b, y_i)}{\\\\partial w}\\\\right]\\\\] where is the learning rate which controls the step-size in the parameter space. The intercept is updated similarly but without regularization (and with additional decay for sparse matrices, as detailed in Implementation details ). The learning rate can be either constant or gradually decaying. For classification, the default learning rate schedule ( ) is given by \\\\[\\\\eta^{(t)}  \\\\frac {1}{\\\\alpha (t_0 + t)}\\\\] where is the time step (there are a total of time steps), is determined based on a heuristic proposed by Lon Bottou such that the expected initial updates are comparable with the expected size of the weights (this assuming that the norm of the training samples is approx. 1). The exact definition can be found in in BaseSGD . For regression the default learning rate schedule is inverse scaling ( ), given by \\\\[\\\\eta^{(t)}  \\\\frac{eta_0}{t^{power\\\\_t}}\\\\] where and are hyperparameters chosen by the user via and , resp. For a constant learning rate use and use to specify the learning rate. For an adaptively decreasing learning rate, use and use to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6. The model parameters can be accessed through the and attributes: holds the weights and holds . When using Averaged SGD (with the parameter), is set to the average weight across all updates: , where is the total number of updates, found in the attribute. \"\\n',\n",
       " '\"sklearn_1_5_stochastic_gradient_descent 1.5. Stochastic Gradient Descent modules/sgd.html  1.5.8. Implementation details  The implementation of SGD is influenced by the of 7 . Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input , the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from 8 . For multi-class classification, a one versus all approach is used. We use the truncated gradient algorithm proposed in 9 for L1 regularization (and the Elastic Net). The code is written in Cython. References: 7 Stochastic Gradient Descent L. Bottou - Website, 2010. 8 Pegasos: Primal estimated sub-gradient solver for svm S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML 07. 9 Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL 09. 10 ( 1 , 2 ) Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent Xu, Wei 11 Regularization and variable selection via the elastic net H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320. 12 Solving large scale linear prediction problems using stochastic gradient descent algorithms T. Zhang - In Proceedings of ICML 04. \"\\n',\n",
       " '\"sklearn_1_6_nearest_neighbors 1.6. Nearest Neighbors modules/neighbors.html  1.6.1. Unsupervised Nearest Neighbors  NearestNeighbors implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree , KDTree , and a brute-force algorithm based on routines in sklearn.metrics.pairwise . The choice of neighbors search algorithm is controlled through the keyword , which must be one of . When the default value is passed, the algorithm attempts to determine the best approach from the training data. For a discussion of the strengths and weaknesses of each option, see Nearest Neighbor Algorithms . Warning Regarding the Nearest Neighbors algorithms, if two neighbors and have identical distances but different labels, the result will depend on the ordering of the training data. 1.6.1.1. Finding the Nearest Neighbors  For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within sklearn.neighbors can be used: Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero. It is also possible to efficiently produce a sparse graph showing the connections between neighboring points: The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see sklearn.manifold.Isomap , sklearn.manifold.LocallyLinearEmbedding , and sklearn.cluster.SpectralClustering . 1.6.1.2. KDTree and BallTree Classes  Alternatively, one can use the KDTree or BallTree classes directly to find nearest neighbors. This is the functionality wrapped by the NearestNeighbors class used above. The Ball Tree and KD Tree have the same interface; well show an example of using the KD Tree here: Refer to the KDTree and BallTree class documentation for more information on the options available for nearest neighbors searches, including specification of query strategies, distance metrics, etc. For a list of available metrics, see the documentation of the DistanceMetric class. \"\\n',\n",
       " '\"sklearn_1_6_nearest_neighbors 1.6. Nearest Neighbors modules/neighbors.html  1.6.2. Nearest Neighbors Classification  Neighbors-based classification is a type of instance-based learning or non-generalizing learning : it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. scikit-learn implements two different nearest neighbors classifiers: KNeighborsClassifier implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user. RadiusNeighborsClassifier implements learning based on the number of neighbors within a fixed radius of each training point, where is a floating-point value specified by the user. The -neighbors classification in KNeighborsClassifier is the most commonly used technique. The optimal choice of the value is highly data-dependent: in general a larger suppresses the effects of noise, but makes the classification boundaries less distinct. In cases where the data is not uniformly sampled, radius-based neighbors classification in RadiusNeighborsClassifier can be a better choice. The user specifies a fixed radius , such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called curse of dimensionality. The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the keyword. The default value, , assigns uniform weights to each neighbor. assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights. Examples: Nearest Neighbors Classification : an example of classification using nearest neighbors. \"\\n',\n",
       " '\"sklearn_1_6_nearest_neighbors 1.6. Nearest Neighbors modules/neighbors.html  1.6.3. Nearest Neighbors Regression  Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors. scikit-learn implements two different neighbors regressors: KNeighborsRegressor implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user. RadiusNeighborsRegressor implements learning based on the neighbors within a fixed radius of the query point, where is a floating-point value specified by the user. The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the keyword. The default value, , assigns equal weights to all points. assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights. The use of multi-output nearest neighbors for regression is demonstrated in Face completion with a multi-output estimators . In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces. Examples: Nearest Neighbors regression : an example of regression using nearest neighbors. Face completion with a multi-output estimators : an example of multi-output regression using nearest neighbors. \"\\n',\n",
       " '\"sklearn_1_6_nearest_neighbors 1.6. Nearest Neighbors modules/neighbors.html  1.6.4. Nearest Neighbor Algorithms  1.6.4.1. Brute Force  Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for samples in dimensions, this approach scales as . Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples grows, the brute-force approach quickly becomes infeasible. In the classes within sklearn.neighbors , brute-force neighbors searches are specified using the keyword , and are computed using the routines available in sklearn.metrics.pairwise . 1.6.4.2. K-D Tree  To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point is very distant from point , and point is very close to point , then we know that points and are very distant, without having to explicitly calculate their distance . In this way, the computational cost of a nearest neighbors search can be reduced to or better. This is a significant improvement over brute-force for large . An early approach to taking advantage of this aggregate information was the KD tree data structure (short for K-dimensional tree ), which generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no -dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only distance computations. Though the KD tree approach is very fast for low-dimensional ( ) neighbors searches, it becomes inefficient as grows very large: this is one manifestation of the so-called curse of dimensionality. In scikit-learn, KD tree neighbors searches are specified using the keyword , and are computed using the class KDTree . References: Multidimensional binary search trees used for associative searching , Bentley, J.L., Communications of the ACM (1975) 1.6.4.3. Ball Tree  To address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions. A ball tree recursively divides the data into nodes defined by a centroid and radius , such that each point in the node lies within the hyper-sphere defined by and . The number of candidate points for a neighbor search is reduced through use of the triangle inequality : \\\\[|x+y| \\\\leq |x| + |y|\\\\] With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword , and are computed using the class sklearn.neighbors.BallTree . Alternatively, the user can work with the BallTree class directly. References: Five balltree construction algorithms , Omohundro, S.M., International Computer Science Institute Technical Report (1989) 1.6.4.4. Choice of Nearest Neighbors Algorithm  The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors: number of samples (i.e. ) and dimensionality (i.e. ). Brute force query time grows as Ball tree query time grows as approximately KD tree query time changes with in a way that is difficult to precisely characterise. For small (less than 20 or so) the cost is approximately , and the KD tree query can be very efficient. For larger , the cost increases to nearly , and the overhead due to the tree structure can lead to queries which are slower than brute force. For small data sets ( less than 30 or so), is comparable to , and brute force algorithms can be more efficient than a tree-based approach. Both KDTree and BallTree address this through providing a leaf size parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small . data structure: intrinsic dimensionality of the data and/or sparsity of the data. Intrinsic dimensionality refers to the dimension of a manifold on which the data lies, which can be linearly or non-linearly embedded in the parameter space. Sparsity refers to the degree to which the data fills the parameter space (this is to be distinguished from the concept as used in sparse matrices. The data matrix may have no zero entries, but the structure can still be sparse in this sense). Brute force query time is unchanged by data structure. Ball tree and KD tree query times can be greatly influenced by data structure. In general, sparser data with a smaller intrinsic dimensionality leads to faster query times. Because the KD tree internal representation is aligned with the parameter axes, it will not generally show as much improvement as ball tree for arbitrarily structured data. Datasets used in machine learning tend to be very structured, and are very well-suited for tree-based queries. number of neighbors requested for a query point. Brute force query time is largely unaffected by the value of Ball tree and KD tree query time will become slower as increases. This is due to two effects: first, a larger leads to the necessity to search a larger portion of the parameter space. Second, using requires internal queueing of results as the tree is traversed. As becomes large compared to , the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient. number of query points. Both the ball tree and the KD Tree require a construction phase. The cost of this construction becomes negligible when amortized over many queries. If only a small number of queries will be performed, however, the construction can make up a significant fraction of the total cost. If very few query points will be required, brute force is better than a tree-based method. Currently, selects if , the input data is sparse, or isnt in the list for either or . Otherwise, it selects the first out of and that has in its list. This choice is based on the assumption that the number of query points is at least the same order as the number of training points, and that is close to its default value of . 1.6.4.5. Effect of  As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter . This parameter choice has many effects: construction time A larger leads to a faster tree construction time, because fewer nodes need to be created query time Both a large or small can lead to suboptimal query cost. For approaching 1, the overhead involved in traversing nodes can significantly slow query times. For approaching the size of the training set, queries become essentially brute force. A good compromise between these is , the default value of the parameter. memory As increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a -dimensional centroid for each node. The required storage space for BallTree is approximately times the size of the training set. is not referenced for brute force queries. \"\\n',\n",
       " '\"sklearn_1_6_nearest_neighbors 1.6. Nearest Neighbors modules/neighbors.html  1.6.5. Nearest Centroid Classifier  The NearestCentroid classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the sklearn.cluster.KMeans algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis ( sklearn.discriminant_analysis.LinearDiscriminantAnalysis ) and Quadratic Discriminant Analysis ( sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis ) for more complex methods that do not make this assumption. Usage of the default NearestCentroid is simple: 1.6.5.1. Nearest Shrunken Centroid  The NearestCentroid classifier has a parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by . Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features. In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82. Examples: Nearest Centroid Classification : an example of classification using nearest centroid with different shrink thresholds. \"\\n',\n",
       " '\"sklearn_1_6_nearest_neighbors 1.6. Nearest Neighbors modules/neighbors.html  1.6.6. Nearest Neighbors Transformer  Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as KNeighborsClassifier and KNeighborsRegressor , but also some clustering methods such as DBSCAN and SpectralClustering , and some manifold embeddings such as TSNE and Isomap . All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors sparse graph , as given by kneighbors_graph and radius_neighbors_graph . With mode , these functions return a binary adjacency sparse graph as required, for instance, in SpectralClustering . Whereas with , they return a distance sparse graph as required, for instance, in DBSCAN . To include these functions in a scikit-learn pipeline, one can also use the corresponding classes KNeighborsTransformer and RadiusNeighborsTransformer . The benefits of this sparse graph API are multiple. First, the precomputed graph can be re-used multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline: Second, precomputing the graph can give finer control on the nearest neighbors estimation, for instance enabling multiprocessing though the parameter , which might not be available in all estimators. Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors sparse graph needs to be formatted as in radius_neighbors_graph output: a CSR matrix (although COO, CSC or LIL will be accepted). only explicitly store nearest neighborhoods of each sample with respect to the training data. This should include those at 0 distance from a query point, including the matrix diagonal when computing the nearest neighborhoods between the training data and itself. each rows should store the distance in increasing order (optional. Unsorted data will be stable-sorted, adding a computational overhead). all values in data should be non-negative. there should be no duplicate in any row (see https://github.com/scipy/scipy/issues/5807 ). if the algorithm being passed the precomputed matrix uses k nearest neighbors (as opposed to radius neighborhood), at least k neighbors must be stored in each row (or k+1, as explained in the following note). Note When a specific number of neighbors is queried (using KNeighborsTransformer ), the definition of is ambiguous since it can either include each training point as its own neighbor, or exclude them. Neither choice is perfect, since including them leads to a different number of non-self neighbors during training and testing, while excluding them leads to a difference between and , which is against scikit-learn API. In KNeighborsTransformer we use the definition which includes each training point as its own neighbor in the count of . However, for compatibility reasons with other estimators which use the other definition, one extra neighbor will be computed when . To maximise compatibility with all estimators, a safe choice is to always include one extra neighbor in a custom nearest neighbors estimator, since unnecessary neighbors will be filtered by following estimators. Examples: Approximate nearest neighbors in TSNE : an example of pipelining KNeighborsTransformer and TSNE . Also proposes two custom nearest neighbors estimators based on external packages. Caching nearest neighbors : an example of pipelining KNeighborsTransformer and KNeighborsClassifier to enable caching of the neighbors graph during a hyper-parameter grid-search. \"\\n',\n",
       " '\"sklearn_1_6_nearest_neighbors 1.6. Nearest Neighbors modules/neighbors.html  1.6.7. Neighborhood Components Analysis  Neighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis ) is a distance metric learning algorithm which aims to improve the accuracy of nearest neighbors classification compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It can also learn a low-dimensional linear projection of data that can be used for data visualization and fast classification. In the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the mathematical formulation for more details. 1.6.7.1. Classification  Combined with a nearest neighbors classifier ( KNeighborsClassifier ), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user. NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries. To use this model for classification, one needs to combine a NeighborhoodComponentsAnalysis instance that learns the optimal transformation with a KNeighborsClassifier instance that performs the classification in the projected space. Here is an example using the two classes: The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes. 1.6.7.2. Dimensionality reduction  NCA can be used to perform supervised dimensionality reduction. The input data are projected onto a linear subspace consisting of the directions which minimize the NCA objective. The desired dimensionality can be set using the parameter . For instance, the following figure shows a comparison of dimensionality reduction with Principal Component Analysis ( sklearn.decomposition.PCA ), Linear Discriminant Analysis ( sklearn.discriminant_analysis.LinearDiscriminantAnalysis ) and Neighborhood Component Analysis ( NeighborhoodComponentsAnalysis ) on the Digits dataset, a dataset with size and . The data set is split into a training and a test set of equal size, then standardized. For evaluation the 3-nearest neighbor classification accuracy is computed on the 2-dimensional projected points found by each method. Each data sample belongs to one of 10 classes. Examples: Comparing Nearest Neighbors with and without Neighborhood Components Analysis Dimensionality Reduction with Neighborhood Components Analysis Manifold learning on handwritten digits: Locally Linear Embedding, Isomap 1.6.7.3. Mathematical formulation  The goal of NCA is to learn an optimal linear transformation matrix of size , which maximises the sum over all samples of the probability that is correctly classified, i.e.: \\\\[\\\\underset{L}{\\\\arg\\\\max} \\\\sum\\\\limits_{i0}^{N - 1} p_{i}\\\\] with  and the probability of sample being correctly classified according to a stochastic nearest neighbors rule in the learned embedded space: \\\\[p_{i}\\\\sum\\\\limits_{j \\\\in C_i}{p_{i j}}\\\\] where is the set of points in the same class as sample , and is the softmax over Euclidean distances in the embedded space: \\\\[p_{i j}  \\\\frac{\\\\exp(-||L x_i - L x_j||^2)}{\\\\sum\\\\limits_{k \\\\ne i} {\\\\exp{-(||L x_i - L x_k||^2)}}} , \\\\quad p_{i i}  0\\\\] 1.6.7.3.1. Mahalanobis distance  NCA can be seen as learning a (squared) Mahalanobis distance metric: \\\\[|| L(x_i - x_j)||^2  (x_i - x_j)^TM(x_i - x_j),\\\\] where is a symmetric positive semi-definite matrix of size . 1.6.7.4. Implementation  This implementation follows what is explained in the original paper 1 . For the optimisation method, it currently uses scipys L-BFGS-B with a full gradient computation at each iteration, to avoid to tune the learning rate and provide stable learning. See the examples below and the docstring of NeighborhoodComponentsAnalysis.fit for further information. 1.6.7.5. Complexity  1.6.7.5.1. Training  NCA stores a matrix of pairwise distances, taking memory. Time complexity depends on the number of iterations done by the optimisation algorithm. However, one can set the maximum number of iterations with the argument . For each iteration, time complexity is . 1.6.7.5.2. Transform  Here the operation returns , therefore its time complexity equals . There is no added space complexity in the operation. References: 1 Neighbourhood Components Analysis , J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520. Wikipedia entry on Neighborhood Components Analysis \"\\n',\n",
       " '\"sklearn_1_7_gaussian_processes 1.7. Gaussian Processes modules/gaussian_process.html  1.7.1. Gaussian Process Regression (GPR)  The GaussianProcessRegressor implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for ) or the training datas mean (for ). The priors covariance is specified by passing a kernel object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed . As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying . The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, can be passed as optimizer. The noise level in the targets can be specified by passing it via the parameter , either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below). The implementation is based on Algorithm 2.1 of [RW2006] . In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor: allows prediction without prior fitting (based on the GP prior) provides an additional method , which evaluates samples drawn from the GPR (prior or posterior) at given inputs exposes a method , which can be used externally for other ways of selecting hyperparameters, e.g., via Markov chain Monte Carlo. \"\\n',\n",
       " '\"sklearn_1_7_gaussian_processes 1.7. Gaussian Processes modules/gaussian_process.html  1.7.2. GPR examples  1.7.2.1. GPR with noise-level estimation  This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML. The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise. The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations. 1.7.2.2. Comparison of GPR and Kernel Ridge Regression  Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the kernel trick. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction. A major difference is that GPR can choose the kernels hyperparameters based on gradient-ascent on the marginal likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus provide meaningful confidence intervals and posterior samples along with the predictions while KRR only provides predictions. The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernels hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR. The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly (6.28), while KRR chooses the doubled periodicity . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (curse of dimensionality). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean. 1.7.2.3. GPR on Mauna Loa CO2 data  This example is based on Section 5.4.3 of [RW2006] . It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t. The kernel is composed of several terms that are responsible for explaining different properties of the signal: a long term, smooth rising trend is to be explained by an RBF kernel. The RBF kernel with a large length-scale enforces this component to be smooth; it is not enforced that the trend is rising which leaves this choice to the GP. The specific length-scale and the amplitude are free hyperparameters. a seasonal component, which is to be explained by the periodic ExpSineSquared kernel with a fixed periodicity of 1 year. The length-scale of this periodic component, controlling its smoothness, is a free parameter. In order to allow decaying away from exact periodicity, the product with an RBF kernel is taken. The length-scale of this RBF component controls the decay time and is a further free parameter. smaller, medium term irregularities are to be explained by a RationalQuadratic kernel component, whose length-scale and alpha parameter, which determines the diffuseness of the length-scales, are to be determined. According to [RW2006] , these irregularities can better be explained by a RationalQuadratic than an RBF kernel component, probably because it can accommodate several length-scales. a noise term, consisting of an RBF kernel contribution, which shall explain the correlated noise components such as local weather phenomena, and a WhiteKernel contribution for the white noise. The relative amplitudes and the RBFs length scale are further free parameters. Maximizing the log-marginal-likelihood after subtracting the targets mean yields the following kernel with an LML of -83.214: Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015 \"\\n',\n",
       " '\"sklearn_1_7_gaussian_processes 1.7. Gaussian Processes modules/gaussian_process.html  1.7.3. Gaussian Process Classification (GPC)  The GaussianProcessClassifier implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function , which is then squashed through a link function to obtain the probabilistic classification. The latent function is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case. In contrast to the regression setting, the posterior of the latent function is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of [RW2006] . The GP prior mean is assumed to be zero. The priors covariance is specified by passing a kernel object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed . As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying . The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, can be passed as optimizer. GaussianProcessClassifier supports multi-class classification by performing either one-versus-rest or one-versus-one based training and prediction. In one-versus-rest, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In one_vs_one, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. See the section on multi-class classification for more details. In the case of Gaussian process classification, one_vs_one might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that one_vs_one does not support predicting probability estimates but only plain predictions. Moreover, note that GaussianProcessClassifier does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one. \"\\n',\n",
       " '\"sklearn_1_7_gaussian_processes 1.7. Gaussian Processes modules/gaussian_process.html  1.7.4. GPC examples  1.7.4.1. Probabilistic predictions with GPC  This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML). While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC. The second figure shows the log-marginal-likelihood for different choices of the kernels hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots. 1.7.4.2. Illustration of GPC on the XOR dataset  This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel ( RBF ) and a non-stationary kernel ( DotProduct ). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as RBF often obtain better results. 1.7.4.3. Gaussian process classification (GPC) on iris dataset  This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions. \"\\n',\n",
       " '\"sklearn_1_7_gaussian_processes 1.7. Gaussian Processes modules/gaussian_process.html  1.7.5. Kernels for Gaussian Processes  Kernels (also called covariance functions in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the similarity of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of [RW2006] . For guidance on how to best combine different kernels, we refer to [Duv2014] . 1.7.5.1. Gaussian Process Kernel API  The main usage of a Kernel is to compute the GPs covariance between datapoints. For this, the method of the kernel can be called. This method can either be used to compute the auto-covariance of all pairs of datapoints in a 2d array X, or the cross-covariance of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the WhiteKernel ): If only the diagonal of the auto-covariance is being used, the method of a kernel can be called, which is more computationally efficient than the equivalent call to : Kernels are parameterized by a vector of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernels auto-covariance with respect to via setting in the method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of , which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of can be get and set via the property of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of Hyperparameter in the respective kernel. Note that a kernel using a hyperparameter with name x must have the attributes self.x and self.x_bounds. The abstract base class for all kernels is Kernel . Kernel implements a similar interface as Estimator , providing the methods , , and . This allows setting kernel values also via meta-estimators such as Pipeline or GridSearch . Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with and parameters of the right operand with . An additional convenience method is , which returns a cloned version of the kernel but with the hyperparameters set to . An illustrative example: All Gaussian process kernels are interoperable with sklearn.metrics.pairwise and vice versa: instances of subclasses of Kernel can be passed as to from sklearn.metrics.pairwise . Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class PairwiseKernel . The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed. 1.7.5.2. Basic kernels  The ConstantKernel kernel can be used as part of a Product kernel where it scales the magnitude of the other factor (kernel) or as part of a Sum kernel, where it modifies the mean of the Gaussian process. It depends on a parameter . It is defined as: \\\\[k(x_i, x_j)  constant\\\\_value \\\\;\\\\forall\\\\; x_1, x_2\\\\] The main use-case of the WhiteKernel kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level. It is defined as: \\\\[k(x_i, x_j)  noise\\\\_level \\\\text{ if } x_i  x_j \\\\text{ else } 0\\\\] 1.7.5.3. Kernel operators  Kernel operators take one or two base kernels and combine them into a new kernel. The Sum kernel takes two kernels and and combines them via . The Product kernel takes two kernels and and combines them via . The Exponentiation kernel takes one base kernel and a scalar parameter and combines them via . Note that magic methods , and are overridden on the Kernel objects, so one can use e.g. as a shortcut for . 1.7.5.4. Radial-basis function (RBF) kernel  The RBF kernel is a stationary kernel. It is also known as the squared exponential kernel. It is parameterized by a length-scale parameter , which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs (anisotropic variant of the kernel). The kernel is given by: \\\\[k(x_i, x_j)  \\\\text{exp}\\\\left(- \\\\frac{d(x_i, x_j)^2}{2l^2} \\\\right)\\\\] where is the Euclidean distance. This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure: 1.7.5.5. Matrn kernel  The Matern kernel is a stationary kernel and a generalization of the RBF kernel. It has an additional parameter which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter , which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs (anisotropic variant of the kernel). The kernel is given by: \\\\[k(x_i, x_j)  \\\\frac{1}{\\\\Gamma(\\\\nu)2^{\\\\nu-1}}\\\\Bigg(\\\\frac{\\\\sqrt{2\\\\nu}}{l} d(x_i , x_j )\\\\Bigg)^\\\\nu K_\\\\nu\\\\Bigg(\\\\frac{\\\\sqrt{2\\\\nu}}{l} d(x_i , x_j )\\\\Bigg),\\\\] where is the Euclidean distance, is a modified Bessel function and is the gamma function. As , the Matrn kernel converges to the RBF kernel. When , the Matrn kernel becomes identical to the absolute exponential kernel, i.e., \\\\[k(x_i, x_j)  \\\\exp \\\\Bigg(- \\\\frac{1}{l} d(x_i , x_j ) \\\\Bigg) \\\\quad \\\\quad \\\\nu \\\\tfrac{1}{2}\\\\] In particular, : \\\\[k(x_i, x_j)  \\\\Bigg(1 + \\\\frac{\\\\sqrt{3}}{l} d(x_i , x_j )\\\\Bigg) \\\\exp \\\\Bigg(-\\\\frac{\\\\sqrt{3}}{l} d(x_i , x_j ) \\\\Bigg) \\\\quad \\\\quad \\\\nu \\\\tfrac{3}{2}\\\\] and : \\\\[k(x_i, x_j)  \\\\Bigg(1 + \\\\frac{\\\\sqrt{5}}{l} d(x_i , x_j ) +\\\\frac{5}{3l} d(x_i , x_j )^2 \\\\Bigg) \\\\exp \\\\Bigg(-\\\\frac{\\\\sqrt{5}}{l} d(x_i , x_j ) \\\\Bigg) \\\\quad \\\\quad \\\\nu \\\\tfrac{5}{2}\\\\] are popular choices for learning functions that are not infinitely differentiable (as assumed by the RBF kernel) but at least once ( ) or twice differentiable ( ). The flexibility of controlling the smoothness of the learned function via allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Matrn kernel are shown in the following figure: See [RW2006] , pp84 for further details regarding the different variants of the Matrn kernel. 1.7.5.6. Rational quadratic kernel  The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter and a scale mixture parameter Only the isotropic variant where is a scalar is supported at the moment. The kernel is given by: \\\\[k(x_i, x_j)  \\\\left(1 + \\\\frac{d(x_i, x_j)^2}{2\\\\alpha l^2}\\\\right)^{-\\\\alpha}\\\\] The prior and posterior of a GP resulting from a RationalQuadratic kernel are shown in the following figure: 1.7.5.7. Exp-Sine-Squared kernel  The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter and a periodicity parameter . Only the isotropic variant where is a scalar is supported at the moment. The kernel is given by: \\\\[k(x_i, x_j)  \\\\text{exp}\\\\left(- \\\\frac{ 2\\\\sin^2(\\\\pi d(x_i, x_j) / p) }{ l^ 2} \\\\right)\\\\] The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure: 1.7.5.8. Dot-Product kernel  The DotProduct kernel is non-stationary and can be obtained from linear regression by putting priors on the coefficients of and a prior of on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter . For , the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by \\\\[k(x_i, x_j)  \\\\sigma_0 ^ 2 + x_i \\\\cdot x_j\\\\] The DotProduct kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure: 1.7.5.9. References  RW2006 ( 1 , 2 , 3 , 4 , 5 , 6 ) Carl Eduard Rasmussen and Christopher K.I. Williams, Gaussian Processes for Machine Learning, MIT Press 2006, Link to an official complete PDF version of the book here . Duv2014 David Duvenaud, The Kernel Cookbook: Advice on Covariance functions, 2014, Link . \"\\n',\n",
       " '\"sklearn_1_9_naive_bayes 1.9. Naive Bayes modules/naive_bayes.html  1.9.1. Gaussian Naive Bayes  GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian: \\\\[P(x_i \\\\mid y)  \\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma^2_y}} \\\\exp\\\\left(-\\\\frac{(x_i - \\\\mu_y)^2}{2\\\\sigma^2_y}\\\\right)\\\\] The parameters and are estimated using maximum likelihood. \"\\n',\n",
       " '\"sklearn_1_9_naive_bayes 1.9. Naive Bayes modules/naive_bayes.html  1.9.2. Multinomial Naive Bayes  MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors for each class , where is the number of features (in text classification, the size of the vocabulary) and is the probability of feature appearing in a sample belonging to class . The parameters is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting: \\\\[\\\\hat{\\\\theta}_{yi}  \\\\frac{ N_{yi} + \\\\alpha}{N_y + \\\\alpha n}\\\\] where is the number of times feature appears in a sample of class in the training set , and is the total count of all features for class . The smoothing priors accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting is called Laplace smoothing, while is called Lidstone smoothing. \"\\n',\n",
       " '\"sklearn_1_9_naive_bayes 1.9. Naive Bayes modules/naive_bayes.html  1.9.3. Complement Naive Bayes  ComplementNB implements the complement naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Specifically, CNB uses statistics from the complement of each class to compute the models weights. The inventors of CNB show empirically that the parameter estimates for CNB are more stable than those for MNB. Further, CNB regularly outperforms MNB (often by a considerable margin) on text classification tasks. The procedure for calculating the weights is as follows: \\\\[ \\\\begin{align}\\\\begin{aligned}\\\\hat{\\\\theta}_{ci}  \\\\frac{\\\\alpha_i + \\\\sum_{j:y_j \\\\neq c} d_{ij}} {\\\\alpha + \\\\sum_{j:y_j \\\\neq c} \\\\sum_{k} d_{kj}}\\\\\\\\w_{ci}  \\\\log \\\\hat{\\\\theta}_{ci}\\\\\\\\w_{ci}  \\\\frac{w_{ci}}{\\\\sum_{j} |w_{cj}|}\\\\end{aligned}\\\\end{align} \\\\] where the summations are over all documents not in class , is either the count or tf-idf value of term in document , is a smoothing hyperparameter like that found in MNB, and . The second normalization addresses the tendency for longer documents to dominate parameter estimates in MNB. The classification rule is: \\\\[\\\\hat{c}  \\\\arg\\\\min_c \\\\sum_{i} t_i w_{ci}\\\\] i.e., a document is assigned to the class that is the poorest complement match. References: Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). Tackling the poor assumptions of naive bayes text classifiers. In ICML (Vol. 3, pp. 616-623). \"\\n',\n",
       " '\"sklearn_1_9_naive_bayes 1.9. Naive Bayes modules/naive_bayes.html  1.9.4. Bernoulli Naive Bayes  BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a instance may binarize its input (depending on the parameter). The decision rule for Bernoulli naive Bayes is based on \\\\[P(x_i \\\\mid y)  P(i \\\\mid y) x_i + (1 - P(i \\\\mid y)) (1 - x_i)\\\\] which differs from multinomial NBs rule in that it explicitly penalizes the non-occurrence of a feature that is an indicator for class , where the multinomial variant would simply ignore a non-occurring feature. In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits. References: C.D. Manning, P. Raghavan and H. Schtze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. A. McCallum and K. Nigam (1998). A comparison of event models for Naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48. V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with Naive Bayes  Which Naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS). \"\\n',\n",
       " '\"sklearn_1_9_naive_bayes 1.9. Naive Bayes modules/naive_bayes.html  1.9.5. Categorical Naive Bayes  CategoricalNB implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index , has its own categorical distribution. For each feature in the training set , CategoricalNB estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as , with as the number of samples. The probability of category in feature given class is estimated as: \\\\[P(x_i  t \\\\mid y  c \\\\: ;\\\\, \\\\alpha)  \\\\frac{ N_{tic} + \\\\alpha}{N_{c} + \\\\alpha n_i},\\\\] where is the number of times category appears in the samples , which belong to class , is the number of samples with class c, is a smoothing parameter and is the number of available categories of feature . CategoricalNB assumes that the sample matrix is encoded (for instance with the help of OrdinalEncoder ) such that all categories for each feature are represented with numbers where is the number of available categories of feature . \"\\n',\n",
       " '\"sklearn_1_9_naive_bayes 1.9. Naive Bayes modules/naive_bayes.html  1.9.6. Out-of-core naive Bayes model fitting  Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB , BernoulliNB , and GaussianNB expose a method that can be used incrementally as done with other classifiers as demonstrated in Out-of-core classification of text documents . All naive Bayes classifiers support sample weighting. Contrary to the method, the first call to needs to be passed the list of all the expected class labels. For an overview of available strategies in scikit-learn, see also the out-of-core learning documentation. Note The method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows. \"\\n',\n",
       " '\"sklearn_2_1_gaussian_mixture_models 2.1. Gaussian mixture models modules/mixture.html  2.1.1. Gaussian Mixture  The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A GaussianMixture.fit method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the GaussianMixture.predict method. The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance. Examples: See GMM covariances for an example of using the Gaussian mixture as clustering on the iris dataset. See Density Estimation for a Gaussian mixture for an example on plotting the density estimation. 2.1.1.1. Pros and cons of class GaussianMixture  2.1.1.1.1. Pros  Speed It is the fastest algorithm for learning mixture models Agnostic As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply. 2.1.1.1.2. Cons  Singularities When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially. Number of components This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues. 2.1.1.2. Selecting the number of components in a classical Gaussian Mixture Model  The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a Variational Bayesian Gaussian mixture avoids the specification of the number of components for a Gaussian mixture model. Examples: See Gaussian Mixture Model Selection for an example of model selection performed with classical Gaussian mixture. 2.1.1.3. Estimation algorithm Expectation-maximization  The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesnt know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). Expectation-maximization is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum. \"\\n',\n",
       " '\"sklearn_2_1_gaussian_mixture_models 2.1. Gaussian mixture models modules/mixture.html  2.1.2. Variational Bayesian Gaussian Mixture  The BayesianGaussianMixture object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar as the one defined by GaussianMixture . 2.1.2.1. Estimation algorithm: variational inference  Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical. Due to its Bayesian nature, the variational algorithm needs more hyper- parameters than expectation-maximization, the most important of these being the concentration parameter . Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture. The parameters implementation of the BayesianGaussianMixture class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data. The next figure compares the results obtained for the different type of the weight concentration prior (parameter ) for different values of . Here, we can see the value of the parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is dirichlet_distribution while this is not necessarily the case for the dirichlet_process type (used by default). The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component. On the following figure we are fitting a dataset not well-depicted by a Gaussian mixture. Adjusting the , parameter of the BayesianGaussianMixture controls the number of components used to fit this data. We also present on the last two plots a random sampling generated from the two resulting mixtures. Examples: See Gaussian Mixture Model Ellipsoids for an example on plotting the confidence ellipsoids for both GaussianMixture and BayesianGaussianMixture . Gaussian Mixture Model Sine Curve shows using GaussianMixture and BayesianGaussianMixture to fit a sine wave. See Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture for an example plotting the confidence ellipsoids for the BayesianGaussianMixture with different for different values of the parameter . 2.1.2.2. Pros and cons of variational inference with BayesianGaussianMixture  2.1.2.2.1. Pros  Automatic selection when is small enough and is larger than what is found necessary by the model, the Variational Bayesian mixture model has a natural tendency to set some mixture weights values close to zero. This makes it possible to let the model choose a suitable number of effective components automatically. Only an upper bound of this number needs to be provided. Note however that the ideal number of active components is very application specific and is typically ill-defined in a data exploration setting. Less sensitivity to the number of parameters unlike finite models, which will almost always use all components as much as they can, and hence will produce wildly different solutions for different numbers of components, the variational inference with a Dirichlet process prior ( ) wont change much with changes to the parameters, leading to more stability and less tuning. Regularization due to the incorporation of prior information, variational solutions have less pathological special cases than expectation-maximization solutions. 2.1.2.2.2. Cons  Speed the extra parametrization necessary for variational inference make inference slower, although not by much. Hyperparameters this algorithm needs an extra hyperparameter that might need experimental tuning via cross-validation. Bias there are many implicit biases in the inference algorithms (and also in the Dirichlet process if used), and whenever there is a mismatch between these biases and the data it might be possible to fit better models using a finite mixture. 2.1.2.3. The Dirichlet Process  Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on clusterings with an infinite, unbounded, number of partitions . Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model. An important question is how can the Dirichlet process use an infinite, unbounded number of clusters and still be consistent. While a full explanation doesnt fit this manual, one can think of its stick breaking process analogy to help understanding it. The stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to the proportion of points that falls into a group of the mixture. At the end, to represent the infinite mixture, we associate the last remaining piece of the stick to the proportion of points that dont fall into all the other groups. The length of each piece is a random variable with probability proportional to the concentration parameter. Smaller value of the concentration will divide the unit-length into larger pieces of the stick (defining more concentrated distribution). Larger concentration values will create smaller pieces of the stick (increasing the number of components with non zero weights). Variational inference techniques for the Dirichlet process still work with a finite approximation to this infinite mixture model, but instead of having to specify a priori how many components one wants to use, one just specifies the concentration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher than the true number of components, affects only algorithmic complexity, not the actual number of components used). \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.10. Tips on practical use  Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See StandardScaler for convenient ways of scaling heterogeneous data. The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a -dimensional manifold embedded in a -dimensional parameter space, the reconstruction error will decrease as is increased until . Note that noisy data can short-circuit the manifold, in essence acting as a bridge between parts of the manifold that would otherwise be well-separated. Manifold learning on noisy and/or incomplete data is an active area of research. Certain input configurations can lead to singular weight matrices, for example when more than two points in the dataset are identical, or when the data is split into disjointed groups. In this case, will fail to find the null space. The easiest way to address this is to use which will work on a singular matrix, though it may be very slow depending on the number of input points. Alternatively, one can attempt to understand the source of the singularity: if it is due to disjoint sets, increasing may help. If it is due to identical points in the dataset, removing these points may help. See also Totally Random Trees Embedding can also be useful to derive non-linear representations of feature space, also it does not perform dimensionality reduction. \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.1. Introduction  High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way. The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost. To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an interesting linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data. Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications. Examples: See Manifold learning on handwritten digits: Locally Linear Embedding, Isomap for an example of dimensionality reduction on handwritten digits. See Comparison of Manifold Learning methods for an example of dimensionality reduction on a toy S-curve dataset. The manifold learning implementations available in scikit-learn are summarized below \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.2. Isomap  One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be performed with the object Isomap . 2.2.2.1. Complexity  The Isomap algorithm comprises three stages: Nearest neighbor search. Isomap uses sklearn.neighbors.BallTree for efficient neighbor search. The cost is approximately , for nearest neighbors of points in dimensions. Shortest-path graph search. The most efficient known algorithms for this are Dijkstras Algorithm , which is approximately , or the Floyd-Warshall algorithm , which is . The algorithm can be selected by the user with the keyword of . If unspecified, the code attempts to choose the best algorithm for the input data. Partial eigenvalue decomposition. The embedding is encoded in the eigenvectors corresponding to the largest eigenvalues of the isomap kernel. For a dense solver, the cost is approximately . This cost can often be improved using the solver. The eigensolver can be specified by the user with the keyword of . If unspecified, the code attempts to choose the best algorithm for the input data. The overall complexity of Isomap is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: A global geometric framework for nonlinear dimensionality reduction Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. Science 290 (5500) \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.3. Locally Linear Embedding  Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding. Locally linear embedding can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding . 2.2.3.1. Complexity  The standard LLE algorithm comprises three stages: Nearest Neighbors Search . See discussion under Isomap above. Weight Matrix Construction . . The construction of the LLE weight matrix involves the solution of a linear equation for each of the local neighborhoods Partial Eigenvalue Decomposition . See discussion under Isomap above. The overall complexity of standard LLE is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: Nonlinear dimensionality reduction by locally linear embedding Roweis, S. & Saul, L. Science 290:2323 (2000) \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.4. Modified Locally Linear Embedding  One well-known issue with LLE is the regularization problem. When the number of neighbors is greater than the number of input dimensions, the matrix defining each local neighborhood is rank-deficient. To address this, standard LLE applies an arbitrary regularization parameter , which is chosen relative to the trace of the local weight matrix. Though it can be shown formally that as , the solution converges to the desired embedding, there is no guarantee that the optimal solution will be found for . This problem manifests itself in embeddings which distort the underlying geometry of the manifold. One method to address the regularization problem is to use multiple weight vectors in each neighborhood. This is the essence of modified locally linear embedding (MLLE). MLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding , with the keyword . It requires . 2.2.4.1. Complexity  The MLLE algorithm comprises three stages: Nearest Neighbors Search . Same as standard LLE Weight Matrix Construction . Approximately . The first term is exactly equivalent to that of standard LLE. The second term has to do with constructing the weight matrix from multiple weights. In practice, the added cost of constructing the MLLE weight matrix is relatively small compared to the cost of stages 1 and 3. Partial Eigenvalue Decomposition . Same as standard LLE The overall complexity of MLLE is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: MLLE: Modified Locally Linear Embedding Using Multiple Weights Zhang, Z. & Wang, J. \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.5. Hessian Eigenmapping  Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding , with the keyword . It requires . 2.2.5.1. Complexity  The HLLE algorithm comprises three stages: Nearest Neighbors Search . Same as standard LLE Weight Matrix Construction . Approximately . The first term reflects a similar cost to that of standard LLE. The second term comes from a QR decomposition of the local hessian estimator. Partial Eigenvalue Decomposition . Same as standard LLE The overall complexity of standard HLLE is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003) \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.6. Spectral Embedding  Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed with the function spectral_embedding or its object-oriented counterpart SpectralEmbedding . 2.2.6.1. Complexity  The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages: Weighted Graph Construction . Transform the raw input data into graph representation using affinity (adjacency) matrix representation. Graph Laplacian Construction . unnormalized Graph Laplacian is constructed as for and normalized one as . Partial Eigenvalue Decomposition . Eigenvalue decomposition is done on graph Laplacian The overall complexity of spectral embedding is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: Laplacian Eigenmaps for Dimensionality Reduction and Data Representation M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396 \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.7. Local Tangent Space Alignment  Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding , with the keyword . 2.2.7.1. Complexity  The LTSA algorithm comprises three stages: Nearest Neighbors Search . Same as standard LLE Weight Matrix Construction . Approximately . The first term reflects a similar cost to that of standard LLE. Partial Eigenvalue Decomposition . Same as standard LLE The overall complexity of standard LTSA is . : number of training data points : input dimension : number of nearest neighbors : output dimension References: Principal manifolds and nonlinear dimensionality reduction via tangent space alignment Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004) \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.8. Multi-dimensional Scaling (MDS)  Multidimensional scaling ( MDS ) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space. In general, MDS is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries. There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class MDS implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities. Let be the similarity matrix, and the coordinates of the input points. Disparities are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by 2.2.8.1. Metric MDS  The simplest metric MDS model, called absolute MDS , disparities are defined by . With absolute MDS, the value should then correspond exactly to the distance between point and in the embedding point. Most commonly, disparities are set to . 2.2.8.2. Nonmetric MDS  Non metric MDS focuses on the ordination of the data. If , then the embedding should enforce . A simple algorithm to enforce that is to use a monotonic regression of on , yielding disparities in the same order as . A trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities are normalized. References: Modern Multidimensional Scaling - Theory and Applications Borg, I.; Groenen P. Springer Series in Statistics (1997) Nonmetric multidimensional scaling: a numerical method Kruskal, J. Psychometrika, 29 (1964) Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis Kruskal, J. Psychometrika, 29, (1964) \"\\n',\n",
       " '\"sklearn_2_2_manifold_learning 2.2. Manifold learning modules/manifold.html  2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)  t-SNE ( TSNE ) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Students t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques: Revealing the structure at many scales on a single map Revealing data that lie in multiple, different, manifolds or clusters Reducing the tendency to crowd points together at the center While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset. The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence. The disadvantages to using t-SNE are roughly: t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings. The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error. Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using ). 2.2.9.1. Optimizing t-SNE  The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions. Optimizing the KL divergence can be a little bit tricky sometimes. There are five parameters that control the optimization of t-SNE and therefore possibly the quality of the resulting embedding: perplexity early exaggeration factor learning rate maximum number of iterations angle (not used in the exact method) The perplexity is defined as where is the Shannon entropy of the conditional probability distribution. The perplexity of a -sided die is , so that is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise. The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maatens FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results. How to Use t-SNE Effectively provides a good discussion of the effects of the various parameters, as well as interactive plots to explore the effects of different parameters. 2.2.9.2. Barnes-Hut t-SNE  The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is , where is the number of output dimensions and is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is , but has several other notable differences: The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations. Barnes-Hut only works with dense input data. Sparse data matrices can only be embedded with the exact method or can be approximated by a dense low rank projection for instance using sklearn.decomposition.TruncatedSVD Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle parameter, therefore the angle parameter is unused when methodexact Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points while the exact method can handle thousands of samples before becoming computationally intractable For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints. Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not low enough to accurately represents the internal structure of the data. References: Visualizing High-Dimensional Data Using t-SNE van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008) t-Distributed Stochastic Neighbor Embedding van der Maaten, L.J.P. Accelerating t-SNE using Tree-Based Algorithms. L.J.P. van der Maaten. Journal of Machine Learning Research 15(Oct):3221-3245, 2014. \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.10. Clustering performance evaluation  Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar than members of different classes according to some similarity metric. 2.3.10.1. Adjusted Rand index  Given the knowledge of the ground truth class assignments and our clustering algorithm assignments of the same samples , the adjusted Rand index is a function that measures the similarity of the two assignments, ignoring permutations and with chance normalization : One can permute 0 and 1 in the predicted labels, rename 2 to 3, and get the same score: Furthermore, adjusted_rand_score is symmetric : swapping the argument does not change the score. It can thus be used as a consensus measure : Perfect labeling is scored 1.0: Bad (e.g. independent labelings) have negative or close to 0.0 scores: 2.3.10.1.1. Advantages  Random (uniform) label assignments have a ARI score close to 0.0 for any value of and (which is not the case for raw Rand index or the V-measure for instance). Bounded range [-1, 1] : negative values are bad (independent labelings), similar clusterings have a positive ARI, 1.0 is the perfect match score. No assumption is made on the cluster structure : can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with folded shapes. 2.3.10.1.2. Drawbacks  Contrary to inertia, ARI requires knowledge of the ground truth classes while is almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting). However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO). Examples: Adjustment for chance in clustering performance evaluation : Analysis of the impact of the dataset size on the value of clustering measures for random assignments. 2.3.10.1.3. Mathematical formulation  If C is a ground truth class assignment and K the clustering, let us define and as: , the number of pairs of elements that are in the same set in C and in the same set in K , the number of pairs of elements that are in different sets in C and in different sets in K The raw (unadjusted) Rand index is then given by: \\\\[\\\\text{RI}  \\\\frac{a + b}{C_2^{n_{samples}}}\\\\] Where is the total number of possible pairs in the dataset (without ordering). However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples). To counter this effect we can discount the expected RI of random labelings by defining the adjusted Rand index as follows: \\\\[\\\\text{ARI}  \\\\frac{\\\\text{RI} - E[\\\\text{RI}]}{\\\\max(\\\\text{RI}) - E[\\\\text{RI}]}\\\\] References Comparing Partitions L. Hubert and P. Arabie, Journal of Classification 1985 Wikipedia entry for the adjusted Rand index 2.3.10.2. Mutual Information based scores  Given the knowledge of the ground truth class assignments and our clustering algorithm assignments of the same samples , the Mutual Information is a function that measures the agreement of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, Normalized Mutual Information (NMI) and Adjusted Mutual Information (AMI) . NMI is often used in the literature, while AMI was proposed more recently and is normalized against chance : One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score: All, mutual_info_score , adjusted_mutual_info_score and normalized_mutual_info_score are symmetric: swapping the argument does not change the score. Thus they can be used as a consensus measure : Perfect labeling is scored 1.0: This is not true for , which is therefore harder to judge: Bad (e.g. independent labelings) have non-positive scores: 2.3.10.2.1. Advantages  Random (uniform) label assignments have a AMI score close to 0.0 for any value of and (which is not the case for raw Mutual Information or the V-measure for instance). Upper bound of 1 : Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, an AMI of exactly 1 indicates that the two label assignments are equal (with or without permutation). 2.3.10.2.2. Drawbacks  Contrary to inertia, MI-based measures require the knowledge of the ground truth classes while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting). However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection. NMI and MI are not adjusted against chance. Examples: Adjustment for chance in clustering performance evaluation : Analysis of the impact of the dataset size on the value of clustering measures for random assignments. This example also includes the Adjusted Rand Index. 2.3.10.2.3. Mathematical formulation  Assume two label assignments (of the same N objects), and . Their entropy is the amount of uncertainty for a partition set, defined by: \\\\[H(U)  - \\\\sum_{i1}^{|U|}P(i)\\\\log(P(i))\\\\] where is the probability that an object picked at random from falls into class . Likewise for : \\\\[H(V)  - \\\\sum_{j1}^{|V|}P\\'(j)\\\\log(P\\'(j))\\\\] With . The mutual information (MI) between and is calculated by: \\\\[\\\\text{MI}(U, V)  \\\\sum_{i1}^{|U|}\\\\sum_{j1}^{|V|}P(i, j)\\\\log\\\\left(\\\\frac{P(i,j)}{P(i)P\\'(j)}\\\\right)\\\\] where is the probability that an object picked at random falls into both classes and . It also can be expressed in set cardinality formulation: \\\\[\\\\text{MI}(U, V)  \\\\sum_{i1}^{|U|} \\\\sum_{j1}^{|V|} \\\\frac{|U_i \\\\cap V_j|}{N}\\\\log\\\\left(\\\\frac{N|U_i \\\\cap V_j|}{|U_i||V_j|}\\\\right)\\\\] The normalized mutual information is defined as \\\\[\\\\text{NMI}(U, V)  \\\\frac{\\\\text{MI}(U, V)}{\\\\text{mean}(H(U), H(V))}\\\\] This value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase as the number of different labels (clusters) increases, regardless of the actual amount of mutual information between the label assignments. The expected value for the mutual information can be calculated using the following equation [VEB2009] . In this equation, (the number of elements in ) and (the number of elements in ). \\\\[E[\\\\text{MI}(U,V)]\\\\sum_{i1}^{|U|} \\\\sum_{j1}^{|V|} \\\\sum_{n_{ij}(a_i+b_j-N)^+ }^{\\\\min(a_i, b_j)} \\\\frac{n_{ij}}{N}\\\\log \\\\left( \\\\frac{ N.n_{ij}}{a_i b_j}\\\\right) \\\\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})! (N-a_i-b_j+n_{ij})!}\\\\] Using the expected value, the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index: \\\\[\\\\text{AMI}  \\\\frac{\\\\text{MI} - E[\\\\text{MI}]}{\\\\text{mean}(H(U), H(V)) - E[\\\\text{MI}]}\\\\] For normalized mutual information and adjusted mutual information, the normalizing value is typically some generalized mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides qualitatively similar behaviours [YAT2016] . In our implementation, this is controlled by the parameter. Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010] . Their sqrt and sum averages are the geometric and arithmetic means; we use these more broadly common names. References Strehl, Alexander, and Joydeep Ghosh (2002). Cluster ensembles  a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research 3: 583617. doi:10.1162/153244303321897735 . Wikipedia entry for the (normalized) Mutual Information Wikipedia entry for the Adjusted Mutual Information VEB2009 Vinh, Epps, and Bailey, (2009). Information theoretic measures for clusterings comparison. Proceedings of the 26th Annual International Conference on Machine Learning - ICML 09. doi:10.1145/1553374.1553511 . ISBN 9781605585161. VEB2010 Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance. JMLR < http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf > YAT2016 Yang, Algesheimer, and Tessone, (2016). A comparative analysis of community detection algorithms on artificial networks. Scientific Reports 6: 30750. doi:10.1038/srep30750 . 2.3.10.3. Homogeneity, completeness and V-measure  Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis. In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment: homogeneity : each cluster contains only members of a single class. completeness : all members of a given class are assigned to the same cluster. We can turn those concept as scores homogeneity_score and completeness_score . Both are bounded below by 0.0 and above by 1.0 (higher is better): Their harmonic mean called V-measure is computed by v_measure_score : This functions formula is as follows: \\\\[v  \\\\frac{(1 + \\\\beta) \\\\times \\\\text{homogeneity} \\\\times \\\\text{completeness}}{(\\\\beta \\\\times \\\\text{homogeneity} + \\\\text{completeness})}\\\\] defaults to a value of 1.0, but for using a value less than 1 for beta: more weight will be attributed to homogeneity, and using a value greater than 1: more weight will be attributed to completeness. The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean [B2011] . Homogeneity, completeness and V-measure can be computed at once using homogeneity_completeness_v_measure as follows: The following clustering assignment is slightly better, since it is homogeneous but not complete: Note v_measure_score is symmetric : it can be used to evaluate the agreement of two independent assignments on the same dataset. This is not the case for completeness_score and homogeneity_score : both are bound by the relationship: 2.3.10.3.1. Advantages  Bounded scores : 0.0 is as bad as it can be, 1.0 is a perfect score. Intuitive interpretation: clustering with bad V-measure can be qualitatively analyzed in terms of homogeneity and completeness to better feel what kind of mistakes is done by the assignment. No assumption is made on the cluster structure : can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with folded shapes. 2.3.10.3.2. Drawbacks  The previously introduced metrics are not normalized with regards to random labeling : this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular random labeling wont yield zero scores especially when the number of clusters is large . This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10. For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI) . These metrics require the knowledge of the ground truth classes while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting). Examples: Adjustment for chance in clustering performance evaluation : Analysis of the impact of the dataset size on the value of clustering measures for random assignments. 2.3.10.3.3. Mathematical formulation  Homogeneity and completeness scores are formally given by: \\\\[h  1 - \\\\frac{H(C|K)}{H(C)}\\\\] \\\\[c  1 - \\\\frac{H(K|C)}{H(K)}\\\\] where is the conditional entropy of the classes given the cluster assignments and is given by: \\\\[H(C|K)  - \\\\sum_{c1}^{|C|} \\\\sum_{k1}^{|K|} \\\\frac{n_{c,k}}{n} \\\\cdot \\\\log\\\\left(\\\\frac{n_{c,k}}{n_k}\\\\right)\\\\] and is the entropy of the classes and is given by: \\\\[H(C)  - \\\\sum_{c1}^{|C|} \\\\frac{n_c}{n} \\\\cdot \\\\log\\\\left(\\\\frac{n_c}{n}\\\\right)\\\\] with the total number of samples, and the number of samples respectively belonging to class and cluster , and finally the number of samples from class assigned to cluster . The conditional entropy of clusters given class and the entropy of clusters are defined in a symmetric manner. Rosenberg and Hirschberg further define V-measure as the harmonic mean of homogeneity and completeness : \\\\[v  2 \\\\cdot \\\\frac{h \\\\cdot c}{h + c}\\\\] References V-Measure: A conditional entropy-based external cluster evaluation measure Andrew Rosenberg and Julia Hirschberg, 2007 B2011 Identication and Characterization of Events in Social Media , Hila Becker, PhD Thesis. 2.3.10.4. Fowlkes-Mallows scores  The Fowlkes-Mallows index ( sklearn.metrics.fowlkes_mallows_score ) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall: \\\\[\\\\text{FMI}  \\\\frac{\\\\text{TP}}{\\\\sqrt{(\\\\text{TP} + \\\\text{FP}) (\\\\text{TP} + \\\\text{FN})}}\\\\] Where is the number of True Positive (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), is the number of False Positive (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and is the number of False Negative (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels). The score ranges from 0 to 1. A high value indicates a good similarity between two clusters. One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score: Perfect labeling is scored 1.0: Bad (e.g. independent labelings) have zero scores: 2.3.10.4.1. Advantages  Random (uniform) label assignments have a FMI score close to 0.0 for any value of and (which is not the case for raw Mutual Information or the V-measure for instance). Upper-bounded at 1 : Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, values of exactly 0 indicate purely independent label assignments and a FMI of exactly 1 indicates that the two label assignments are equal (with or without permutation). No assumption is made on the cluster structure : can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with folded shapes. 2.3.10.4.2. Drawbacks  Contrary to inertia, FMI-based measures require the knowledge of the ground truth classes while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting). References E. B. Fowkles and C. L. Mallows, 1983. A method for comparing two hierarchical clusterings. Journal of the American Statistical Association. http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf Wikipedia entry for the Fowlkes-Mallows Index 2.3.10.5. Silhouette Coefficient  If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient ( sklearn.metrics.silhouette_score ) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores: a : The mean distance between a sample and all other points in the same class. b : The mean distance between a sample and all other points in the next nearest cluster . The Silhouette Coefficient s for a single sample is then given as: \\\\[s  \\\\frac{b - a}{max(a, b)}\\\\] The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample. In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis. References Peter J. Rousseeuw (1987). Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis. Computational and Applied Mathematics 20: 5365. doi:10.1016/0377-0427(87)90125-7 . 2.3.10.5.1. Advantages  The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster. 2.3.10.5.2. Drawbacks  The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. Examples: Selecting the number of clusters with silhouette analysis on KMeans clustering : In this example the silhouette analysis is used to choose an optimal value for n_clusters. 2.3.10.6. Calinski-Harabasz Index  If the ground truth labels are not known, the Calinski-Harabasz index ( sklearn.metrics.calinski_harabasz_score ) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabasz score relates to a model with better defined clusters. The index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared): In normal usage, the Calinski-Harabasz index is applied to the results of a cluster analysis: 2.3.10.6.1. Advantages  The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster. The score is fast to compute. 2.3.10.6.2. Drawbacks  The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. 2.3.10.6.3. Mathematical formulation  For a set of data of size which has been clustered into clusters, the Calinski-Harabasz score is defined as the ratio of the between-clusters dispersion mean and the within-cluster dispersion: \\\\[s  \\\\frac{\\\\mathrm{tr}(B_k)}{\\\\mathrm{tr}(W_k)} \\\\times \\\\frac{n_E - k}{k - 1}\\\\] where is trace of the between group dispersion matrix and is the trace of the within-cluster dispersion matrix defined by: \\\\[W_k  \\\\sum_{q1}^k \\\\sum_{x \\\\in C_q} (x - c_q) (x - c_q)^T\\\\] \\\\[B_k  \\\\sum_{q1}^k n_q (c_q - c_E) (c_q - c_E)^T\\\\] with the set of points in cluster , the center of cluster , the center of , and the number of points in cluster . References Caliski, T., & Harabasz, J. (1974). A Dendrite Method for Cluster Analysis . Communications in Statistics-theory and Methods 3: 1-27. doi:10.1080/03610927408827101 . 2.3.10.7. Davies-Bouldin Index  If the ground truth labels are not known, the Davies-Bouldin index ( sklearn.metrics.davies_bouldin_score ) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters. This index signifies the average similarity between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Zero is the lowest possible score. Values closer to zero indicate a better partition. In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows: 2.3.10.7.1. Advantages  The computation of Davies-Bouldin is simpler than that of Silhouette scores. The index is computed only quantities and features inherent to the dataset. 2.3.10.7.2. Drawbacks  The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN. The usage of centroid distance limits the distance metric to Euclidean space. 2.3.10.7.3. Mathematical formulation  The index is defined as the average similarity between each cluster for and its most similar one . In the context of this index, similarity is defined as a measure that trades off: , the average distance between each point of cluster and the centroid of that cluster  also know as cluster diameter. , the distance between cluster centroids and . A simple choice to construct so that it is nonnegative and symmetric is: \\\\[R_{ij}  \\\\frac{s_i + s_j}{d_{ij}}\\\\] Then the Davies-Bouldin index is defined as: \\\\[DB  \\\\frac{1}{k} \\\\sum_{i1}^k \\\\max_{i \\\\neq j} R_{ij}\\\\] References Davies, David L.; Bouldin, Donald W. (1979). A Cluster Separation Measure IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227. doi:10.1109/TPAMI.1979.4766909 . Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). On Clustering Validation Techniques Journal of Intelligent Information Systems, 17(2-3), 107-145. doi:10.1023/A:1012801612483 . Wikipedia entry for Davies-Bouldin index . 2.3.10.8. Contingency Matrix  Contingency matrix ( sklearn.metrics.cluster.contingency_matrix ) reports the intersection cardinality for every true/predicted cluster pair. The contingency matrix provides sufficient statistics for all clustering metrics where the samples are independent and identically distributed and one doesnt need to account for some instances not being clustered. Here is an example: The first row of output array indicates that there are three samples whose true cluster is a. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is b. Of them, none is in predicted cluster 0, one is in 1 and two are in 2. A confusion matrix for classification is a square contingency matrix where the order of rows and columns correspond to a list of classes. 2.3.10.8.1. Advantages  Allows to examine the spread of each true cluster across predicted clusters and vice versa. The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings. 2.3.10.8.2. Drawbacks  Contingency matrix is easy to interpret for a small number of clusters, but becomes very hard to interpret for a large number of clusters. It doesnt give a single metric to use as an objective for clustering optimisation. References Wikipedia entry for contingency matrix \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.1. Overview of clustering methods  A comparison of the clustering algorithms in scikit-learn  Method name Parameters Scalability Usecase Geometry (metric used) K-Means number of clusters Very large , medium with MiniBatch code General-purpose, even cluster size, flat geometry, not too many clusters Distances between points Affinity propagation damping, sample preference Not scalable with n_samples Many clusters, uneven cluster size, non-flat geometry Graph distance (e.g. nearest-neighbor graph) Mean-shift bandwidth Not scalable with Many clusters, uneven cluster size, non-flat geometry Distances between points Spectral clustering number of clusters Medium , small Few clusters, even cluster size, non-flat geometry Graph distance (e.g. nearest-neighbor graph) Ward hierarchical clustering number of clusters or distance threshold Large and Many clusters, possibly connectivity constraints Distances between points Agglomerative clustering number of clusters or distance threshold, linkage type, distance Large and Many clusters, possibly connectivity constraints, non Euclidean distances Any pairwise distance DBSCAN neighborhood size Very large , medium Non-flat geometry, uneven cluster sizes Distances between nearest points OPTICS minimum cluster membership Very large , large Non-flat geometry, uneven cluster sizes, variable cluster density Distances between points Gaussian mixtures many Not scalable Flat geometry, good for density estimation Mahalanobis distances to centers Birch branching factor, threshold, optional global clusterer. Large and Large dataset, outlier removal, data reduction. Euclidean distance between points Non-flat geometry clustering is useful when the clusters have a specific shape, i.e. a non-flat manifold, and the standard euclidean distance is not the right metric. This case arises in the two top rows of the figure above. Gaussian mixture models, useful for clustering, are described in another chapter of the documentation dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component. \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.2. K-means  The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields. The k-means algorithm divides a set of samples into disjoint clusters , each described by the mean of the samples in the cluster. The means are commonly called the cluster centroids; note that they are not, in general, points from , although they live in the same space. The K-means algorithm aims to choose centroids that minimise the inertia , or within-cluster sum-of-squares criterion : \\\\[\\\\sum_{i0}^{n}\\\\min_{\\\\mu_j \\\\in C}(||x_i - \\\\mu_j||^2)\\\\] Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks: Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes. Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called curse of dimensionality). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations. K-means is often referred to as Lloyds algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose samples from the dataset . After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly. K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix. The algorithm can also be understood through the concept of Voronoi diagrams . First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance. Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference. The algorithm supports sample weights, which can be given by a parameter . This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset . K-means can be used for vector quantization. This is achieved using the transform method of a trained model of KMeans . 2.3.2.1. Low-level parallelism  KMeans benefits from OpenMP based parallelism through Cython. Small chunks of data (256 samples) are processed in parallel, which in addition yields a low memory footprint. For more details on how to control the number of threads, please refer to our Parallelism notes. Examples: Demonstration of k-means assumptions : Demonstrating when k-means performs intuitively and when it does not A demo of K-Means clustering on the handwritten digits data : Clustering handwritten digits References: k-means++: The advantages of careful seeding Arthur, David, and Sergei Vassilvitskii, Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms , Society for Industrial and Applied Mathematics (2007) 2.3.2.2. Mini Batch K-Means  The MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm. The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached. MiniBatchKMeans converges faster than KMeans , but the quality of the results is reduced. In practice this difference in quality can be quite small, as shown in the example and cited reference. Examples: Comparison of the K-Means and MiniBatchKMeans clustering algorithms : Comparison of KMeans and MiniBatchKMeans Clustering text documents using k-means : Document clustering using sparse MiniBatchKMeans Online learning of a dictionary of parts of faces References: Web Scale K-Means clustering D. Sculley, Proceedings of the 19th international conference on World wide web (2010) \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.3. Affinity Propagation  AffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given. Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference , which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages. The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order , where is the number of samples and is the number of iterations until convergence. Further, the memory complexity is of the order if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets. Examples: Demo of affinity propagation clustering algorithm : Affinity Propagation on a synthetic 2D datasets with 3 classes. Visualizing the stock market structure Affinity Propagation on Financial time series to find groups of companies Algorithm description: The messages sent between points belong to one of two categories. The first is the responsibility , which is the accumulated evidence that sample should be the exemplar for sample . The second is the availability which is the accumulated evidence that sample should choose sample to be its exemplar, and considers the values for all other samples that should be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves. More formally, the responsibility of a sample to be the exemplar of sample is given by: \\\\[r(i, k) \\\\leftarrow s(i, k) - max [ a(i, k\\') + s(i, k\\') \\\\forall k\\' \\\\neq k ]\\\\] Where is the similarity between samples and . The availability of sample to be the exemplar of sample is given by: \\\\[a(i, k) \\\\leftarrow min [0, r(k, k) + \\\\sum_{i\\'~s.t.~i\\' \\\\notin \\\\{i, k\\\\}}{r(i\\', k)}]\\\\] To begin with, all values for and are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor is introduced to iteration process: \\\\[r_{t+1}(i, k)  \\\\lambda\\\\cdot r_{t}(i, k) + (1-\\\\lambda)\\\\cdot r_{t+1}(i, k)\\\\] \\\\[a_{t+1}(i, k)  \\\\lambda\\\\cdot a_{t}(i, k) + (1-\\\\lambda)\\\\cdot a_{t+1}(i, k)\\\\] where indicates the iteration times. \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.4. Mean Shift  MeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids. Given a candidate centroid for iteration , the candidate is updated according to the following equation: \\\\[x_i^{t+1}  m(x_i^t)\\\\] Where is the neighborhood of samples within a given distance around and is the mean shift vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. This is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood: \\\\[m(x_i)  \\\\frac{\\\\sum_{x_j \\\\in N(x_i)}K(x_j - x_i)x_j}{\\\\sum_{x_j \\\\in N(x_i)}K(x_j - x_i)}\\\\] The algorithm automatically sets the number of clusters, instead of relying on a parameter , which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided function, which is called if the bandwidth is not set. The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small. Labelling a new sample is performed by finding the nearest centroid for a given sample. Examples: A demo of the mean-shift clustering algorithm : Mean Shift clustering on a synthetic 2D datasets with 3 classes. References: Mean shift: A robust approach toward feature space analysis. D. Comaniciu and P. Meer, IEEE Transactions on Pattern Analysis and Machine Intelligence (2002) \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.5. Spectral clustering  SpectralClustering performs a low-dimension embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space. It is especially computationally efficient if the affinity matrix is sparse and the solver is used for the eigenvalue problem (Note, the solver requires that the pyamg module is installed.) The present version of SpectralClustering requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters. For two clusters, SpectralClustering solves a convex relaxation of the normalised cuts problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image. Warning Transforming distance to well-behaved similarities Note that if the values of your similarity matrix are not well distributed, e.g. with negative values or with a distance matrix rather than a similarity, the spectral problem will be singular and the problem not solvable. In which case it is advised to apply a transformation to the entries of the matrix. For instance, in the case of a signed distance matrix, is common to apply a heat kernel: See the examples for such an application. Examples: Spectral clustering for image segmentation : Segmenting objects from a noisy background using spectral clustering. Segmenting the picture of greek coins in regions : Spectral clustering to split the image of coins in regions. 2.3.5.1. Different label assignment strategies  Different label assignment strategies can be used, corresponding to the parameter of SpectralClustering . strategy can match finer details, but can be unstable. In particular, unless you control the , it may not be reproducible from run-to-run, as it depends on random initialization. The alternative strategy is 100% reproducible, but tends to create parcels of fairly even and geometrical shape. 2.3.5.2. Spectral Clustering Graphs  Spectral Clustering can also be used to partition graphs via their spectral embeddings. In this case, the affinity matrix is the adjacency matrix of the graph, and SpectralClustering is initialized with : References: A Tutorial on Spectral Clustering Ulrike von Luxburg, 2007 Normalized cuts and image segmentation Jianbo Shi, Jitendra Malik, 2000 A Random Walks View of Spectral Segmentation Marina Meila, Jianbo Shi, 2001 On Spectral Clustering: Analysis and an algorithm Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001 Preconditioned Spectral Clustering for Stochastic Block Partition Streaming Graph Challenge David Zhuzhunashvili, Andrew Knyazev \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.6. Hierarchical clustering  Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the Wikipedia page for more details. The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy: Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach. Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters. Average linkage minimizes the average of the distances between all observations of pairs of clusters. Single linkage minimizes the distance between the closest observations of pairs of clusters. AgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges. FeatureAgglomeration The FeatureAgglomeration uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see Unsupervised dimensionality reduction . 2.3.6.1. Different linkage type: Ward, complete, average, and single linkage  AgglomerativeClustering supports Ward, single, average, and complete linkage strategies. Agglomerative cluster has a rich get richer behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data. Examples: Various Agglomerative Clustering on a 2D embedding of digits : exploration of the different linkage strategies in a real dataset. 2.3.6.2. Visualization of cluster hierarchy  Its possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes. 2.3.6.3. Adding connectivity constraints  An interesting aspect of AgglomerativeClustering is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll. These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high. The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using sklearn.neighbors.kneighbors_graph to restrict merging to nearest neighbors as in this example , or using sklearn.feature_extraction.image.grid_to_graph to enable only merging of neighboring pixels on an image, as in the coin example. Examples: A demo of structured Ward hierarchical clustering on an image of coins : Ward clustering to split the image of coins in regions. Hierarchical clustering: structured vs unstructured ward : Example of Ward algorithm on a swiss-roll, comparison of structured approaches versus unstructured approaches. Feature agglomeration vs. univariate selection : Example of dimensionality reduction with feature agglomeration based on Ward hierarchical clustering. Agglomerative clustering with and without structure Warning Connectivity constraints with single, average and complete linkage Connectivity constraints and single, complete or average linkage can enhance the rich getting richer aspect of agglomerative clustering, particularly so if they are built with sklearn.neighbors.kneighbors_graph . In the limit of a small number of clusters, they tend to give a few macroscopically occupied clusters and almost empty ones. (see the discussion in Agglomerative clustering with and without structure ). Single linkage is the most brittle linkage option with regard to this issue. 2.3.6.4. Varying the metric  Single, average and complete linkage can be used with a variety of distances (or affinities), in particular Euclidean distance ( l2 ), Manhattan distance (or Cityblock, or l1 ), cosine distance, or any precomputed affinity matrix. l1 distance is often good for sparse features, or sparse noise: i.e. many of the features are zero, as in text mining using occurrences of rare words. cosine distance is interesting because it is invariant to global scalings of the signal. The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class. Examples: Agglomerative clustering with different metrics \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.7. DBSCAN  The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples , which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, and , which define formally what we mean when we say dense . Higher or lower indicate higher density necessary to form a cluster. More formally, we define a core sample as being a sample in the dataset such that there exist other samples within a distance of , which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least in distance from any core sample, is considered an outlier by the algorithm. While the parameter primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as for noise). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below). In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below. Examples: Demo of DBSCAN clustering algorithm Implementation The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering. The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see NearestNeighbors . Memory consumption for large sample sizes This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume n^2 floats. A couple of mechanisms for getting around this are: Use OPTICS clustering in conjunction with the method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n). A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with . See sklearn.neighbors.NearestNeighbors.radius_neighbors_graph . The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a when fitting DBSCAN. References: A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226231. 1996 DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19. \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.8. OPTICS  The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a distance, and a spot within the cluster attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for , then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given value using the method. Setting to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points. The reachability distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining reachability distances and data set produces a reachability plot , where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. Cutting the reachability plot at a single value produces DBSCAN like results; all points above the cut are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter . There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster. Examples: Demo of OPTICS clustering algorithm Comparison with DBSCAN The results from OPTICS method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise. Note that for any single value of , DBSCAN will tend to have a shorter run time than OPTICS; however, for repeated runs at varying values, a single run of OPTICS may require less cumulative runtime than DBSCAN. It is also important to note that OPTICS output is close to DBSCANs only if and are close. Computational Complexity Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the keyword. For large datasets, similar (but not identical) results can be obtained via HDBSCAN . The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain n (as opposed to n^2 ) memory scaling; however, tuning of the parameter will likely need to be used to give a solution in a reasonable amount of wall time. References: OPTICS: ordering points to identify the clustering structure. Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jrg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999. \"\\n',\n",
       " '\"sklearn_2_3_clustering 2.3. Clustering modules/clustering.html  2.3.9. Birch  The Birch builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children. The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes: Number of samples in a subcluster. Linear Sum - A n-dimensional vector holding the sum of all samples Squared Sum - Sum of the squared L2 norm of all samples. Centroids - To avoid recalculation linear sum / n_samples. Squared norm of the centroids. The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters. This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by . If is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster. Algorithm description: A new sample is inserted into the root of the CF Tree which is a CF Node. It is then merged with the subcluster of the root, that has the smallest radius after merging, constrained by the threshold and branching factor conditions. If the subcluster has any child node, then this is done repeatedly till it reaches a leaf. After finding the nearest subcluster in the leaf, the properties of this subcluster and the parent subclusters are recursively updated. If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters. If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root. Birch or MiniBatchKMeans? Birch does not scale very well to high dimensional data. As a rule of thumb if is greater than twenty, it is generally better to use MiniBatchKMeans. If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans. How to use partial_fit? To avoid the computation of global clustering, for every call of the user is advised To set initially Train all data by multiple calls to partial_fit. Set to a required value using . Call finally with no arguments, i.e. which performs the global clustering. References: Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm https://code.google.com/archive/p/jbirch \"\\n',\n",
       " '\"sklearn_2_4_biclustering 2.4. Biclustering modules/biclustering.html  2.4.1. Spectral Co-Clustering  The SpectralCoclustering algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal: Note The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs. 2.4.1.1. Mathematical formulation  An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix has shape , the Laplacian matrix for the corresponding bipartite graph has shape . However, in this case it is possible to work directly with , which is smaller and more efficient. The input matrix is preprocessed as follows: \\\\[A_n  R^{-1/2} A C^{-1/2}\\\\] Where is the diagonal matrix with entry equal to and is the diagonal matrix with entry equal to . The singular value decomposition, , provides the partitions of the rows and columns of . A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions. The singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix : \\\\[\\\\begin{split}Z  \\\\begin{bmatrix} R^{-1/2} U \\\\\\\\\\\\\\\\ C^{-1/2} V \\\\end{bmatrix}\\\\end{split}\\\\] where the columns of are , and similarly for . Then the rows of are clustered using k-means . The first labels provide the row partitioning, and the remaining labels provide the column partitioning. Examples: A demo of the Spectral Co-Clustering algorithm : A simple example showing how to generate a data matrix with biclusters and apply this method to it. Biclustering documents with the Spectral Co-clustering algorithm : An example of finding biclusters in the twenty newsgroup dataset. References: Dhillon, Inderjit S, 2001. Co-clustering documents and words using bipartite spectral graph partitioning . \"\\n',\n",
       " '\"sklearn_2_4_biclustering 2.4. Biclustering modules/biclustering.html  2.4.2. Spectral Biclustering  The SpectralBiclustering algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters. The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix. 2.4.2.1. Mathematical formulation  The input matrix is first normalized to make the checkerboard pattern more obvious. There are three possible methods: Independent row and column normalization , as in Spectral Co-Clustering. This method makes the rows sum to a constant and the columns sum to a different constant. Bistochastization : repeated row and column normalization until convergence. This method makes both rows and columns sum to the same constant. Log normalization : the log of the data matrix is computed: . Then the column mean , row mean , and overall mean of are computed. The final matrix is computed according to the formula \\\\[K_{ij}  L_{ij} - \\\\overline{L_{i \\\\cdot}} - \\\\overline{L_{\\\\cdot j}} + \\\\overline{L_{\\\\cdot \\\\cdot}}\\\\] After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm. If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, and . are discarded. From now on, the first singular vectors refers to and except in the case of log normalization. Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered. For instance, if singular vectors were calculated, the best are found as described, where . Let be the matrix with columns the best left singular vectors, and similarly for the right. To partition the rows, the rows of are projected to a dimensional space: . Treating the rows of this matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to and clustering this matrix yields the column labels. Examples: A demo of the Spectral Biclustering algorithm : a simple example showing how to generate a checkerboard matrix and bicluster it. References: Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray data: coclustering genes and conditions . \"\\n',\n",
       " '\"sklearn_2_4_biclustering 2.4. Biclustering modules/biclustering.html  2.4.3. Biclustering evaluation  There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known. To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score. To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented: \\\\[J(A, B)  \\\\frac{|A \\\\cap B|}{|A| + |B| - |A \\\\cap B|}\\\\] where and are biclusters, is the number of elements in their intersection. The Jaccard index achieves its minimum of 0 when the biclusters to not overlap at all and its maximum of 1 when they are identical. Several methods have been developed to compare two sets of biclusters. For now, only consensus_score (Hochreiter et. al., 2010) is available: Compute bicluster similarities for pairs of biclusters, one in each set, using the Jaccard index or a similar measure. Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed using the Hungarian algorithm. The final sum of similarities is divided by the size of the larger set. The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical. References: Hochreiter, Bodenhofer, et. al., 2010. FABIA: factor analysis for bicluster acquisition . \"\\n',\n",
       " '\"sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems 2.5. Decomposing signals in components (matrix factorization problems) modules/decomposition.html  2.5.1. Principal component analysis (PCA)  2.5.1.1. Exact PCA and probabilistic interpretation  PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns components in its method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm. Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance: The PCA object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a score method that can be used in cross-validation: Examples: Comparison of LDA and PCA 2D projection of Iris dataset Model selection with Probabilistic PCA and Factor Analysis (FA) 2.5.1.2. Incremental PCA  The PCA object is very useful, but has certain limitations for large datasets. The biggest limitation is that PCA only supports batch processing, which means all of the data to be processed must fit in main memory. The IncrementalPCA object uses a different form of processing and allows for partial computations which almost exactly match the results of PCA while processing the data in a minibatch fashion. IncrementalPCA makes it possible to implement out-of-core Principal Component Analysis either by: Using its method on chunks of data fetched sequentially from the local hard drive or a network database. Calling its fit method on a sparse matrix or a memory mapped file using . IncrementalPCA only stores estimates of component and noise variances, in order update incrementally. This is why memory usage depends on the number of samples per batch, rather than the number of samples to be processed in the dataset. As in PCA , IncrementalPCA centers but does not scale the input data for each feature before applying the SVD. Examples: Incremental PCA 2.5.1.3. PCA using randomized SVD  It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values. For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time. The class PCA used with the optional parameter is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform. For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size and , the computation time is less than 1s: If we note and , the time complexity of the randomized PCA is instead of for the exact method implemented in PCA . The memory footprint of randomized PCA is also proportional to instead of for the exact method. Note: the implementation of in PCA with is not the exact inverse transform of even when (default). Examples: Faces recognition example using eigenfaces and SVMs Faces dataset decompositions References: Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 2.5.1.4. Kernel PCA  KernelPCA is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels (see Pairwise metrics, Affinities and Kernels ). It has many applications including denoising, compression and structured prediction (kernel dependency estimation). KernelPCA supports both and . Examples: Kernel PCA 2.5.1.5. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)  SparsePCA is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the data. Mini-batch sparse PCA ( MiniBatchSparsePCA ) is a variant of SparsePCA that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations. Principal component analysis ( PCA ) has the disadvantage that the components extracted by this method have exclusively dense expressions, i.e. they have non-zero coefficients when expressed as linear combinations of the original variables. This can make interpretation difficult. In many cases, the real underlying components can be more naturally imagined as sparse vectors; for example in face recognition, components might naturally map to parts of faces. Sparse principal components yields a more parsimonious, interpretable representation, clearly emphasizing which of the original features contribute to the differences between samples. The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector , and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see [Jen09] for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below. Note that there are many different formulations for the Sparse PCA problem. The one implemented here is based on [Mrl09] . The optimization problem solved is a PCA problem (dictionary learning) with an penalty on the components: \\\\[\\\\begin{split}(U^*, V^*)  \\\\underset{U, V}{\\\\operatorname{arg\\\\,min\\\\,}} & \\\\frac{1}{2} ||X-UV||_2^2+\\\\alpha||V||_1 \\\\\\\\ \\\\text{subject to } & ||U_k||_2  1 \\\\text{ for all } 0 \\\\leq k < n_{components}\\\\end{split}\\\\] The sparsity-inducing norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter . Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero. Note While in the spirit of an online algorithm, the class MiniBatchSparsePCA does not implement because the algorithm is online along the features direction, not the samples direction. Examples: Faces dataset decompositions References: Mrl09 Online Dictionary Learning for Sparse Coding J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009 Jen09 Structured Sparse Principal Component Analysis R. Jenatton, G. Obozinski, F. Bach, 2009 \"\\n',\n",
       " '\"sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems 2.5. Decomposing signals in components (matrix factorization problems) modules/decomposition.html  2.5.2. Truncated singular value decomposition and latent semantic analysis  TruncatedSVD implements a variant of singular value decomposition (SVD) that only computes the largest singular values, where is a user-specified parameter. When truncated SVD is applied to term-document matrices (as returned by CountVectorizer or TfidfVectorizer ), this transformation is known as latent semantic analysis (LSA), because it transforms such matrices to a semantic space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity. Note LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes. Mathematically, truncated SVD applied to training samples produces a low-rank approximation : \\\\[X \\\\approx X_k  U_k \\\\Sigma_k V_k^\\\\top\\\\] After this operation, is the transformed training set with features (called in the API). To also transform a test set , we multiply it with : \\\\[X\\'  X V_k\\\\] Note Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix so that it has shape  . We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same. TruncatedSVD is very similar to PCA , but differs in that the matrix does not need to be centered. When the columnwise (per-feature) means of are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA. In practical terms, this means that the TruncatedSVD transformer accepts matrices without the need to densify them, as densifying may fill up memory even for medium-sized document collections. While the TruncatedSVD transformer works with any feature matrix, using it on tfidf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on ( ) to bring the feature values closer to a Gaussian distribution, compensating for LSAs erroneous assumptions about textual data. Examples: Clustering text documents using k-means References: Christopher D. Manning, Prabhakar Raghavan and Hinrich Schtze (2008), Introduction to Information Retrieval , Cambridge University Press, chapter 18: Matrix decompositions & latent semantic indexing \"\\n',\n",
       " '\"sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems 2.5. Decomposing signals in components (matrix factorization problems) modules/decomposition.html  2.5.3. Dictionary Learning  2.5.3.1. Sparse coding with a precomputed dictionary  The SparseCoder object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the initialization parameter: Orthogonal matching pursuit ( Orthogonal Matching Pursuit (OMP) ) Least-angle regression ( Least Angle Regression ) Lasso computed by least-angle regression Lasso using coordinate descent ( Lasso ) Thresholding Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction. The dictionary learning objects offer, via the parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading. The split code for a single sample has length and is constructed using the following rule: First, the regular code of length is computed. Then, the first entries of the are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative. Examples: Sparse coding with a precomputed dictionary 2.5.3.2. Generic dictionary learning  Dictionary learning ( DictionaryLearning ) is a matrix factorization problem that amounts to finding a (usually overcomplete) dictionary that will perform well at sparsely encoding the fitted data. Representing data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the mammalian primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for supervised recognition tasks. Dictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to multiple Lasso problems, considering the dictionary fixed, and then updating the dictionary to best fit the sparse code. \\\\[\\\\begin{split}(U^*, V^*)  \\\\underset{U, V}{\\\\operatorname{arg\\\\,min\\\\,}} & \\\\frac{1}{2} ||X-UV||_2^2+\\\\alpha||U||_1 \\\\\\\\ \\\\text{subject to } & ||V_k||_2  1 \\\\text{ for all } 0 \\\\leq k < n_{\\\\mathrm{atoms}}\\\\end{split}\\\\] After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see Sparse coding with a precomputed dictionary ). It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros. The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like. Examples: Image denoising using dictionary learning References: Online dictionary learning for sparse coding J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009 2.5.3.3. Mini-batch dictionary learning  MiniBatchDictionaryLearning implements a faster, but less accurate version of the dictionary learning algorithm that is better suited for large datasets. By default, MiniBatchDictionaryLearning divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition. The estimator also implements , which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory. Clustering for dictionary learning Note that when using dictionary learning to extract a representation (e.g. for sparse coding) clustering can be a good proxy to learn the dictionary. For instance the MiniBatchKMeans estimator is computationally efficient and implements on-line learning with a method. Example: Online learning of a dictionary of parts of faces \"\\n',\n",
       " '\"sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems 2.5. Decomposing signals in components (matrix factorization problems) modules/decomposition.html  2.5.4. Factor Analysis  In unsupervised learning we only have a dataset . How can this dataset be described mathematically? A very simple model for is \\\\[x_i  W h_i + \\\\mu + \\\\epsilon\\\\] The vector is called latent because it is unobserved. is considered a noise term distributed according to a Gaussian with mean 0 and covariance (i.e. ), is some arbitrary offset vector. Such a model is called generative as it describes how is generated from . If we use all the s as columns to form a matrix and all the s as columns of a matrix then we can write (with suitably defined and ): \\\\[\\\\mathbf{X}  W \\\\mathbf{H} + \\\\mathbf{M} + \\\\mathbf{E}\\\\] In other words, we decomposed matrix . If is given, the above equation automatically implies the following probabilistic interpretation: \\\\[p(x_i|h_i)  \\\\mathcal{N}(Wh_i + \\\\mu, \\\\Psi)\\\\] For a complete probabilistic model we also need a prior distribution for the latent variable . The most straightforward assumption (based on the nice properties of the Gaussian distribution) is . This yields a Gaussian as the marginal distribution of : \\\\[p(x)  \\\\mathcal{N}(\\\\mu, WW^T + \\\\Psi)\\\\] Now, without any further assumptions the idea of having a latent variable would be superfluous  can be completely modelled with a mean and a covariance. We need to impose some more specific structure on one of these two parameters. A simple additional assumption regards the structure of the error covariance : : This assumption leads to the probabilistic model of PCA . : This model is called FactorAnalysis , a classical statistical model. The matrix W is sometimes called the factor loading matrix. Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. FastICA ) if non-Gaussian priors on the latent variables are assumed. Factor analysis can produce similar components (the columns of its loading matrix) to PCA . However, one can not make any general statements about these components (e.g. whether they are orthogonal): The main advantage for Factor Analysis over PCA is that it can model the variance in every direction of the input space independently (heteroscedastic noise): This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise: Examples: Model selection with Probabilistic PCA and Factor Analysis (FA) \"\\n',\n",
       " '\"sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems 2.5. Decomposing signals in components (matrix factorization problems) modules/decomposition.html  2.5.5. Independent component analysis (ICA)  Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the Fast ICA algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants. It is classically used to separate mixed signals (a problem known as blind source separation ), as in the example below: ICA can also be used as yet another non linear decomposition that finds components with some sparsity: Examples: Blind source separation using FastICA FastICA on 2D point clouds Faces dataset decompositions \"\\n',\n",
       " '\"sklearn_2_5_decomposing_signals_in_components_matrix_factorization_problems 2.5. Decomposing signals in components (matrix factorization problems) modules/decomposition.html  2.5.7. Latent Dirichlet Allocation (LDA)  Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents. The graphical model of LDA is a three-level generative model: Note on notations presented in the graphical model above, which can be found in Hoffman et al. (2013): The corpus is a collection of documents. A document is a sequence of words. There are topics in the corpus. The boxes represent repeated sampling. In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure. When modeling text corpora, the model assumes the following generative process for a corpus with documents and topics, with corresponding to n_components in the API: For each topic , draw . This provides a distribution over the words, i.e. the probability of a word appearing in topic . corresponds to topic_word_prior . For each document , draw the topic proportions . corresponds to doc_topic_prior . For each word in document : Draw the topic assignment Draw the observed word For parameter estimation, the posterior distribution is: \\\\[p(z, \\\\theta, \\\\beta |w, \\\\alpha, \\\\eta)  \\\\frac{p(z, \\\\theta, \\\\beta|\\\\alpha, \\\\eta)}{p(w|\\\\alpha, \\\\eta)}\\\\] Since the posterior is intractable, variational Bayesian method uses a simpler distribution to approximate it, and those variational parameters , , are optimized to maximize the Evidence Lower Bound (ELBO): \\\\[\\\\log\\\\: P(w | \\\\alpha, \\\\eta) \\\\geq L(w,\\\\phi,\\\\gamma,\\\\lambda) \\\\overset{\\\\triangle}{} E_{q}[\\\\log\\\\:p(w,z,\\\\theta,\\\\beta|\\\\alpha,\\\\eta)] - E_{q}[\\\\log\\\\:q(z, \\\\theta, \\\\beta)]\\\\] Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between and the true posterior . LatentDirichletAllocation implements the online variational Bayes algorithm and supports both online and batch update methods. While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points. Note Although the online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting. When LatentDirichletAllocation is applied on a document-term matrix, the matrix will be decomposed into a topic-term matrix and a document-topic matrix. While topic-term matrix is stored as components_ in the model, document-topic matrix can be calculated from method. LatentDirichletAllocation also implements method. This is used when data can be fetched sequentially. Examples: Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation References: Latent Dirichlet Allocation D. Blei, A. Ng, M. Jordan, 2003 Online Learning for Latent Dirichlet Allocation M. Hoffman, D. Blei, F. Bach, 2010 Stochastic Variational Inference M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013 See also Dimensionality reduction for dimensionality reduction with Neighborhood Components Analysis. \"\\n',\n",
       " '\"sklearn_2_6_covariance_estimation 2.6. Covariance estimation modules/covariance.html  2.6.1. Empirical covariance  The covariance matrix of a data set is known to be well approximated by the classical maximum likelihood estimator (or empirical covariance), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding populations covariance matrix. The empirical covariance matrix of a sample can be computed using the empirical_covariance function of the package, or by fitting an EmpiricalCovariance object to the data sample with the EmpiricalCovariance.fit method. Be careful that results depend on whether the data are centered, so one may want to use the parameter accurately. More precisely, if , then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and should be used. Examples: See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to fit an EmpiricalCovariance object to data. \"\\n',\n",
       " '\"sklearn_2_6_covariance_estimation 2.6. Covariance estimation modules/covariance.html  2.6.2. Shrunk Covariance  2.6.2.1. Basic shrinkage  Despite being an unbiased estimator of the covariance matrix, the Maximum Likelihood Estimator is not a good estimator of the eigenvalues of the covariance matrix, so the precision matrix obtained from its inversion is not accurate. Sometimes, it even occurs that the empirical covariance matrix cannot be inverted for numerical reasons. To avoid such an inversion problem, a transformation of the empirical covariance matrix has been introduced: the . In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the shrunk_covariance method. Also, a shrunk estimator of the covariance can be fitted to data with a ShrunkCovariance object and its ShrunkCovariance.fit method. Again, results depend on whether the data are centered, so one may want to use the parameter accurately. Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : . Choosing the amount of shrinkage, amounts to setting a bias/variance trade-off, and is discussed below. Examples: See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to fit a ShrunkCovariance object to data. 2.6.2.2. Ledoit-Wolf shrinkage  In their 2004 paper 1 , O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient that minimizes the Mean Squared Error between the estimated and the real covariance matrix. The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the ledoit_wolf function of the sklearn.covariance package, or it can be otherwise obtained by fitting a LedoitWolf object to the same sample. Note Case when population covariance matrix is isotropic It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this. This, however, is not the case in the Ledoit-Wolf procedure when the population covariance happens to be a multiple of the identity matrix. In this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of samples increases. This indicates that the optimal estimate of the covariance matrix in the Ledoit-Wolf sense is multiple of the identity. Since the population covariance is already a multiple of the identity matrix, the Ledoit-Wolf solution is indeed a reasonable estimate. Examples: See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to fit a LedoitWolf object to data and for visualizing the performances of the Ledoit-Wolf estimator in terms of likelihood. References: 1 O. Ledoit and M. Wolf, A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411. 2.6.2.3. Oracle Approximating Shrinkage  Under the assumption that the data are Gaussian distributed, Chen et al. 2 derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolfs formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance. The OAS estimator of the covariance matrix can be computed on a sample with the oas function of the sklearn.covariance package, or it can be otherwise obtained by fitting an OAS object to the same sample. Bias-variance trade-off when setting the shrinkage: comparing the choices of Ledoit-Wolf and OAS estimators  References: 2 Chen et al., Shrinkage Algorithms for MMSE Covariance Estimation, IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010. Examples: See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to fit an OAS object to data. See Ledoit-Wolf vs OAS estimation to visualize the Mean Squared Error difference between a LedoitWolf and an OAS estimator of the covariance. \"\\n',\n",
       " '\"sklearn_2_6_covariance_estimation 2.6. Covariance estimation modules/covariance.html  2.6.3. Sparse inverse covariance  The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as covariance selection . In the small-samples situation, in which is on the order of or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure. The GraphicalLasso estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its parameter, the more sparse the precision matrix. The corresponding GraphicalLassoCV object uses cross-validation to automatically set the parameter. A comparison of maximum likelihood, shrinkage and sparse estimates of the covariance and precision matrix in the very small samples settings.  Note Structure recovery Recovering a graphical structure from correlations in the data is a challenging thing. If you are interested in such recovery keep in mind that: Recovery is easier from a correlation matrix than a covariance matrix: standardize your observations before running GraphicalLasso If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections. If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it. Even if you are in favorable recovery conditions, the alpha parameter chosen by cross-validation (e.g. using the GraphicalLassoCV object) will lead to selecting too many edges. However, the relevant edges will have heavier weights than the irrelevant ones. The mathematical formulation is the following: \\\\[\\\\hat{K}  \\\\mathrm{argmin}_K \\\\big( \\\\mathrm{tr} S K - \\\\mathrm{log} \\\\mathrm{det} K + \\\\alpha \\\\|K\\\\|_1 \\\\big)\\\\] Where is the precision matrix to be estimated, and is the sample covariance matrix. is the sum of the absolute values of off-diagonal coefficients of . The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R package. Examples: Sparse inverse covariance estimation : example on synthetic data showing some recovery of a structure, and comparing to other covariance estimators. Visualizing the stock market structure : example on real stock market data, finding which symbols are most linked. References: Friedman et al, Sparse inverse covariance estimation with the graphical lasso , Biostatistics 9, pp 432, 2008 \"\\n',\n",
       " '\"sklearn_2_6_covariance_estimation 2.6. Covariance estimation modules/covariance.html  2.6.4. Robust Covariance Estimation  Real data sets are often subject to measurement or recording errors. Regular but uncommon observations may also appear for a variety of reasons. Observations which are very uncommon are called outliers. The empirical covariance estimator and the shrunk covariance estimators presented above are very sensitive to the presence of outliers in the data. Therefore, one should use robust covariance estimators to estimate the covariance of its real data sets. Alternatively, robust covariance estimators can be used to perform outlier detection and discard/downweight some observations according to further processing of the data. The package implements a robust estimator of covariance, the Minimum Covariance Determinant 3 . 2.6.4.1. Minimum Covariance Determinant  The Minimum Covariance Determinant estimator is a robust estimator of a data sets covariance introduced by P.J. Rousseeuw in 3 . The idea is to find a given proportion (h) of good observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (consistency step). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (reweighting step). Rousseeuw and Van Driessen 4 developed the FastMCD algorithm in order to compute the Minimum Covariance Determinant. This algorithm is used in scikit-learn when fitting an MCD object to data. The FastMCD algorithm also computes a robust estimate of the data set location at the same time. Raw estimates can be accessed as and attributes of a MinCovDet robust covariance estimator object. References: 3 ( 1 , 2 ) P. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871, 1984. 4 A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS. Examples: See Robust vs Empirical covariance estimate for an example on how to fit a MinCovDet object to data and see how the estimate remains accurate despite the presence of outliers. See Robust covariance estimation and Mahalanobis distances relevance to visualize the difference between EmpiricalCovariance and MinCovDet covariance estimators in terms of Mahalanobis distance (so we get a better estimate of the precision matrix too). Influence of outliers on location and covariance estimates Separating inliers from outliers using a Mahalanobis distance \"\\n',\n",
       " '\"sklearn_2_7_novelty_and_outlier_detection 2.7. Novelty and Outlier Detection modules/outlier_detection.html  2.7.1. Overview of outlier detection methods  A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection. ensemble.IsolationForest and neighbors.LocalOutlierFactor perform reasonably well on the data sets considered here. The svm.OneClassSVM is known to be sensitive to outliers and thus does not perform very well for outlier detection. Finally, covariance.EllipticEnvelope assumes the data is Gaussian and learns an ellipse. For more details on the different estimators refer to the example Comparing anomaly detection algorithms for outlier detection on toy datasets and the sections hereunder. Examples: See Comparing anomaly detection algorithms for outlier detection on toy datasets for a comparison of the svm.OneClassSVM , the ensemble.IsolationForest , the neighbors.LocalOutlierFactor and covariance.EllipticEnvelope . \"\\n',\n",
       " '\"sklearn_2_7_novelty_and_outlier_detection 2.7. Novelty and Outlier Detection modules/outlier_detection.html  2.7.2. Novelty Detection  Consider a data set of observations from the same distribution described by features. Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods. In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding -dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment. The One-Class SVM has been introduced by Schlkopf et al. for that purpose and implemented in the Support Vector Machines module in the svm.OneClassSVM object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier. References: Estimating the support of a high-dimensional distribution Schlkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471. Examples: See One-class SVM with non-linear kernel (RBF) for visualizing the frontier learned around some data by a svm.OneClassSVM object. Species distribution modeling \"\\n',\n",
       " '\"sklearn_2_7_novelty_and_outlier_detection 2.7. Novelty and Outlier Detection modules/outlier_detection.html  2.7.3. Outlier Detection  Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called outliers . Yet, in the case of outlier detection, we dont have a clean data set representing the population of regular observations that can be used to train any tool. 2.7.3.1. Fitting an elliptic envelope  One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the shape of the data, and can define outlying observations as observations which stand far enough from the fit shape. The scikit-learn provides an object covariance.EllipticEnvelope that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode. For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below. Examples: See Robust covariance estimation and Mahalanobis distances relevance for an illustration of the difference between using a standard ( covariance.EmpiricalCovariance ) or a robust estimate ( covariance.MinCovDet ) of location and covariance to assess the degree of outlyingness of an observation. References: Rousseeuw, P.J., Van Driessen, K. A fast algorithm for the minimum covariance determinant estimator Technometrics 41(3), 212 (1999) 2.7.3.2. Isolation Forest  One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The ensemble.IsolationForest isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies. The implementation of ensemble.IsolationForest is based on an ensemble of tree.ExtraTreeRegressor . Following Isolation Forest original paper, the maximum depth of each tree is set to where is the number of samples used to build the tree (see (Liu et al., 2008) for more details). This algorithm is illustrated below. The ensemble.IsolationForest supports which allows you to add more trees to an already fitted model: Examples: See IsolationForest example for an illustration of the use of IsolationForest. See Comparing anomaly detection algorithms for outlier detection on toy datasets for a comparison of ensemble.IsolationForest with neighbors.LocalOutlierFactor , svm.OneClassSVM (tuned to perform like an outlier detection method) and a covariance-based outlier detection with covariance.EllipticEnvelope . References: Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. Isolation forest. Data Mining, 2008. ICDM08. Eighth IEEE International Conference on. 2.7.3.3. Local Outlier Factor  Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm. The neighbors.LocalOutlierFactor (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors. In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density. The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors35 in the example below). The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood. When applying LOF for outlier detection, there are no , and methods but only a method. The scores of abnormality of the training samples are accessible through the attribute. Note that , and can be used on new unseen data when LOF is applied for novelty detection, i.e. when the parameter is set to . See Novelty detection with Local Outlier Factor . This strategy is illustrated below. Examples: See Outlier detection with Local Outlier Factor (LOF) for an illustration of the use of neighbors.LocalOutlierFactor . See Comparing anomaly detection algorithms for outlier detection on toy datasets for a comparison with other anomaly detection methods. References: Breunig, Kriegel, Ng, and Sander (2000) LOF: identifying density-based local outliers. Proc. ACM SIGMOD \"\\n',\n",
       " '\"sklearn_2_7_novelty_and_outlier_detection 2.7. Novelty and Outlier Detection modules/outlier_detection.html  2.7.4. Novelty detection with Local Outlier Factor  To use neighbors.LocalOutlierFactor for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the parameter set to before fitting the estimator: Note that is not available in this case. Warning Novelty detection with Local Outlier Factor` When is set to be aware that you must only use , and on new unseen data and not on the training samples as this would lead to wrong results. The scores of abnormality of the training samples are always accessible through the attribute. Novelty detection with Local Outlier Factor is illustrated below. \"\\n',\n",
       " '\"sklearn_2_8_density_estimation 2.8. Density Estimation modules/density.html  2.8.1. Density Estimation: Histograms  A histogram is a simple visualization of data where bins are defined, and the number of data points within each bin is tallied. An example of a histogram can be seen in the upper-left panel of the following figure: A major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the resulting visualization. Consider the upper-right panel of the above figure. It shows a histogram over the same data, with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different interpretations of the data. Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data. This visualization is an example of a kernel density estimation , in this case with a top-hat kernel (i.e. a square block at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution of points. \"\\n',\n",
       " '\"sklearn_2_8_density_estimation 2.8. Density Estimation modules/density.html  2.8.2. Kernel Density Estimation  Kernel density estimation in scikit-learn is implemented in the sklearn.neighbors.KernelDensity estimator, which uses the Ball Tree or KD Tree for efficient queries (see Nearest Neighbors for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions. In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels: Its clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows: Here we have used , as seen above. Mathematically, a kernel is a positive function which is controlled by the bandwidth parameter . Given this kernel form, the density estimate at a point within a group of points is given by: \\\\[\\\\rho_K(y)  \\\\sum_{i1}^{N} K(y - x_i; h)\\\\] The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution. sklearn.neighbors.KernelDensity implements several common kernel forms, which are shown in the following figure: The form of these kernels is as follows: Gaussian kernel ( ) Tophat kernel ( ) if Epanechnikov kernel ( ) Exponential kernel ( ) Linear kernel ( ) if Cosine kernel ( ) if The kernel density estimator can be used with any of the valid distance metrics (see sklearn.neighbors.DistanceMetric for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the Haversine distance which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent: One other useful application of kernel density estimation is to learn a non-parametric generative model of a dataset in order to efficiently draw new samples from this generative model. Here is an example of using this process to create a new set of hand-written digits, using a Gaussian kernel learned on a PCA projection of the data: The new data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model. Examples: Simple 1D Kernel Density Estimation : computation of simple kernel density estimates in one dimension. Kernel Density Estimation : an example of using Kernel Density estimation to learn a generative model of the hand-written digits data, and drawing new samples from this model. Kernel Density Estimate of Species Distributions : an example of Kernel Density estimation using the Haversine distance metric to visualize geospatial data \"\\n',\n",
       " '\"sklearn_2_9_neural_network_models_unsupervised 2.9. Neural network models (unsupervised) modules/neural_networks_unsupervised.html  2.9.1. Restricted Boltzmann machines  Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classifier such as a linear SVM or a perceptron. The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides BernoulliRBM , which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on. The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used ( Stochastic Maximum Likelihood ) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation. The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training. Examples: Restricted Boltzmann Machine features for digit classification 2.9.1.1. Graphical model and parametrization  The graphical model of an RBM is a fully-connected bipartite graph. The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity. The energy function measures the quality of a joint assignment: \\\\[E(\\\\mathbf{v}, \\\\mathbf{h})  -\\\\sum_i \\\\sum_j w_{ij}v_ih_j - \\\\sum_i b_iv_i - \\\\sum_j c_jh_j\\\\] In the formula above, and are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy: \\\\[P(\\\\mathbf{v}, \\\\mathbf{h})  \\\\frac{e^{-E(\\\\mathbf{v}, \\\\mathbf{h})}}{Z}\\\\] The word restricted refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed: \\\\[\\\\begin{split}h_i \\\\bot h_j | \\\\mathbf{v} \\\\\\\\ v_i \\\\bot v_j | \\\\mathbf{h}\\\\end{split}\\\\] The bipartite structure allows for the use of efficient block Gibbs sampling for inference. 2.9.1.2. Bernoulli Restricted Boltzmann machines  In the BernoulliRBM , all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which arent. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values. The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives: \\\\[\\\\begin{split}P(v_i1|\\\\mathbf{h})  \\\\sigma(\\\\sum_j w_{ij}h_j + b_i) \\\\\\\\ P(h_i1|\\\\mathbf{v})  \\\\sigma(\\\\sum_i w_{ij}v_i + c_j)\\\\end{split}\\\\] where is the logistic sigmoid function: \\\\[\\\\sigma(x)  \\\\frac{1}{1 + e^{-x}}\\\\] 2.9.1.3. Stochastic Maximum Likelihood learning  The training algorithm implemented in BernoulliRBM is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood: \\\\[\\\\log P(v)  \\\\log \\\\sum_h e^{-E(v, h)} - \\\\log \\\\sum_{x, y} e^{-E(x, y)}\\\\] For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples. In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of and given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes. The Contrastive Divergence method suggests to stop the chain after a small number of iterations, , usually even 1. This method is fast and has low variance, but the samples are far from the model distribution. Persistent Contrastive Divergence addresses this. Instead of starting a new chain each time the gradient is needed, and performing only one Gibbs sampling step, in PCD we keep a number of chains (fantasy particles) that are updated Gibbs steps after each weight update. This allows the particles to explore the space more thoroughly. References: A fast learning algorithm for deep belief nets G. Hinton, S. Osindero, Y.-W. Teh, 2006 Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient T. Tieleman, 2008 \"\\n',\n",
       " '\"sklearn_3_1_cross-validation_evaluating_estimator_performance 3.1. Cross-validation: evaluating estimator performance modules/cross_validation.html  3.1.1. Computing cross-validated metrics  The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset. The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time): The mean score and the 95% confidence interval of the score estimate are hence given by: By default, the score computed at each CV iteration is the method of the estimator. It is possible to change this by using the scoring parameter: See The scoring parameter: defining model evaluation rules for details. In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal. When the argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin . It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance: Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example: Data transformation with held out data Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar data transformations similarly should be learnt from a training set and applied to held-out data for prediction: A Pipeline makes it easier to compose estimators, providing this behavior under cross-validation: See Pipelines and composite estimators . 3.1.1.1. The cross_validate function and multiple metric evaluation  The cross_validate function differs from cross_val_score in two ways: It allows specifying multiple metrics for evaluation. It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score. For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - And for multiple metric evaluation, the return value is a dict with the following keys - is set to by default to save computation time. To evaluate the scores on the training set as well you need to be set to . You may also retain the estimator fitted on each training set by setting . The multiple metrics can be specified either as a list, tuple or set of predefined scorer names: Or as a dict mapping scorer name to a predefined or custom scoring function: Here is an example of using a single metric: 3.1.1.2. Obtaining predictions by cross-validation  The function cross_val_predict has a similar interface to cross_val_score , but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised). Warning Note on inappropriate usage of cross_val_predict The result of cross_val_predict may be different from those obtained using cross_val_score as the elements are grouped in different ways. The function cross_val_score takes an average over cross-validation folds, whereas cross_val_predict simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, cross_val_predict is not an appropriate measure of generalisation error. The function cross_val_predict is appropriate for: Visualization of predictions obtained from different models. Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods. The available cross validation iterators are introduced in the following section. Examples Receiver Operating Characteristic (ROC) with cross validation , Recursive feature elimination with cross-validation , Parameter estimation using grid search with cross-validation , Sample pipeline for text feature extraction and evaluation , Plotting Cross-Validated Predictions , Nested versus non-nested cross-validation . \"\\n',\n",
       " '\"sklearn_3_1_cross-validation_evaluating_estimator_performance 3.1. Cross-validation: evaluating estimator performance modules/cross_validation.html  3.1.2. Cross validation iterators  The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies. 3.1.2.1. Cross-validation iterators for i.i.d. data  Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples. The following cross-validators can be used in such cases. NOTE While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it is safer to use a time-series aware cross-validation scheme . Similarly, if we know that the generative process has a group structure (samples collected from different subjects, experiments, measurement devices), it is safer to use group-wise cross-validation . 3.1.2.1.1. K-fold  KFold divides all the samples in groups of samples, called folds (if , this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using folds, and the fold left out is used for test. Example of 2-fold cross-validation on a dataset with 4 samples: Here is a visualization of the cross-validation behavior. Note that KFold is not affected by classes or groups. Each fold is constituted by two arrays: the first one is related to the training set , and the second one to the test set . Thus, one can create the training/test sets using numpy indexing: 3.1.2.1.2. Repeated K-Fold  RepeatedKFold repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition. Example of 2-fold K-Fold repeated 2 times: Similarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times with different randomization in each repetition. 3.1.2.1.3. Leave One Out (LOO)  LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for samples, we have different training sets and different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set: Potential users of LOO for model selection should weigh a few known caveats. When compared with -fold cross validation, one builds models from samples instead of models, where . Moreover, each is trained on samples rather than . In both ways, assuming is not too large and , LOO is more computationally expensive than -fold cross validation. In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since of the samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set. However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error. As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO. References: http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html ; T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning , Springer 2009 L. Breiman, P. Spector Submodel selection and evaluation in regression: The X-random case , International Statistical Review 1992; R. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection , Intl. Jnt. Conf. AI R. Bharat Rao, G. Fung, R. Rosales, On the Dangers of Cross-Validation. An Experimental Evaluation , SIAM 2008; G. James, D. Witten, T. Hastie, R Tibshirani, An Introduction to Statistical Learning , Springer 2013. 3.1.2.1.4. Leave P Out (LPO)  LeavePOut is very similar to LeaveOneOut as it creates all the possible training/test sets by removing samples from the complete set. For samples, this produces train-test pairs. Unlike LeaveOneOut and KFold , the test sets will overlap for . Example of Leave-2-Out on a dataset with 4 samples: 3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split  ShuffleSplit The ShuffleSplit iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets. It is possible to control the randomness for reproducibility of the results by explicitly seeding the pseudo random number generator. Here is a usage example: Here is a visualization of the cross-validation behavior. Note that ShuffleSplit is not affected by classes or groups. ShuffleSplit is thus a good alternative to KFold cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split. 3.1.2.2. Cross-validation iterators with stratification based on class labels.  Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in StratifiedKFold and StratifiedShuffleSplit to ensure that relative class frequencies is approximately preserved in each train and validation fold. 3.1.2.2.1. Stratified k-fold  StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with KFold . We can see that StratifiedKFold preserves the class ratios (approximately 1 / 10) in both train and test dataset. Here is a visualization of the cross-validation behavior. RepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times with different randomization in each repetition. 3.1.2.2.2. Stratified Shuffle Split  StratifiedShuffleSplit is a variation of ShuffleSplit , which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set. Here is a visualization of the cross-validation behavior. 3.1.2.3. Cross-validation iterators for grouped data.  The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples. Such a grouping of data is domain specific. An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier. In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold. The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the parameter. 3.1.2.3.1. Group k-fold  GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations. Imagine you have three subjects, each with an associated number from 1 to 3: Each subject is in a different testing fold, and the same subject is never in both testing and training. Notice that the folds do not have exactly the same size due to the imbalance in the data. Here is a visualization of the cross-validation behavior. 3.1.2.3.2. Leave One Group Out  LeaveOneGroupOut is a cross-validation scheme which holds out the samples according to a third-party provided array of integer groups. This group information can be used to encode arbitrary domain specific pre-defined cross-validation folds. Each training set is thus constituted by all the samples except the ones related to a specific group. For example, in the cases of multiple experiments, LeaveOneGroupOut can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one: Another common application is to use time information: for instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits. 3.1.2.3.3. Leave P Groups Out  LeavePGroupsOut is similar as LeaveOneGroupOut , but removes samples related to groups for each training/test set. Example of Leave-2-Group Out: 3.1.2.3.4. Group Shuffle Split  The GroupShuffleSplit iterator behaves as a combination of ShuffleSplit and LeavePGroupsOut , and generates a sequence of randomized partitions in which a subset of groups are held out for each split. Here is a usage example: Here is a visualization of the cross-validation behavior. This class is useful when the behavior of LeavePGroupsOut is desired, but the number of groups is large enough that generating all possible partitions with groups withheld would be prohibitively expensive. In such a scenario, GroupShuffleSplit provides a random sample (with replacement) of the train / test splits generated by LeavePGroupsOut . 3.1.2.4. Predefined Fold-Splits / Validation-Sets  For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using PredefinedSplit it is possible to use these folds e.g. when searching for hyperparameters. For example, when using a validation set, set the to 0 for all samples that are part of the validation set, and to -1 for all other samples. 3.1.2.5. Cross validation of time series data  Time series data is characterised by the correlation between observations that are near in time ( autocorrelation ). However, classical cross-validation techniques such as KFold and ShuffleSplit assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the future observations least like those that are used to train the model. To achieve this, one solution is provided by TimeSeriesSplit . 3.1.2.5.1. Time Series Split  TimeSeriesSplit is a variation of k-fold which returns first folds as train set and the th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model. This class can be used to cross-validate time series data samples that are observed at fixed time intervals. Example of 3-split time series cross-validation on a dataset with 6 samples: Here is a visualization of the cross-validation behavior. \"\\n',\n",
       " '\"sklearn_3_1_cross-validation_evaluating_estimator_performance 3.1. Cross-validation: evaluating estimator performance modules/cross_validation.html  3.1.3. A note on shuffling  If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples. Some cross validation iterators, such as KFold , have an inbuilt option to shuffle the data indices before splitting them. Note that: This consumes less memory than shuffling the data directly. By default no shuffling occurs, including for the (stratified) K fold cross- validation performed by specifying to cross_val_score , grid search, etc. Keep in mind that train_test_split still returns a random split. The parameter defaults to , meaning that the shuffling will be different every time is iterated. However, will use the same shuffling for each set of parameters validated by a single call to its method. To get identical results for each split, set to an integer. \"\\n',\n",
       " '\"sklearn_3_1_cross-validation_evaluating_estimator_performance 3.1. Cross-validation: evaluating estimator performance modules/cross_validation.html  3.1.4. Cross validation and model selection  Cross validation iterators can also be used to directly perform model selection using Grid Search for the optimal hyperparameters of the model. This is the topic of the next section: Tuning the hyper-parameters of an estimator . \"\\n',\n",
       " '\"sklearn_3_2_tuning_the_hyper-parameters_of_an_estimator 3.2. Tuning the hyper-parameters of an estimator modules/grid_search.html  3.2.1. Exhaustive Grid Search  The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the parameter. For instance, the following : specifies that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second one with an RBF kernel, and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001]. The GridSearchCV instance implements the usual estimator API: when fitting it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained. Examples: See Parameter estimation using grid search with cross-validation for an example of Grid Search computation on the digits dataset. See Sample pipeline for text feature extraction and evaluation for an example of Grid Search coupling parameters from a text documents feature extractor (n-gram count vectorizer and TF-IDF transformer) with a classifier (here a linear SVM trained with SGD with either elastic net or L2 penalty) using a pipeline.Pipeline instance. See Nested versus non-nested cross-validation for an example of Grid Search within a cross validation loop on the iris dataset. This is the best practice for evaluating the performance of a model with grid search. See Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV for an example of GridSearchCV being used to evaluate multiple metrics simultaneously. See Balance model complexity and cross-validated score for an example of using interface in GridSearchCV . The example shows how this interface adds certain amount of flexibility in identifying the best estimator. This interface can also be used in multiple metrics evaluation. \"\\n',\n",
       " '\"sklearn_3_2_tuning_the_hyper-parameters_of_an_estimator 3.2. Tuning the hyper-parameters of an estimator modules/grid_search.html  3.2.2. Randomized Parameter Optimization  While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search: A budget can be chosen independent of the number of parameters and possible values. Adding parameters that do not influence the performance does not decrease efficiency. Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV . Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified: This example uses the module, which contains many useful distributions for sampling parameters, such as , , or . In principle, any function can be passed that provides a (random variate sample) method to sample a value. A call to the function should provide independent random samples from possible parameter values on consecutive calls. Warning The distributions in prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via or set using . However, beginning scikit-learn 0.18, the sklearn.model_selection module sets the random state provided by the user if scipy > 0.16 is also available. For continuous parameters, such as above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing will always lead to a finer search. A continuous log-uniform random variable is available through loguniform . This is a continuous version of log-spaced parameters. For example to specify above, can be used instead of or . This is an alias to SciPys stats.reciprocal . Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between and : Examples: Comparing randomized search and grid search for hyperparameter estimation compares the usage and efficiency of randomized search and grid search. References: Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012) \"\\n',\n",
       " '\"sklearn_3_2_tuning_the_hyper-parameters_of_an_estimator 3.2. Tuning the hyper-parameters of an estimator modules/grid_search.html  3.2.3. Tips for parameter search  3.2.3.1. Specifying an objective metric  By default, parameter search uses the function of the estimator to evaluate a parameter setting. These are the sklearn.metrics.accuracy_score for classification and sklearn.metrics.r2_score for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the parameter to GridSearchCV , RandomizedSearchCV and many of the specialized cross-validation tools described below. See The scoring parameter: defining model evaluation rules for more details. 3.2.3.2. Specifying multiple metrics for evaluation  and allow specifying multiple metrics for the parameter. Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See Using multiple metric evaluation for more details. When specifying multiple metrics, the parameter must be set to the metric (string) for which the will be found and used to build the on the whole dataset. If the search should not be refit, set . Leaving refit to the default value will result in an error when using multiple metrics. See Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV for an example usage. 3.2.3.3. Composite estimators and parameter spaces  GridSearchCV and RandomizedSearchCV allow searching over parameters of composite or nested estimators such as Pipeline , ColumnTransformer , VotingClassifier or CalibratedClassifierCV using a dedicated syntax: Here, is the parameter name of the nested estimator, in this case . If the meta-estimator is constructed as a collection of estimators as in , then refers to the name of the estimator, see Nested parameters . In practice, there can be several levels of nesting: 3.2.3.4. Model selection: development and evaluation  Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to train the parameters of the grid. When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the instance) and an evaluation set to compute performance metrics. This can be done by using the train_test_split utility function. 3.2.3.5. Parallelism  GridSearchCV and RandomizedSearchCV evaluate each parameter setting independently. Computations can be run in parallel if your OS supports it, by using the keyword . See function signature for more details. 3.2.3.6. Robustness to failure  Some parameter settings may result in a failure to one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. Setting (or ) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or ), but completing the search. \"\\n',\n",
       " '\"sklearn_3_2_tuning_the_hyper-parameters_of_an_estimator 3.2. Tuning the hyper-parameters of an estimator modules/grid_search.html  3.2.4. Alternatives to brute force parameter search  3.2.4.1. Model specific cross-validation  Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter. The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the regularization path of the estimator. Here is the list of such models: linear_model.ElasticNetCV (*[,\\xa0l1_ratio,\\xa0]) Elastic Net model with iterative fitting along a regularization path. linear_model.LarsCV (*[,\\xa0fit_intercept,\\xa0]) Cross-validated Least Angle Regression model. linear_model.LassoCV (*[,\\xa0eps,\\xa0n_alphas,\\xa0]) Lasso linear model with iterative fitting along a regularization path. linear_model.LassoLarsCV (*[,\\xa0fit_intercept,\\xa0]) Cross-validated Lasso, using the LARS algorithm. linear_model.LogisticRegressionCV (*[,\\xa0Cs,\\xa0]) Logistic Regression CV (aka logit, MaxEnt) classifier. linear_model.MultiTaskElasticNetCV (*[,\\xa0]) Multi-task L1/L2 ElasticNet with built-in cross-validation. linear_model.MultiTaskLassoCV (*[,\\xa0eps,\\xa0]) Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. linear_model.OrthogonalMatchingPursuitCV (*) Cross-validated Orthogonal Matching Pursuit model (OMP). linear_model.RidgeCV ([alphas,\\xa0]) Ridge regression with built-in cross-validation. linear_model.RidgeClassifierCV ([alphas,\\xa0]) Ridge classifier with built-in cross-validation. 3.2.4.2. Information Criterion  Some models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation). Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection: linear_model.LassoLarsIC ([criterion,\\xa0]) Lasso model fit with Lars using BIC or AIC for model selection 3.2.4.3. Out of Bag Estimates  When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out. This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes for free as no additional data is needed and can be used for model selection. This is currently implemented in the following classes: ensemble.RandomForestClassifier ([]) A random forest classifier. ensemble.RandomForestRegressor ([]) A random forest regressor. ensemble.ExtraTreesClassifier ([]) An extra-trees classifier. ensemble.ExtraTreesRegressor ([n_estimators,\\xa0]) An extra-trees regressor. ensemble.GradientBoostingClassifier (*[,\\xa0]) Gradient Boosting for classification. ensemble.GradientBoostingRegressor (*[,\\xa0]) Gradient Boosting for regression. \"\\n',\n",
       " '\"sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions 3.3. Metrics and scoring: quantifying the quality of predictions modules/model_evaluation.html  3.3.1. The parameter: defining model evaluation rules  Model selection and evaluation using tools, such as model_selection.GridSearchCV and model_selection.cross_val_score , take a parameter that controls what metric they apply to the estimators evaluated. 3.3.1.1. Common cases: predefined values  For the most common use cases, you can designate a scorer object with the parameter; the table below shows all possible values. All scorer objects follow the convention that higher return values are better than lower return values . Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error , are available as neg_mean_squared_error which return the negated value of the metric. Scoring Function Comment Classification accuracy metrics.accuracy_score balanced_accuracy metrics.balanced_accuracy_score average_precision metrics.average_precision_score neg_brier_score metrics.brier_score_loss f1 metrics.f1_score for binary targets f1_micro metrics.f1_score micro-averaged f1_macro metrics.f1_score macro-averaged f1_weighted metrics.f1_score weighted average f1_samples metrics.f1_score by multilabel sample neg_log_loss metrics.log_loss requires support precision etc. metrics.precision_score suffixes apply as with f1 recall etc. metrics.recall_score suffixes apply as with f1 jaccard etc. metrics.jaccard_score suffixes apply as with f1 roc_auc metrics.roc_auc_score roc_auc_ovr metrics.roc_auc_score roc_auc_ovo metrics.roc_auc_score roc_auc_ovr_weighted metrics.roc_auc_score roc_auc_ovo_weighted metrics.roc_auc_score Clustering adjusted_mutual_info_score metrics.adjusted_mutual_info_score adjusted_rand_score metrics.adjusted_rand_score completeness_score metrics.completeness_score fowlkes_mallows_score metrics.fowlkes_mallows_score homogeneity_score metrics.homogeneity_score mutual_info_score metrics.mutual_info_score normalized_mutual_info_score metrics.normalized_mutual_info_score v_measure_score metrics.v_measure_score Regression explained_variance metrics.explained_variance_score max_error metrics.max_error neg_mean_absolute_error metrics.mean_absolute_error neg_mean_squared_error metrics.mean_squared_error neg_root_mean_squared_error metrics.mean_squared_error neg_mean_squared_log_error metrics.mean_squared_log_error neg_median_absolute_error metrics.median_absolute_error r2 metrics.r2_score neg_mean_poisson_deviance metrics.mean_poisson_deviance neg_mean_gamma_deviance metrics.mean_gamma_deviance Usage examples: Note The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary . 3.3.1.2. Defining your scoring strategy from metric functions  The module sklearn.metrics also exposes a set of simple functions measuring a prediction error given ground truth and prediction: functions ending with return a value to maximize, the higher the better. functions ending with or return a value to minimize, the lower the better. When converting into a scorer object using make_scorer , set the parameter to False (True by default; see the parameter description below). Metrics available for various machine learning tasks are detailed in sections below. Many metrics are not given names to be used as values, sometimes because they require additional parameters, such as fbeta_score . In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using make_scorer . That function converts metrics into callables that can be used for model evaluation. One typical use case is to wrap an existing metric function from the library with non-default values for its parameters, such as the parameter for the fbeta_score function: The second use case is to build a completely custom scorer object from a simple python function using make_scorer , which can take several parameters: the python function you want to use ( in the example below) whether the python function returns a score ( , the default) or a loss ( ). If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return higher values for better models. for classification metrics only: whether the python function you provided requires continuous decision certainties ( ). The default value is False. any additional parameters, such as or in f1_score . Here is an example of building custom scorers, and of using the parameter: 3.3.1.3. Implementing your own scoring object  You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the make_scorer factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules: It can be called with parameters , where is the model that should be evaluated, is validation data, and is the ground truth target for (in the supervised case) or (in the unsupervised case). It returns a floating point number that quantifies the prediction quality on , with reference to . Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated. Note Using custom scorers in functions where n_jobs > 1 While defining the custom scoring function alongside the calling function should work out of the box with the default joblib backend (loky), importing it from another module will be a more robust approach and work independently of the joblib backend. For example, to use greater than 1 in the example below, function is saved in a user-created module ( ) and imported: 3.3.1.4. Using multiple metric evaluation  Scikit-learn also permits evaluation of multiple metrics in , and . There are two ways to specify multiple scoring metrics for the parameter: As an iterable of string metrics:: As a mapping the scorer name to the scoring function:: Note that the dict values can either be scorer functions or one of the predefined metric strings. Currently only those scorer functions that return a single score can be passed inside the dict. Scorer functions that return multiple values are not permitted and will require a wrapper to return a single metric: \"\\n',\n",
       " '\"sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions 3.3. Metrics and scoring: quantifying the quality of predictions modules/model_evaluation.html  3.3.2. Classification metrics  The sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the parameter. Some of these are restricted to the binary classification case: precision_recall_curve (y_true,\\xa0probas_pred,\\xa0*) Compute precision-recall pairs for different probability thresholds roc_curve (y_true,\\xa0y_score,\\xa0*[,\\xa0pos_label,\\xa0]) Compute Receiver operating characteristic (ROC) Others also work in the multiclass case: balanced_accuracy_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0]) Compute the balanced accuracy cohen_kappa_score (y1,\\xa0y2,\\xa0*[,\\xa0labels,\\xa0]) Cohens kappa: a statistic that measures inter-annotator agreement. confusion_matrix (y_true,\\xa0y_pred,\\xa0*[,\\xa0]) Compute confusion matrix to evaluate the accuracy of a classification. hinge_loss (y_true,\\xa0pred_decision,\\xa0*[,\\xa0]) Average hinge loss (non-regularized) matthews_corrcoef (y_true,\\xa0y_pred,\\xa0*[,\\xa0]) Compute the Matthews correlation coefficient (MCC) roc_auc_score (y_true,\\xa0y_score,\\xa0*[,\\xa0average,\\xa0]) Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. Some also work in the multilabel case: accuracy_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0]) Accuracy classification score. classification_report (y_true,\\xa0y_pred,\\xa0*[,\\xa0]) Build a text report showing the main classification metrics. f1_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0labels,\\xa0]) Compute the F1 score, also known as balanced F-score or F-measure fbeta_score (y_true,\\xa0y_pred,\\xa0*,\\xa0beta[,\\xa0]) Compute the F-beta score hamming_loss (y_true,\\xa0y_pred,\\xa0*[,\\xa0sample_weight]) Compute the average Hamming loss. jaccard_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0labels,\\xa0]) Jaccard similarity coefficient score log_loss (y_true,\\xa0y_pred,\\xa0*[,\\xa0eps,\\xa0]) Log loss, aka logistic loss or cross-entropy loss. multilabel_confusion_matrix (y_true,\\xa0y_pred,\\xa0*) Compute a confusion matrix for each class or sample precision_recall_fscore_support (y_true,\\xa0) Compute precision, recall, F-measure and support for each class precision_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0labels,\\xa0]) Compute the precision recall_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0labels,\\xa0]) Compute the recall roc_auc_score (y_true,\\xa0y_score,\\xa0*[,\\xa0average,\\xa0]) Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. zero_one_loss (y_true,\\xa0y_pred,\\xa0*[,\\xa0]) Zero-one classification loss. And some work with binary and multilabel (but not multiclass) problems: average_precision_score (y_true,\\xa0y_score,\\xa0*) Compute average precision (AP) from prediction scores In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition. 3.3.2.1. From binary to multiclass and multilabel  Some metrics are essentially defined for binary classification tasks (e.g. f1_score , roc_auc_score ). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled (though this may be configurable through the parameter). In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the parameter. simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class. accounts for class imbalance by computing the average of binary metrics in which each classs score is weighted by its presence in the true data sample. gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored. applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes for each sample in the evaluation data, and returning their ( -weighted) average. Selecting will return an array with the score for each class. While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell has value 1 if sample has label and value 0 otherwise. 3.3.2.2. Accuracy score  The accuracy_score function computes the accuracy , either the fraction (default) or the count (normalizeFalse) of correct predictions. In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0. If is the predicted value of the -th sample and is the corresponding true value, then the fraction of correct predictions over is defined as \\\\[\\\\texttt{accuracy}(y, \\\\hat{y})  \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}_i  y_i)\\\\] where is the indicator function . In the multilabel case with binary label indicators: Example: See Test with permutations the significance of a classification score for an example of accuracy score usage using permutations of the dataset. 3.3.2.3. Balanced accuracy score  The balanced_accuracy_score function computes the balanced accuracy , which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy. In the binary case, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate), or the area under the ROC curve with binary predictions rather than scores: \\\\[\\\\texttt{balanced-accuracy}  \\\\frac{1}{2}\\\\left( \\\\frac{TP}{TP + FN} + \\\\frac{TN}{TN + FP}\\\\right )\\\\] If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions). In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to . The score ranges from 0 to 1, or when is used, it rescaled to the range to 1, inclusive, with performance at random scoring 0. If is the true value of the -th sample, and is the corresponding sample weight, then we adjust the sample weight to: \\\\[\\\\hat{w}_i  \\\\frac{w_i}{\\\\sum_j{1(y_j  y_i) w_j}}\\\\] where is the indicator function . Given predicted for sample , balanced accuracy is defined as: \\\\[\\\\texttt{balanced-accuracy}(y, \\\\hat{y}, w)  \\\\frac{1}{\\\\sum{\\\\hat{w}_i}} \\\\sum_i 1(\\\\hat{y}_i  y_i) \\\\hat{w}_i\\\\] With , balanced accuracy reports the relative increase from . In the binary case, this is also known as *Youdens J statistic* , or informedness . Note The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature: Our definition: [Mosley2013] , [Kelleher2015] and [Guyon2015] , where [Guyon2015] adopt the adjusted version to ensure that random predictions have a score of and perfect predictions have a score of .. Class balanced accuracy as described in [Mosley2013] : the minimum between the precision and the recall for each class is computed. Those values are then averaged over the total number of classes to get the balanced accuracy. Balanced Accuracy as described in [Urbanowicz2015] : the average of sensitivity and specificity is computed for each class and then averaged over total number of classes. References: Guyon2015 ( 1 , 2 ) I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Maci, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, Design of the 2015 ChaLearn AutoML Challenge , IJCNN 2015. Mosley2013 ( 1 , 2 ) L. Mosley, A balanced approach to the multi-class imbalance problem , IJCV 2010. Kelleher2015 John. D. Kelleher, Brian Mac Namee, Aoife DArcy, Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies , 2015. Urbanowicz2015 Urbanowicz R.J., Moore, J.H. ExSTraCS 2.0: description and evaluation of a scalable learning classifier system , Evol. Intel. (2015) 8: 89. 3.3.2.4. Cohens kappa  The function cohen_kappa_score computes Cohens kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth. The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels). Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators. 3.3.2.5. Confusion matrix  The confusion_matrix function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class (Wikipedia and other references may use different convention for axes). By definition, entry in a confusion matrix is the number of observations actually in group , but predicted to be in group . Here is an example: plot_confusion_matrix can be used to visually represent a confusion matrix as shown in the Confusion matrix example, which creates the following figure: The parameter allows to report ratios instead of counts. The confusion matrix can be normalized in 3 different ways: , , and which will divide the counts by the sum of each columns, rows, or the entire matrix, respectively. For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows: Example: See Confusion matrix for an example of using a confusion matrix to evaluate classifier output quality. See Recognizing hand-written digits for an example of using a confusion matrix to classify hand-written digits. See Classification of text documents using sparse features for an example of using a confusion matrix to classify text documents. 3.3.2.6. Classification report  The classification_report function builds a text report showing the main classification metrics. Here is a small example with custom and inferred labels: Example: See Recognizing hand-written digits for an example of classification report usage for hand-written digits. See Classification of text documents using sparse features for an example of classification report usage for text documents. See Parameter estimation using grid search with cross-validation for an example of classification report usage for grid search with nested cross-validation. 3.3.2.7. Hamming loss  The hamming_loss computes the average Hamming loss or Hamming distance between two sets of samples. If is the predicted value for the -th label of a given sample, is the corresponding true value, and is the number of classes or labels, then the Hamming loss between two samples is defined as: \\\\[L_{Hamming}(y, \\\\hat{y})  \\\\frac{1}{n_\\\\text{labels}} \\\\sum_{j0}^{n_\\\\text{labels} - 1} 1(\\\\hat{y}_j \\\\not y_j)\\\\] where is the indicator function . In the multilabel case with binary label indicators: Note In multiclass classification, the Hamming loss corresponds to the Hamming distance between and which is similar to the Zero one loss function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive. 3.3.2.8. Precision, recall and F-measures  Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples. The F-measure ( and measures) can be interpreted as a weighted harmonic mean of the precision and recall. A measure reaches its best value at 1 and its worst score at 0. With , and are equivalent, and the recall and the precision are equally important. The precision_recall_curve computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold. The average_precision_score function computes the average precision (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as \\\\[\\\\text{AP}  \\\\sum_n (R_n - R_{n-1}) P_n\\\\] where and are the precision and recall at the nth threshold. With random predictions, the AP is the fraction of positive samples. References [Manning2008] and [Everingham2010] present alternative variants of AP that interpolate the precision-recall curve. Currently, average_precision_score does not implement any interpolated variant. References [Davis2006] and [Flach2015] describe why a linear interpolation of points on the precision-recall curve provides an overly-optimistic measure of classifier performance. This linear interpolation is used when computing area under the curve with the trapezoidal rule in auc . Several functions allow you to analyze the precision, recall and F-measures score: average_precision_score (y_true,\\xa0y_score,\\xa0*) Compute average precision (AP) from prediction scores f1_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0labels,\\xa0]) Compute the F1 score, also known as balanced F-score or F-measure fbeta_score (y_true,\\xa0y_pred,\\xa0*,\\xa0beta[,\\xa0]) Compute the F-beta score precision_recall_curve (y_true,\\xa0probas_pred,\\xa0*) Compute precision-recall pairs for different probability thresholds precision_recall_fscore_support (y_true,\\xa0) Compute precision, recall, F-measure and support for each class precision_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0labels,\\xa0]) Compute the precision recall_score (y_true,\\xa0y_pred,\\xa0*[,\\xa0labels,\\xa0]) Compute the recall Note that the precision_recall_curve function is restricted to the binary case. The average_precision_score function works only in binary classification and multilabel indicator format. The plot_precision_recall_curve function plots the precision recall as follows. Examples: See Classification of text documents using sparse features for an example of f1_score usage to classify text documents. See Parameter estimation using grid search with cross-validation for an example of precision_score and recall_score usage to estimate parameters using grid search with nested cross-validation. See Precision-Recall for an example of precision_recall_curve usage to evaluate classifier output quality. References: Manning2008 C.D. Manning, P. Raghavan, H. Schtze, Introduction to Information Retrieval , 2008. Everingham2010 M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, The Pascal Visual Object Classes (VOC) Challenge , IJCV 2010. Davis2006 J. Davis, M. Goadrich, The Relationship Between Precision-Recall and ROC Curves , ICML 2006. Flach2015 P.A. Flach, M. Kull, Precision-Recall-Gain Curves: PR Analysis Done Right , NIPS 2015. 3.3.2.8.1. Binary classification  In a binary classification task, the terms positive and negative refer to the classifiers prediction, and the terms true and false refer to whether that prediction corresponds to the external judgment (sometimes known as the observation). Given these definitions, we can formulate the following table: Actual class (observation) Predicted class (expectation) tp (true positive) Correct result fp (false positive) Unexpected result fn (false negative) Missing result tn (true negative) Correct absence of result In this context, we can define the notions of precision, recall and F-measure: \\\\[\\\\text{precision}  \\\\frac{tp}{tp + fp},\\\\] \\\\[\\\\text{recall}  \\\\frac{tp}{tp + fn},\\\\] \\\\[F_\\\\beta  (1 + \\\\beta^2) \\\\frac{\\\\text{precision} \\\\times \\\\text{recall}}{\\\\beta^2 \\\\text{precision} + \\\\text{recall}}.\\\\] Here are some small examples in binary classification: 3.3.2.8.2. Multiclass and multilabel classification  In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the argument to the average_precision_score (multilabel only), f1_score , fbeta_score , precision_recall_fscore_support , precision_score and recall_score functions, as described above . Note that if all labels are included, micro-averaging in a multiclass setting will produce precision, recall and that are all identical to accuracy. Also note that weighted averaging may produce an F-score that is not between precision and recall. To make this more explicit, consider the following notation: the set of predicted pairs the set of true pairs the set of labels the set of samples the subset of with sample , i.e. the subset of with label similarly, and are subsets of for some sets and (Conventions vary on handling ; this implementation uses , and similar for .) Then the metrics are defined as: Precision Recall F_beta For multiclass classification with a negative class, it is possible to exclude some labels: Similarly, labels not present in the data sample may be accounted for in macro-averaging. 3.3.2.9. Jaccard similarity coefficient score  The jaccard_score function computes the average of Jaccard similarity coefficients , also called the Jaccard index, between pairs of label sets. The Jaccard similarity coefficient of the -th samples, with a ground truth label set and predicted label set , is defined as \\\\[J(y_i, \\\\hat{y}_i)  \\\\frac{|y_i \\\\cap \\\\hat{y}_i|}{|y_i \\\\cup \\\\hat{y}_i|}.\\\\] jaccard_score works like precision_recall_fscore_support as a naively set-wise measure applying natively to binary targets, and extended to apply to multilabel and multiclass through the use of (see above ). In the binary case: In the multilabel case with binary label indicators: Multiclass problems are binarized and treated like the corresponding multilabel problem: 3.3.2.10. Hinge loss  The hinge_loss function computes the average distance between the model and the data using hinge loss , a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.) If the labels are encoded with +1 and -1, : is the true value, and is the predicted decisions as output by , then the hinge loss is defined as: \\\\[L_\\\\text{Hinge}(y, w)  \\\\max\\\\left\\\\{1 - wy, 0\\\\right\\\\}  \\\\left|1 - wy\\\\right|_+\\\\] If there are more than two labels, hinge_loss uses a multiclass variant due to Crammer & Singer. Here is the paper describing it. If is the predicted decision for true label and is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by: \\\\[L_\\\\text{Hinge}(y_w, y_t)  \\\\max\\\\left\\\\{1 + y_t - y_w, 0\\\\right\\\\}\\\\] Here a small example demonstrating the use of the hinge_loss function with a svm classifier in a binary class problem: Here is an example demonstrating the use of the hinge_loss function with a svm classifier in a multiclass problem: 3.3.2.11. Log loss  Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs ( ) of a classifier instead of its discrete predictions. For binary classification with a true label and a probability estimate , the log loss per sample is the negative log-likelihood of the classifier given the true label: \\\\[L_{\\\\log}(y, p)  -\\\\log \\\\operatorname{Pr}(y|p)  -(y \\\\log (p) + (1 - y) \\\\log (1 - p))\\\\] This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix , i.e., if sample has label taken from a set of labels. Let be a matrix of probability estimates, with . Then the log loss of the whole set is \\\\[L_{\\\\log}(Y, P)  -\\\\log \\\\operatorname{Pr}(Y|P)  - \\\\frac{1}{N} \\\\sum_{i0}^{N-1} \\\\sum_{k0}^{K-1} y_{i,k} \\\\log p_{i,k}\\\\] To see how this generalizes the binary log loss given above, note that in the binary case, and , so expanding the inner sum over gives the binary log loss. The log_loss function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimators method. The first in denotes 90% probability that the first sample has label 0. The log loss is non-negative. 3.3.2.12. Matthews correlation coefficient  The matthews_corrcoef function computes the Matthews correlation coefficient (MCC) for binary classes. Quoting Wikipedia: The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. In the binary (two-class) case, , , and are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as \\\\[MCC  \\\\frac{tp \\\\times tn - fp \\\\times fn}{\\\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\\\\] In the multiclass case, the Matthews correlation coefficient can be defined in terms of a confusion_matrix for classes. To simplify the definition consider the following intermediate variables: the number of times class truly occurred, the number of times class was predicted, the total number of samples correctly predicted, the total number of samples. Then the multiclass MCC is defined as: \\\\[MCC  \\\\frac{ c \\\\times s - \\\\sum_{k}^{K} p_k \\\\times t_k }{\\\\sqrt{ (s^2 - \\\\sum_{k}^{K} p_k^2) \\\\times (s^2 - \\\\sum_{k}^{K} t_k^2) }}\\\\] When there are more than two labels, the value of the MCC will no longer range between -1 and +1. Instead the minimum value will be somewhere between -1 and 0 depending on the number and distribution of ground true labels. The maximum value is always +1. Here is a small example illustrating the usage of the matthews_corrcoef function: 3.3.2.13. Multi-label confusion matrix  The multilabel_confusion_matrix function computes class-wise (default) or sample-wise (samplewiseTrue) multilabel confusion matrix to evaluate the accuracy of a classification. multilabel_confusion_matrix also treats multiclass data as if it were multilabel, as this is a transformation commonly applied to evaluate multiclass problems with binary classification metrics (such as precision, recall, etc.). When calculating class-wise multilabel confusion matrix , the count of true negatives for class is , false negatives is , true positives is and false positives is . Here is an example demonstrating the use of the multilabel_confusion_matrix function with multilabel indicator matrix input: Or a confusion matrix can be constructed for each samples labels: Here is an example demonstrating the use of the multilabel_confusion_matrix function with multiclass input: Here are some examples demonstrating the use of the multilabel_confusion_matrix function to calculate recall (or sensitivity), specificity, fall out and miss rate for each class in a problem with multilabel indicator matrix input. Calculating recall (also called the true positive rate or the sensitivity) for each class: Calculating specificity (also called the true negative rate) for each class: Calculating fall out (also called the false positive rate) for each class: Calculating miss rate (also called the false negative rate) for each class: 3.3.2.14. Receiver operating characteristic (ROC)  The function roc_curve computes the receiver operating characteristic curve, or ROC curve . Quoting Wikipedia : A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR  true positive rate) vs. the fraction of false positives out of the negatives (FPR  false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate. This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the roc_curve function: This figure shows an example of such an ROC curve: The roc_auc_score function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. For more information see the Wikipedia article on AUC . In multi-label classification, the roc_auc_score function is extended by averaging over the labels as above . Compared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesnt require optimizing a threshold for each label. The roc_auc_score function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the predicted labels are provided in an array with values from 0 to , and the scores correspond to the probability estimates that a sample belongs to a particular class. The OvO and OvR algorithms support weighting uniformly ( ) and by prevalence ( ). One-vs-one Algorithm : Computes the average AUC of all possible pairwise combinations of classes. [HT2001] defines a multiclass AUC metric weighted uniformly: \\\\[\\\\frac{2}{c(c-1)}\\\\sum_{j1}^{c}\\\\sum_{k > j}^c (\\\\text{AUC}(j | k) + \\\\text{AUC}(k | j))\\\\] where is the number of classes and is the AUC with class as the positive class and class as the negative class. In general, in the multiclass case. This algorithm is used by setting the keyword argument to and to . The [HT2001] multiclass AUC metric can be extended to be weighted by the prevalence: \\\\[\\\\frac{2}{c(c-1)}\\\\sum_{j1}^{c}\\\\sum_{k > j}^c p(j \\\\cup k)( \\\\text{AUC}(j | k) + \\\\text{AUC}(k | j))\\\\] where is the number of classes. This algorithm is used by setting the keyword argument to and to . The option returns a prevalence-weighted average as described in [FC2009] . One-vs-rest Algorithm : Computes the AUC of each class against the rest [PD2000] . The algorithm is functionally the same as the multilabel case. To enable this algorithm set the keyword argument to . Like OvO, OvR supports two types of averaging: [F2006] and [F2001] . In applications where a high false positive rate is not tolerable the parameter of roc_auc_score can be used to summarize the ROC curve up to the given limit. Examples: See Receiver Operating Characteristic (ROC) for an example of using ROC to evaluate the quality of the output of a classifier. See Receiver Operating Characteristic (ROC) with cross validation for an example of using ROC to evaluate classifier output quality, using cross-validation. See Species distribution modeling for an example of using ROC to model species distribution. References: HT2001 ( 1 , 2 ) Hand, D.J. and Till, R.J., (2001). A simple generalisation of the area under the ROC curve for multiple class classification problems. Machine learning, 45(2), pp.171-186. FC2009 Ferri, Csar & Hernandez-Orallo, Jose & Modroiu, R. (2009). An Experimental Comparison of Performance Measures for Classification. Pattern Recognition Letters. 30. 27-38. PD2000 Provost, F., Domingos, P. (2000). Well-trained PETs: Improving probability estimation trees (Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business, New York University. F2006 Fawcett, T., 2006. An introduction to ROC analysis. Pattern Recognition Letters, 27(8), pp. 861-874. F2001 Fawcett, T., 2001. Using rule sets to maximize ROC performance In Data Mining, 2001. Proceedings IEEE International Conference, pp. 131-138. 3.3.2.15. Zero one loss  The zero_one_loss function computes the sum or the average of the 0-1 classification loss ( ) over . By default, the function normalizes over the sample. To get the sum of the , set to . In multilabel classification, the zero_one_loss scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set to If is the predicted value of the -th sample and is the corresponding true value, then the 0-1 loss is defined as: \\\\[L_{0-1}(y_i, \\\\hat{y}_i)  1(\\\\hat{y}_i \\\\not y_i)\\\\] where is the indicator function . In the multilabel case with binary label indicators, where the first label set [0,1] has an error: Example: See Recursive feature elimination with cross-validation for an example of zero one loss usage to perform recursive feature elimination with cross-validation. 3.3.2.16. Brier score loss  The brier_score_loss function computes the Brier score for binary classes. Quoting Wikipedia: The Brier score is a proper score function that measures the accuracy of probabilistic predictions. It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes. This function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1. The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the calibration of a set of probabilistic predictions. \\\\[BS  \\\\frac{1}{N} \\\\sum_{t1}^{N}(f_t - o_t)^2\\\\] where : is the total number of predictions, is the predicted probability of the actual outcome . Here is a small example of usage of this function:: Example: See Probability calibration of classifiers for an example of Brier score loss usage to perform probability calibration of classifiers. References: G. Brier, Verification of forecasts expressed in terms of probability , Monthly weather review 78.1 (1950) \"\\n',\n",
       " '\"sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions 3.3. Metrics and scoring: quantifying the quality of predictions modules/model_evaluation.html  3.3.3. Multilabel ranking metrics  In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels. 3.3.3.1. Coverage error  The coverage_error function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels. Note Our implementations score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels. Formally, given a binary indicator matrix of the ground truth labels and the score associated with each label , the coverage is defined as \\\\[coverage(y, \\\\hat{f})  \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i0}^{n_{\\\\text{samples}} - 1} \\\\max_{j:y_{ij}  1} \\\\text{rank}_{ij}\\\\] with . Given the rank definition, ties in are broken by giving the maximal rank that would have been assigned to all tied values. Here is a small example of usage of this function: 3.3.3.2. Label ranking average precision  The label_ranking_average_precision_score function implements label ranking average precision (LRAP). This metric is linked to the average_precision_score function, but is based on the notion of label ranking instead of precision and recall. Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank . Formally, given a binary indicator matrix of the ground truth labels and the score associated with each label , the average precision is defined as \\\\[LRAP(y, \\\\hat{f})  \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i0}^{n_{\\\\text{samples}} - 1} \\\\frac{1}{||y_i||_0} \\\\sum_{j:y_{ij}  1} \\\\frac{|\\\\mathcal{L}_{ij}|}{\\\\text{rank}_{ij}}\\\\] where , , computes the cardinality of the set (i.e., the number of elements in the set), and is the norm (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: 3.3.3.3. Ranking loss  The label_ranking_loss function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero. Formally, given a binary indicator matrix of the ground truth labels and the score associated with each label , the ranking loss is defined as \\\\[ranking\\\\_loss(y, \\\\hat{f})  \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i0}^{n_{\\\\text{samples}} - 1} \\\\frac{1}{||y_i||_0(n_\\\\text{labels} - ||y_i||_0)} \\\\left|\\\\left\\\\{(k, l): \\\\hat{f}_{ik} \\\\leq \\\\hat{f}_{il}, y_{ik}  1, y_{il}  0\\xa0\\\\right\\\\}\\\\right|\\\\] where computes the cardinality of the set (i.e., the number of elements in the set) and is the norm (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: References: Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US. 3.3.3.4. Normalized Discounted Cumulative Gain  Discounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain (NDCG) are ranking metrics; they compare a predicted order to ground-truth scores, such as the relevance of answers to a query. From the Wikipedia page for Discounted Cumulative Gain: Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks DCG orders the true targets (e.g. relevance of query answers) in the predicted order, then multiplies them by a logarithmic decay and sums the result. The sum can be truncated after the first results, in which case we call it DCG@K. NDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so that it is always between 0 and 1. Usually, NDCG is preferred to DCG. Compared with the ranking loss, NDCG can take into account relevance scores, rather than a ground-truth ranking. So if the ground-truth consists only of an ordering, the ranking loss should be preferred; if the ground-truth consists of actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very relevant), NDCG can be used. For one sample, given the vector of continuous ground-truth values for each target , where is the number of outputs, and the prediction , which induces the ranking function , the DCG score is \\\\[\\\\sum_{r1}^{\\\\min(K, M)}\\\\frac{y_{f(r)}}{\\\\log(1 + r)}\\\\] and the NDCG score is the DCG score divided by the DCG score obtained for . References: Wikipedia entry for Discounted Cumulative Gain Jarvelin, K., & Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4), 422-446. Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May). A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013) McSherry, F., & Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg. \"\\n',\n",
       " '\"sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions 3.3. Metrics and scoring: quantifying the quality of predictions modules/model_evaluation.html  3.3.4. Regression metrics  The sklearn.metrics module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: mean_squared_error , mean_absolute_error , explained_variance_score and r2_score . These functions have an keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is , which specifies a uniformly weighted mean over outputs. If an of shape is passed, then its entries are interpreted as weights and an according weighted average is returned. If is is specified, then all unaltered individual scores or losses will be returned in an array of shape . The r2_score and explained_variance_score accept an additional value for the parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. is the default value for r2_score for backward compatibility. This will be changed to in the future. 3.3.4.1. Explained variance score  The explained_variance_score computes the explained variance regression score . If is the estimated target output, the corresponding (correct) target output, and is Variance , the square of the standard deviation, then the explained variance is estimated as follow: \\\\[explained\\\\_{}variance(y, \\\\hat{y})  1 - \\\\frac{Var\\\\{ y - \\\\hat{y}\\\\}}{Var\\\\{y\\\\}}\\\\] The best possible score is 1.0, lower values are worse. Here is a small example of usage of the explained_variance_score function: 3.3.4.2. Max error  The max_error function computes the maximum residual error , a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, would be on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted. If is the predicted value of the -th sample, and is the corresponding true value, then the max error is defined as \\\\[\\\\text{Max Error}(y, \\\\hat{y})  max(| y_i - \\\\hat{y}_i |)\\\\] Here is a small example of usage of the max_error function: The max_error does not support multioutput. 3.3.4.3. Mean absolute error  The mean_absolute_error function computes mean absolute error , a risk metric corresponding to the expected value of the absolute error loss or -norm loss. If is the predicted value of the -th sample, and is the corresponding true value, then the mean absolute error (MAE) estimated over is defined as \\\\[\\\\text{MAE}(y, \\\\hat{y})  \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i0}^{n_{\\\\text{samples}}-1} \\\\left| y_i - \\\\hat{y}_i \\\\right|.\\\\] Here is a small example of usage of the mean_absolute_error function: 3.3.4.4. Mean squared error  The mean_squared_error function computes mean square error , a risk metric corresponding to the expected value of the squared (quadratic) error or loss. If is the predicted value of the -th sample, and is the corresponding true value, then the mean squared error (MSE) estimated over is defined as \\\\[\\\\text{MSE}(y, \\\\hat{y})  \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i0}^{n_\\\\text{samples} - 1} (y_i - \\\\hat{y}_i)^2.\\\\] Here is a small example of usage of the mean_squared_error function: Examples: See Gradient Boosting regression for an example of mean squared error usage to evaluate gradient boosting regression. 3.3.4.5. Mean squared logarithmic error  The mean_squared_log_error function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss. If is the predicted value of the -th sample, and is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over is defined as \\\\[\\\\text{MSLE}(y, \\\\hat{y})  \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i0}^{n_\\\\text{samples} - 1} (\\\\log_e (1 + y_i) - \\\\log_e (1 + \\\\hat{y}_i) )^2.\\\\] Where means the natural logarithm of . This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate. Here is a small example of usage of the mean_squared_log_error function: 3.3.4.6. Median absolute error  The median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction. If is the predicted value of the -th sample and is the corresponding true value, then the median absolute error (MedAE) estimated over is defined as \\\\[\\\\text{MedAE}(y, \\\\hat{y})  \\\\text{median}(\\\\mid y_1 - \\\\hat{y}_1 \\\\mid, \\\\ldots, \\\\mid y_n - \\\\hat{y}_n \\\\mid).\\\\] The median_absolute_error does not support multioutput. Here is a small example of usage of the median_absolute_error function: 3.3.4.7. R score, the coefficient of determination  The r2_score function computes the coefficient of determination , usually denoted as R. It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. As such variance is dataset dependent, R may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R score of 0.0. If is the predicted value of the -th sample and is the corresponding true value for total samples, the estimated R is defined as: \\\\[R^2(y, \\\\hat{y})  1 - \\\\frac{\\\\sum_{i1}^{n} (y_i - \\\\hat{y}_i)^2}{\\\\sum_{i1}^{n} (y_i - \\\\bar{y})^2}\\\\] where and . Note that r2_score calculates unadjusted R without correcting for bias in sample variance of y. Here is a small example of usage of the r2_score function: Example: See Lasso and Elastic Net for Sparse Signals for an example of R score usage to evaluate Lasso and Elastic Net on sparse signals. 3.3.4.8. Mean Poisson, Gamma, and Tweedie deviances  The mean_tweedie_deviance function computes the mean Tweedie deviance error with a parameter ( ). This is a metric that elicits predicted expectation values of regression targets. Following special cases exist, when it is equivalent to mean_squared_error . when it is equivalent to mean_poisson_deviance . when it is equivalent to mean_gamma_deviance . If is the predicted value of the -th sample, and is the corresponding true value, then the mean Tweedie deviance error (D) for power , estimated over is defined as \\\\[\\\\begin{split}\\\\text{D}(y, \\\\hat{y})  \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i0}^{n_\\\\text{samples} - 1} \\\\begin{cases} (y_i-\\\\hat{y}_i)^2, & \\\\text{for }p0\\\\text{ (Normal)}\\\\\\\\ 2(y_i \\\\log(y/\\\\hat{y}_i) + \\\\hat{y}_i - y_i), & \\\\text{for}p1\\\\text{ (Poisson)}\\\\\\\\ 2(\\\\log(\\\\hat{y}_i/y_i) + y_i/\\\\hat{y}_i - 1), & \\\\text{for}p2\\\\text{ (Gamma)}\\\\\\\\ 2\\\\left(\\\\frac{\\\\max(y_i,0)^{2-p}}{(1-p)(2-p)}- \\\\frac{y\\\\,\\\\hat{y}^{1-p}_i}{1-p}+\\\\frac{\\\\hat{y}^{2-p}_i}{2-p}\\\\right), & \\\\text{otherwise} \\\\end{cases}\\\\end{split}\\\\] Tweedie deviance is a homogeneous function of degree . Thus, Gamma distribution with means that simultaneously scaling and has no effect on the deviance. For Poisson distribution the deviance scales linearly, and for Normal distribution ( ), quadratically. In general, the higher the less weight is given to extreme deviations between true and predicted targets. For instance, lets compare the two predictions 1.0 and 100 that are both 50% of their corresponding true value. The mean squared error ( ) is very sensitive to the prediction difference of the second point,: If we increase to 1,: the difference in errors decreases. Finally, by setting, : we would get identical errors. The deviance when is thus only sensitive to relative errors. \"\\n',\n",
       " '\"sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions 3.3. Metrics and scoring: quantifying the quality of predictions modules/model_evaluation.html  3.3.5. Clustering metrics  The sklearn.metrics module implements several loss, score, and utility functions. For more information see the Clustering performance evaluation section for instance clustering, and Biclustering evaluation for biclustering. \"\\n',\n",
       " '\"sklearn_3_3_metrics_and_scoring_quantifying_the_quality_of_predictions 3.3. Metrics and scoring: quantifying the quality of predictions modules/model_evaluation.html  3.3.6. Dummy estimators  When doing supervised learning, a simple sanity check consists of comparing ones estimator against simple rules of thumb. DummyClassifier implements several such simple strategies for classification: generates random predictions by respecting the training set class distribution. always predicts the most frequent label in the training set. always predicts the class that maximizes the class prior (like ) and returns the class prior. generates predictions uniformly at random. always predicts a constant label that is provided by the user. A major motivation of this method is F1-scoring, when the positive class is in the minority. Note that with all these strategies, the method completely ignores the input data! To illustrate DummyClassifier , first lets create an imbalanced dataset: Next, lets compare the accuracy of and : We see that doesnt do much better than a dummy classifier. Now, lets change the kernel: We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the Cross-validation: evaluating estimator performance section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator section for details. More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc DummyRegressor also implements four simple rules of thumb for regression: always predicts the mean of the training targets. always predicts the median of the training targets. always predicts a user provided quantile of the training targets. always predicts a constant value that is provided by the user. In all these strategies, the method completely ignores the input data. \"\\n',\n",
       " '\"sklearn_3_4_model_persistence 3.4. Model persistence modules/model_persistence.html  3.4.1. Persistence example  It is possible to save a model in scikit-learn by using Pythons built-in persistence model, namely pickle : In the specific case of scikit-learn, it may be better to use joblibs replacement of pickle ( & ), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string: Later you can load back the pickled model (possibly in another Python process) with: Note and functions also accept file-like object instead of filenames. More information on data persistence with Joblib is available here . \"\\n',\n",
       " '\"sklearn_3_4_model_persistence 3.4. Model persistence modules/model_persistence.html  3.4.2. Security & maintainability limitations  pickle (and joblib by extension), has some issues regarding maintainability and security. Because of this, Never unpickle untrusted data as it could lead to malicious code being executed upon loading. While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results. In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model: The training data, e.g. a reference to an immutable snapshot The python source code used to generate the model The versions of scikit-learn and its dependencies The cross validation score obtained on the training data This should make it possible to check that the cross-validation score is in the same range as before. Since a model internal representation may be different on two different architectures, dumping a model on one architecture and loading it on another architecture is not supported. If you want to know more about these issues and explore other possible serialization methods, please refer to this talk by Alex Gaynor . \"\\n',\n",
       " '\"sklearn_3_5_validation_curves_plotting_scores_to_evaluate_models 3.5. Validation curves: plotting scores to evaluate models modules/learning_curve.html  3.5.1. Validation curve  To validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions ), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods (see Tuning the hyper-parameters of an estimator ) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set. However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values. The function validation_curve can help in this case: If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter of an SVM on the digits dataset. \"\\n',\n",
       " '\"sklearn_3_5_validation_curves_plotting_scores_to_evaluate_models 3.5. Validation curves: plotting scores to evaluate models modules/learning_curve.html  3.5.2. Learning curve  A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. Consider the following example where we plot the learning curve of a naive Bayes classifier and an SVM. For the naive Bayes, both the validation score and the training score converge to a value that is quite low with increasing size of the training set. Thus, we will probably not benefit much from more training data. In contrast, for small amounts of data, the training score of the SVM is much greater than the validation score. Adding more training samples will most likely increase generalization. We can use the function learning_curve to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets): \"\\n',\n",
       " '\"sklearn_4_1_partial_dependence_plots 4.1. Partial dependence plots modules/partial_dependence.html  4.1.1. Mathematical Definition  Let be the set of target features (i.e. the parameter) and let be its complement. The partial dependence of the response at a point is defined as: \\\\[\\\\begin{split}pd_{X_S}(x_S) &\\\\overset{def}{} \\\\mathbb{E}_{X_C}\\\\left[ f(x_S, X_C) \\\\right]\\\\\\\\ & \\\\int f(x_S, x_C) p(x_C) dx_C,\\\\end{split}\\\\] where is the response function ( predict , predict_proba or decision_function ) for a given sample whose values are defined by for the features in , and by for the features in . Note that and may be tuples. Computing this integral for various values of produces a plot as above. \"\\n',\n",
       " '\"sklearn_4_1_partial_dependence_plots 4.1. Partial dependence plots modules/partial_dependence.html  4.1.2. Computation methods  There are two main methods to approximate the integral above, namely the brute and recursion methods. The parameter controls which method to use. The brute method is a generic method that works with any estimator. It approximates the above integral by computing an average over the data : \\\\[pd_{X_S}(x_S) \\\\approx \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i1}^n f(x_S, x_C^{(i)}),\\\\] where is the value of the i-th sample for the features in . For each value of , this method requires a full pass over the dataset which is computationally intensive. The recursion method is faster than the brute method, but it is only supported by some tree-based estimators. It is computed as follows. For a given point , a weighted tree traversal is performed: if a split node involves a target feature, the corresponding left or right branch is followed; otherwise both branches are followed, each branch being weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all the visited leaves values. With the brute method, the parameter is used both for generating the grid of values and the complement feature values . However with the recursion method, is only used for the grid values: implicitly, the values are those of the training data. By default, the recursion method is used on tree-based estimators that support it, and brute is used for the rest. Note While both methods should be close in general, they might differ in some specific settings. The brute method assumes the existence of the data points . When the features are correlated, such artificial samples may have a very low probability mass. The brute and recursion methods will likely disagree regarding the value of the partial dependence, because they will treat these unlikely samples differently. Remember, however, that the primary assumption for interpreting PDPs is that the features should be independent. Footnotes 1 For classification, the target response may be the probability of a class (the positive class for binary classification), or the decision function. Examples: Partial Dependence Plots References T. Hastie, R. Tibshirani and J. Friedman, The Elements of Statistical Learning , Second Edition, Section 10.13.2, Springer, 2009. C. Molnar, Interpretable Machine Learning , Section 5.1, 2019. \"\\n',\n",
       " '\"sklearn_4_2_permutation_feature_importance 4.2. Permutation feature importance modules/permutation_importance.html  4.2.1. Outline of the permutation importance algorithm  Inputs: fitted predictive model , tabular dataset (training or validation) . Compute the reference score of the model on data (for instance the accuracy for a classifier or the for a regressor). For each feature (column of ): For each repetition in : Randomly shuffle column of dataset to generate a corrupted version of the data named . Compute the score of model on corrupted data . Compute importance for feature defined as: \\\\[i_j  s - \\\\frac{1}{K} \\\\sum_{k1}^{K} s_{k,j}\\\\] \"\\n',\n",
       " '\"sklearn_4_2_permutation_feature_importance 4.2. Permutation feature importance modules/permutation_importance.html  4.2.2. Relation to impurity-based importance in trees  Tree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Entropy or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data. Furthermore, impurity-based feature importance for trees are strongly biased and favor high cardinality features (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories. Permutation-based feature importances do not exhibit such a bias. Additionally, the permutation feature importance may be computed performance metric on the model predictions predictions and can be used to analyze any model class (not just tree-based models). The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: Permutation Importance vs Random Forest Feature Importance (MDI) . \"\\n',\n",
       " '\"sklearn_4_2_permutation_feature_importance 4.2. Permutation feature importance modules/permutation_importance.html  4.2.3. Misleading values on strongly correlated features  When two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important. One way to handle this is to cluster features that are correlated and only keep one feature from each cluster. This strategy is explored in the following example: Permutation Importance with Multicollinear or Correlated Features . Examples: Permutation Importance vs Random Forest Feature Importance (MDI) Permutation Importance with Multicollinear or Correlated Features References: 1 L. Breiman, Random Forests, Machine Learning, 45(1), 5-32, 2001. https://doi.org/10.1023/A:1010933404324 \"\\n',\n",
       " '\"sklearn_5_1_available_plotting_utilities 5.1. Available Plotting Utilities visualizations.html#available-plotting-utilities  5.1. Available Plotting Utilities  5.1.1. Functions  inspection.plot_partial_dependence ([,\\xa0]) Partial dependence plots. metrics.plot_confusion_matrix (estimator,\\xa0X,\\xa0) Plot Confusion Matrix. metrics.plot_precision_recall_curve ([,\\xa0]) Plot Precision Recall Curve for binary classifiers. metrics.plot_roc_curve (estimator,\\xa0X,\\xa0y,\\xa0*[,\\xa0]) Plot Receiver operating characteristic (ROC) curve. 5.1.2. Display Objects  inspection.PartialDependenceDisplay () Partial Dependence Plot (PDP) visualization. metrics.ConfusionMatrixDisplay ([,\\xa0]) Confusion Matrix visualization. metrics.PrecisionRecallDisplay (precision,\\xa0) Precision Recall visualization. metrics.RocCurveDisplay (*,\\xa0fpr,\\xa0tpr[,\\xa0]) ROC Curve visualization. \"\\n',\n",
       " '\"sklearn_6_1_pipelines_and_composite_estimators 6.1. Pipelines and composite estimators modules/compose.html  6.1.1. Pipeline: chaining estimators  Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here: Convenience and encapsulation You only have to call fit and predict once on your data to fit a whole sequence of estimators. Joint parameter selection You can grid search over parameters of all estimators in the pipeline at once. Safety Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors. All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.). 6.1.1.1. Usage  6.1.1.1.1. Construction  The Pipeline is built using a list of pairs, where the is a string containing the name you want to give this step and is an estimator object: The utility function make_pipeline is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically: 6.1.1.1.2. Accessing steps  The estimators of a pipeline are stored as a list in the attribute, but can be accessed by index or name by indexing (with ) the Pipeline: Pipelines attribute allows accessing steps by name with tab completion in interactive environments: A sub-pipeline can also be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse): 6.1.1.1.3. Nested parameters  Parameters of the estimators in the pipeline can be accessed using the syntax: This is particularly important for doing grid searches: Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to : The estimators of the pipeline can be retrieved by index: or by name: Examples: Pipeline Anova SVM Sample pipeline for text feature extraction and evaluation Pipelining: chaining a PCA and a logistic regression Explicit feature map approximation for RBF kernels SVM-Anova: SVM with univariate feature selection Selecting dimensionality reduction with Pipeline and GridSearchCV See also: Composite estimators and parameter spaces 6.1.1.2. Notes  Calling on the pipeline is the same as calling on each estimator in turn, the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline. 6.1.1.3. Caching transformers: avoid repeated computation  Fitting transformers may be computationally expensive. With its parameter set, Pipeline will cache each transformer after calling . This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration. The parameter is needed in order to cache the transformers. can be either a string containing the directory where to cache the transformers or a joblib.Memory object: Warning Side effect of caching transformers Using a Pipeline without cache enabled, it is possible to inspect the original instance such as: Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. In following example, accessing the PCA instance will raise an since will be an unfitted transformer. Instead, use the attribute to inspect estimators within the pipeline: Examples: Selecting dimensionality reduction with Pipeline and GridSearchCV \"\\n',\n",
       " '\"sklearn_6_1_pipelines_and_composite_estimators 6.1. Pipelines and composite estimators modules/compose.html  6.1.2. Transforming target in regression  TransformedTargetRegressor transforms the targets before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable: For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping: Subsequently, the object is created as: By default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting to : Note The transformation can be triggered by setting either or the pair of functions and . However, setting both options will raise an error. Examples: Effect of transforming the targets in regression model \"\\n',\n",
       " '\"sklearn_6_1_pipelines_and_composite_estimators 6.1. Pipelines and composite estimators modules/compose.html  6.1.3. FeatureUnion: composite feature spaces  FeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix. When you want to apply different transformations to each field of the data, see the related class sklearn.compose.ColumnTransformer (see user guide ). FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation. FeatureUnion and Pipeline can be combined to create complex models. (A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the callers responsibility.) 6.1.3.1. Usage  A FeatureUnion is built using a list of pairs, where the is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and is an estimator object: Like pipelines, feature unions have a shorthand constructor called make_union that does not require explicit naming of the components. Like , individual steps may be replaced using , and ignored by setting to : Examples: Concatenating multiple feature extraction methods \"\\n',\n",
       " '\"sklearn_6_1_pipelines_and_composite_estimators 6.1. Pipelines and composite estimators modules/compose.html  6.1.4. ColumnTransformer for heterogeneous data  Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using pandas . Processing your data before passing it to scikit-learn might be problematic for one of the following reasons: Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as data leakage ), for example in the case of scalers or imputing missing values. You may want to include the parameters of the preprocessors in a parameter search . The ColumnTransformer helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. ColumnTransformer works on arrays, sparse matrices, and pandas DataFrames . To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method: For this data, we might want to encode the column as a categorical variable using preprocessing.OneHotEncoder but apply a feature_extraction.text.CountVectorizer to the column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say and . By default, the remaining rating columns are ignored ( ): In the above example, the CountVectorizer expects a 1D array as input and therefore the columns were specified as a string ( ). However, preprocessing.OneHotEncoder as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings ( ). Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a make_column_selector . The make_column_selector is used to select columns based on data type or column name: Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns. We can keep the remaining rating columns by setting . The values are appended to the end of the transformation: The parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation: The make_column_transformer function is available to more easily create a ColumnTransformer object. Specifically, the names will be given automatically. The equivalent for the above example would be: \"\\n',\n",
       " '\"sklearn_6_1_pipelines_and_composite_estimators 6.1. Pipelines and composite estimators modules/compose.html  6.1.5. Visualizing Composite Estimators  Estimators can be displayed with a HTML representation when shown in a jupyter notebook. This can be useful to diagnose or visualize a Pipeline with many estimators. This visualization is activated by setting the option in sklearn.set_config : An example of the HTML output can be seen in the HTML representation of Pipeline section of Column Transformer with Mixed Types . As an alternative, the HTML can be written to a file using estimator_html_repr : Examples: Column Transformer with Heterogeneous Data Sources Column Transformer with Mixed Types \"\\n',\n",
       " '\"sklearn_6_2_feature_extraction 6.2. Feature extraction modules/feature_extraction.html  6.2.1. Loading features from dicts  The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python objects to the NumPy/SciPy representation used by scikit-learn estimators. While not particularly fast to process, Pythons has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values. DictVectorizer implements what is called one-of-K or one-hot coding for categorical (aka nominal, discrete) features. Categorical features are attribute-value pairs where the value is restricted to a list of discrete of possibilities without ordering (e.g. topic identifiers, types of objects, tags, names). In the following, city is a categorical attribute while temperature is a traditional numerical feature: DictVectorizer is also a useful representation transformation for training sequence classifiers in Natural Language Processing models that typically work by extracting feature windows around a particular word of interest. For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word sat in the sentence The cat sat on the mat.: This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a text.TfidfTransformer for normalization): As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the class uses a matrix by default instead of a . \"\\n',\n",
       " '\"sklearn_6_2_feature_extraction 6.2. Feature extraction modules/feature_extraction.html  6.2.2. Feature hashing  The class FeatureHasher is a high-speed, low-memory vectorizer that uses a technique known as feature hashing , or the hashing trick. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of FeatureHasher apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no method. Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output features value is zero. This mechanism is enabled by default with and is particularly useful for small hash table sizes ( ). For large hash table sizes, it can be disabled, to allow the output to be passed to estimators like sklearn.naive_bayes.MultinomialNB or sklearn.feature_selection.chi2 feature selectors that expect non-negative inputs. FeatureHasher accepts either mappings (like Pythons and its variants in the module), pairs, or strings, depending on the constructor parameter . Mapping are treated as lists of pairs, while single strings have an implicit value of 1, so is interpreted as . If a single feature occurs multiple times in a sample, the associated values will be summed (so and become ). The output from FeatureHasher is always a matrix in the CSR format. Feature hashing can be employed in document classification, but unlike text.CountVectorizer , FeatureHasher does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see Vectorizing a large text corpus with the hashing trick , below, for a combined tokenizer/hasher. As an example, consider a word-level natural language processing task that needs features extracted from pairs. One could use a Python generator function to extract features: Then, the to be fed to can be constructed using: and fed to a hasher with: to get a matrix . Note the use of a generator comprehension, which introduces laziness into the feature extraction: tokens are only processed on demand from the hasher. 6.2.2.1. Implementation details  FeatureHasher uses the signed 32-bit variant of MurmurHash3. As a result (and because of limitations in ), the maximum number of features supported is currently . The original formulation of the hashing trick by Weinberger et al. used two separate hash functions and to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits. Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the parameter; otherwise the features will not be mapped evenly to the columns. References: Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). Feature hashing for large scale multitask learning . Proc. ICML. MurmurHash3 . \"\\n',\n",
       " '\"sklearn_6_2_feature_extraction 6.2. Feature extraction modules/feature_extraction.html  6.2.3. Text feature extraction  6.2.3.1. The Bag of Words representation  Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length. In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely: tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators. counting the occurrences of tokens in each document. normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents. In this scheme, features and samples are defined as follows: each individual token occurrence frequency (normalized or not) is treated as a feature . the vector of all the token frequencies for a given document is considered a multivariate sample . A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus. We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or Bag of n-grams representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document. 6.2.3.2. Sparsity  As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them). For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually. In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the package. 6.2.3.3. Common Vectorizer usage  CountVectorizer implements both tokenization and occurrence counting in a single class: This model has many parameters, however the default values are quite reasonable (please see the reference documentation for the details): Lets use it to tokenize and count the word occurrences of a minimalistic corpus of text documents: The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly: Each term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows: The converse mapping from feature name to column index is stored in the attribute of the vectorizer: Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method: Note that in the previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. In particular we lose the information that the last document is an interrogative form. To preserve some of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words): The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns: In particular the interrogative form Is this is only present in the last document: 6.2.3.3.1. Using stop words  Stop words are words like and, the, him, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality. There are several known issues in our provided english stop word list. It does not aim to be a general, one-size-fits-all solution as some tasks may require a more custom solution. See [NQY18] for more details. Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as computer . You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word weve is split into we and ve by CountVectorizers default tokenizer, so if weve is in , but ve is not, ve will be retained from weve in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies. References NQY18 J. Nothman, H. Qin and R. Yurchak (2018). Stop Word Lists in Free Open-source Software Packages . In Proc. Workshop for NLP Open Source Software . 6.2.3.4. Tfidf term weighting  In a large text corpus, some words will be very present (e.g. the, a, is in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms. In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tfidf transform. Tf means term-frequency while tfidf means term-frequency times inverse document-frequency : . Using the s default settings, the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as , where is the total number of documents in the document set, and is the number of documents in the document set that contain term . The resulting tf-idf vectors are then normalized by the Euclidean norm: . This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering. The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learns TfidfTransformer and TfidfVectorizer differ slightly from the standard textbook notation that defines the idf as In the TfidfTransformer and TfidfVectorizer with , the 1 count is added to the idf instead of the idfs denominator: This normalization is implemented by the TfidfTransformer class: Again please see the reference documentation for the details on all the parameters. Lets take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents: Each row is normalized to have unit Euclidean norm: For example, we can compute the tf-idf of the first term in the first document in the array as follows: Now, if we repeat this computation for the remaining 2 terms in the document, we get and the vector of raw tf-idfs: Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1: Furthermore, the default parameter adds 1 to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: Using this modification, the tf-idf of the third term in document 1 changes to 1.8473: And the L2-normalized tf-idf changes to : The weights of each feature computed by the method call are stored in a model attribute: As tfidf is very often used for text features, there is also another class called TfidfVectorizer that combines all the options of CountVectorizer and TfidfTransformer in a single model: While the tfidf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the parameter of CountVectorizer . In particular, some estimators such as Bernoulli Naive Bayes explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tfidf values while the binary occurrence info is more stable. As usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by pipelining the feature extractor with a classifier: Sample pipeline for text feature extraction and evaluation 6.2.3.5. Decoding text files  Text is made of characters, but files are made of bytes. These bytes represent characters according to some encoding . To work with text files in Python, their bytes must be decoded to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist. Note An encoding can also be called a character set, but this term is less accurate: several encodings can exist for a single character set. The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The CountVectorizer takes an parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default ( ). If the text you are loading is not actually encoded with UTF-8, however, you will get a . The vectorizers can be told to be silent about decoding errors by setting the parameter to either or . See the documentation for the Python function for more details (type at the Python prompt). If you are having trouble decoding text, here are some things to try: Find out what the actual encoding of the text is. The file might come with a header or README that tells you the encoding, or there might be some standard encoding you can assume based on where the text comes from. You may be able to find out what kind of encoding it is in general using the UNIX command . The Python module comes with a script called that will guess the specific encoding, though you cannot rely on its guess being correct. You could try UTF-8 and disregard the errors. You can decode byte strings with to replace all decoding errors with a meaningless character, or set in the vectorizer. This may damage the usefulness of your features. Real text may come from a variety of sources that may have used different encodings, or even be sloppily decoded in a different encoding than the one it was encoded with. This is common in text retrieved from the Web. The Python package ftfy can automatically sort out some classes of decoding errors, so you could try decoding the unknown text as and then using to fix errors. If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as . Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature. For example, the following snippet uses (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here. (Depending on the version of , it might get the first one wrong.) For an introduction to Unicode and character encodings in general, see Joel Spolskys Absolute Minimum Every Software Developer Must Know About Unicode . 6.2.3.6. Applications and examples  The bag of words representation is quite simplistic but surprisingly useful in practice. In particular in a supervised setting it can be successfully combined with fast and scalable linear models to train document classifiers , for instance: Classification of text documents using sparse features In an unsupervised setting it can be used to group similar documents together by applying clustering algorithms such as K-means : Clustering text documents using k-means Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering, for instance by using Non-negative matrix factorization (NMF or NNMF) : Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation 6.2.3.7. Limitations of the Bag of Words representation  A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesnt account for potential misspellings or word derivations. N-grams to the rescue! Instead of building a simple collection of unigrams (n1), one might prefer a collection of bigrams (n2), where occurrences of pairs of consecutive words are counted. One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations. For example, lets say were dealing with a corpus of two documents: . The second document contains a misspelling of the word words. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better: In the above example, analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The analyzer, alternatively, creates n-grams that span across words: The word boundaries-aware variant is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations. While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried by that internal structure. In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as Structured output problems which are currently outside of the scope of scikit-learn. 6.2.3.8. Vectorizing a large text corpus with the hashing trick  The above vectorization scheme is simple but the fact that it holds an in- memory mapping from the string tokens to the integer feature indices (the attribute) causes several problems when dealing with large datasets : the larger the corpus, the larger the vocabulary will grow and hence the memory use too, fitting requires the allocation of intermediate data structures of size proportional to that of the original dataset. building the word-mapping requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner. pickling and un-pickling vectorizers with a large can be very slow (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size), it is not easily possible to split the vectorization work into concurrent sub tasks as the attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers performance to the point of making them slower than the sequential variant. It is possible to overcome those limitations by combining the hashing trick ( Feature hashing ) implemented by the sklearn.feature_extraction.FeatureHasher class and the text preprocessing and tokenization features of the CountVectorizer . This combination is implementing in HashingVectorizer , a transformer class that is mostly API compatible with CountVectorizer . HashingVectorizer is stateless, meaning that you dont have to call on it: You can see that 16 non-zero feature tokens were extracted in the vector output: this is less than the 19 non-zeros extracted previously by the CountVectorizer on the same toy corpus. The discrepancy comes from hash function collisions because of the low value of the parameter. In a real world setting, the parameter can be left to its default value of (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as might help without introducing too many additional collisions on typical text classification tasks. Note that the dimensionality does not affect the CPU training time of algorithms which operate on CSR matrices ( , , , ) but it does for algorithms that work with CSC matrices ( , , etc). Lets try again with the default setting: We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of course, other terms than the 19 used here might still collide with each other. The HashingVectorizer also comes with the following limitations: it is not possible to invert the model (no method), nor to access the original string representation of the features, because of the one-way nature of the hash function that performs the mapping. it does not provide IDF weighting as that would introduce statefulness in the model. A TfidfTransformer can be appended to it in a pipeline if required. 6.2.3.9. Performing out-of-core scaling with HashingVectorizer  An interesting development of using a HashingVectorizer is the ability to perform out-of-core scaling. This means that we can learn from data that does not fit into the computers main memory. A strategy to implement out-of-core scaling is to stream data to the estimator in mini-batches. Each mini-batch is vectorized using HashingVectorizer so as to guarantee that the input space of the estimator has always the same dimensionality. The amount of memory used at any time is thus bounded by the size of a mini-batch. Although there is no limit to the amount of data that can be ingested using such an approach, from a practical point of view the learning time is often limited by the CPU time one wants to spend on the task. For a full-fledged example of out-of-core scaling in a text classification task see Out-of-core classification of text documents . 6.2.3.10. Customizing the vectorizer classes  It is possible to customize the behavior by passing a callable to the vectorizer constructor: In particular we name: : a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc. : a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these. : a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps. (Lucene users might recognize these names, but be aware that scikit-learn concepts may not map one-to-one onto Lucene concepts.) To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the class and override the , and factory methods instead of passing custom functions. Some tips and tricks: If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Heres a with a tokenizer and lemmatizer using NLTK : (Note that this will not filter out punctuation.) The following example will, for instance, transform some British spelling to American spelling: for other styles of preprocessing; examples include stemming, lemmatization, or normalizing numerical tokens, with the latter illustrated in: Biclustering documents with the Spectral Co-clustering algorithm Customizing the vectorizer can also be useful when handling Asian languages that do not use an explicit word separator such as whitespace. \"\\n',\n",
       " '\"sklearn_6_2_feature_extraction 6.2. Feature extraction modules/feature_extraction.html  6.2.4. Image feature extraction  6.2.4.1. Patch extraction  The extract_patches_2d function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use reconstruct_from_patches_2d . For example let use generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format): Let us now try to reconstruct the original image from the patches by averaging on overlapping areas: The PatchExtractor class works in the same way as extract_patches_2d , only it supports multiple images as input. It is implemented as an estimator, so it can be used in pipelines. See: 6.2.4.2. Connectivity graph of an image  Several estimators in the scikit-learn can use connectivity information between features or samples. For instance Ward clustering ( Hierarchical clustering ) can cluster together only neighboring pixels of an image, thus forming contiguous patches: For this purpose, the estimators use a connectivity matrix, giving which samples are connected. The function img_to_graph returns such a matrix from a 2D or 3D image. Similarly, grid_to_graph build a connectivity matrix for images given the shape of these image. These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering ( Hierarchical clustering ), but also to build precomputed kernels, or similarity matrices. Note Examples A demo of structured Ward hierarchical clustering on an image of coins Spectral clustering for image segmentation Feature agglomeration vs. univariate selection \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.1. Standardization, or mean removal and variance scaling  Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance . In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. The function scale provides a quick and easy way to perform this operation on a single array-like dataset: Scaled data has zero mean and unit variance: The module further provides a utility class StandardScaler that implements the API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline : The scaler instance can then be used on new data to transform it the same way it did on the training set: It is possible to disable either centering or scaling by either passing or to the constructor of StandardScaler . 6.3.1.1. Scaling features to a range  An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler , respectively. The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data. Here is an example to scale a toy data matrix to the range: The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data: It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data: If MinMaxScaler is given an explicit the full formula is: MaxAbsScaler works in a very similar fashion, but scales in a way that the training data lies within the range by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data. Here is how to use the toy data from the previous example with this scaler: As with scale , the module further provides convenience functions minmax_scale and maxabs_scale if you dont want to create an object. 6.3.1.2. Scaling sparse data  Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales. MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go about this. However, scale and StandardScaler can accept matrices as input, as long as is explicitly passed to the constructor. Otherwise a will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. RobustScaler cannot be fitted to sparse inputs, but you can use the method on sparse inputs. Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see and ). Any other sparse input will be converted to the Compressed Sparse Rows representation . To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream. Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the method of sparse matrices is another option. 6.3.1.3. Scaling data with outliers  If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use robust_scale and RobustScaler as drop-in replacements instead. They use more robust estimates for the center and range of your data. References: Further discussion on the importance of centering and scaling data is available on this FAQ: Should I normalize/standardize/rescale the data? Scaling vs Whitening It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use sklearn.decomposition.PCA with to further remove the linear correlation across features. Scaling a 1D array All above functions (i.e. scale , minmax_scale , maxabs_scale , and robust_scale ) accept 1D array which can be useful in some specific case. 6.3.1.4. Centering kernel matrices  If you have a kernel matrix of a kernel that computes a dot product in a feature space defined by function , a KernelCenterer can transform the kernel matrix so that it contains inner products in the feature space defined by followed by removal of the mean in that space. \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.2. Non-linear transformation  Two types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature. Quantile transforms put all features into the same desired distribution based on the formula where is the cumulative distribution function of the feature and the quantile function of the desired output distribution . This formula is using the two following facts: (i) if is a random variable with a continuous cumulative distribution function then is uniformly distributed on ; (ii) if is a random variable with uniform distribution on then has distribution . By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features. Power transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution. 6.3.2.1. Mapping to a Uniform distribution  QuantileTransformer and quantile_transform provide a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1: This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined: This can be confirmed on a independent testing set with similar remarks: 6.3.2.2. Mapping to a Gaussian distribution  In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness. PowerTransformer currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform. The Yeo-Johnson transform is given by: \\\\[\\\\begin{split}x_i^{(\\\\lambda)}  \\\\begin{cases} [(x_i + 1)^\\\\lambda - 1] / \\\\lambda & \\\\text{if } \\\\lambda \\\\neq 0, x_i \\\\geq 0, \\\\\\\\[8pt] \\\\ln{(x_i + 1)} & \\\\text{if } \\\\lambda  0, x_i \\\\geq 0 \\\\\\\\[8pt] -[(-x_i + 1)^{2 - \\\\lambda} - 1] / (2 - \\\\lambda) & \\\\text{if } \\\\lambda \\\\neq 2, x_i < 0, \\\\\\\\[8pt] - \\\\ln (- x_i + 1) & \\\\text{if } \\\\lambda  2, x_i < 0 \\\\end{cases}\\\\end{split}\\\\] while the Box-Cox transform is given by: \\\\[\\\\begin{split}x_i^{(\\\\lambda)}  \\\\begin{cases} \\\\dfrac{x_i^\\\\lambda - 1}{\\\\lambda} & \\\\text{if } \\\\lambda \\\\neq 0, \\\\\\\\[8pt] \\\\ln{(x_i)} & \\\\text{if } \\\\lambda  0, \\\\end{cases}\\\\end{split}\\\\] Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by , which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution: While the above example sets the option to , PowerTransformer will apply zero-mean, unit-variance normalization to the transformed output by default. Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation. It is also possible to map data to a normal distribution using QuantileTransformer by setting . Using the earlier example with the iris dataset: Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the inputs minimum and maximum  corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively  do not become infinite under the transformation. \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.3. Normalization  Normalization is the process of scaling individual samples to have unit norm . This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the or norms: The module further provides a utility class Normalizer that implements the same operation using the API (even though the method is useless in this case: the class is stateless as this operation treats samples independently). This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline : The normalizer instance can then be used on sample vectors as any transformer: Note: L2 normalization is also known as spatial sign preprocessing. Sparse input normalize and Normalizer accept both dense array-like and sparse matrices from scipy.sparse as input . For sparse input the data is converted to the Compressed Sparse Rows representation (see ) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream. \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.4. Encoding categorical features  Often features are not given as continuous values but categorical. For example a person could have features , , . Such features can be efficiently coded as integers, for instance could be expressed as while would be . To convert categorical features to such integer codes, we can use the OrdinalEncoder . This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1): Such integer representation can, however, not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily). Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder , which transforms each categorical feature with possible values into binary features, with one of them 1, and all others 0. Continuing the example above: By default, the values each feature can take is inferred automatically from the dataset and can be found in the attribute: It is possible to specify this explicitly using the parameter . There are two genders, four possible continents and four web browsers in our dataset: If there is a possibility that the training data might have missing categorical features, it can often be better to specify instead of setting the manually as above. When is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros ( is only supported for one-hot encoding): It is also possible to encode each column into columns instead of columns by using the parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression ( LinearRegression ), since co-linearity would cause the covariance matrix to be non-invertible. When this parameter is not None, must be set to : One might want to drop one of the two columns only for features with 2 categories. In this case, you can set the parameter . In the transformed , the first column is the encoding of the feature with categories male/female, while the remaining 6 columns is the encoding of the 2 features with respectively 3 categories each. See Loading features from dicts for categorical features that are represented as a dict, not as scalars. \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.5. Discretization  Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models. 6.3.5.1. K-bins discretization  KBinsDiscretizer discretizes features into bins: By default the output is one-hot encoded into a sparse matrix (See Encoding categorical features ) and this can be configured with the parameter. For each feature, the bin edges are computed during and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as: feature 1: feature 2: feature 3: Based on these bin intervals, is transformed as follows: The resulting dataset contains ordinal attributes which can be further used in a sklearn.pipeline.Pipeline . Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. KBinsDiscretizer implements different binning strategies, which can be selected with the parameter. The uniform strategy uses constant-width bins. The quantile strategy uses the quantiles values to have equally populated bins in each feature. The kmeans strategy defines bins based on a k-means clustering procedure performed on each feature independently. Examples: Using KBinsDiscretizer to discretize continuous features Feature discretization Demonstrating the different strategies of KBinsDiscretizer 6.3.5.2. Feature binarization  Feature binarization is the process of thresholding numerical features to get boolean values . This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution . For instance, this is the case for the sklearn.neural_network.BernoulliRBM . It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice. As for the Normalizer , the utility class Binarizer is meant to be used in the early stages of sklearn.pipeline.Pipeline . The method does nothing as each sample is treated independently of others: It is possible to adjust the threshold of the binarizer: As for the StandardScaler and Normalizer classes, the preprocessing module provides a companion function binarize to be used when the transformer API is not necessary. Note that the Binarizer is similar to the KBinsDiscretizer when , and when the bin edge is at the value . Sparse input binarize and Binarizer accept both dense array-like and sparse matrices from scipy.sparse as input . For sparse input the data is converted to the Compressed Sparse Rows representation (see ). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream. \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.6. Imputation of missing values  Tools for imputing missing values are discussed at Imputation of missing values . \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.7. Generating polynomial features  Often its useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features high-order and interaction terms. It is implemented in PolynomialFeatures : The features of X have been transformed from to . In some cases, only interaction terms among features are required, and it can be gotten with the setting : The features of X have been transformed from to . Note that polynomial features are used implicitly in kernel methods (e.g., sklearn.svm.SVC , sklearn.decomposition.KernelPCA ) when using polynomial Kernel functions . See Polynomial interpolation for Ridge regression using created polynomial features. \"\\n',\n",
       " '\"sklearn_6_3_preprocessing_data 6.3. Preprocessing data modules/preprocessing.html  6.3.8. Custom transformers  Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with FunctionTransformer . For example, to build a transformer that applies a log transformation in a pipeline, do: You can ensure that and are the inverse of each other by setting and calling before . Please note that a warning is raised and can be turned into an error with a : For a full code example that demonstrates using a FunctionTransformer to do custom feature selection, see Using FunctionTransformer to select columns \"\\n',\n",
       " '\"sklearn_6_4_imputation_of_missing_values 6.4. Imputation of missing values modules/impute.html  6.4.1. Univariate vs. Multivariate Imputation  One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. impute.SimpleImputer ). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer ). \"\\n',\n",
       " '\"sklearn_6_4_imputation_of_missing_values 6.4. Imputation of missing values modules/impute.html  6.4.2. Univariate feature imputation  The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings. The following snippet demonstrates how to replace missing values, encoded as , using the mean value of the columns (axis 0) that contain the missing values: The SimpleImputer class also supports sparse matrices: Note that this format is not meant to be used to implicitly store missing values in the matrix because it would densify it at transform time. Missing values encoded by 0 must be used with dense input. The SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the or strategy: \"\\n',\n",
       " '\"sklearn_6_4_imputation_of_missing_values 6.4. Imputation of missing values modules/impute.html  6.4.3. Multivariate feature imputation  A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output and the other feature columns are treated as inputs . A regressor is fit on for known . Then, the regressor is used to predict the missing values of . This is done for each feature in an iterative fashion, and then is repeated for imputation rounds. The results of the final imputation round are returned. Note This estimator is still experimental for now: default parameters or details of behaviour might change without any deprecation cycle. Resolving the following issues would help stabilize IterativeImputer : convergence criteria ( #14338 ), default estimators ( #13286 ), and use of random state ( #15611 ). To use it, you need to explicitly import . Both SimpleImputer and IterativeImputer can be used in a Pipeline as a way to build a composite estimator that supports imputation. See Imputing missing values before building an estimator . 6.4.3.1. Flexibility of IterativeImputer  There are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with IterativeImputer by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See Imputing missing values with variants of IterativeImputer . 6.4.3.2. Multiple vs. Single Imputation  In the statistics community, it is common practice to perform multiple imputations, generating, for example, separate imputations for a single feature matrix. Each of these imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation. Our implementation of IterativeImputer was inspired by the R MICE package (Multivariate Imputation by Chained Equations) 1 , but differs from it by returning a single imputation instead of multiple imputations. However, IterativeImputer can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when . See 2 , chapter 4 for more discussion on multiple vs. single imputations. It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values. Note that a call to the method of IterativeImputer is not allowed to change the number of samples. Therefore multiple imputations cannot be achieved by a single call to . \"\\n',\n",
       " '\"sklearn_6_4_imputation_of_missing_values 6.4. Imputation of missing values modules/impute.html  6.4.4. References  1 Stef van Buuren, Karin Groothuis-Oudshoorn (2011). mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software 45: 1-67. 2 Roderick J A Little and Donald B Rubin (1986). Statistical Analysis with Missing Data. John Wiley & Sons, Inc., New York, NY, USA. \"\\n',\n",
       " '\"sklearn_6_4_imputation_of_missing_values 6.4. Imputation of missing values modules/impute.html  6.4.5. Nearest neighbors imputation  The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances , is used to find the nearest neighbors. Each missing feature is imputed using values from nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during . For more information on the methodology, see ref. [OL2001] . The following snippet demonstrates how to replace missing values, encoded as , using the mean feature value of the two nearest neighbors of samples with missing values: OL2001 Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17 no. 6, 2001 Pages 520-525. \"\\n',\n",
       " '\"sklearn_6_4_imputation_of_missing_values 6.4. Imputation of missing values modules/impute.html  6.4.6. Marking imputed values  The MissingIndicator transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. Note that both the SimpleImputer and IterativeImputer have the boolean parameter ( by default) which when set to provides a convenient way of stacking the output of the MissingIndicator transformer with the output of the imputer. is usually used as the placeholder for missing values. However, it enforces the data type to be float. The parameter allows to specify other placeholder such as integer. In the following example, we will use as missing values: The parameter is used to choose the features for which the mask is constructed. By default, it is which returns the imputer mask of the features containing missing values at time: The parameter can be set to to return all features whether or not they contain missing values: When using the MissingIndicator in a Pipeline , be sure to use the FeatureUnion or ColumnTransformer to add the indicator features to the regular features. First we obtain the dataset, and add some missing values to it. Now we create a FeatureUnion . All features will be imputed using SimpleImputer , in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator . Of course, we cannot use the transformer to make any predictions. We should wrap this in a Pipeline with a classifier (e.g., a DecisionTreeClassifier ) to be able to make predictions. \"\\n',\n",
       " '\"sklearn_6_5_unsupervised_dimensionality_reduction 6.5. Unsupervised dimensionality reduction modules/unsupervised_reduction.html  6.5.1. PCA: principal component analysis  decomposition.PCA looks for a combination of features that capture well the variance of the original features. See Decomposing signals in components (matrix factorization problems) . Examples Faces recognition example using eigenfaces and SVMs \"\\n',\n",
       " '\"sklearn_6_5_unsupervised_dimensionality_reduction 6.5. Unsupervised dimensionality reduction modules/unsupervised_reduction.html  6.5.2. Random projections  The module: random_projection provides several tools for data reduction by random projections. See the relevant section of the documentation: Random Projection . Examples The Johnson-Lindenstrauss bound for embedding with random projections \"\\n',\n",
       " '\"sklearn_6_5_unsupervised_dimensionality_reduction 6.5. Unsupervised dimensionality reduction modules/unsupervised_reduction.html  6.5.3. Feature agglomeration  cluster.FeatureAgglomeration applies Hierarchical clustering to group together features that behave similarly. Examples Feature agglomeration vs. univariate selection Feature agglomeration Feature scaling Note that if features have very different scaling or statistical properties, cluster.FeatureAgglomeration may not be able to capture the links between related features. Using a preprocessing.StandardScaler can be useful in these settings. \"\\n',\n",
       " '\"sklearn_6_6_random_projection 6.6. Random Projection modules/random_projection.html  6.6.1. The Johnson-Lindenstrauss lemma  The main theoretical result behind the efficiency of random projection is the Johnson-Lindenstrauss lemma (quoting Wikipedia) : In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection. Knowing only the number of samples, the sklearn.random_projection.johnson_lindenstrauss_min_dim estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection: Example: See The Johnson-Lindenstrauss bound for embedding with random projections for a theoretical explication on the Johnson-Lindenstrauss lemma and an empirical validation using sparse random matrices. References: Sanjoy Dasgupta and Anupam Gupta, 1999. An elementary proof of the Johnson-Lindenstrauss Lemma. \"\\n',\n",
       " '\"sklearn_6_6_random_projection 6.6. Random Projection modules/random_projection.html  6.6.2. Gaussian random projection  The sklearn.random_projection.GaussianRandomProjection reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution . Here a small excerpt which illustrates how to use the Gaussian random projection transformer: \"\\n',\n",
       " '\"sklearn_6_6_random_projection 6.6. Random Projection modules/random_projection.html  6.6.3. Sparse random projection  The sklearn.random_projection.SparseRandomProjection reduces the dimensionality by projecting the original input space using a sparse random matrix. Sparse random matrices are an alternative to dense Gaussian random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data. If we define , the elements of the random matrix are drawn from \\\\[\\\\begin{split}\\\\left\\\\{ \\\\begin{array}{c c l} -\\\\sqrt{\\\\frac{s}{n_{\\\\text{components}}}} & & 1 / 2s\\\\\\\\ 0 &\\\\text{with probability} & 1 - 1 / s \\\\\\\\ +\\\\sqrt{\\\\frac{s}{n_{\\\\text{components}}}} & & 1 / 2s\\\\\\\\ \\\\end{array} \\\\right.\\\\end{split}\\\\] where is the size of the projected subspace. By default the density of non zero elements is set to the minimum density as recommended by Ping Li et al.: . Here a small excerpt which illustrates how to use the sparse random projection transformer: References: D. Achlioptas. 2003. Database-friendly random projections: Johnson-Lindenstrauss with binary coins . Journal of Computer and System Sciences 66 (2003) 671687 Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006. Very sparse random projections. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 06). ACM, New York, NY, USA, 287-296. \"\\n',\n",
       " '\"sklearn_6_7_kernel_approximation 6.7. Kernel Approximation modules/kernel_approximation.html  6.7.1. Nystroem Method for Kernel Approximation  The Nystroem method, as implemented in Nystroem is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default Nystroem uses the kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter . \"\\n',\n",
       " '\"sklearn_6_7_kernel_approximation 6.7. Kernel Approximation modules/kernel_approximation.html  6.7.2. Radial Basis Function Kernel  The RBFSampler constructs an approximate mapping for the radial basis function kernel, also known as Random Kitchen Sinks [RR2007] . This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM: The mapping relies on a Monte Carlo approximation to the kernel values. The function performs the Monte Carlo sampling, whereas the method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the function. The function takes two arguments: , which is the target dimensionality of the feature transform, and , the parameter of the RBF-kernel. A higher will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that fitting the feature function does not actually depend on the data given to the function. Only the dimensionality of the data is used. Details on the method can be found in [RR2007] . For a given value of RBFSampler is often less accurate as Nystroem . RBFSampler is cheaper to compute, though, making use of larger feature spaces more efficient. Comparing an exact RBF kernel (left) with the approximation (right)  Examples: Explicit feature map approximation for RBF kernels \"\\n',\n",
       " '\"sklearn_6_7_kernel_approximation 6.7. Kernel Approximation modules/kernel_approximation.html  6.7.3. Additive Chi Squared Kernel  The additive chi squared kernel is a kernel on histograms, often used in computer vision. The additive chi squared kernel as used here is given by \\\\[k(x, y)  \\\\sum_i \\\\frac{2x_iy_i}{x_i+y_i}\\\\] This is not exactly the same as sklearn.metrics.additive_chi2_kernel . The authors of [VZ2010] prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling. The class AdditiveChi2Sampler implements this component wise deterministic sampling. Each component is sampled times, yielding dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, is usually chosen to be 1 or 2, transforming the dataset to size (in the case of ). The approximate feature map provided by AdditiveChi2Sampler can be combined with the approximate feature map provided by RBFSampler to yield an approximate feature map for the exponentiated chi squared kernel. See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler . \"\\n',\n",
       " '\"sklearn_6_7_kernel_approximation 6.7. Kernel Approximation modules/kernel_approximation.html  6.7.4. Skewed Chi Squared Kernel  The skewed chi squared kernel is given by: \\\\[k(x,y)  \\\\prod_i \\\\frac{2\\\\sqrt{x_i+c}\\\\sqrt{y_i+c}}{x_i + y_i + 2c}\\\\] It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map. The usage of the SkewedChi2Sampler is the same as the usage described above for the RBFSampler . The only difference is in the free parameter, that is called . For a motivation for this mapping and the mathematical details see [LS2010] . \"\\n',\n",
       " '\"sklearn_6_7_kernel_approximation 6.7. Kernel Approximation modules/kernel_approximation.html  6.7.5. Mathematical Details  Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function (a so called Mercer kernel), it is guaranteed that there exists a mapping into a Hilbert space , such that \\\\[k(x,y)  \\\\langle \\\\phi(x), \\\\phi(y) \\\\rangle\\\\] Where denotes the inner product in the Hilbert space. If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points , one may use the value of , which corresponds to applying the algorithm to the mapped data points . The advantage of using is that the mapping never has to be calculated explicitly, allowing for arbitrary large features (even infinite). One drawback of kernel methods is, that it might be necessary to store many kernel values during optimization. If a kernelized classifier is applied to new data , needs to be computed to make predictions, possibly for many different in the training set. The classes in this submodule allow to approximate the embedding , thereby working explicitly with the representations , which obviates the need to apply the kernel or store training examples. References: RR2007 ( 1 , 2 ) Random features for large-scale kernel machines Rahimi, A. and Recht, B. - Advances in neural information processing 2007, LS2010 Random Fourier approximations for skewed multiplicative histogram kernels Random Fourier approximations for skewed multiplicative histogram kernels - Lecture Notes for Computer Sciencd (DAGM) VZ2010 ( 1 , 2 ) Efficient additive kernels via explicit feature maps Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010 VVZ2010 Generalized RBF feature maps for Efficient Detection Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010 \"\\n',\n",
       " '\"sklearn_6_8_pairwise_metrics_affinities_and_kernels 6.8. Pairwise metrics, Affinities and Kernels modules/metrics.html  6.8.1. Cosine similarity  cosine_similarity computes the L2-normalized dot product of vectors. That is, if and are row vectors, their cosine similarity is defined as: \\\\[k(x, y)  \\\\frac{x y^\\\\top}{\\\\|x\\\\| \\\\|y\\\\|}\\\\] This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors. This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. cosine_similarity accepts matrices. (Note that the tf-idf functionality in can produce normalized vectors, in which case cosine_similarity is equivalent to linear_kernel , only slower.) References: C.D. Manning, P. Raghavan and H. Schtze (2008). Introduction to Information Retrieval. Cambridge University Press. https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html \"\\n',\n",
       " '\"sklearn_6_8_pairwise_metrics_affinities_and_kernels 6.8. Pairwise metrics, Affinities and Kernels modules/metrics.html  6.8.2. Linear kernel  The function linear_kernel computes the linear kernel, that is, a special case of polynomial_kernel with and (homogeneous). If and are column vectors, their linear kernel is: \\\\[k(x, y)  x^\\\\top y\\\\] \"\\n',\n",
       " '\"sklearn_6_8_pairwise_metrics_affinities_and_kernels 6.8. Pairwise metrics, Affinities and Kernels modules/metrics.html  6.8.3. Polynomial kernel  The function polynomial_kernel computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction. The polynomial kernel is defined as: \\\\[k(x, y)  (\\\\gamma x^\\\\top y +c_0)^d\\\\] where: , are the input vectors is the kernel degree If the kernel is said to be homogeneous. \"\\n',\n",
       " '\"sklearn_6_8_pairwise_metrics_affinities_and_kernels 6.8. Pairwise metrics, Affinities and Kernels modules/metrics.html  6.8.4. Sigmoid kernel  The function sigmoid_kernel computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as: \\\\[k(x, y)  \\\\tanh( \\\\gamma x^\\\\top y + c_0)\\\\] where: , are the input vectors is known as slope is known as intercept \"\\n',\n",
       " '\"sklearn_6_8_pairwise_metrics_affinities_and_kernels 6.8. Pairwise metrics, Affinities and Kernels modules/metrics.html  6.8.5. RBF kernel  The function rbf_kernel computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as: \\\\[k(x, y)  \\\\exp( -\\\\gamma \\\\| x-y \\\\|^2)\\\\] where and are the input vectors. If the kernel is known as the Gaussian kernel of variance . \"\\n',\n",
       " '\"sklearn_6_8_pairwise_metrics_affinities_and_kernels 6.8. Pairwise metrics, Affinities and Kernels modules/metrics.html  6.8.6. Laplacian kernel  The function laplacian_kernel is a variant on the radial basis function kernel defined as: \\\\[k(x, y)  \\\\exp( -\\\\gamma \\\\| x-y \\\\|_1)\\\\] where and are the input vectors and is the Manhattan distance between the input vectors. It has proven useful in ML applied to noiseless data. See e.g. Machine learning for quantum mechanics in a nutshell . \"\\n',\n",
       " '\"sklearn_6_8_pairwise_metrics_affinities_and_kernels 6.8. Pairwise metrics, Affinities and Kernels modules/metrics.html  6.8.7. Chi-squared kernel  The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using chi2_kernel and then passed to an sklearn.svm.SVC with : It can also be directly used as the argument: The chi squared kernel is given by \\\\[k(x, y)  \\\\exp \\\\left (-\\\\gamma \\\\sum_i \\\\frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \\\\right )\\\\] The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions. The chi squared kernel is most commonly used on histograms (bags) of visual words. References: Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf \"\\n',\n",
       " '\"sklearn_6_9_transforming_the_prediction_target_y 6.9. Transforming the prediction target (y) modules/preprocessing_targets.html  6.9.1. Label binarization  LabelBinarizer is a utility class to help create a label indicator matrix from a list of multi-class labels: For multiple labels per instance, use MultiLabelBinarizer : \"\\n',\n",
       " '\"sklearn_6_9_transforming_the_prediction_target_y 6.9. Transforming the prediction target (y) modules/preprocessing_targets.html  6.9.2. Label encoding  LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1. This is sometimes useful for writing efficient Cython routines. LabelEncoder can be used as follows: It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels: \"\\n',\n",
       " '\"sklearn_7_1_general_dataset_api 7.1. General dataset API datasets/index.html#general-dataset-api  7.1. General dataset API  There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset. The dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section. The dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section. Both loaders and fetchers functions return a sklearn.utils.Bunch object holding at least two items: an array of shape * with key (except for 20newsgroups) and a numpy array of length , containing the target values, with key . The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see sklearn.utils.Bunch : Its also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the parameter to . The datasets also contain a full description in their attribute and some contain and . See the dataset descriptions below for details. The dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section. These functions return a tuple consisting of a * numpy array and an array of length containing the targets . In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section. \"\\n',\n",
       " '\"sklearn_7_1_general_dataset_api 7.1. General dataset API datasets/index.html#general-dataset-api  7.2. Toy datasets  scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. They can be loaded using the following functions: load_boston (*[,\\xa0return_X_y]) Load and return the boston house-prices dataset (regression). load_iris (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the iris dataset (classification). load_diabetes (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the diabetes dataset (regression). load_digits (*[,\\xa0n_class,\\xa0return_X_y,\\xa0as_frame]) Load and return the digits dataset (classification). load_linnerud (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the physical excercise linnerud dataset. load_wine (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the wine dataset (classification). load_breast_cancer (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the breast cancer wisconsin dataset (classification). These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks. 7.2.1. Boston house prices dataset  Data Set Characteristics: Number of Instances 506 Number of Attributes 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. Attribute Information (in order) CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable ( 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per $10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT % lower status of the population MEDV Median value of owner-occupied homes in $1000s Missing Attribute Values None Creator Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, Regression diagnostics , Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. References Belsley, Kuh & Welsch, Regression diagnostics: Identifying Influential Data and Sources of Collinearity, Wiley, 1980. 244-261. Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. 7.2.2. Iris plants dataset  Data Set Characteristics: Number of Instances 150 (50 in each of three classes) Number of Attributes 4 numeric, predictive attributes and the class Attribute Information sepal length in cm sepal width in cm petal length in cm petal width in cm class: Iris-Setosa Iris-Versicolour Iris-Virginica Summary Statistics sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) Missing Attribute Values None Class Distribution 33.3% for each of 3 classes. Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fishers paper. Note that its the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fishers paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. References Fisher, R.A. The use of multiple measurements in taxonomic problems Annual Eugenics, 7, Part II, 179-188 (1936); also in Contributions to Mathematical Statistics (John Wiley, NY, 1950). Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. Dasarathy, B.V. (1980) Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. Gates, G.W. (1972) The Reduced Nearest Neighbor Rule. IEEE Transactions on Information Theory, May 1972, 431-433. See also: 1988 MLC Proceedings, 54-64. Cheeseman et als AUTOCLASS II conceptual clustering system finds 3 classes in the data. Many, many more  7.2.3. Diabetes dataset  Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n  442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. Data Set Characteristics: Number of Instances 442 Number of Attributes First 10 columns are numeric predictive values Target Column 11 is a quantitative measure of disease progression one year after baseline Attribute Information age age in years sex bmi body mass index bp average blood pressure s1 tc, T-Cells (a type of white blood cells) s2 ldl, low-density lipoproteins s3 hdl, high-density lipoproteins s4 tch, thyroid stimulating hormone s5 ltg, lamotrigine s6 glu, blood sugar level Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times (i.e. the sum of squares of each column totals 1). Source URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) Least Angle Regression, Annals of Statistics (with discussion), 407-499. ( https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf ) 7.2.4. Optical recognition of handwritten digits dataset  Data Set Characteristics: Number of Instances 5620 Number of Attributes 64 Attribute Information 8x8 image of integer pixels in the range 0..16. Missing Attribute Values None Creator Alpaydin (alpaydin @ boun.edu.tr) Date July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. References C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000. 7.2.5. Linnerrud dataset  Data Set Characteristics: Number of Instances 20 Number of Attributes 3 Missing Attribute Values None The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club: physiological - CSV containing 20 observations on 3 physiological variables: Weight, Waist and Pulse. exercise - CSV containing 20 observations on 3 exercise variables: Chins, Situps and Jumps. References Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. 7.2.6. Wine recognition dataset  Data Set Characteristics: Number of Instances 178 (50 in each of three classes) Number of Attributes 13 numeric, predictive attributes and the class Attribute Information Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline class: class_0 class_1 class_2 Summary Statistics Alcohol: 11.0 14.8 13.0 0.8 Malic Acid: 0.74 5.80 2.34 1.12 Ash: 1.36 3.23 2.36 0.27 Alcalinity of Ash: 10.6 30.0 19.5 3.3 Magnesium: 70.0 162.0 99.7 14.3 Total Phenols: 0.98 3.88 2.29 0.63 Flavanoids: 0.34 5.08 2.03 1.00 Nonflavanoid Phenols: 0.13 0.66 0.36 0.12 Proanthocyanins: 0.41 3.58 1.59 0.57 Colour Intensity: 1.3 13.0 5.1 2.3 Hue: 0.48 1.71 0.96 0.23 OD280/OD315 of diluted wines: 1.27 4.00 2.61 0.71 Proline: 278 1680 746 315 Missing Attribute Values None Class Distribution class_0 (59), class_1 (71), class_2 (48) Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 This is a copy of UCI ML Wine recognition datasets. https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine. Original Owners: Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Citation: Lichman, M. (2013). UCI Machine Learning Repository [ https://archive.ics.uci.edu/ml ]. Irvine, CA: University of California, School of Information and Computer Science. References (1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics). The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique) (2) S. Aeberhard, D. Coomans and O. de Vel, THE CLASSIFICATION PERFORMANCE OF RDA Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics). 7.2.7. Breast cancer wisconsin (diagnostic) dataset  Data Set Characteristics: Number of Instances 569 Number of Attributes 30 numeric, predictive attributes and the class Attribute Information radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (coastline approximation - 1) The mean, standard error, and worst or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius. class: WDBC-Malignant WDBC-Benign Summary Statistics radius (mean): 6.981 28.11 texture (mean): 9.71 39.28 perimeter (mean): 43.79 188.5 area (mean): 143.5 2501.0 smoothness (mean): 0.053 0.163 compactness (mean): 0.019 0.345 concavity (mean): 0.0 0.427 concave points (mean): 0.0 0.201 symmetry (mean): 0.106 0.304 fractal dimension (mean): 0.05 0.097 radius (standard error): 0.112 2.873 texture (standard error): 0.36 4.885 perimeter (standard error): 0.757 21.98 area (standard error): 6.802 542.2 smoothness (standard error): 0.002 0.031 compactness (standard error): 0.002 0.135 concavity (standard error): 0.0 0.396 concave points (standard error): 0.0 0.053 symmetry (standard error): 0.008 0.079 fractal dimension (standard error): 0.001 0.03 radius (worst): 7.93 36.04 texture (worst): 12.02 49.54 perimeter (worst): 50.41 251.2 area (worst): 185.2 4254.0 smoothness (worst): 0.071 0.223 compactness (worst): 0.027 1.058 concavity (worst): 0.0 1.252 concave points (worst): 0.0 0.291 symmetry (worst): 0.156 0.664 fractal dimension (worst): 0.055 0.208 Missing Attribute Values None Class Distribution 212 - Malignant, 357 - Benign Creator Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian Donor Nick Street Date November, 1995 This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2 Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, Decision Tree Construction Via Linear Programming. Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: Robust Linear Programming Discrimination of Two Linearly Inseparable Sets, Optimization Methods and Software 1, 1992, 23-34]. This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/ References W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995. W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171. \"\\n',\n",
       " '\"sklearn_7_1_general_dataset_api 7.1. General dataset API datasets/index.html#general-dataset-api  7.3. Real world datasets  scikit-learn provides tools to load larger datasets, downloading them if necessary. They can be loaded using the following functions: fetch_olivetti_faces (*[,\\xa0data_home,\\xa0]) Load the Olivetti faces data-set from AT&T (classification). fetch_20newsgroups (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the filenames and data from the 20 newsgroups dataset (classification). fetch_20newsgroups_vectorized (*[,\\xa0subset,\\xa0]) Load the 20 newsgroups dataset and vectorize it into token counts (classification). fetch_lfw_people (*[,\\xa0data_home,\\xa0funneled,\\xa0]) Load the Labeled Faces in the Wild (LFW) people dataset (classification). fetch_lfw_pairs (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). fetch_covtype (*[,\\xa0data_home,\\xa0]) Load the covertype dataset (classification). fetch_rcv1 (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the RCV1 multilabel dataset (classification). fetch_kddcup99 (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the kddcup99 dataset (classification). fetch_california_housing (*[,\\xa0data_home,\\xa0]) Load the California housing dataset (regression). 7.3.1. The Olivetti faces dataset  This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T. As described on the original website: There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). Data Set Characteristics: Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms. The target for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective. The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images. When using these images, please give credit to AT&T Laboratories Cambridge. 7.3.2. The 20 newsgroups text dataset  The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups , returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized , returns ready-to-use features, i.e., it is not necessary to use a feature extractor. Data Set Characteristics: Classes 20 Samples total 18846 Dimensionality 1 Features text 7.3.2.1. Usage  The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website , extracts the archive contents in the folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them: The real data lies in the and attributes. The target attribute is the integer index of the category: It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function: 7.3.2.2. Converting text to vectors  In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news: The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features): sklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names. 7.3.2.3. Filtering text for more realistic training  It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that arent from this window of time. For example, lets look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score: (The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of whats going on inside this classifier?) Lets take a look at what the most informative features are: You can now see many things that these features have overfit to: Almost every group is distinguished by whether headers such as and appear more or less often. Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature. The word article is a significant feature, based on how often people quote previous posts like this: In article [article ID], [name] <[e-mail address]> wrote: Other features match the names and e-mail addresses of particular people who were posting at the time. With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level. For this reason, the functions that load 20 Newsgroups data provide a parameter called remove , telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of , telling it to remove headers, signature blocks, and quotation blocks respectively. This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data: Some other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the option to compare the results. Recommendation When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting . The F-score will be lower because it is more realistic. Examples Sample pipeline for text feature extraction and evaluation Classification of text documents using sparse features 7.3.3. The Labeled Faces in the Wild face recognition dataset  This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website: http://vis-www.cs.umass.edu/lfw/ Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person. An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons. Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites. Data Set Characteristics: Classes 5749 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 7.3.3.1. Usage  provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the folder using . The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning): The default slice is a rectangular shape around the face, removing most of the background: Each of the faces is assigned to a single person id in the array: The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person: Both for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing , in that case the shape will be . The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development set, the development set and an evaluation set meant to compute performance metrics using a 10-folds cross validation scheme. References: Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007. 7.3.3.2. Examples  Faces recognition example using eigenfaces and SVMs 7.3.4. Forest covertypes  The samples in this dataset correspond to 3030m patches of forest in the US, collected for the task of predicting each patchs cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the datasets homepage . Some of the features are boolean indicators, while others are discrete or continuous measurements. Data Set Characteristics: Classes 7 Samples total 581012 Dimensionality 54 Features int sklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.5. RCV1 dataset  Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in 1 . Data Set Characteristics: Classes 103 Samples total 804414 Dimensionality 47236 Features real, between 0 and 1 sklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels: It returns a dictionary-like object, with the following attributes: : The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in 1 : The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values: : The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values: : Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596: : The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for GMIL, to 381327 for CCAT: The dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB. References 1 ( 1 , 2 ) Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397. 7.3.6. Kddcup 99 dataset  The KDD Cup 99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the datasets homepage ) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting abnormal data, ie qualitatively different from normal data in large minority among the observations. We thus transform the KDD Data set into two different data sets: SA and SF. -SA is obtained by simply selecting all the normal data, and a small proportion of abnormal data to gives an anomaly proportion of 1%. -SF is obtained as in [2] by simply picking up the data whose attribute logged_in is positive, thus focusing on the intrusion attack, which gives a proportion of 0.3% of attack. -http and smtp are two subsets of SF corresponding with third feature equal to http (resp. to smtp) General KDD structure : Samples total 4898431 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SA structure : Samples total 976158 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SF structure : Samples total 699691 Dimensionality 4 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type http structure : Samples total 619052 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type smtp structure : Samples total 95373 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type sklearn.datasets.fetch_kddcup99 will load the kddcup99 dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.7. California Housing dataset  Data Set Characteristics: Number of Instances 20640 Number of Attributes 8 numeric, predictive attributes and the target Attribute Information MedInc median income in block HouseAge median house age in block AveRooms average number of rooms AveBedrms average number of bedrooms Population block population AveOccup average house occupancy Latitude house block latitude Longitude house block longitude Missing Attribute Values None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the sklearn.datasets.fetch_california_housing function. References Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297 \"\\n',\n",
       " '\"sklearn_7_1_general_dataset_api 7.1. General dataset API datasets/index.html#general-dataset-api  7.4. Generated datasets  In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. 7.4.1. Generators for classification and clustering  These generators produce a matrix of features and corresponding discrete targets. 7.4.1.1. Single label  Both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space. make_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem. make_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles. 7.4.1.2. Multilabel  make_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include: Per-topic word distributions are independently drawn, where in reality all would be affected by a sparse base distribution, and would be correlated. For a document generated from multiple topics, all topics are weighted equally in generating its bag of words. Documents without labels words at random, rather than from a base distribution. 7.4.1.3. Biclustering  make_biclusters (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with constant block diagonal structure for biclustering. make_checkerboard (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with block checkerboard structure for biclustering. 7.4.2. Generators for regression  make_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance). Other regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target. 7.4.3. Generators for manifold learning  make_s_curve ([n_samples,\\xa0noise,\\xa0random_state]) Generate an S curve dataset. make_swiss_roll ([n_samples,\\xa0noise,\\xa0random_state]) Generate a swiss roll dataset. 7.4.4. Generators for decomposition  make_low_rank_matrix ([n_samples,\\xa0]) Generate a mostly low rank matrix with bell-shaped singular values make_sparse_coded_signal (n_samples,\\xa0*,\\xa0) Generate a signal as a sparse combination of dictionary elements. make_spd_matrix (n_dim,\\xa0*[,\\xa0random_state]) Generate a random symmetric, positive-definite matrix. make_sparse_spd_matrix ([dim,\\xa0alpha,\\xa0]) Generate a sparse symmetric definite positive matrix. \"\\n',\n",
       " '\"sklearn_7_1_general_dataset_api 7.1. General dataset API datasets/index.html#general-dataset-api  7.5. Loading other datasets  7.5.1. Sample images  Scikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data. load_sample_images () Load sample images for image manipulation. load_sample_image (image_name) Load the numpy array of a single sample image Warning The default coding of images is based on the dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use dont forget to scale to the range 0 - 1 as done in the following example. Examples: Color Quantization using K-Means 7.5.2. Datasets in svmlight / libsvm format  scikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form . This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for and numpy arrays are used for . You may load a dataset like as follows: You may also load two (or more) datasets at once: In this case, and are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features: Related links: Public datasets in svmlight / libsvm format : https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets Faster API-compatible implementation : https://github.com/mblondel/svmlight-loader 7.5.3. Downloading datasets from the openml.org repository  openml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets. The package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml . For example, to download a dataset of gene expressions in mice brains: To fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes: You can get more information on the dataset by looking at the and attributes: The contains a free-text description of the data, while contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website: The also uniquely identifies a dataset from OpenML: 7.5.3.1. Dataset Versions  A dataset is uniquely specified by its , but not necessarily by its name. Several different versions of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that can yield different results at different times if earlier versions become inactive. You can see that the dataset with 40966 that we fetched above is the version 1 of the miceprotein dataset: In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions: Specifying the dataset by the name iris yields the lowest version, version 1, with the 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset . The other dataset, with 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data: You can also specify both the name and the version, which also uniquely identifies the dataset: References: Vanschoren, van Rijn, Bischl and Torgo OpenML: networked science in machine learning , ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014. 7.5.4. Loading from external datasets  scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable. Here are some recommended ways to load standard columnar data into a format usable by scikit-learn: pandas.io provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn. scipy.io specializes in binary formats often used in scientific computing context such as .mat and .arff numpy/routines.io for standard loading of columnar data into numpy arrays scikit-learns datasets.load_svmlight_file for the svmlight or libSVM sparse format scikit-learns datasets.load_files for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category For some miscellaneous data such as images, videos, and audio, you may wish to refer to: skimage.io or Imageio for loading images and videos into numpy arrays scipy.io.wavfile.read for reading WAV files into a numpy array Categorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data . Note: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format. \"\\n',\n",
       " '\"sklearn_7_2_toy_datasets 7.2. Toy datasets datasets/index.html#toy-datasets  7.1. General dataset API  There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset. The dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section. The dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section. Both loaders and fetchers functions return a sklearn.utils.Bunch object holding at least two items: an array of shape * with key (except for 20newsgroups) and a numpy array of length , containing the target values, with key . The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see sklearn.utils.Bunch : Its also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the parameter to . The datasets also contain a full description in their attribute and some contain and . See the dataset descriptions below for details. The dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section. These functions return a tuple consisting of a * numpy array and an array of length containing the targets . In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section. \"\\n',\n",
       " '\"sklearn_7_2_toy_datasets 7.2. Toy datasets datasets/index.html#toy-datasets  7.2. Toy datasets  scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. They can be loaded using the following functions: load_boston (*[,\\xa0return_X_y]) Load and return the boston house-prices dataset (regression). load_iris (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the iris dataset (classification). load_diabetes (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the diabetes dataset (regression). load_digits (*[,\\xa0n_class,\\xa0return_X_y,\\xa0as_frame]) Load and return the digits dataset (classification). load_linnerud (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the physical excercise linnerud dataset. load_wine (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the wine dataset (classification). load_breast_cancer (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the breast cancer wisconsin dataset (classification). These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks. 7.2.1. Boston house prices dataset  Data Set Characteristics: Number of Instances 506 Number of Attributes 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. Attribute Information (in order) CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable ( 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per $10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT % lower status of the population MEDV Median value of owner-occupied homes in $1000s Missing Attribute Values None Creator Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, Regression diagnostics , Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. References Belsley, Kuh & Welsch, Regression diagnostics: Identifying Influential Data and Sources of Collinearity, Wiley, 1980. 244-261. Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. 7.2.2. Iris plants dataset  Data Set Characteristics: Number of Instances 150 (50 in each of three classes) Number of Attributes 4 numeric, predictive attributes and the class Attribute Information sepal length in cm sepal width in cm petal length in cm petal width in cm class: Iris-Setosa Iris-Versicolour Iris-Virginica Summary Statistics sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) Missing Attribute Values None Class Distribution 33.3% for each of 3 classes. Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fishers paper. Note that its the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fishers paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. References Fisher, R.A. The use of multiple measurements in taxonomic problems Annual Eugenics, 7, Part II, 179-188 (1936); also in Contributions to Mathematical Statistics (John Wiley, NY, 1950). Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. Dasarathy, B.V. (1980) Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. Gates, G.W. (1972) The Reduced Nearest Neighbor Rule. IEEE Transactions on Information Theory, May 1972, 431-433. See also: 1988 MLC Proceedings, 54-64. Cheeseman et als AUTOCLASS II conceptual clustering system finds 3 classes in the data. Many, many more  7.2.3. Diabetes dataset  Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n  442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. Data Set Characteristics: Number of Instances 442 Number of Attributes First 10 columns are numeric predictive values Target Column 11 is a quantitative measure of disease progression one year after baseline Attribute Information age age in years sex bmi body mass index bp average blood pressure s1 tc, T-Cells (a type of white blood cells) s2 ldl, low-density lipoproteins s3 hdl, high-density lipoproteins s4 tch, thyroid stimulating hormone s5 ltg, lamotrigine s6 glu, blood sugar level Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times (i.e. the sum of squares of each column totals 1). Source URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) Least Angle Regression, Annals of Statistics (with discussion), 407-499. ( https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf ) 7.2.4. Optical recognition of handwritten digits dataset  Data Set Characteristics: Number of Instances 5620 Number of Attributes 64 Attribute Information 8x8 image of integer pixels in the range 0..16. Missing Attribute Values None Creator Alpaydin (alpaydin @ boun.edu.tr) Date July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. References C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000. 7.2.5. Linnerrud dataset  Data Set Characteristics: Number of Instances 20 Number of Attributes 3 Missing Attribute Values None The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club: physiological - CSV containing 20 observations on 3 physiological variables: Weight, Waist and Pulse. exercise - CSV containing 20 observations on 3 exercise variables: Chins, Situps and Jumps. References Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. 7.2.6. Wine recognition dataset  Data Set Characteristics: Number of Instances 178 (50 in each of three classes) Number of Attributes 13 numeric, predictive attributes and the class Attribute Information Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline class: class_0 class_1 class_2 Summary Statistics Alcohol: 11.0 14.8 13.0 0.8 Malic Acid: 0.74 5.80 2.34 1.12 Ash: 1.36 3.23 2.36 0.27 Alcalinity of Ash: 10.6 30.0 19.5 3.3 Magnesium: 70.0 162.0 99.7 14.3 Total Phenols: 0.98 3.88 2.29 0.63 Flavanoids: 0.34 5.08 2.03 1.00 Nonflavanoid Phenols: 0.13 0.66 0.36 0.12 Proanthocyanins: 0.41 3.58 1.59 0.57 Colour Intensity: 1.3 13.0 5.1 2.3 Hue: 0.48 1.71 0.96 0.23 OD280/OD315 of diluted wines: 1.27 4.00 2.61 0.71 Proline: 278 1680 746 315 Missing Attribute Values None Class Distribution class_0 (59), class_1 (71), class_2 (48) Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 This is a copy of UCI ML Wine recognition datasets. https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine. Original Owners: Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Citation: Lichman, M. (2013). UCI Machine Learning Repository [ https://archive.ics.uci.edu/ml ]. Irvine, CA: University of California, School of Information and Computer Science. References (1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics). The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique) (2) S. Aeberhard, D. Coomans and O. de Vel, THE CLASSIFICATION PERFORMANCE OF RDA Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics). 7.2.7. Breast cancer wisconsin (diagnostic) dataset  Data Set Characteristics: Number of Instances 569 Number of Attributes 30 numeric, predictive attributes and the class Attribute Information radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (coastline approximation - 1) The mean, standard error, and worst or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius. class: WDBC-Malignant WDBC-Benign Summary Statistics radius (mean): 6.981 28.11 texture (mean): 9.71 39.28 perimeter (mean): 43.79 188.5 area (mean): 143.5 2501.0 smoothness (mean): 0.053 0.163 compactness (mean): 0.019 0.345 concavity (mean): 0.0 0.427 concave points (mean): 0.0 0.201 symmetry (mean): 0.106 0.304 fractal dimension (mean): 0.05 0.097 radius (standard error): 0.112 2.873 texture (standard error): 0.36 4.885 perimeter (standard error): 0.757 21.98 area (standard error): 6.802 542.2 smoothness (standard error): 0.002 0.031 compactness (standard error): 0.002 0.135 concavity (standard error): 0.0 0.396 concave points (standard error): 0.0 0.053 symmetry (standard error): 0.008 0.079 fractal dimension (standard error): 0.001 0.03 radius (worst): 7.93 36.04 texture (worst): 12.02 49.54 perimeter (worst): 50.41 251.2 area (worst): 185.2 4254.0 smoothness (worst): 0.071 0.223 compactness (worst): 0.027 1.058 concavity (worst): 0.0 1.252 concave points (worst): 0.0 0.291 symmetry (worst): 0.156 0.664 fractal dimension (worst): 0.055 0.208 Missing Attribute Values None Class Distribution 212 - Malignant, 357 - Benign Creator Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian Donor Nick Street Date November, 1995 This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2 Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, Decision Tree Construction Via Linear Programming. Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: Robust Linear Programming Discrimination of Two Linearly Inseparable Sets, Optimization Methods and Software 1, 1992, 23-34]. This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/ References W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995. W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171. \"\\n',\n",
       " '\"sklearn_7_2_toy_datasets 7.2. Toy datasets datasets/index.html#toy-datasets  7.3. Real world datasets  scikit-learn provides tools to load larger datasets, downloading them if necessary. They can be loaded using the following functions: fetch_olivetti_faces (*[,\\xa0data_home,\\xa0]) Load the Olivetti faces data-set from AT&T (classification). fetch_20newsgroups (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the filenames and data from the 20 newsgroups dataset (classification). fetch_20newsgroups_vectorized (*[,\\xa0subset,\\xa0]) Load the 20 newsgroups dataset and vectorize it into token counts (classification). fetch_lfw_people (*[,\\xa0data_home,\\xa0funneled,\\xa0]) Load the Labeled Faces in the Wild (LFW) people dataset (classification). fetch_lfw_pairs (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). fetch_covtype (*[,\\xa0data_home,\\xa0]) Load the covertype dataset (classification). fetch_rcv1 (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the RCV1 multilabel dataset (classification). fetch_kddcup99 (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the kddcup99 dataset (classification). fetch_california_housing (*[,\\xa0data_home,\\xa0]) Load the California housing dataset (regression). 7.3.1. The Olivetti faces dataset  This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T. As described on the original website: There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). Data Set Characteristics: Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms. The target for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective. The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images. When using these images, please give credit to AT&T Laboratories Cambridge. 7.3.2. The 20 newsgroups text dataset  The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups , returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized , returns ready-to-use features, i.e., it is not necessary to use a feature extractor. Data Set Characteristics: Classes 20 Samples total 18846 Dimensionality 1 Features text 7.3.2.1. Usage  The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website , extracts the archive contents in the folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them: The real data lies in the and attributes. The target attribute is the integer index of the category: It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function: 7.3.2.2. Converting text to vectors  In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news: The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features): sklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names. 7.3.2.3. Filtering text for more realistic training  It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that arent from this window of time. For example, lets look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score: (The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of whats going on inside this classifier?) Lets take a look at what the most informative features are: You can now see many things that these features have overfit to: Almost every group is distinguished by whether headers such as and appear more or less often. Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature. The word article is a significant feature, based on how often people quote previous posts like this: In article [article ID], [name] <[e-mail address]> wrote: Other features match the names and e-mail addresses of particular people who were posting at the time. With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level. For this reason, the functions that load 20 Newsgroups data provide a parameter called remove , telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of , telling it to remove headers, signature blocks, and quotation blocks respectively. This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data: Some other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the option to compare the results. Recommendation When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting . The F-score will be lower because it is more realistic. Examples Sample pipeline for text feature extraction and evaluation Classification of text documents using sparse features 7.3.3. The Labeled Faces in the Wild face recognition dataset  This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website: http://vis-www.cs.umass.edu/lfw/ Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person. An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons. Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites. Data Set Characteristics: Classes 5749 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 7.3.3.1. Usage  provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the folder using . The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning): The default slice is a rectangular shape around the face, removing most of the background: Each of the faces is assigned to a single person id in the array: The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person: Both for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing , in that case the shape will be . The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development set, the development set and an evaluation set meant to compute performance metrics using a 10-folds cross validation scheme. References: Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007. 7.3.3.2. Examples  Faces recognition example using eigenfaces and SVMs 7.3.4. Forest covertypes  The samples in this dataset correspond to 3030m patches of forest in the US, collected for the task of predicting each patchs cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the datasets homepage . Some of the features are boolean indicators, while others are discrete or continuous measurements. Data Set Characteristics: Classes 7 Samples total 581012 Dimensionality 54 Features int sklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.5. RCV1 dataset  Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in 1 . Data Set Characteristics: Classes 103 Samples total 804414 Dimensionality 47236 Features real, between 0 and 1 sklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels: It returns a dictionary-like object, with the following attributes: : The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in 1 : The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values: : The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values: : Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596: : The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for GMIL, to 381327 for CCAT: The dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB. References 1 ( 1 , 2 ) Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397. 7.3.6. Kddcup 99 dataset  The KDD Cup 99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the datasets homepage ) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting abnormal data, ie qualitatively different from normal data in large minority among the observations. We thus transform the KDD Data set into two different data sets: SA and SF. -SA is obtained by simply selecting all the normal data, and a small proportion of abnormal data to gives an anomaly proportion of 1%. -SF is obtained as in [2] by simply picking up the data whose attribute logged_in is positive, thus focusing on the intrusion attack, which gives a proportion of 0.3% of attack. -http and smtp are two subsets of SF corresponding with third feature equal to http (resp. to smtp) General KDD structure : Samples total 4898431 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SA structure : Samples total 976158 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SF structure : Samples total 699691 Dimensionality 4 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type http structure : Samples total 619052 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type smtp structure : Samples total 95373 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type sklearn.datasets.fetch_kddcup99 will load the kddcup99 dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.7. California Housing dataset  Data Set Characteristics: Number of Instances 20640 Number of Attributes 8 numeric, predictive attributes and the target Attribute Information MedInc median income in block HouseAge median house age in block AveRooms average number of rooms AveBedrms average number of bedrooms Population block population AveOccup average house occupancy Latitude house block latitude Longitude house block longitude Missing Attribute Values None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the sklearn.datasets.fetch_california_housing function. References Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297 \"\\n',\n",
       " '\"sklearn_7_2_toy_datasets 7.2. Toy datasets datasets/index.html#toy-datasets  7.4. Generated datasets  In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. 7.4.1. Generators for classification and clustering  These generators produce a matrix of features and corresponding discrete targets. 7.4.1.1. Single label  Both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space. make_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem. make_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles. 7.4.1.2. Multilabel  make_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include: Per-topic word distributions are independently drawn, where in reality all would be affected by a sparse base distribution, and would be correlated. For a document generated from multiple topics, all topics are weighted equally in generating its bag of words. Documents without labels words at random, rather than from a base distribution. 7.4.1.3. Biclustering  make_biclusters (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with constant block diagonal structure for biclustering. make_checkerboard (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with block checkerboard structure for biclustering. 7.4.2. Generators for regression  make_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance). Other regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target. 7.4.3. Generators for manifold learning  make_s_curve ([n_samples,\\xa0noise,\\xa0random_state]) Generate an S curve dataset. make_swiss_roll ([n_samples,\\xa0noise,\\xa0random_state]) Generate a swiss roll dataset. 7.4.4. Generators for decomposition  make_low_rank_matrix ([n_samples,\\xa0]) Generate a mostly low rank matrix with bell-shaped singular values make_sparse_coded_signal (n_samples,\\xa0*,\\xa0) Generate a signal as a sparse combination of dictionary elements. make_spd_matrix (n_dim,\\xa0*[,\\xa0random_state]) Generate a random symmetric, positive-definite matrix. make_sparse_spd_matrix ([dim,\\xa0alpha,\\xa0]) Generate a sparse symmetric definite positive matrix. \"\\n',\n",
       " '\"sklearn_7_2_toy_datasets 7.2. Toy datasets datasets/index.html#toy-datasets  7.5. Loading other datasets  7.5.1. Sample images  Scikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data. load_sample_images () Load sample images for image manipulation. load_sample_image (image_name) Load the numpy array of a single sample image Warning The default coding of images is based on the dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use dont forget to scale to the range 0 - 1 as done in the following example. Examples: Color Quantization using K-Means 7.5.2. Datasets in svmlight / libsvm format  scikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form . This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for and numpy arrays are used for . You may load a dataset like as follows: You may also load two (or more) datasets at once: In this case, and are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features: Related links: Public datasets in svmlight / libsvm format : https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets Faster API-compatible implementation : https://github.com/mblondel/svmlight-loader 7.5.3. Downloading datasets from the openml.org repository  openml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets. The package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml . For example, to download a dataset of gene expressions in mice brains: To fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes: You can get more information on the dataset by looking at the and attributes: The contains a free-text description of the data, while contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website: The also uniquely identifies a dataset from OpenML: 7.5.3.1. Dataset Versions  A dataset is uniquely specified by its , but not necessarily by its name. Several different versions of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that can yield different results at different times if earlier versions become inactive. You can see that the dataset with 40966 that we fetched above is the version 1 of the miceprotein dataset: In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions: Specifying the dataset by the name iris yields the lowest version, version 1, with the 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset . The other dataset, with 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data: You can also specify both the name and the version, which also uniquely identifies the dataset: References: Vanschoren, van Rijn, Bischl and Torgo OpenML: networked science in machine learning , ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014. 7.5.4. Loading from external datasets  scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable. Here are some recommended ways to load standard columnar data into a format usable by scikit-learn: pandas.io provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn. scipy.io specializes in binary formats often used in scientific computing context such as .mat and .arff numpy/routines.io for standard loading of columnar data into numpy arrays scikit-learns datasets.load_svmlight_file for the svmlight or libSVM sparse format scikit-learns datasets.load_files for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category For some miscellaneous data such as images, videos, and audio, you may wish to refer to: skimage.io or Imageio for loading images and videos into numpy arrays scipy.io.wavfile.read for reading WAV files into a numpy array Categorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data . Note: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format. \"\\n',\n",
       " '\"sklearn_7_3_real_world_datasets 7.3. Real world datasets datasets/index.html#real-world-datasets  7.1. General dataset API  There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset. The dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section. The dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section. Both loaders and fetchers functions return a sklearn.utils.Bunch object holding at least two items: an array of shape * with key (except for 20newsgroups) and a numpy array of length , containing the target values, with key . The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see sklearn.utils.Bunch : Its also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the parameter to . The datasets also contain a full description in their attribute and some contain and . See the dataset descriptions below for details. The dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section. These functions return a tuple consisting of a * numpy array and an array of length containing the targets . In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section. \"\\n',\n",
       " '\"sklearn_7_3_real_world_datasets 7.3. Real world datasets datasets/index.html#real-world-datasets  7.2. Toy datasets  scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. They can be loaded using the following functions: load_boston (*[,\\xa0return_X_y]) Load and return the boston house-prices dataset (regression). load_iris (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the iris dataset (classification). load_diabetes (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the diabetes dataset (regression). load_digits (*[,\\xa0n_class,\\xa0return_X_y,\\xa0as_frame]) Load and return the digits dataset (classification). load_linnerud (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the physical excercise linnerud dataset. load_wine (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the wine dataset (classification). load_breast_cancer (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the breast cancer wisconsin dataset (classification). These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks. 7.2.1. Boston house prices dataset  Data Set Characteristics: Number of Instances 506 Number of Attributes 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. Attribute Information (in order) CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable ( 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per $10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT % lower status of the population MEDV Median value of owner-occupied homes in $1000s Missing Attribute Values None Creator Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, Regression diagnostics , Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. References Belsley, Kuh & Welsch, Regression diagnostics: Identifying Influential Data and Sources of Collinearity, Wiley, 1980. 244-261. Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. 7.2.2. Iris plants dataset  Data Set Characteristics: Number of Instances 150 (50 in each of three classes) Number of Attributes 4 numeric, predictive attributes and the class Attribute Information sepal length in cm sepal width in cm petal length in cm petal width in cm class: Iris-Setosa Iris-Versicolour Iris-Virginica Summary Statistics sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) Missing Attribute Values None Class Distribution 33.3% for each of 3 classes. Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fishers paper. Note that its the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fishers paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. References Fisher, R.A. The use of multiple measurements in taxonomic problems Annual Eugenics, 7, Part II, 179-188 (1936); also in Contributions to Mathematical Statistics (John Wiley, NY, 1950). Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. Dasarathy, B.V. (1980) Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. Gates, G.W. (1972) The Reduced Nearest Neighbor Rule. IEEE Transactions on Information Theory, May 1972, 431-433. See also: 1988 MLC Proceedings, 54-64. Cheeseman et als AUTOCLASS II conceptual clustering system finds 3 classes in the data. Many, many more  7.2.3. Diabetes dataset  Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n  442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. Data Set Characteristics: Number of Instances 442 Number of Attributes First 10 columns are numeric predictive values Target Column 11 is a quantitative measure of disease progression one year after baseline Attribute Information age age in years sex bmi body mass index bp average blood pressure s1 tc, T-Cells (a type of white blood cells) s2 ldl, low-density lipoproteins s3 hdl, high-density lipoproteins s4 tch, thyroid stimulating hormone s5 ltg, lamotrigine s6 glu, blood sugar level Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times (i.e. the sum of squares of each column totals 1). Source URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) Least Angle Regression, Annals of Statistics (with discussion), 407-499. ( https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf ) 7.2.4. Optical recognition of handwritten digits dataset  Data Set Characteristics: Number of Instances 5620 Number of Attributes 64 Attribute Information 8x8 image of integer pixels in the range 0..16. Missing Attribute Values None Creator Alpaydin (alpaydin @ boun.edu.tr) Date July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. References C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000. 7.2.5. Linnerrud dataset  Data Set Characteristics: Number of Instances 20 Number of Attributes 3 Missing Attribute Values None The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club: physiological - CSV containing 20 observations on 3 physiological variables: Weight, Waist and Pulse. exercise - CSV containing 20 observations on 3 exercise variables: Chins, Situps and Jumps. References Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. 7.2.6. Wine recognition dataset  Data Set Characteristics: Number of Instances 178 (50 in each of three classes) Number of Attributes 13 numeric, predictive attributes and the class Attribute Information Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline class: class_0 class_1 class_2 Summary Statistics Alcohol: 11.0 14.8 13.0 0.8 Malic Acid: 0.74 5.80 2.34 1.12 Ash: 1.36 3.23 2.36 0.27 Alcalinity of Ash: 10.6 30.0 19.5 3.3 Magnesium: 70.0 162.0 99.7 14.3 Total Phenols: 0.98 3.88 2.29 0.63 Flavanoids: 0.34 5.08 2.03 1.00 Nonflavanoid Phenols: 0.13 0.66 0.36 0.12 Proanthocyanins: 0.41 3.58 1.59 0.57 Colour Intensity: 1.3 13.0 5.1 2.3 Hue: 0.48 1.71 0.96 0.23 OD280/OD315 of diluted wines: 1.27 4.00 2.61 0.71 Proline: 278 1680 746 315 Missing Attribute Values None Class Distribution class_0 (59), class_1 (71), class_2 (48) Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 This is a copy of UCI ML Wine recognition datasets. https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine. Original Owners: Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Citation: Lichman, M. (2013). UCI Machine Learning Repository [ https://archive.ics.uci.edu/ml ]. Irvine, CA: University of California, School of Information and Computer Science. References (1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics). The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique) (2) S. Aeberhard, D. Coomans and O. de Vel, THE CLASSIFICATION PERFORMANCE OF RDA Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics). 7.2.7. Breast cancer wisconsin (diagnostic) dataset  Data Set Characteristics: Number of Instances 569 Number of Attributes 30 numeric, predictive attributes and the class Attribute Information radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (coastline approximation - 1) The mean, standard error, and worst or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius. class: WDBC-Malignant WDBC-Benign Summary Statistics radius (mean): 6.981 28.11 texture (mean): 9.71 39.28 perimeter (mean): 43.79 188.5 area (mean): 143.5 2501.0 smoothness (mean): 0.053 0.163 compactness (mean): 0.019 0.345 concavity (mean): 0.0 0.427 concave points (mean): 0.0 0.201 symmetry (mean): 0.106 0.304 fractal dimension (mean): 0.05 0.097 radius (standard error): 0.112 2.873 texture (standard error): 0.36 4.885 perimeter (standard error): 0.757 21.98 area (standard error): 6.802 542.2 smoothness (standard error): 0.002 0.031 compactness (standard error): 0.002 0.135 concavity (standard error): 0.0 0.396 concave points (standard error): 0.0 0.053 symmetry (standard error): 0.008 0.079 fractal dimension (standard error): 0.001 0.03 radius (worst): 7.93 36.04 texture (worst): 12.02 49.54 perimeter (worst): 50.41 251.2 area (worst): 185.2 4254.0 smoothness (worst): 0.071 0.223 compactness (worst): 0.027 1.058 concavity (worst): 0.0 1.252 concave points (worst): 0.0 0.291 symmetry (worst): 0.156 0.664 fractal dimension (worst): 0.055 0.208 Missing Attribute Values None Class Distribution 212 - Malignant, 357 - Benign Creator Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian Donor Nick Street Date November, 1995 This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2 Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, Decision Tree Construction Via Linear Programming. Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: Robust Linear Programming Discrimination of Two Linearly Inseparable Sets, Optimization Methods and Software 1, 1992, 23-34]. This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/ References W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995. W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171. \"\\n',\n",
       " '\"sklearn_7_3_real_world_datasets 7.3. Real world datasets datasets/index.html#real-world-datasets  7.3. Real world datasets  scikit-learn provides tools to load larger datasets, downloading them if necessary. They can be loaded using the following functions: fetch_olivetti_faces (*[,\\xa0data_home,\\xa0]) Load the Olivetti faces data-set from AT&T (classification). fetch_20newsgroups (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the filenames and data from the 20 newsgroups dataset (classification). fetch_20newsgroups_vectorized (*[,\\xa0subset,\\xa0]) Load the 20 newsgroups dataset and vectorize it into token counts (classification). fetch_lfw_people (*[,\\xa0data_home,\\xa0funneled,\\xa0]) Load the Labeled Faces in the Wild (LFW) people dataset (classification). fetch_lfw_pairs (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). fetch_covtype (*[,\\xa0data_home,\\xa0]) Load the covertype dataset (classification). fetch_rcv1 (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the RCV1 multilabel dataset (classification). fetch_kddcup99 (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the kddcup99 dataset (classification). fetch_california_housing (*[,\\xa0data_home,\\xa0]) Load the California housing dataset (regression). 7.3.1. The Olivetti faces dataset  This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T. As described on the original website: There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). Data Set Characteristics: Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms. The target for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective. The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images. When using these images, please give credit to AT&T Laboratories Cambridge. 7.3.2. The 20 newsgroups text dataset  The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups , returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized , returns ready-to-use features, i.e., it is not necessary to use a feature extractor. Data Set Characteristics: Classes 20 Samples total 18846 Dimensionality 1 Features text 7.3.2.1. Usage  The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website , extracts the archive contents in the folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them: The real data lies in the and attributes. The target attribute is the integer index of the category: It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function: 7.3.2.2. Converting text to vectors  In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news: The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features): sklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names. 7.3.2.3. Filtering text for more realistic training  It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that arent from this window of time. For example, lets look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score: (The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of whats going on inside this classifier?) Lets take a look at what the most informative features are: You can now see many things that these features have overfit to: Almost every group is distinguished by whether headers such as and appear more or less often. Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature. The word article is a significant feature, based on how often people quote previous posts like this: In article [article ID], [name] <[e-mail address]> wrote: Other features match the names and e-mail addresses of particular people who were posting at the time. With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level. For this reason, the functions that load 20 Newsgroups data provide a parameter called remove , telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of , telling it to remove headers, signature blocks, and quotation blocks respectively. This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data: Some other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the option to compare the results. Recommendation When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting . The F-score will be lower because it is more realistic. Examples Sample pipeline for text feature extraction and evaluation Classification of text documents using sparse features 7.3.3. The Labeled Faces in the Wild face recognition dataset  This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website: http://vis-www.cs.umass.edu/lfw/ Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person. An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons. Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites. Data Set Characteristics: Classes 5749 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 7.3.3.1. Usage  provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the folder using . The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning): The default slice is a rectangular shape around the face, removing most of the background: Each of the faces is assigned to a single person id in the array: The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person: Both for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing , in that case the shape will be . The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development set, the development set and an evaluation set meant to compute performance metrics using a 10-folds cross validation scheme. References: Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007. 7.3.3.2. Examples  Faces recognition example using eigenfaces and SVMs 7.3.4. Forest covertypes  The samples in this dataset correspond to 3030m patches of forest in the US, collected for the task of predicting each patchs cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the datasets homepage . Some of the features are boolean indicators, while others are discrete or continuous measurements. Data Set Characteristics: Classes 7 Samples total 581012 Dimensionality 54 Features int sklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.5. RCV1 dataset  Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in 1 . Data Set Characteristics: Classes 103 Samples total 804414 Dimensionality 47236 Features real, between 0 and 1 sklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels: It returns a dictionary-like object, with the following attributes: : The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in 1 : The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values: : The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values: : Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596: : The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for GMIL, to 381327 for CCAT: The dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB. References 1 ( 1 , 2 ) Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397. 7.3.6. Kddcup 99 dataset  The KDD Cup 99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the datasets homepage ) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting abnormal data, ie qualitatively different from normal data in large minority among the observations. We thus transform the KDD Data set into two different data sets: SA and SF. -SA is obtained by simply selecting all the normal data, and a small proportion of abnormal data to gives an anomaly proportion of 1%. -SF is obtained as in [2] by simply picking up the data whose attribute logged_in is positive, thus focusing on the intrusion attack, which gives a proportion of 0.3% of attack. -http and smtp are two subsets of SF corresponding with third feature equal to http (resp. to smtp) General KDD structure : Samples total 4898431 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SA structure : Samples total 976158 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SF structure : Samples total 699691 Dimensionality 4 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type http structure : Samples total 619052 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type smtp structure : Samples total 95373 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type sklearn.datasets.fetch_kddcup99 will load the kddcup99 dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.7. California Housing dataset  Data Set Characteristics: Number of Instances 20640 Number of Attributes 8 numeric, predictive attributes and the target Attribute Information MedInc median income in block HouseAge median house age in block AveRooms average number of rooms AveBedrms average number of bedrooms Population block population AveOccup average house occupancy Latitude house block latitude Longitude house block longitude Missing Attribute Values None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the sklearn.datasets.fetch_california_housing function. References Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297 \"\\n',\n",
       " '\"sklearn_7_3_real_world_datasets 7.3. Real world datasets datasets/index.html#real-world-datasets  7.4. Generated datasets  In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. 7.4.1. Generators for classification and clustering  These generators produce a matrix of features and corresponding discrete targets. 7.4.1.1. Single label  Both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space. make_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem. make_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles. 7.4.1.2. Multilabel  make_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include: Per-topic word distributions are independently drawn, where in reality all would be affected by a sparse base distribution, and would be correlated. For a document generated from multiple topics, all topics are weighted equally in generating its bag of words. Documents without labels words at random, rather than from a base distribution. 7.4.1.3. Biclustering  make_biclusters (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with constant block diagonal structure for biclustering. make_checkerboard (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with block checkerboard structure for biclustering. 7.4.2. Generators for regression  make_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance). Other regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target. 7.4.3. Generators for manifold learning  make_s_curve ([n_samples,\\xa0noise,\\xa0random_state]) Generate an S curve dataset. make_swiss_roll ([n_samples,\\xa0noise,\\xa0random_state]) Generate a swiss roll dataset. 7.4.4. Generators for decomposition  make_low_rank_matrix ([n_samples,\\xa0]) Generate a mostly low rank matrix with bell-shaped singular values make_sparse_coded_signal (n_samples,\\xa0*,\\xa0) Generate a signal as a sparse combination of dictionary elements. make_spd_matrix (n_dim,\\xa0*[,\\xa0random_state]) Generate a random symmetric, positive-definite matrix. make_sparse_spd_matrix ([dim,\\xa0alpha,\\xa0]) Generate a sparse symmetric definite positive matrix. \"\\n',\n",
       " '\"sklearn_7_3_real_world_datasets 7.3. Real world datasets datasets/index.html#real-world-datasets  7.5. Loading other datasets  7.5.1. Sample images  Scikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data. load_sample_images () Load sample images for image manipulation. load_sample_image (image_name) Load the numpy array of a single sample image Warning The default coding of images is based on the dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use dont forget to scale to the range 0 - 1 as done in the following example. Examples: Color Quantization using K-Means 7.5.2. Datasets in svmlight / libsvm format  scikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form . This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for and numpy arrays are used for . You may load a dataset like as follows: You may also load two (or more) datasets at once: In this case, and are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features: Related links: Public datasets in svmlight / libsvm format : https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets Faster API-compatible implementation : https://github.com/mblondel/svmlight-loader 7.5.3. Downloading datasets from the openml.org repository  openml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets. The package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml . For example, to download a dataset of gene expressions in mice brains: To fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes: You can get more information on the dataset by looking at the and attributes: The contains a free-text description of the data, while contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website: The also uniquely identifies a dataset from OpenML: 7.5.3.1. Dataset Versions  A dataset is uniquely specified by its , but not necessarily by its name. Several different versions of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that can yield different results at different times if earlier versions become inactive. You can see that the dataset with 40966 that we fetched above is the version 1 of the miceprotein dataset: In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions: Specifying the dataset by the name iris yields the lowest version, version 1, with the 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset . The other dataset, with 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data: You can also specify both the name and the version, which also uniquely identifies the dataset: References: Vanschoren, van Rijn, Bischl and Torgo OpenML: networked science in machine learning , ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014. 7.5.4. Loading from external datasets  scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable. Here are some recommended ways to load standard columnar data into a format usable by scikit-learn: pandas.io provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn. scipy.io specializes in binary formats often used in scientific computing context such as .mat and .arff numpy/routines.io for standard loading of columnar data into numpy arrays scikit-learns datasets.load_svmlight_file for the svmlight or libSVM sparse format scikit-learns datasets.load_files for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category For some miscellaneous data such as images, videos, and audio, you may wish to refer to: skimage.io or Imageio for loading images and videos into numpy arrays scipy.io.wavfile.read for reading WAV files into a numpy array Categorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data . Note: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format. \"\\n',\n",
       " '\"sklearn_7_4_generated_datasets 7.4. Generated datasets datasets/index.html#generated-datasets  7.1. General dataset API  There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset. The dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section. The dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section. Both loaders and fetchers functions return a sklearn.utils.Bunch object holding at least two items: an array of shape * with key (except for 20newsgroups) and a numpy array of length , containing the target values, with key . The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see sklearn.utils.Bunch : Its also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the parameter to . The datasets also contain a full description in their attribute and some contain and . See the dataset descriptions below for details. The dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section. These functions return a tuple consisting of a * numpy array and an array of length containing the targets . In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section. \"\\n',\n",
       " '\"sklearn_7_4_generated_datasets 7.4. Generated datasets datasets/index.html#generated-datasets  7.2. Toy datasets  scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. They can be loaded using the following functions: load_boston (*[,\\xa0return_X_y]) Load and return the boston house-prices dataset (regression). load_iris (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the iris dataset (classification). load_diabetes (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the diabetes dataset (regression). load_digits (*[,\\xa0n_class,\\xa0return_X_y,\\xa0as_frame]) Load and return the digits dataset (classification). load_linnerud (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the physical excercise linnerud dataset. load_wine (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the wine dataset (classification). load_breast_cancer (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the breast cancer wisconsin dataset (classification). These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks. 7.2.1. Boston house prices dataset  Data Set Characteristics: Number of Instances 506 Number of Attributes 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. Attribute Information (in order) CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable ( 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per $10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT % lower status of the population MEDV Median value of owner-occupied homes in $1000s Missing Attribute Values None Creator Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, Regression diagnostics , Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. References Belsley, Kuh & Welsch, Regression diagnostics: Identifying Influential Data and Sources of Collinearity, Wiley, 1980. 244-261. Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. 7.2.2. Iris plants dataset  Data Set Characteristics: Number of Instances 150 (50 in each of three classes) Number of Attributes 4 numeric, predictive attributes and the class Attribute Information sepal length in cm sepal width in cm petal length in cm petal width in cm class: Iris-Setosa Iris-Versicolour Iris-Virginica Summary Statistics sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) Missing Attribute Values None Class Distribution 33.3% for each of 3 classes. Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fishers paper. Note that its the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fishers paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. References Fisher, R.A. The use of multiple measurements in taxonomic problems Annual Eugenics, 7, Part II, 179-188 (1936); also in Contributions to Mathematical Statistics (John Wiley, NY, 1950). Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. Dasarathy, B.V. (1980) Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. Gates, G.W. (1972) The Reduced Nearest Neighbor Rule. IEEE Transactions on Information Theory, May 1972, 431-433. See also: 1988 MLC Proceedings, 54-64. Cheeseman et als AUTOCLASS II conceptual clustering system finds 3 classes in the data. Many, many more  7.2.3. Diabetes dataset  Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n  442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. Data Set Characteristics: Number of Instances 442 Number of Attributes First 10 columns are numeric predictive values Target Column 11 is a quantitative measure of disease progression one year after baseline Attribute Information age age in years sex bmi body mass index bp average blood pressure s1 tc, T-Cells (a type of white blood cells) s2 ldl, low-density lipoproteins s3 hdl, high-density lipoproteins s4 tch, thyroid stimulating hormone s5 ltg, lamotrigine s6 glu, blood sugar level Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times (i.e. the sum of squares of each column totals 1). Source URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) Least Angle Regression, Annals of Statistics (with discussion), 407-499. ( https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf ) 7.2.4. Optical recognition of handwritten digits dataset  Data Set Characteristics: Number of Instances 5620 Number of Attributes 64 Attribute Information 8x8 image of integer pixels in the range 0..16. Missing Attribute Values None Creator Alpaydin (alpaydin @ boun.edu.tr) Date July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. References C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000. 7.2.5. Linnerrud dataset  Data Set Characteristics: Number of Instances 20 Number of Attributes 3 Missing Attribute Values None The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club: physiological - CSV containing 20 observations on 3 physiological variables: Weight, Waist and Pulse. exercise - CSV containing 20 observations on 3 exercise variables: Chins, Situps and Jumps. References Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. 7.2.6. Wine recognition dataset  Data Set Characteristics: Number of Instances 178 (50 in each of three classes) Number of Attributes 13 numeric, predictive attributes and the class Attribute Information Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline class: class_0 class_1 class_2 Summary Statistics Alcohol: 11.0 14.8 13.0 0.8 Malic Acid: 0.74 5.80 2.34 1.12 Ash: 1.36 3.23 2.36 0.27 Alcalinity of Ash: 10.6 30.0 19.5 3.3 Magnesium: 70.0 162.0 99.7 14.3 Total Phenols: 0.98 3.88 2.29 0.63 Flavanoids: 0.34 5.08 2.03 1.00 Nonflavanoid Phenols: 0.13 0.66 0.36 0.12 Proanthocyanins: 0.41 3.58 1.59 0.57 Colour Intensity: 1.3 13.0 5.1 2.3 Hue: 0.48 1.71 0.96 0.23 OD280/OD315 of diluted wines: 1.27 4.00 2.61 0.71 Proline: 278 1680 746 315 Missing Attribute Values None Class Distribution class_0 (59), class_1 (71), class_2 (48) Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 This is a copy of UCI ML Wine recognition datasets. https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine. Original Owners: Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Citation: Lichman, M. (2013). UCI Machine Learning Repository [ https://archive.ics.uci.edu/ml ]. Irvine, CA: University of California, School of Information and Computer Science. References (1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics). The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique) (2) S. Aeberhard, D. Coomans and O. de Vel, THE CLASSIFICATION PERFORMANCE OF RDA Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics). 7.2.7. Breast cancer wisconsin (diagnostic) dataset  Data Set Characteristics: Number of Instances 569 Number of Attributes 30 numeric, predictive attributes and the class Attribute Information radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (coastline approximation - 1) The mean, standard error, and worst or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius. class: WDBC-Malignant WDBC-Benign Summary Statistics radius (mean): 6.981 28.11 texture (mean): 9.71 39.28 perimeter (mean): 43.79 188.5 area (mean): 143.5 2501.0 smoothness (mean): 0.053 0.163 compactness (mean): 0.019 0.345 concavity (mean): 0.0 0.427 concave points (mean): 0.0 0.201 symmetry (mean): 0.106 0.304 fractal dimension (mean): 0.05 0.097 radius (standard error): 0.112 2.873 texture (standard error): 0.36 4.885 perimeter (standard error): 0.757 21.98 area (standard error): 6.802 542.2 smoothness (standard error): 0.002 0.031 compactness (standard error): 0.002 0.135 concavity (standard error): 0.0 0.396 concave points (standard error): 0.0 0.053 symmetry (standard error): 0.008 0.079 fractal dimension (standard error): 0.001 0.03 radius (worst): 7.93 36.04 texture (worst): 12.02 49.54 perimeter (worst): 50.41 251.2 area (worst): 185.2 4254.0 smoothness (worst): 0.071 0.223 compactness (worst): 0.027 1.058 concavity (worst): 0.0 1.252 concave points (worst): 0.0 0.291 symmetry (worst): 0.156 0.664 fractal dimension (worst): 0.055 0.208 Missing Attribute Values None Class Distribution 212 - Malignant, 357 - Benign Creator Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian Donor Nick Street Date November, 1995 This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2 Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, Decision Tree Construction Via Linear Programming. Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: Robust Linear Programming Discrimination of Two Linearly Inseparable Sets, Optimization Methods and Software 1, 1992, 23-34]. This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/ References W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995. W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171. \"\\n',\n",
       " '\"sklearn_7_4_generated_datasets 7.4. Generated datasets datasets/index.html#generated-datasets  7.3. Real world datasets  scikit-learn provides tools to load larger datasets, downloading them if necessary. They can be loaded using the following functions: fetch_olivetti_faces (*[,\\xa0data_home,\\xa0]) Load the Olivetti faces data-set from AT&T (classification). fetch_20newsgroups (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the filenames and data from the 20 newsgroups dataset (classification). fetch_20newsgroups_vectorized (*[,\\xa0subset,\\xa0]) Load the 20 newsgroups dataset and vectorize it into token counts (classification). fetch_lfw_people (*[,\\xa0data_home,\\xa0funneled,\\xa0]) Load the Labeled Faces in the Wild (LFW) people dataset (classification). fetch_lfw_pairs (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). fetch_covtype (*[,\\xa0data_home,\\xa0]) Load the covertype dataset (classification). fetch_rcv1 (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the RCV1 multilabel dataset (classification). fetch_kddcup99 (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the kddcup99 dataset (classification). fetch_california_housing (*[,\\xa0data_home,\\xa0]) Load the California housing dataset (regression). 7.3.1. The Olivetti faces dataset  This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T. As described on the original website: There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). Data Set Characteristics: Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms. The target for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective. The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images. When using these images, please give credit to AT&T Laboratories Cambridge. 7.3.2. The 20 newsgroups text dataset  The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups , returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized , returns ready-to-use features, i.e., it is not necessary to use a feature extractor. Data Set Characteristics: Classes 20 Samples total 18846 Dimensionality 1 Features text 7.3.2.1. Usage  The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website , extracts the archive contents in the folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them: The real data lies in the and attributes. The target attribute is the integer index of the category: It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function: 7.3.2.2. Converting text to vectors  In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news: The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features): sklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names. 7.3.2.3. Filtering text for more realistic training  It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that arent from this window of time. For example, lets look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score: (The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of whats going on inside this classifier?) Lets take a look at what the most informative features are: You can now see many things that these features have overfit to: Almost every group is distinguished by whether headers such as and appear more or less often. Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature. The word article is a significant feature, based on how often people quote previous posts like this: In article [article ID], [name] <[e-mail address]> wrote: Other features match the names and e-mail addresses of particular people who were posting at the time. With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level. For this reason, the functions that load 20 Newsgroups data provide a parameter called remove , telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of , telling it to remove headers, signature blocks, and quotation blocks respectively. This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data: Some other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the option to compare the results. Recommendation When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting . The F-score will be lower because it is more realistic. Examples Sample pipeline for text feature extraction and evaluation Classification of text documents using sparse features 7.3.3. The Labeled Faces in the Wild face recognition dataset  This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website: http://vis-www.cs.umass.edu/lfw/ Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person. An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons. Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites. Data Set Characteristics: Classes 5749 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 7.3.3.1. Usage  provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the folder using . The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning): The default slice is a rectangular shape around the face, removing most of the background: Each of the faces is assigned to a single person id in the array: The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person: Both for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing , in that case the shape will be . The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development set, the development set and an evaluation set meant to compute performance metrics using a 10-folds cross validation scheme. References: Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007. 7.3.3.2. Examples  Faces recognition example using eigenfaces and SVMs 7.3.4. Forest covertypes  The samples in this dataset correspond to 3030m patches of forest in the US, collected for the task of predicting each patchs cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the datasets homepage . Some of the features are boolean indicators, while others are discrete or continuous measurements. Data Set Characteristics: Classes 7 Samples total 581012 Dimensionality 54 Features int sklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.5. RCV1 dataset  Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in 1 . Data Set Characteristics: Classes 103 Samples total 804414 Dimensionality 47236 Features real, between 0 and 1 sklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels: It returns a dictionary-like object, with the following attributes: : The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in 1 : The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values: : The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values: : Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596: : The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for GMIL, to 381327 for CCAT: The dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB. References 1 ( 1 , 2 ) Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397. 7.3.6. Kddcup 99 dataset  The KDD Cup 99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the datasets homepage ) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting abnormal data, ie qualitatively different from normal data in large minority among the observations. We thus transform the KDD Data set into two different data sets: SA and SF. -SA is obtained by simply selecting all the normal data, and a small proportion of abnormal data to gives an anomaly proportion of 1%. -SF is obtained as in [2] by simply picking up the data whose attribute logged_in is positive, thus focusing on the intrusion attack, which gives a proportion of 0.3% of attack. -http and smtp are two subsets of SF corresponding with third feature equal to http (resp. to smtp) General KDD structure : Samples total 4898431 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SA structure : Samples total 976158 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SF structure : Samples total 699691 Dimensionality 4 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type http structure : Samples total 619052 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type smtp structure : Samples total 95373 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type sklearn.datasets.fetch_kddcup99 will load the kddcup99 dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.7. California Housing dataset  Data Set Characteristics: Number of Instances 20640 Number of Attributes 8 numeric, predictive attributes and the target Attribute Information MedInc median income in block HouseAge median house age in block AveRooms average number of rooms AveBedrms average number of bedrooms Population block population AveOccup average house occupancy Latitude house block latitude Longitude house block longitude Missing Attribute Values None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the sklearn.datasets.fetch_california_housing function. References Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297 \"\\n',\n",
       " '\"sklearn_7_4_generated_datasets 7.4. Generated datasets datasets/index.html#generated-datasets  7.4. Generated datasets  In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. 7.4.1. Generators for classification and clustering  These generators produce a matrix of features and corresponding discrete targets. 7.4.1.1. Single label  Both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space. make_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem. make_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles. 7.4.1.2. Multilabel  make_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include: Per-topic word distributions are independently drawn, where in reality all would be affected by a sparse base distribution, and would be correlated. For a document generated from multiple topics, all topics are weighted equally in generating its bag of words. Documents without labels words at random, rather than from a base distribution. 7.4.1.3. Biclustering  make_biclusters (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with constant block diagonal structure for biclustering. make_checkerboard (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with block checkerboard structure for biclustering. 7.4.2. Generators for regression  make_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance). Other regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target. 7.4.3. Generators for manifold learning  make_s_curve ([n_samples,\\xa0noise,\\xa0random_state]) Generate an S curve dataset. make_swiss_roll ([n_samples,\\xa0noise,\\xa0random_state]) Generate a swiss roll dataset. 7.4.4. Generators for decomposition  make_low_rank_matrix ([n_samples,\\xa0]) Generate a mostly low rank matrix with bell-shaped singular values make_sparse_coded_signal (n_samples,\\xa0*,\\xa0) Generate a signal as a sparse combination of dictionary elements. make_spd_matrix (n_dim,\\xa0*[,\\xa0random_state]) Generate a random symmetric, positive-definite matrix. make_sparse_spd_matrix ([dim,\\xa0alpha,\\xa0]) Generate a sparse symmetric definite positive matrix. \"\\n',\n",
       " '\"sklearn_7_4_generated_datasets 7.4. Generated datasets datasets/index.html#generated-datasets  7.5. Loading other datasets  7.5.1. Sample images  Scikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data. load_sample_images () Load sample images for image manipulation. load_sample_image (image_name) Load the numpy array of a single sample image Warning The default coding of images is based on the dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use dont forget to scale to the range 0 - 1 as done in the following example. Examples: Color Quantization using K-Means 7.5.2. Datasets in svmlight / libsvm format  scikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form . This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for and numpy arrays are used for . You may load a dataset like as follows: You may also load two (or more) datasets at once: In this case, and are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features: Related links: Public datasets in svmlight / libsvm format : https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets Faster API-compatible implementation : https://github.com/mblondel/svmlight-loader 7.5.3. Downloading datasets from the openml.org repository  openml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets. The package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml . For example, to download a dataset of gene expressions in mice brains: To fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes: You can get more information on the dataset by looking at the and attributes: The contains a free-text description of the data, while contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website: The also uniquely identifies a dataset from OpenML: 7.5.3.1. Dataset Versions  A dataset is uniquely specified by its , but not necessarily by its name. Several different versions of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that can yield different results at different times if earlier versions become inactive. You can see that the dataset with 40966 that we fetched above is the version 1 of the miceprotein dataset: In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions: Specifying the dataset by the name iris yields the lowest version, version 1, with the 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset . The other dataset, with 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data: You can also specify both the name and the version, which also uniquely identifies the dataset: References: Vanschoren, van Rijn, Bischl and Torgo OpenML: networked science in machine learning , ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014. 7.5.4. Loading from external datasets  scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable. Here are some recommended ways to load standard columnar data into a format usable by scikit-learn: pandas.io provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn. scipy.io specializes in binary formats often used in scientific computing context such as .mat and .arff numpy/routines.io for standard loading of columnar data into numpy arrays scikit-learns datasets.load_svmlight_file for the svmlight or libSVM sparse format scikit-learns datasets.load_files for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category For some miscellaneous data such as images, videos, and audio, you may wish to refer to: skimage.io or Imageio for loading images and videos into numpy arrays scipy.io.wavfile.read for reading WAV files into a numpy array Categorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data . Note: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format. \"\\n',\n",
       " '\"sklearn_7_5_loading_other_datasets 7.5. Loading other datasets datasets/index.html#loading-other-datasets  7.1. General dataset API  There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset. The dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section. The dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section. Both loaders and fetchers functions return a sklearn.utils.Bunch object holding at least two items: an array of shape * with key (except for 20newsgroups) and a numpy array of length , containing the target values, with key . The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see sklearn.utils.Bunch : Its also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the parameter to . The datasets also contain a full description in their attribute and some contain and . See the dataset descriptions below for details. The dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section. These functions return a tuple consisting of a * numpy array and an array of length containing the targets . In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section. \"\\n',\n",
       " '\"sklearn_7_5_loading_other_datasets 7.5. Loading other datasets datasets/index.html#loading-other-datasets  7.2. Toy datasets  scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. They can be loaded using the following functions: load_boston (*[,\\xa0return_X_y]) Load and return the boston house-prices dataset (regression). load_iris (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the iris dataset (classification). load_diabetes (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the diabetes dataset (regression). load_digits (*[,\\xa0n_class,\\xa0return_X_y,\\xa0as_frame]) Load and return the digits dataset (classification). load_linnerud (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the physical excercise linnerud dataset. load_wine (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the wine dataset (classification). load_breast_cancer (*[,\\xa0return_X_y,\\xa0as_frame]) Load and return the breast cancer wisconsin dataset (classification). These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks. 7.2.1. Boston house prices dataset  Data Set Characteristics: Number of Instances 506 Number of Attributes 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. Attribute Information (in order) CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable ( 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per $10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT % lower status of the population MEDV Median value of owner-occupied homes in $1000s Missing Attribute Values None Creator Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, Regression diagnostics , Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. References Belsley, Kuh & Welsch, Regression diagnostics: Identifying Influential Data and Sources of Collinearity, Wiley, 1980. 244-261. Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. 7.2.2. Iris plants dataset  Data Set Characteristics: Number of Instances 150 (50 in each of three classes) Number of Attributes 4 numeric, predictive attributes and the class Attribute Information sepal length in cm sepal width in cm petal length in cm petal width in cm class: Iris-Setosa Iris-Versicolour Iris-Virginica Summary Statistics sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) Missing Attribute Values None Class Distribution 33.3% for each of 3 classes. Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fishers paper. Note that its the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fishers paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. References Fisher, R.A. The use of multiple measurements in taxonomic problems Annual Eugenics, 7, Part II, 179-188 (1936); also in Contributions to Mathematical Statistics (John Wiley, NY, 1950). Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. Dasarathy, B.V. (1980) Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. Gates, G.W. (1972) The Reduced Nearest Neighbor Rule. IEEE Transactions on Information Theory, May 1972, 431-433. See also: 1988 MLC Proceedings, 54-64. Cheeseman et als AUTOCLASS II conceptual clustering system finds 3 classes in the data. Many, many more  7.2.3. Diabetes dataset  Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n  442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. Data Set Characteristics: Number of Instances 442 Number of Attributes First 10 columns are numeric predictive values Target Column 11 is a quantitative measure of disease progression one year after baseline Attribute Information age age in years sex bmi body mass index bp average blood pressure s1 tc, T-Cells (a type of white blood cells) s2 ldl, low-density lipoproteins s3 hdl, high-density lipoproteins s4 tch, thyroid stimulating hormone s5 ltg, lamotrigine s6 glu, blood sugar level Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times (i.e. the sum of squares of each column totals 1). Source URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) Least Angle Regression, Annals of Statistics (with discussion), 407-499. ( https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf ) 7.2.4. Optical recognition of handwritten digits dataset  Data Set Characteristics: Number of Instances 5620 Number of Attributes 64 Attribute Information 8x8 image of integer pixels in the range 0..16. Missing Attribute Values None Creator Alpaydin (alpaydin @ boun.edu.tr) Date July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. References C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000. 7.2.5. Linnerrud dataset  Data Set Characteristics: Number of Instances 20 Number of Attributes 3 Missing Attribute Values None The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club: physiological - CSV containing 20 observations on 3 physiological variables: Weight, Waist and Pulse. exercise - CSV containing 20 observations on 3 exercise variables: Chins, Situps and Jumps. References Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. 7.2.6. Wine recognition dataset  Data Set Characteristics: Number of Instances 178 (50 in each of three classes) Number of Attributes 13 numeric, predictive attributes and the class Attribute Information Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline class: class_0 class_1 class_2 Summary Statistics Alcohol: 11.0 14.8 13.0 0.8 Malic Acid: 0.74 5.80 2.34 1.12 Ash: 1.36 3.23 2.36 0.27 Alcalinity of Ash: 10.6 30.0 19.5 3.3 Magnesium: 70.0 162.0 99.7 14.3 Total Phenols: 0.98 3.88 2.29 0.63 Flavanoids: 0.34 5.08 2.03 1.00 Nonflavanoid Phenols: 0.13 0.66 0.36 0.12 Proanthocyanins: 0.41 3.58 1.59 0.57 Colour Intensity: 1.3 13.0 5.1 2.3 Hue: 0.48 1.71 0.96 0.23 OD280/OD315 of diluted wines: 1.27 4.00 2.61 0.71 Proline: 278 1680 746 315 Missing Attribute Values None Class Distribution class_0 (59), class_1 (71), class_2 (48) Creator R.A. Fisher Donor Michael Marshall ( MARSHALL%PLU @ io . arc . nasa . gov ) Date July, 1988 This is a copy of UCI ML Wine recognition datasets. https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine. Original Owners: Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Citation: Lichman, M. (2013). UCI Machine Learning Repository [ https://archive.ics.uci.edu/ml ]. Irvine, CA: University of California, School of Information and Computer Science. References (1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics). The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique) (2) S. Aeberhard, D. Coomans and O. de Vel, THE CLASSIFICATION PERFORMANCE OF RDA Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics). 7.2.7. Breast cancer wisconsin (diagnostic) dataset  Data Set Characteristics: Number of Instances 569 Number of Attributes 30 numeric, predictive attributes and the class Attribute Information radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (coastline approximation - 1) The mean, standard error, and worst or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius. class: WDBC-Malignant WDBC-Benign Summary Statistics radius (mean): 6.981 28.11 texture (mean): 9.71 39.28 perimeter (mean): 43.79 188.5 area (mean): 143.5 2501.0 smoothness (mean): 0.053 0.163 compactness (mean): 0.019 0.345 concavity (mean): 0.0 0.427 concave points (mean): 0.0 0.201 symmetry (mean): 0.106 0.304 fractal dimension (mean): 0.05 0.097 radius (standard error): 0.112 2.873 texture (standard error): 0.36 4.885 perimeter (standard error): 0.757 21.98 area (standard error): 6.802 542.2 smoothness (standard error): 0.002 0.031 compactness (standard error): 0.002 0.135 concavity (standard error): 0.0 0.396 concave points (standard error): 0.0 0.053 symmetry (standard error): 0.008 0.079 fractal dimension (standard error): 0.001 0.03 radius (worst): 7.93 36.04 texture (worst): 12.02 49.54 perimeter (worst): 50.41 251.2 area (worst): 185.2 4254.0 smoothness (worst): 0.071 0.223 compactness (worst): 0.027 1.058 concavity (worst): 0.0 1.252 concave points (worst): 0.0 0.291 symmetry (worst): 0.156 0.664 fractal dimension (worst): 0.055 0.208 Missing Attribute Values None Class Distribution 212 - Malignant, 357 - Benign Creator Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian Donor Nick Street Date November, 1995 This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2 Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, Decision Tree Construction Via Linear Programming. Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: Robust Linear Programming Discrimination of Two Linearly Inseparable Sets, Optimization Methods and Software 1, 1992, 23-34]. This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/ References W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995. W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171. \"\\n',\n",
       " '\"sklearn_7_5_loading_other_datasets 7.5. Loading other datasets datasets/index.html#loading-other-datasets  7.3. Real world datasets  scikit-learn provides tools to load larger datasets, downloading them if necessary. They can be loaded using the following functions: fetch_olivetti_faces (*[,\\xa0data_home,\\xa0]) Load the Olivetti faces data-set from AT&T (classification). fetch_20newsgroups (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the filenames and data from the 20 newsgroups dataset (classification). fetch_20newsgroups_vectorized (*[,\\xa0subset,\\xa0]) Load the 20 newsgroups dataset and vectorize it into token counts (classification). fetch_lfw_people (*[,\\xa0data_home,\\xa0funneled,\\xa0]) Load the Labeled Faces in the Wild (LFW) people dataset (classification). fetch_lfw_pairs (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the Labeled Faces in the Wild (LFW) pairs dataset (classification). fetch_covtype (*[,\\xa0data_home,\\xa0]) Load the covertype dataset (classification). fetch_rcv1 (*[,\\xa0data_home,\\xa0subset,\\xa0]) Load the RCV1 multilabel dataset (classification). fetch_kddcup99 (*[,\\xa0subset,\\xa0data_home,\\xa0]) Load the kddcup99 dataset (classification). fetch_california_housing (*[,\\xa0data_home,\\xa0]) Load the California housing dataset (regression). 7.3.1. The Olivetti faces dataset  This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T. As described on the original website: There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). Data Set Characteristics: Classes 40 Samples total 400 Dimensionality 4096 Features real, between 0 and 1 The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms. The target for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective. The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images. When using these images, please give credit to AT&T Laboratories Cambridge. 7.3.2. The 20 newsgroups text dataset  The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups , returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized , returns ready-to-use features, i.e., it is not necessary to use a feature extractor. Data Set Characteristics: Classes 20 Samples total 18846 Dimensionality 1 Features text 7.3.2.1. Usage  The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website , extracts the archive contents in the folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them: The real data lies in the and attributes. The target attribute is the integer index of the category: It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function: 7.3.2.2. Converting text to vectors  In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news: The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features): sklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names. 7.3.2.3. Filtering text for more realistic training  It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that arent from this window of time. For example, lets look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score: (The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of whats going on inside this classifier?) Lets take a look at what the most informative features are: You can now see many things that these features have overfit to: Almost every group is distinguished by whether headers such as and appear more or less often. Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature. The word article is a significant feature, based on how often people quote previous posts like this: In article [article ID], [name] <[e-mail address]> wrote: Other features match the names and e-mail addresses of particular people who were posting at the time. With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level. For this reason, the functions that load 20 Newsgroups data provide a parameter called remove , telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of , telling it to remove headers, signature blocks, and quotation blocks respectively. This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data: Some other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the option to compare the results. Recommendation When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting . The F-score will be lower because it is more realistic. Examples Sample pipeline for text feature extraction and evaluation Classification of text documents using sparse features 7.3.3. The Labeled Faces in the Wild face recognition dataset  This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website: http://vis-www.cs.umass.edu/lfw/ Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person. An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons. Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites. Data Set Characteristics: Classes 5749 Samples total 13233 Dimensionality 5828 Features real, between 0 and 255 7.3.3.1. Usage  provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the folder using . The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning): The default slice is a rectangular shape around the face, removing most of the background: Each of the faces is assigned to a single person id in the array: The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person: Both for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing , in that case the shape will be . The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development set, the development set and an evaluation set meant to compute performance metrics using a 10-folds cross validation scheme. References: Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007. 7.3.3.2. Examples  Faces recognition example using eigenfaces and SVMs 7.3.4. Forest covertypes  The samples in this dataset correspond to 3030m patches of forest in the US, collected for the task of predicting each patchs cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the datasets homepage . Some of the features are boolean indicators, while others are discrete or continuous measurements. Data Set Characteristics: Classes 7 Samples total 581012 Dimensionality 54 Features int sklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.5. RCV1 dataset  Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in 1 . Data Set Characteristics: Classes 103 Samples total 804414 Dimensionality 47236 Features real, between 0 and 1 sklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels: It returns a dictionary-like object, with the following attributes: : The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in 1 : The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values: : The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values: : Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596: : The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for GMIL, to 381327 for CCAT: The dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB. References 1 ( 1 , 2 ) Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397. 7.3.6. Kddcup 99 dataset  The KDD Cup 99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the datasets homepage ) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting abnormal data, ie qualitatively different from normal data in large minority among the observations. We thus transform the KDD Data set into two different data sets: SA and SF. -SA is obtained by simply selecting all the normal data, and a small proportion of abnormal data to gives an anomaly proportion of 1%. -SF is obtained as in [2] by simply picking up the data whose attribute logged_in is positive, thus focusing on the intrusion attack, which gives a proportion of 0.3% of attack. -http and smtp are two subsets of SF corresponding with third feature equal to http (resp. to smtp) General KDD structure : Samples total 4898431 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SA structure : Samples total 976158 Dimensionality 41 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type SF structure : Samples total 699691 Dimensionality 4 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type http structure : Samples total 619052 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type smtp structure : Samples total 95373 Dimensionality 3 Features discrete (int) or continuous (float) Targets str, normal. or name of the anomaly type sklearn.datasets.fetch_kddcup99 will load the kddcup99 dataset; it returns a dictionary-like object with the feature matrix in the member and the target values in . The dataset will be downloaded from the web if necessary. 7.3.7. California Housing dataset  Data Set Characteristics: Number of Instances 20640 Number of Attributes 8 numeric, predictive attributes and the target Attribute Information MedInc median income in block HouseAge median house age in block AveRooms average number of rooms AveBedrms average number of bedrooms Population block population AveOccup average house occupancy Latitude house block latitude Longitude house block longitude Missing Attribute Values None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the sklearn.datasets.fetch_california_housing function. References Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297 \"\\n',\n",
       " '\"sklearn_7_5_loading_other_datasets 7.5. Loading other datasets datasets/index.html#loading-other-datasets  7.4. Generated datasets  In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. 7.4.1. Generators for classification and clustering  These generators produce a matrix of features and corresponding discrete targets. 7.4.1.1. Single label  Both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space. make_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem. make_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles. 7.4.1.2. Multilabel  make_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include: Per-topic word distributions are independently drawn, where in reality all would be affected by a sparse base distribution, and would be correlated. For a document generated from multiple topics, all topics are weighted equally in generating its bag of words. Documents without labels words at random, rather than from a base distribution. 7.4.1.3. Biclustering  make_biclusters (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with constant block diagonal structure for biclustering. make_checkerboard (shape,\\xa0n_clusters,\\xa0*[,\\xa0]) Generate an array with block checkerboard structure for biclustering. 7.4.2. Generators for regression  make_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance). Other regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target. 7.4.3. Generators for manifold learning  make_s_curve ([n_samples,\\xa0noise,\\xa0random_state]) Generate an S curve dataset. make_swiss_roll ([n_samples,\\xa0noise,\\xa0random_state]) Generate a swiss roll dataset. 7.4.4. Generators for decomposition  make_low_rank_matrix ([n_samples,\\xa0]) Generate a mostly low rank matrix with bell-shaped singular values make_sparse_coded_signal (n_samples,\\xa0*,\\xa0) Generate a signal as a sparse combination of dictionary elements. make_spd_matrix (n_dim,\\xa0*[,\\xa0random_state]) Generate a random symmetric, positive-definite matrix. make_sparse_spd_matrix ([dim,\\xa0alpha,\\xa0]) Generate a sparse symmetric definite positive matrix. \"\\n',\n",
       " '\"sklearn_7_5_loading_other_datasets 7.5. Loading other datasets datasets/index.html#loading-other-datasets  7.5. Loading other datasets  7.5.1. Sample images  Scikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data. load_sample_images () Load sample images for image manipulation. load_sample_image (image_name) Load the numpy array of a single sample image Warning The default coding of images is based on the dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use dont forget to scale to the range 0 - 1 as done in the following example. Examples: Color Quantization using K-Means 7.5.2. Datasets in svmlight / libsvm format  scikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form . This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for and numpy arrays are used for . You may load a dataset like as follows: You may also load two (or more) datasets at once: In this case, and are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features: Related links: Public datasets in svmlight / libsvm format : https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets Faster API-compatible implementation : https://github.com/mblondel/svmlight-loader 7.5.3. Downloading datasets from the openml.org repository  openml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets. The package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml . For example, to download a dataset of gene expressions in mice brains: To fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes: You can get more information on the dataset by looking at the and attributes: The contains a free-text description of the data, while contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website: The also uniquely identifies a dataset from OpenML: 7.5.3.1. Dataset Versions  A dataset is uniquely specified by its , but not necessarily by its name. Several different versions of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that can yield different results at different times if earlier versions become inactive. You can see that the dataset with 40966 that we fetched above is the version 1 of the miceprotein dataset: In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions: Specifying the dataset by the name iris yields the lowest version, version 1, with the 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset . The other dataset, with 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data: You can also specify both the name and the version, which also uniquely identifies the dataset: References: Vanschoren, van Rijn, Bischl and Torgo OpenML: networked science in machine learning , ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014. 7.5.4. Loading from external datasets  scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable. Here are some recommended ways to load standard columnar data into a format usable by scikit-learn: pandas.io provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn. scipy.io specializes in binary formats often used in scientific computing context such as .mat and .arff numpy/routines.io for standard loading of columnar data into numpy arrays scikit-learns datasets.load_svmlight_file for the svmlight or libSVM sparse format scikit-learns datasets.load_files for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category For some miscellaneous data such as images, videos, and audio, you may wish to refer to: skimage.io or Imageio for loading images and videos into numpy arrays scipy.io.wavfile.read for reading WAV files into a numpy array Categorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data . Note: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format. \"\\n',\n",
       " '\"sklearn_8_1_strategies_to_scale_computationally_bigger_data 8.1. Strategies to scale computationally: bigger data modules/computing.html#strategies-to-scale-computationally-bigger-data  8.1. Strategies to scale computationally: bigger data  For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale. 8.1.1. Scaling with instances using out-of-core learning  Out-of-core (or external memory) learning is a technique used to learn from data that cannot fit in a computers main memory (RAM). Here is a sketch of a system designed to achieve this goal: a way to stream instances a way to extract features from instances an incremental algorithm 8.1.1.1. Streaming instances  Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation. 8.1.1.2. Extracting features  2. could be any relevant way to extract features among the different feature extraction methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by sklearn.feature_extraction.FeatureHasher for datasets with categorical variables represented as list of Python dicts or sklearn.feature_extraction.text.HashingVectorizer for text documents. 8.1.1.3. Incremental learning  Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called online learning) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning 1 . Here is a list of incremental estimators for different tasks: Classification sklearn.naive_bayes.MultinomialNB sklearn.naive_bayes.BernoulliNB sklearn.linear_model.Perceptron sklearn.linear_model.SGDClassifier sklearn.linear_model.PassiveAggressiveClassifier sklearn.neural_network.MLPClassifier Regression sklearn.linear_model.SGDRegressor sklearn.linear_model.PassiveAggressiveRegressor sklearn.neural_network.MLPRegressor Clustering sklearn.cluster.MiniBatchKMeans sklearn.cluster.Birch Decomposition / feature Extraction sklearn.decomposition.MiniBatchDictionaryLearning sklearn.decomposition.IncrementalPCA sklearn.decomposition.LatentDirichletAllocation Preprocessing sklearn.preprocessing.StandardScaler sklearn.preprocessing.MinMaxScaler sklearn.preprocessing.MaxAbsScaler For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first call using the parameter. Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the is still sensitive to badly labeled examples even after many examples whereas the and families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time. 8.1.1.4. Examples  Finally, we have a full-fledged example of Out-of-core classification of text documents . It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above. Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples. Now looking at the computation time of the different parts, we see that the vectorization is much more expensive than learning itself. From the different algorithms, is the most expensive, but its overhead can be mitigated by increasing the size of the mini-batches (exercise: change to 100 and 10000 in the program and compare). 8.1.1.5. Notes  1 Depending on the algorithm the mini-batch size can influence results or not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly online and are not affected by batch size. Conversely, MiniBatchKMeans convergence rate is affected by the batch size. Also, its memory footprint can vary dramatically with batch size. \"\\n',\n",
       " '\"sklearn_8_1_strategies_to_scale_computationally_bigger_data 8.1. Strategies to scale computationally: bigger data modules/computing.html#strategies-to-scale-computationally-bigger-data  8.2. Computational Performance  For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline). We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks. Prediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile). Prediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second). An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones. 8.2.1. Prediction Latency  One of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment. The main factors that influence the prediction latency are Number of features Input data representation and sparsity Model complexity Feature extraction A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode. 8.2.1.1. Bulk versus Atomic mode  In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude: To benchmark different estimators for your case you can simply change the parameter in this example: Prediction Latency . This should give you an estimate of the order of magnitude of the prediction latency. 8.2.1.2. Configuring Scikit-learn for reduced validation overhead  Scikit-learn does some validation on data that increases the overhead per call to and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable to a non-empty string before importing scikit-learn, or configure it in Python with sklearn.set_config . For more control than these global settings, a config_context allows you to set this configuration within a specified context: Note that this will affect all uses of sklearn.utils.assert_all_finite within the context. 8.2.1.3. Influence of the Number of Features  Obviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of instances with features, the space complexity is in . From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features: Overall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator). 8.2.1.4. Influence of the Input Data Representation  Scipy provides sparse matrix data structures which are optimized for storing sparse data. The main feature of sparse formats is that you dont store zeros so if your data is sparse then you use much less memory. A non-zero value in a sparse ( CSR or CSC ) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation instead of 1e6. Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation. Here is sample code to test the sparsity of your input: As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipys sparse matrix formats documentation for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the and formats work best. 8.2.1.5. Influence of the Model Complexity  Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models. For sklearn.linear_model (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent. Here is an example using sklearn.linear_model.SGDClassifier with the penalty. The regularization strength is globally controlled by the parameter. With a sufficiently high , one can then increase the parameter of to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients. For the sklearn.svm family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the parameter of sklearn.svm.NuSVR was used to influence the number of support vectors. For sklearn.ensemble of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the parameter of sklearn.ensemble.gradient_boosting.GradientBoostingRegressor . In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process. 8.2.1.6. Feature Extraction Latency  Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model. In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application. 8.2.2. Prediction Throughput  Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the Prediction Latency example that measures this quantity for a number of estimators on synthetic data: These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the GIL ) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though. 8.2.3. Tips and Tricks  8.2.3.1. Linear algebra libraries  As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK library. Not all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs ( , , , ). On the other hand a linear model implemented with a BLAS DGEMM call (via ) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS. You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands: Optimized BLAS / LAPACK implementations include: Atlas (need hardware specific tuning by rebuilding on the target machine) OpenBLAS MKL Apple Accelerate and vecLib frameworks (OSX only) More information can be found on the Scipy install page and in this blog post from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu. 8.2.3.2. Limiting Working Memory  Some calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using sklearn.set_config or config_context . The following suggests to limit temporary working memory to 128 MiB: An example of a chunked operation adhering to this setting is metric.pairwise_distances_chunked , which facilitates computing row-wise reductions of a pairwise distance matrix. 8.2.3.3. Model Compression  Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation. Here is sample code that illustrates the use of the method: In this example we prefer the penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the parameter (in combination with the regularization strength ) to control this tradeoff. A typical benchmark on synthetic data yields a >30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers. 8.2.3.4. Model Reshaping  Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in format), it is generally sufficient to not generate the relevant features, leaving their columns empty. 8.2.3.5. Links  scikit-learn developer performance documentation Scipy sparse matrix formats documentation \"\\n',\n",
       " '\"sklearn_8_1_strategies_to_scale_computationally_bigger_data 8.1. Strategies to scale computationally: bigger data modules/computing.html#strategies-to-scale-computationally-bigger-data  8.3. Parallelism, resource management, and configuration  8.3.1. Parallelism  Some scikit-learn estimators and utilities can parallelize costly operations using multiple CPU cores, thanks to the following components: via the joblib library. In this case the number of threads or processes can be controlled with the parameter. via OpenMP, used in C or Cython code. In addition, some of the numpy routines that are used internally by scikit-learn may also be parallelized if numpy is installed with specific numerical libraries such as MKL, OpenBLAS, or BLIS. We describe these 3 scenarios in the following subsections. 8.3.1.1. Joblib-based parallelism  When the underlying implementation uses joblib, the number of workers (threads or processes) that are spawned in parallel can be controlled via the parameter. Note Where (and how) parallelization happens in the estimators is currently poorly documented. Please help us by improving our docs and tackle issue 14228 ! Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the backend that its using. Scikit-learn generally relies on the backend, which is joblibs default backend. Loky is a multi-processing backend. When doing multi-processing, in order to avoid duplicating the memory in each process (which isnt reasonable with big datasets), joblib will create a memmap that all processes can share, when the data is bigger than 1MB. In some specific cases (when the code that is run in parallel releases the GIL), scikit-learn will indicate to that a multi-threading backend is preferable. As a user, you may control the backend that joblib will use (regardless of what scikit-learn recommends) by using a context manager: Please refer to the joblibs docs for more details. In practice, whether parallelism is helpful at improving runtime depends on many factors. It is usually a good idea to experiment rather than assuming that increasing the number of workers is always a good thing. In some cases it can be highly detrimental to performance to run multiple copies of some estimators or functions in parallel (see oversubscription below). 8.3.1.2. OpenMP-based parallelism  OpenMP is used to parallelize code written in Cython or C, relying on multi-threading exclusively. By default (and unless joblib is trying to avoid oversubscription), the implementation will use as many threads as possible. You can control the exact number of threads that are used via the environment variable: 8.3.1.3. Parallel Numpy routines from numerical libraries  Scikit-learn relies heavily on NumPy and SciPy, which internally call multi-threaded linear algebra routines implemented in libraries such as MKL, OpenBLAS or BLIS. The number of threads used by the OpenBLAS, MKL or BLIS libraries can be set via the , , and environment variables. Please note that scikit-learn has no direct control over these implementations. Scikit-learn solely relies on Numpy and Scipy. Note At the time of writing (2019), NumPy and SciPy packages distributed on pypi.org (used by ) and on the conda-forge channel are linked with OpenBLAS, while conda packages shipped on the defaults channel from anaconda.org are linked by default with MKL. 8.3.1.4. Oversubscription: spawning too many threads  It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time. Suppose you have a machine with 8 CPUs. Consider a case where youre running a GridSearchCV (parallelized with joblib) with over a HistGradientBoostingClassifier (parallelized with OpenMP). Each instance of HistGradientBoostingClassifier will spawn 8 threads (since you have 8 CPUs). Thats a total of threads, which leads to oversubscription of physical CPU resources and to scheduling overhead. Oversubscription can arise in the exact same fashion with parallelized routines from MKL, OpenBLAS or BLIS that are nested in joblib calls. Starting from , when the backend is used (which is the default), joblib will tell its child processes to limit the number of threads they can use, so as to avoid oversubscription. In practice the heuristic that joblib uses is to tell the processes to use , via their corresponding environment variable. Back to our example from above, since the joblib backend of GridSearchCV is , each process will only be able to use 1 thread instead of 8, thus mitigating the oversubscription issue. Note that: Manually setting one of the environment variables ( , , , or ) will take precedence over what joblib tries to do. The total number of threads will be . Note that setting this limit will also impact your computations in the main process, which will only use . Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below). Joblib is currently unable to avoid oversubscription in a multi-threading context. It can only do so with the backend (which spawns processes). You will find additional details about joblib mitigation of oversubscription in joblib documentation . 8.3.2. Configuration switches  8.3.2.1. Python runtime  sklearn.set_config controls the following behaviors: assume_finite used to skip validation, which enables faster computations but may lead to segmentation faults if the data contains NaNs. working_memory the optimal size of temporary arrays used by some algorithms. 8.3.2.2. Environment variables  These environment variables should be set before importing scikit-learn. SKLEARN_SITE_JOBLIB When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikit-learn and joblib need to be compatible. Currently, joblib 0.11+ is supported. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets. Deprecated since version 0.21: As of version 0.21 this parameter has no effect, vendored joblib was removed and site joblib is always used. SKLEARN_ASSUME_FINITE Sets the default value for the argument of sklearn.set_config . SKLEARN_WORKING_MEMORY Sets the default value for the argument of sklearn.set_config . SKLEARN_SEED Sets the seed of the global random generator when running the tests, for reproducibility. SKLEARN_SKIP_NETWORK_TESTS When this environment variable is set to a non zero value, the tests that need network access are skipped. \"\\n',\n",
       " '\"sklearn_8_2_computational_performance 8.2. Computational Performance modules/computing.html#computational-performance  8.1. Strategies to scale computationally: bigger data  For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale. 8.1.1. Scaling with instances using out-of-core learning  Out-of-core (or external memory) learning is a technique used to learn from data that cannot fit in a computers main memory (RAM). Here is a sketch of a system designed to achieve this goal: a way to stream instances a way to extract features from instances an incremental algorithm 8.1.1.1. Streaming instances  Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation. 8.1.1.2. Extracting features  2. could be any relevant way to extract features among the different feature extraction methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by sklearn.feature_extraction.FeatureHasher for datasets with categorical variables represented as list of Python dicts or sklearn.feature_extraction.text.HashingVectorizer for text documents. 8.1.1.3. Incremental learning  Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called online learning) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning 1 . Here is a list of incremental estimators for different tasks: Classification sklearn.naive_bayes.MultinomialNB sklearn.naive_bayes.BernoulliNB sklearn.linear_model.Perceptron sklearn.linear_model.SGDClassifier sklearn.linear_model.PassiveAggressiveClassifier sklearn.neural_network.MLPClassifier Regression sklearn.linear_model.SGDRegressor sklearn.linear_model.PassiveAggressiveRegressor sklearn.neural_network.MLPRegressor Clustering sklearn.cluster.MiniBatchKMeans sklearn.cluster.Birch Decomposition / feature Extraction sklearn.decomposition.MiniBatchDictionaryLearning sklearn.decomposition.IncrementalPCA sklearn.decomposition.LatentDirichletAllocation Preprocessing sklearn.preprocessing.StandardScaler sklearn.preprocessing.MinMaxScaler sklearn.preprocessing.MaxAbsScaler For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first call using the parameter. Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the is still sensitive to badly labeled examples even after many examples whereas the and families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time. 8.1.1.4. Examples  Finally, we have a full-fledged example of Out-of-core classification of text documents . It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above. Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples. Now looking at the computation time of the different parts, we see that the vectorization is much more expensive than learning itself. From the different algorithms, is the most expensive, but its overhead can be mitigated by increasing the size of the mini-batches (exercise: change to 100 and 10000 in the program and compare). 8.1.1.5. Notes  1 Depending on the algorithm the mini-batch size can influence results or not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly online and are not affected by batch size. Conversely, MiniBatchKMeans convergence rate is affected by the batch size. Also, its memory footprint can vary dramatically with batch size. \"\\n',\n",
       " '\"sklearn_8_2_computational_performance 8.2. Computational Performance modules/computing.html#computational-performance  8.2. Computational Performance  For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline). We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks. Prediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile). Prediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second). An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones. 8.2.1. Prediction Latency  One of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment. The main factors that influence the prediction latency are Number of features Input data representation and sparsity Model complexity Feature extraction A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode. 8.2.1.1. Bulk versus Atomic mode  In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude: To benchmark different estimators for your case you can simply change the parameter in this example: Prediction Latency . This should give you an estimate of the order of magnitude of the prediction latency. 8.2.1.2. Configuring Scikit-learn for reduced validation overhead  Scikit-learn does some validation on data that increases the overhead per call to and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable to a non-empty string before importing scikit-learn, or configure it in Python with sklearn.set_config . For more control than these global settings, a config_context allows you to set this configuration within a specified context: Note that this will affect all uses of sklearn.utils.assert_all_finite within the context. 8.2.1.3. Influence of the Number of Features  Obviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of instances with features, the space complexity is in . From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features: Overall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator). 8.2.1.4. Influence of the Input Data Representation  Scipy provides sparse matrix data structures which are optimized for storing sparse data. The main feature of sparse formats is that you dont store zeros so if your data is sparse then you use much less memory. A non-zero value in a sparse ( CSR or CSC ) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation instead of 1e6. Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation. Here is sample code to test the sparsity of your input: As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipys sparse matrix formats documentation for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the and formats work best. 8.2.1.5. Influence of the Model Complexity  Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models. For sklearn.linear_model (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent. Here is an example using sklearn.linear_model.SGDClassifier with the penalty. The regularization strength is globally controlled by the parameter. With a sufficiently high , one can then increase the parameter of to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients. For the sklearn.svm family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the parameter of sklearn.svm.NuSVR was used to influence the number of support vectors. For sklearn.ensemble of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the parameter of sklearn.ensemble.gradient_boosting.GradientBoostingRegressor . In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process. 8.2.1.6. Feature Extraction Latency  Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model. In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application. 8.2.2. Prediction Throughput  Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the Prediction Latency example that measures this quantity for a number of estimators on synthetic data: These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the GIL ) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though. 8.2.3. Tips and Tricks  8.2.3.1. Linear algebra libraries  As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK library. Not all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs ( , , , ). On the other hand a linear model implemented with a BLAS DGEMM call (via ) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS. You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands: Optimized BLAS / LAPACK implementations include: Atlas (need hardware specific tuning by rebuilding on the target machine) OpenBLAS MKL Apple Accelerate and vecLib frameworks (OSX only) More information can be found on the Scipy install page and in this blog post from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu. 8.2.3.2. Limiting Working Memory  Some calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using sklearn.set_config or config_context . The following suggests to limit temporary working memory to 128 MiB: An example of a chunked operation adhering to this setting is metric.pairwise_distances_chunked , which facilitates computing row-wise reductions of a pairwise distance matrix. 8.2.3.3. Model Compression  Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation. Here is sample code that illustrates the use of the method: In this example we prefer the penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the parameter (in combination with the regularization strength ) to control this tradeoff. A typical benchmark on synthetic data yields a >30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers. 8.2.3.4. Model Reshaping  Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in format), it is generally sufficient to not generate the relevant features, leaving their columns empty. 8.2.3.5. Links  scikit-learn developer performance documentation Scipy sparse matrix formats documentation \"\\n',\n",
       " '\"sklearn_8_2_computational_performance 8.2. Computational Performance modules/computing.html#computational-performance  8.3. Parallelism, resource management, and configuration  8.3.1. Parallelism  Some scikit-learn estimators and utilities can parallelize costly operations using multiple CPU cores, thanks to the following components: via the joblib library. In this case the number of threads or processes can be controlled with the parameter. via OpenMP, used in C or Cython code. In addition, some of the numpy routines that are used internally by scikit-learn may also be parallelized if numpy is installed with specific numerical libraries such as MKL, OpenBLAS, or BLIS. We describe these 3 scenarios in the following subsections. 8.3.1.1. Joblib-based parallelism  When the underlying implementation uses joblib, the number of workers (threads or processes) that are spawned in parallel can be controlled via the parameter. Note Where (and how) parallelization happens in the estimators is currently poorly documented. Please help us by improving our docs and tackle issue 14228 ! Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the backend that its using. Scikit-learn generally relies on the backend, which is joblibs default backend. Loky is a multi-processing backend. When doing multi-processing, in order to avoid duplicating the memory in each process (which isnt reasonable with big datasets), joblib will create a memmap that all processes can share, when the data is bigger than 1MB. In some specific cases (when the code that is run in parallel releases the GIL), scikit-learn will indicate to that a multi-threading backend is preferable. As a user, you may control the backend that joblib will use (regardless of what scikit-learn recommends) by using a context manager: Please refer to the joblibs docs for more details. In practice, whether parallelism is helpful at improving runtime depends on many factors. It is usually a good idea to experiment rather than assuming that increasing the number of workers is always a good thing. In some cases it can be highly detrimental to performance to run multiple copies of some estimators or functions in parallel (see oversubscription below). 8.3.1.2. OpenMP-based parallelism  OpenMP is used to parallelize code written in Cython or C, relying on multi-threading exclusively. By default (and unless joblib is trying to avoid oversubscription), the implementation will use as many threads as possible. You can control the exact number of threads that are used via the environment variable: 8.3.1.3. Parallel Numpy routines from numerical libraries  Scikit-learn relies heavily on NumPy and SciPy, which internally call multi-threaded linear algebra routines implemented in libraries such as MKL, OpenBLAS or BLIS. The number of threads used by the OpenBLAS, MKL or BLIS libraries can be set via the , , and environment variables. Please note that scikit-learn has no direct control over these implementations. Scikit-learn solely relies on Numpy and Scipy. Note At the time of writing (2019), NumPy and SciPy packages distributed on pypi.org (used by ) and on the conda-forge channel are linked with OpenBLAS, while conda packages shipped on the defaults channel from anaconda.org are linked by default with MKL. 8.3.1.4. Oversubscription: spawning too many threads  It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time. Suppose you have a machine with 8 CPUs. Consider a case where youre running a GridSearchCV (parallelized with joblib) with over a HistGradientBoostingClassifier (parallelized with OpenMP). Each instance of HistGradientBoostingClassifier will spawn 8 threads (since you have 8 CPUs). Thats a total of threads, which leads to oversubscription of physical CPU resources and to scheduling overhead. Oversubscription can arise in the exact same fashion with parallelized routines from MKL, OpenBLAS or BLIS that are nested in joblib calls. Starting from , when the backend is used (which is the default), joblib will tell its child processes to limit the number of threads they can use, so as to avoid oversubscription. In practice the heuristic that joblib uses is to tell the processes to use , via their corresponding environment variable. Back to our example from above, since the joblib backend of GridSearchCV is , each process will only be able to use 1 thread instead of 8, thus mitigating the oversubscription issue. Note that: Manually setting one of the environment variables ( , , , or ) will take precedence over what joblib tries to do. The total number of threads will be . Note that setting this limit will also impact your computations in the main process, which will only use . Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below). Joblib is currently unable to avoid oversubscription in a multi-threading context. It can only do so with the backend (which spawns processes). You will find additional details about joblib mitigation of oversubscription in joblib documentation . 8.3.2. Configuration switches  8.3.2.1. Python runtime  sklearn.set_config controls the following behaviors: assume_finite used to skip validation, which enables faster computations but may lead to segmentation faults if the data contains NaNs. working_memory the optimal size of temporary arrays used by some algorithms. 8.3.2.2. Environment variables  These environment variables should be set before importing scikit-learn. SKLEARN_SITE_JOBLIB When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikit-learn and joblib need to be compatible. Currently, joblib 0.11+ is supported. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets. Deprecated since version 0.21: As of version 0.21 this parameter has no effect, vendored joblib was removed and site joblib is always used. SKLEARN_ASSUME_FINITE Sets the default value for the argument of sklearn.set_config . SKLEARN_WORKING_MEMORY Sets the default value for the argument of sklearn.set_config . SKLEARN_SEED Sets the seed of the global random generator when running the tests, for reproducibility. SKLEARN_SKIP_NETWORK_TESTS When this environment variable is set to a non zero value, the tests that need network access are skipped. \"\\n',\n",
       " '\"sklearn_8_3_parallelism_resource_management_and_configuration 8.3. Parallelism, resource management, and configuration modules/computing.html#parallelism-resource-management-and-configuration  8.1. Strategies to scale computationally: bigger data  For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale. 8.1.1. Scaling with instances using out-of-core learning  Out-of-core (or external memory) learning is a technique used to learn from data that cannot fit in a computers main memory (RAM). Here is a sketch of a system designed to achieve this goal: a way to stream instances a way to extract features from instances an incremental algorithm 8.1.1.1. Streaming instances  Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation. 8.1.1.2. Extracting features  2. could be any relevant way to extract features among the different feature extraction methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by sklearn.feature_extraction.FeatureHasher for datasets with categorical variables represented as list of Python dicts or sklearn.feature_extraction.text.HashingVectorizer for text documents. 8.1.1.3. Incremental learning  Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called online learning) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning 1 . Here is a list of incremental estimators for different tasks: Classification sklearn.naive_bayes.MultinomialNB sklearn.naive_bayes.BernoulliNB sklearn.linear_model.Perceptron sklearn.linear_model.SGDClassifier sklearn.linear_model.PassiveAggressiveClassifier sklearn.neural_network.MLPClassifier Regression sklearn.linear_model.SGDRegressor sklearn.linear_model.PassiveAggressiveRegressor sklearn.neural_network.MLPRegressor Clustering sklearn.cluster.MiniBatchKMeans sklearn.cluster.Birch Decomposition / feature Extraction sklearn.decomposition.MiniBatchDictionaryLearning sklearn.decomposition.IncrementalPCA sklearn.decomposition.LatentDirichletAllocation Preprocessing sklearn.preprocessing.StandardScaler sklearn.preprocessing.MinMaxScaler sklearn.preprocessing.MaxAbsScaler For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first call using the parameter. Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the is still sensitive to badly labeled examples even after many examples whereas the and families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time. 8.1.1.4. Examples  Finally, we have a full-fledged example of Out-of-core classification of text documents . It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above. Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples. Now looking at the computation time of the different parts, we see that the vectorization is much more expensive than learning itself. From the different algorithms, is the most expensive, but its overhead can be mitigated by increasing the size of the mini-batches (exercise: change to 100 and 10000 in the program and compare). 8.1.1.5. Notes  1 Depending on the algorithm the mini-batch size can influence results or not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly online and are not affected by batch size. Conversely, MiniBatchKMeans convergence rate is affected by the batch size. Also, its memory footprint can vary dramatically with batch size. \"\\n',\n",
       " '\"sklearn_8_3_parallelism_resource_management_and_configuration 8.3. Parallelism, resource management, and configuration modules/computing.html#parallelism-resource-management-and-configuration  8.2. Computational Performance  For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline). We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks. Prediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile). Prediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second). An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones. 8.2.1. Prediction Latency  One of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment. The main factors that influence the prediction latency are Number of features Input data representation and sparsity Model complexity Feature extraction A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode. 8.2.1.1. Bulk versus Atomic mode  In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude: To benchmark different estimators for your case you can simply change the parameter in this example: Prediction Latency . This should give you an estimate of the order of magnitude of the prediction latency. 8.2.1.2. Configuring Scikit-learn for reduced validation overhead  Scikit-learn does some validation on data that increases the overhead per call to and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable to a non-empty string before importing scikit-learn, or configure it in Python with sklearn.set_config . For more control than these global settings, a config_context allows you to set this configuration within a specified context: Note that this will affect all uses of sklearn.utils.assert_all_finite within the context. 8.2.1.3. Influence of the Number of Features  Obviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of instances with features, the space complexity is in . From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features: Overall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator). 8.2.1.4. Influence of the Input Data Representation  Scipy provides sparse matrix data structures which are optimized for storing sparse data. The main feature of sparse formats is that you dont store zeros so if your data is sparse then you use much less memory. A non-zero value in a sparse ( CSR or CSC ) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation instead of 1e6. Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation. Here is sample code to test the sparsity of your input: As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipys sparse matrix formats documentation for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the and formats work best. 8.2.1.5. Influence of the Model Complexity  Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models. For sklearn.linear_model (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent. Here is an example using sklearn.linear_model.SGDClassifier with the penalty. The regularization strength is globally controlled by the parameter. With a sufficiently high , one can then increase the parameter of to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients. For the sklearn.svm family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the parameter of sklearn.svm.NuSVR was used to influence the number of support vectors. For sklearn.ensemble of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the parameter of sklearn.ensemble.gradient_boosting.GradientBoostingRegressor . In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process. 8.2.1.6. Feature Extraction Latency  Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model. In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application. 8.2.2. Prediction Throughput  Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the Prediction Latency example that measures this quantity for a number of estimators on synthetic data: These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the GIL ) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though. 8.2.3. Tips and Tricks  8.2.3.1. Linear algebra libraries  As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK library. Not all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs ( , , , ). On the other hand a linear model implemented with a BLAS DGEMM call (via ) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS. You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands: Optimized BLAS / LAPACK implementations include: Atlas (need hardware specific tuning by rebuilding on the target machine) OpenBLAS MKL Apple Accelerate and vecLib frameworks (OSX only) More information can be found on the Scipy install page and in this blog post from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu. 8.2.3.2. Limiting Working Memory  Some calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using sklearn.set_config or config_context . The following suggests to limit temporary working memory to 128 MiB: An example of a chunked operation adhering to this setting is metric.pairwise_distances_chunked , which facilitates computing row-wise reductions of a pairwise distance matrix. 8.2.3.3. Model Compression  Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation. Here is sample code that illustrates the use of the method: In this example we prefer the penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the parameter (in combination with the regularization strength ) to control this tradeoff. A typical benchmark on synthetic data yields a >30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers. 8.2.3.4. Model Reshaping  Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in format), it is generally sufficient to not generate the relevant features, leaving their columns empty. 8.2.3.5. Links  scikit-learn developer performance documentation Scipy sparse matrix formats documentation \"\\n',\n",
       " '\"sklearn_8_3_parallelism_resource_management_and_configuration 8.3. Parallelism, resource management, and configuration modules/computing.html#parallelism-resource-management-and-configuration  8.3. Parallelism, resource management, and configuration  8.3.1. Parallelism  Some scikit-learn estimators and utilities can parallelize costly operations using multiple CPU cores, thanks to the following components: via the joblib library. In this case the number of threads or processes can be controlled with the parameter. via OpenMP, used in C or Cython code. In addition, some of the numpy routines that are used internally by scikit-learn may also be parallelized if numpy is installed with specific numerical libraries such as MKL, OpenBLAS, or BLIS. We describe these 3 scenarios in the following subsections. 8.3.1.1. Joblib-based parallelism  When the underlying implementation uses joblib, the number of workers (threads or processes) that are spawned in parallel can be controlled via the parameter. Note Where (and how) parallelization happens in the estimators is currently poorly documented. Please help us by improving our docs and tackle issue 14228 ! Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the backend that its using. Scikit-learn generally relies on the backend, which is joblibs default backend. Loky is a multi-processing backend. When doing multi-processing, in order to avoid duplicating the memory in each process (which isnt reasonable with big datasets), joblib will create a memmap that all processes can share, when the data is bigger than 1MB. In some specific cases (when the code that is run in parallel releases the GIL), scikit-learn will indicate to that a multi-threading backend is preferable. As a user, you may control the backend that joblib will use (regardless of what scikit-learn recommends) by using a context manager: Please refer to the joblibs docs for more details. In practice, whether parallelism is helpful at improving runtime depends on many factors. It is usually a good idea to experiment rather than assuming that increasing the number of workers is always a good thing. In some cases it can be highly detrimental to performance to run multiple copies of some estimators or functions in parallel (see oversubscription below). 8.3.1.2. OpenMP-based parallelism  OpenMP is used to parallelize code written in Cython or C, relying on multi-threading exclusively. By default (and unless joblib is trying to avoid oversubscription), the implementation will use as many threads as possible. You can control the exact number of threads that are used via the environment variable: 8.3.1.3. Parallel Numpy routines from numerical libraries  Scikit-learn relies heavily on NumPy and SciPy, which internally call multi-threaded linear algebra routines implemented in libraries such as MKL, OpenBLAS or BLIS. The number of threads used by the OpenBLAS, MKL or BLIS libraries can be set via the , , and environment variables. Please note that scikit-learn has no direct control over these implementations. Scikit-learn solely relies on Numpy and Scipy. Note At the time of writing (2019), NumPy and SciPy packages distributed on pypi.org (used by ) and on the conda-forge channel are linked with OpenBLAS, while conda packages shipped on the defaults channel from anaconda.org are linked by default with MKL. 8.3.1.4. Oversubscription: spawning too many threads  It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time. Suppose you have a machine with 8 CPUs. Consider a case where youre running a GridSearchCV (parallelized with joblib) with over a HistGradientBoostingClassifier (parallelized with OpenMP). Each instance of HistGradientBoostingClassifier will spawn 8 threads (since you have 8 CPUs). Thats a total of threads, which leads to oversubscription of physical CPU resources and to scheduling overhead. Oversubscription can arise in the exact same fashion with parallelized routines from MKL, OpenBLAS or BLIS that are nested in joblib calls. Starting from , when the backend is used (which is the default), joblib will tell its child processes to limit the number of threads they can use, so as to avoid oversubscription. In practice the heuristic that joblib uses is to tell the processes to use , via their corresponding environment variable. Back to our example from above, since the joblib backend of GridSearchCV is , each process will only be able to use 1 thread instead of 8, thus mitigating the oversubscription issue. Note that: Manually setting one of the environment variables ( , , , or ) will take precedence over what joblib tries to do. The total number of threads will be . Note that setting this limit will also impact your computations in the main process, which will only use . Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below). Joblib is currently unable to avoid oversubscription in a multi-threading context. It can only do so with the backend (which spawns processes). You will find additional details about joblib mitigation of oversubscription in joblib documentation . 8.3.2. Configuration switches  8.3.2.1. Python runtime  sklearn.set_config controls the following behaviors: assume_finite used to skip validation, which enables faster computations but may lead to segmentation faults if the data contains NaNs. working_memory the optimal size of temporary arrays used by some algorithms. 8.3.2.2. Environment variables  These environment variables should be set before importing scikit-learn. SKLEARN_SITE_JOBLIB When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikit-learn and joblib need to be compatible. Currently, joblib 0.11+ is supported. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets. Deprecated since version 0.21: As of version 0.21 this parameter has no effect, vendored joblib was removed and site joblib is always used. SKLEARN_ASSUME_FINITE Sets the default value for the argument of sklearn.set_config . SKLEARN_WORKING_MEMORY Sets the default value for the argument of sklearn.set_config . SKLEARN_SEED Sets the seed of the global random generator when running the tests, for reproducibility. SKLEARN_SKIP_NETWORK_TESTS When this environment variable is set to a non zero value, the tests that need network access are skipped. \"\\n',\n",
       " '\"scipy_compressed_sparse_graph_routines_scipy_sparse_csgraph Compressed Sparse Graph Routines (scipy.sparse.csgraph) csgraph.html  Example: Word Ladders  A Word Ladder is a word game invented by Lewis Carroll, in which players find paths between words by switching one letter at a time. For example, one can link ape and man in the following way: \\\\[{\\\\rm ape \\\\to apt \\\\to ait \\\\to bit \\\\to big \\\\to bag \\\\to mag \\\\to man}\\\\] Note that each step involves changing just one letter of the word. This is just one possible path from ape to man, but is it the shortest possible path? If we desire to find the shortest word-ladder path between two given words, the sparse graph submodule can help. First, we need a list of valid words. Many operating systems have such a list built in. For example, on linux, a word list can often be found at one of the following locations: Another easy source for words are the Scrabble word lists available at various sites around the internet (search with your favorite search engine). Well first create this list. The system word lists consist of a file with one word per line. The following should be modified to use the particular word list you have available: We want to look at words of length 3, so lets select just those words of the correct length. Well also eliminate words which start with upper-case (proper nouns) or contain non-alphanumeric characters, like apostrophes and hyphens. Finally, well make sure everything is lower-case for comparison later: Now we have a list of 586 valid three-letter words (the exact number may change depending on the particular list used). Each of these words will become a node in our graph, and we will create edges connecting the nodes associated with each pair of words which differs by only one letter. There are efficient ways to do this, and inefficient ways to do this. To do this as efficiently as possible, were going to use some sophisticated numpy array manipulation: We have an array where each entry is three unicode characters long. Wed like to find all pairs where exactly one character is different. Well start by converting each word to a 3-D vector: Now, well use the Hamming distance between each point to determine which pairs of words are connected. The Hamming distance measures the fraction of entries between two vectors which differ: any two words with a Hamming distance equal to , where is the number of letters, are connected in the word ladder: When comparing the distances, we dont use an equality because this can be unstable for floating point values. The inequality produces the desired result, as long as no two entries of the word list are identical. Now, that our graph is set up, well use a shortest path search to find the path between any two words in the graph: We need to check that these match, because if the words are not in the list, that will not be the case. Now, all we need is to find the shortest path between these two indices in the graph. Well use Dijkstras algorithm , because it allows us to find the path for just one node: So we see that the shortest path between ape and man contains only five steps. We can use the predecessors returned by the algorithm to reconstruct this path: This is three fewer links than our initial example: the path from ape to man is only five steps. Using other tools in the module, we can answer other questions. For example, are there three-letter words which are not linked in a word ladder? This is a question of connected components in the graph: In this particular sample of three-letter words, there are 15 connected components: that is, 15 distinct sets of words with no paths between the sets. How many words are there in each of these sets? We can learn this from the list of components: There is one large connected set and 14 smaller ones. Lets look at the words in the smaller ones: These are all the three-letter words which do not connect to others via a word ladder. We might also be curious about which words are maximally separated. Which two words take the most links to connect? We can determine this by computing the matrix of all shortest paths. Note that, by convention, the distance between two non-connected points is reported to be infinity, so well need to remove these before finding the maximum: So, there is at least one pair of words which takes 13 steps to get from one to the other! Lets determine which these are: We see that there are two pairs of words which are maximally separated from each other: imp and ump on the one hand, and ohm and ohs on the other. We can find the connecting list in the same way as above: This gives us the path we desired to see. Word ladders are just one potential application of scipys fast graph algorithms for sparse matrices. Graph theory makes appearances in many areas of mathematics, data analysis, and machine learning. The sparse graph tools are flexible enough to handle many of these situations. \"\\n',\n",
       " '\"scipy_file_io_scipy_io File IO (scipy.io) io.html  Arff files ( scipy.io.arff )  loadarff (f) Read an arff file. \"\\n',\n",
       " '\"scipy_file_io_scipy_io File IO (scipy.io) io.html  IDL files  readsav (file_name[,\\xa0idict,\\xa0python_dict,\\xa0]) Read an IDL .sav file. \"\\n',\n",
       " '\"scipy_file_io_scipy_io File IO (scipy.io) io.html  MATLAB files  loadmat (file_name[,\\xa0mdict,\\xa0appendmat]) Load MATLAB file. savemat (file_name,\\xa0mdict[,\\xa0appendmat,\\xa0]) Save a dictionary of names and arrays into a MATLAB-style .mat file. whosmat (file_name[,\\xa0appendmat]) List variables inside a MATLAB file. The basic functions  Well start by importing scipy.io and calling it for convenience: If you are using IPython, try tab-completing on . Among the many options, you will find: These are the high-level functions you will most likely use when working with MATLAB files. Youll also find: This is the package from which , , and are imported. Within , you will find the module This module contains the machinery that and use. From time to time you may find yourself re-using this machinery. How do I start?  You may have a file that you want to read into SciPy. Or, you want to pass some variables from SciPy / NumPy into MATLAB. To save us using a MATLAB license, lets start in Octave . Octave has MATLAB-compatible save and load functions. Start Octave ( at the command line for me): Now, to Python: Now lets try the other way round: Then back to Octave: If you want to inspect the contents of a MATLAB file without reading the data into memory, use the command: returns a list of tuples, one for each array (or other object) in the file. Each tuple contains the name, shape and data type of the array. MATLAB structs  MATLAB structs are a little bit like Python dicts, except the field names must be strings. Any MATLAB object can be a value of a field. As for all objects in MATLAB, structs are, in fact, arrays of structs, where a single struct is an array of shape (1, 1). We can load this in Python: In the SciPy versions from 0.12.0, MATLAB structs come back as NumPy structured arrays, with fields named for the struct fields. You can see the field names in the output above. Note also: and: So, in MATLAB, the struct array must be at least 2-D, and we replicate that when we read into SciPy. If you want all length 1 dimensions squeezed out, try this: Sometimes, its more convenient to load the MATLAB structs as Python objects rather than NumPy structured arrays - it can make the access syntax in Python a bit more similar to that in MATLAB. In order to do this, use the parameter setting to . works nicely with : Saving struct arrays can be done in various ways. One simple method is to use dicts: loaded as: You can also save structs back again to MATLAB (or Octave in our case) like this: MATLAB cell arrays  Cell arrays in MATLAB are rather like Python lists, in the sense that the elements in the arrays can contain any type of MATLAB object. In fact, they are most similar to NumPy object arrays, and that is how we load them into NumPy. Back to Python: Saving to a MATLAB cell array just involves making a NumPy object array: \"\\n',\n",
       " '\"scipy_file_io_scipy_io File IO (scipy.io) io.html  Matrix Market files  mminfo (source) Return size and storage parameters from Matrix Market file-like source. mmread (source) Reads the contents of a Matrix Market file-like source into a matrix. mmwrite (target,\\xa0a[,\\xa0comment,\\xa0field,\\xa0]) Writes the sparse or dense array a to Matrix Market file-like target . \"\\n',\n",
       " '\"scipy_file_io_scipy_io File IO (scipy.io) io.html  Netcdf  netcdf_file (filename[,\\xa0mode,\\xa0mmap,\\xa0version,\\xa0]) A file object for NetCDF data. Allows reading of NetCDF files (version of pupynere package) \"\\n',\n",
       " '\"scipy_file_io_scipy_io File IO (scipy.io) io.html  Wav sound files ( scipy.io.wavfile )  read (filename[,\\xa0mmap]) Open a WAV file write (filename,\\xa0rate,\\xa0data) Write a NumPy array as a WAV file. \"\\n',\n",
       " '\"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  Discrete Cosine Transforms  SciPy provides a DCT with the function dct and a corresponding IDCT with the function idct . There are 8 types of the DCT [WPC] , [Mak] ; however, only the first 3 types are implemented in scipy. The DCT generally refers to DCT type 2, and the Inverse DCT generally refers to DCT type 3. In addition, the DCT coefficients can be normalized differently (for most types, scipy provides and ). Two parameters of the dct/idct function calls allow setting the DCT type and coefficient normalization. For a single dimension array x, dct(x, normortho) is equal to MATLAB dct(x). Type I DCT  SciPy uses the following definition of the unnormalized DCT-I ( ): \\\\[y[k]  x_0 + (-1)^k x_{N-1} + 2\\\\sum_{n1}^{N-2} x[n] \\\\cos\\\\left(\\\\frac{\\\\pi nk}{N-1}\\\\right), \\\\qquad 0 \\\\le k < N.\\\\] Note that the DCT-I is only supported for input size > 1. Type II DCT  SciPy uses the following definition of the unnormalized DCT-II ( ): \\\\[y[k]  2 \\\\sum_{n0}^{N-1} x[n] \\\\cos \\\\left({\\\\pi(2n+1)k \\\\over 2N} \\\\right) \\\\qquad 0 \\\\le k < N.\\\\] In case of the normalized DCT ( ), the DCT coefficients are multiplied by a scaling factor f : \\\\[\\\\begin{split}f  \\\\begin{cases} \\\\sqrt{1/(4N)}, & \\\\text{if $k  0$} \\\\\\\\ \\\\sqrt{1/(2N)}, & \\\\text{otherwise} \\\\end{cases} \\\\, .\\\\end{split}\\\\] In this case, the DCT base functions become orthonormal: \\\\[\\\\sum_{n0}^{N-1} \\\\phi_k[n] \\\\phi_l[n]  \\\\delta_{lk}.\\\\] Type III DCT  SciPy uses the following definition of the unnormalized DCT-III ( ): \\\\[y[k]  x_0 + 2 \\\\sum_{n1}^{N-1} x[n] \\\\cos\\\\left({\\\\pi n(2k+1) \\\\over 2N}\\\\right) \\\\qquad 0 \\\\le k < N,\\\\] or, for : \\\\[y[k]  {x_0\\\\over\\\\sqrt{N}} + {2\\\\over\\\\sqrt{N}} \\\\sum_{n1}^{N-1} x[n] \\\\cos\\\\left({\\\\pi n(2k+1) \\\\over 2N}\\\\right) \\\\qquad 0 \\\\le k < N.\\\\] DCT and IDCT  The (unnormalized) DCT-III is the inverse of the (unnormalized) DCT-II, up to a factor of 2N . The orthonormalized DCT-III is exactly the inverse of the orthonormalized DCT- II. The function idct performs the mappings between the DCT and IDCT types, as well as the correct normalization. The following example shows the relation between DCT and IDCT for different types and normalizations. The DCT-II and DCT-III are each others inverses, so for an orthonormal transform we return back to the original signal. Doing the same under default normalization, however, we pick up an extra scaling factor of since the forward transform is unnormalized. For this reason, we should use the function idct using the same type for both, giving a correctly normalized result. Analogous results can be seen for the DCT-I, which is its own inverse up to a factor of . And for the DCT-IV, which is also its own inverse up to a factor of . Example  The DCT exhibits the energy compaction property, meaning that for many signals only the first few DCT coefficients have significant magnitude. Zeroing out the other coefficients leads to a small reconstruction error, a fact which is exploited in lossy signal compression (e.g. JPEG compression). The example below shows a signal x and two reconstructions ( and ) from the signals DCT coefficients. The signal is reconstructed from the first 20 DCT coefficients, is reconstructed from the first 15 DCT coefficients. It can be seen that the relative error of using 20 coefficients is still very small (~0.1%), but provides a five-fold compression rate. \"\\n',\n",
       " '\"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  Discrete Sine Transforms  SciPy provides a DST [Mak] with the function dst and a corresponding IDST with the function idst . There are, theoretically, 8 types of the DST for different combinations of even/odd boundary conditions and boundary off sets [WPS] , only the first 3 types are implemented in scipy. Type I DST  DST-I assumes the input is odd around n-1 and nN. SciPy uses the following definition of the unnormalized DST-I ( ): \\\\[y[k]  2\\\\sum_{n0}^{N-1} x[n] \\\\sin\\\\left( \\\\pi {(n+1) (k+1)}\\\\over{N+1} \\\\right), \\\\qquad 0 \\\\le k < N.\\\\] Note also that the DST-I is only supported for input size > 1. The (unnormalized) DST-I is its own inverse, up to a factor of 2(N+1) . Type II DST  DST-II assumes the input is odd around n-1/2 and even around nN. SciPy uses the following definition of the unnormalized DST-II ( ): \\\\[y[k]  2 \\\\sum_{n0}^{N-1} x[n] \\\\sin\\\\left( {\\\\pi (n+1/2)(k+1)} \\\\over N \\\\right), \\\\qquad 0 \\\\le k < N.\\\\] Type III DST  DST-III assumes the input is odd around n-1 and even around nN-1. SciPy uses the following definition of the unnormalized DST-III ( ): \\\\[y[k]  (-1)^k x[N-1] + 2 \\\\sum_{n0}^{N-2} x[n] \\\\sin \\\\left( {\\\\pi (n+1)(k+1/2)} \\\\over N \\\\right), \\\\qquad 0 \\\\le k < N.\\\\] DST and IDST  The following example shows the relation between DST and IDST for different types and normalizations. The DST-II and DST-III are each others inverses, so for an orthonormal transform we return back to the original signal. Doing the same under default normalization, however, we pick up an extra scaling factor of since the forward transform is unnormalized. For this reason, we should use the function idst using the same type for both, giving a correctly normalized result. Analogous results can be seen for the DST-I, which is its own inverse up to a factor of . And for the DST-IV, which is also its own inverse up to a factor of . \"\\n',\n",
       " '\"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  Fast Fourier transforms  1-D discrete Fourier transforms  The FFT y[k] of length of the length- sequence x[n] is defined as \\\\[y[k]  \\\\sum_{n0}^{N-1} e^{-2 \\\\pi j \\\\frac{k n}{N} } x[n] \\\\, ,\\\\] and the inverse transform is defined as follows \\\\[x[n]  \\\\frac{1}{N} \\\\sum_{k0}^{N-1} e^{2 \\\\pi j \\\\frac{k n}{N} } y[k] \\\\, .\\\\] These transforms can be calculated by means of fft and ifft , respectively, as shown in the following example. From the definition of the FFT it can be seen that \\\\[y[0]  \\\\sum_{n0}^{N-1} x[n] \\\\, .\\\\] In the example which corresponds to . For N even, the elements contain the positive-frequency terms, and the elements contain the negative-frequency terms, in order of decreasingly negative frequency. For N odd, the elements contain the positive-frequency terms, and the elements contain the negative-frequency terms, in order of decreasingly negative frequency. In case the sequence x is real-valued, the values of for positive frequencies is the conjugate of the values for negative frequencies (because the spectrum is symmetric). Typically, only the FFT corresponding to positive frequencies is plotted. The example plots the FFT of the sum of two sines. The FFT input signal is inherently truncated. This truncation can be modeled as multiplication of an infinite signal with a rectangular window function. In the spectral domain this multiplication becomes convolution of the signal spectrum with the window function spectrum, being of form . This convolution is the cause of an effect called spectral leakage (see [WPW] ). Windowing the signal with a dedicated window function helps mitigate spectral leakage. The example below uses a Blackman window from scipy.signal and shows the effect of windowing (the zero component of the FFT has been truncated for illustrative purposes). In case the sequence x is complex-valued, the spectrum is no longer symmetric. To simplify working with the FFT functions, scipy provides the following two helper functions. The function fftfreq returns the FFT sample frequency points. In a similar spirit, the function fftshift allows swapping the lower and upper halves of a vector, so that it becomes suitable for display. The example below plots the FFT of two complex exponentials; note the asymmetric spectrum. The function rfft calculates the FFT of a real sequence and outputs the complex FFT coefficients for only half of the frequency range. The remaining negative frequency components are implied by the Hermitian symmetry of the FFT for a real input ( ). In case of N being even: ; in case of N being odd . The terms shown explicitly as are restricted to be purely real since, by the hermitian property, they are their own complex conjugate. The corresponding function irfft calculates the IFFT of the FFT coefficients with this special ordering. Notice that the rfft of odd and even length signals are of the same shape. By default, irfft assumes the output signal should be of even length. And so, for odd signals, it will give the wrong result: To recover the original odd-length signal, we must pass the output shape by the n parameter. 2- and N-D discrete Fourier transforms  The functions fft2 and ifft2 provide 2-D FFT and IFFT, respectively. Similarly, fftn and ifftn provide N-D FFT, and IFFT, respectively. For real-input signals, similarly to rfft , we have the functions rfft2 and irfft2 for 2-D real transforms; rfftn and irfftn for N-D real transforms. The example below demonstrates a 2-D IFFT and plots the resulting (2-D) time-domain signals. \"\\n',\n",
       " '\"scipy_fourier_transforms_scipy_fft Fourier Transforms (scipy.fft) fft.html  References  CT65 Cooley, James W., and John W. Tukey, 1965, An algorithm for the machine calculation of complex Fourier series, Math. Comput. 19: 297-301. NR07 Press, W., Teukolsky, S., Vetterline, W.T., and Flannery, B.P., 2007, Numerical Recipes: The Art of Scientific Computing , ch. 12-13. Cambridge Univ. Press, Cambridge, UK. Mak ( 1 , 2 ) J. Makhoul, 1980, A Fast Cosine Transform in One and Two Dimensions, IEEE Transactions on acoustics, speech and signal processing vol. 28(1), pp. 27-34, DOI:10.1109/TASSP.1980.1163351 WPW https://en.wikipedia.org/wiki/Window_function WPC https://en.wikipedia.org/wiki/Discrete_cosine_transform WPS https://en.wikipedia.org/wiki/Discrete_sine_transform \"\\n',\n",
       " '\"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Faster integration using low-level callback functions  A user desiring reduced integration times may pass a C function pointer through scipy.LowLevelCallable to quad , dblquad , tplquad or nquad and it will be integrated and return a result in Python. The performance increase here arises from two factors. The primary improvement is faster function evaluation, which is provided by compilation of the function itself. Additionally we have a speedup provided by the removal of function calls between C and Python in quad . This method may provide a speed improvements of ~2x for trivial functions such as sine but can produce a much more noticeable improvements (10x+) for more complex functions. This feature then, is geared towards a user with numerically intensive integrations willing to write a little C to reduce computation time significantly. The approach can be used, for example, via ctypes in a few simple steps: 1.) Write an integrand function in C with the function signature , where is an array containing the point the function f is evaluated at, and to arbitrary additional data you want to provide. 2.) Now compile this file to a shared/dynamic library (a quick search will help with this as it is OS-dependent). The user must link any math libraries, etc., used. On linux this looks like: The output library will be referred to as , but it may have a different file extension. A library has now been created that can be loaded into Python with ctypes . 3.) Load shared library into Python using ctypes and set and - this allows SciPy to interpret the function correctly: The last in the function is optional and can be omitted (both in the C function and ctypes argtypes) if not needed. Note that the coordinates are passed in as an array of doubles rather than a separate argument. 4.) Now integrate the library function as normally, here using nquad : The Python tuple is returned as expected in a reduced amount of time. All optional parameters can be used with this method including specifying singularities, infinite bounds, etc. \"\\n',\n",
       " '\"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Gaussian quadrature  A few functions are also provided in order to perform simple Gaussian quadrature over a fixed interval. The first is fixed_quad , which performs fixed-order Gaussian quadrature. The second function is quadrature , which performs Gaussian quadrature of multiple orders until the difference in the integral estimate is beneath some tolerance supplied by the user. These functions both use the module , which can calculate the roots and quadrature weights of a large variety of orthogonal polynomials (the polynomials themselves are available as special functions returning instances of the polynomial class  e.g., special.legendre ). \"\\n',\n",
       " '\"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  General integration ( quad )  The function quad is provided to integrate a function of one variable between two points. The points can be ( ) to indicate infinite limits. For example, suppose you wish to integrate a bessel function along the interval \\\\[I\\\\int_{0}^{4.5}J_{2.5}\\\\left(x\\\\right)\\\\, dx.\\\\] This could be computed using quad : The first argument to quad is a callable Python object (i.e., a function, method, or class instance). Notice the use of a lambda- function in this case as the argument. The next two arguments are the limits of integration. The return value is a tuple, with the first element holding the estimated value of the integral and the second element holding an upper bound on the error. Notice, that in this case, the true value of this integral is \\\\[I\\\\sqrt{\\\\frac{2}{\\\\pi}}\\\\left(\\\\frac{18}{27}\\\\sqrt{2}\\\\cos\\\\left(4.5\\\\right)-\\\\frac{4}{27}\\\\sqrt{2}\\\\sin\\\\left(4.5\\\\right)+\\\\sqrt{2\\\\pi}\\\\textrm{Si}\\\\left(\\\\frac{3}{\\\\sqrt{\\\\pi}}\\\\right)\\\\right),\\\\] where \\\\[\\\\textrm{Si}\\\\left(x\\\\right)\\\\int_{0}^{x}\\\\sin\\\\left(\\\\frac{\\\\pi}{2}t^{2}\\\\right)\\\\, dt.\\\\] is the Fresnel sine integral. Note that the numerically-computed integral is within of the exact result  well below the reported error bound. If the function to integrate takes additional parameters, they can be provided in the args argument. Suppose that the following integral shall be calculated: \\\\[I(a,b)\\\\int_{0}^{1} ax^2+b \\\\, dx.\\\\] This integral can be evaluated by using the following code: Infinite inputs are also allowed in quad by using as one of the arguments. For example, suppose that a numerical value for the exponential integral: \\\\[E_{n}\\\\left(x\\\\right)\\\\int_{1}^{\\\\infty}\\\\frac{e^{-xt}}{t^{n}}\\\\, dt.\\\\] is desired (and the fact that this integral can be computed as is forgotten). The functionality of the function special.expn can be replicated by defining a new function based on the routine quad : The function which is integrated can even use the quad argument (though the error bound may underestimate the error due to possible numerical error in the integrand from the use of quad ). The integral in this case is \\\\[I_{n}\\\\int_{0}^{\\\\infty}\\\\int_{1}^{\\\\infty}\\\\frac{e^{-xt}}{t^{n}}\\\\, dt\\\\, dx\\\\frac{1}{n}.\\\\] This last example shows that multiple integration can be handled using repeated calls to quad . \"\\n',\n",
       " '\"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  General multiple integration ( dblquad , tplquad , nquad )  The mechanics for double and triple integration have been wrapped up into the functions dblquad and tplquad . These functions take the function to integrate and four, or six arguments, respectively. The limits of all inner integrals need to be defined as functions. An example of using double integration to compute several values of is shown below: As example for non-constant limits consider the integral \\\\[I\\\\int_{y0}^{1/2}\\\\int_{x0}^{1-2y} x y \\\\, dx\\\\, dy\\\\frac{1}{96}.\\\\] This integral can be evaluated using the expression below (Note the use of the non-constant lambda functions for the upper limit of the inner integral): For n-fold integration, scipy provides the function nquad . The integration bounds are an iterable object: either a list of constant bounds, or a list of functions for the non-constant integration bounds. The order of integration (and therefore the bounds) is from the innermost integral to the outermost one. The integral from above \\\\[I_{n}\\\\int_{0}^{\\\\infty}\\\\int_{1}^{\\\\infty}\\\\frac{e^{-xt}}{t^{n}}\\\\, dt\\\\, dx\\\\frac{1}{n}\\\\] can be calculated as Note that the order of arguments for f must match the order of the integration bounds; i.e., the inner integral with respect to is on the interval and the outer integral with respect to is on the interval . Non-constant integration bounds can be treated in a similar manner; the example from above \\\\[I\\\\int_{y0}^{1/2}\\\\int_{x0}^{1-2y} x y \\\\, dx\\\\, dy\\\\frac{1}{96}.\\\\] can be evaluated by means of which is the same result as before. \"\\n',\n",
       " '\"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Integrating using Samples  If the samples are equally-spaced and the number of samples available is for some integer , then Romberg romb integration can be used to obtain high-precision estimates of the integral using the available samples. Romberg integration uses the trapezoid rule at step-sizes related by a power of two and then performs Richardson extrapolation on these estimates to approximate the integral with a higher degree of accuracy. In case of arbitrary spaced samples, the two functions trapz and simps are available. They are using Newton-Coates formulas of order 1 and 2 respectively to perform integration. The trapezoidal rule approximates the function as a straight line between adjacent points, while Simpsons rule approximates the function between three adjacent points as a parabola. For an odd number of samples that are equally spaced Simpsons rule is exact if the function is a polynomial of order 3 or less. If the samples are not equally spaced, then the result is exact only if the function is a polynomial of order 2 or less. This corresponds exactly to \\\\[\\\\int_{1}^{4} x^2 \\\\, dx  21,\\\\] whereas integrating the second function does not correspond to \\\\[\\\\int_{1}^{4} x^3 \\\\, dx  63.75\\\\] because the order of the polynomial in f2 is larger than two. \"\\n',\n",
       " '\"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Ordinary differential equations ( solve_ivp )  Integrating a set of ordinary differential equations (ODEs) given initial conditions is another useful example. The function solve_ivp is available in SciPy for integrating a first-order vector differential equation: \\\\[\\\\frac{d\\\\mathbf{y}}{dt}\\\\mathbf{f}\\\\left(\\\\mathbf{y},t\\\\right),\\\\] given initial conditions , where is a length vector and is a mapping from to A higher-order ordinary differential equation can always be reduced to a differential equation of this type by introducing intermediate derivatives into the vector. For example, suppose it is desired to find the solution to the following second-order differential equation: \\\\[\\\\frac{d^{2}w}{dz^{2}}-zw(z)0\\\\] with initial conditions and It is known that the solution to this differential equation with these boundary conditions is the Airy function \\\\[w\\\\textrm{Ai}\\\\left(z\\\\right),\\\\] which gives a means to check the integrator using special.airy . First, convert this ODE into standard form by setting and . Thus, the differential equation becomes \\\\[\\\\begin{split}\\\\frac{d\\\\mathbf{y}}{dt}\\\\left[\\\\begin{array}{c} ty_{1}\\\\\\\\ y_{0}\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{cc} 0 & t\\\\\\\\ 1 & 0\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c} y_{0}\\\\\\\\ y_{1}\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{cc} 0 & t\\\\\\\\ 1 & 0\\\\end{array}\\\\right]\\\\mathbf{y}.\\\\end{split}\\\\] In other words, \\\\[\\\\mathbf{f}\\\\left(\\\\mathbf{y},t\\\\right)\\\\mathbf{A}\\\\left(t\\\\right)\\\\mathbf{y}.\\\\] As an interesting reminder, if commutes with under matrix multiplication, then this linear differential equation has an exact solution using the matrix exponential: \\\\[\\\\mathbf{y}\\\\left(t\\\\right)\\\\exp\\\\left(\\\\int_{0}^{t}\\\\mathbf{A}\\\\left(\\\\tau\\\\right)d\\\\tau\\\\right)\\\\mathbf{y}\\\\left(0\\\\right),\\\\] However, in this case, and its integral do not commute. This differential equation can be solved using the function solve_ivp . It requires the derivative, fprime , the time span [t_start, t_end] and the initial conditions vector, y0 , as input arguments and returns an object whose y field is an array with consecutive solution values as columns. The initial conditions are therefore given in the first output column. As it can be seen solve_ivp determines its time steps automatically if not specified otherwise. To compare the solution of solve_ivp with the airy function the time vector created by solve_ivp is passed to the airy function. The solution of solve_ivp with its standard parameters shows a big deviation to the airy function. To minimize this deviation, relative and absolute tolerances can be used. To specify user defined time points for the solution of solve_ivp , solve_ivp offers two possibilities that can also be used complementarily. By passing the t_eval option to the function call solve_ivp returns the solutions of these time points of t_eval in its output. If the jacobian matrix of function is known, it can be passed to the solve_ivp to achieve better results. Please be aware however that the default integration method RK45 does not support jacobian matrices and thereby another integration method has to be chosen. One of the integration methods that support a jacobian matrix is the for example the Radau method of following example. Solving a system with a banded Jacobian matrix  odeint can be told that the Jacobian is banded . For a large system of differential equations that are known to be stiff, this can improve performance significantly. As an example, well solve the 1-D Gray-Scott partial differential equations using the method of lines [MOL] . The Gray-Scott equations for the functions and on the interval are \\\\[\\\\begin{split}\\\\begin{split} \\\\frac{\\\\partial u}{\\\\partial t}  D_u \\\\frac{\\\\partial^2 u}{\\\\partial x^2} - uv^2 + f(1-u) \\\\\\\\ \\\\frac{\\\\partial v}{\\\\partial t}  D_v \\\\frac{\\\\partial^2 v}{\\\\partial x^2} + uv^2 - (f + k)v \\\\\\\\ \\\\end{split}\\\\end{split}\\\\] where and are the diffusion coefficients of the components and , respectively, and and are constants. (For more information about the system, see http://groups.csail.mit.edu/mac/projects/amorphous/GrayScott/ ) Well assume Neumann (i.e., no flux) boundary conditions: \\\\[\\\\frac{\\\\partial u}{\\\\partial x}(0,t)  0, \\\\quad \\\\frac{\\\\partial v}{\\\\partial x}(0,t)  0, \\\\quad \\\\frac{\\\\partial u}{\\\\partial x}(L,t)  0, \\\\quad \\\\frac{\\\\partial v}{\\\\partial x}(L,t)  0\\\\] To apply the method of lines, we discretize the variable by defining the uniformly spaced grid of points , with and . We define and , and replace the derivatives with finite differences. That is, \\\\[\\\\frac{\\\\partial^2 u}{\\\\partial x^2}(x_j, t) \\\\rightarrow \\\\frac{u_{j-1}(t) - 2 u_{j}(t) + u_{j+1}(t)}{(\\\\Delta x)^2}\\\\] We then have a system of ordinary differential equations: (1)  \\\\[\\\\begin{split} \\\\begin{split} \\\\frac{du_j}{dt}  \\\\frac{D_u}{(\\\\Delta x)^2} \\\\left(u_{j-1} - 2 u_{j} + u_{j+1}\\\\right) -u_jv_j^2 + f(1 - u_j) \\\\\\\\ \\\\frac{dv_j}{dt}  \\\\frac{D_v}{(\\\\Delta x)^2} \\\\left(v_{j-1} - 2 v_{j} + v_{j+1}\\\\right) + u_jv_j^2 - (f + k)v_j \\\\end{split}\\\\end{split}\\\\] For convenience, the arguments have been dropped. To enforce the boundary conditions, we introduce ghost points and , and define , ; and are defined analogously. Then (2)  \\\\[\\\\begin{split} \\\\begin{split} \\\\frac{du_0}{dt}  \\\\frac{D_u}{(\\\\Delta x)^2} \\\\left(2u_{1} - 2 u_{0}\\\\right) -u_0v_0^2 + f(1 - u_0) \\\\\\\\ \\\\frac{dv_0}{dt}  \\\\frac{D_v}{(\\\\Delta x)^2} \\\\left(2v_{1} - 2 v_{0}\\\\right) + u_0v_0^2 - (f + k)v_0 \\\\end{split}\\\\end{split}\\\\] and (3)  \\\\[\\\\begin{split} \\\\begin{split} \\\\frac{du_{N-1}}{dt}  \\\\frac{D_u}{(\\\\Delta x)^2} \\\\left(2u_{N-2} - 2 u_{N-1}\\\\right) -u_{N-1}v_{N-1}^2 + f(1 - u_{N-1}) \\\\\\\\ \\\\frac{dv_{N-1}}{dt}  \\\\frac{D_v}{(\\\\Delta x)^2} \\\\left(2v_{N-2} - 2 v_{N-1}\\\\right) + u_{N-1}v_{N-1}^2 - (f + k)v_{N-1} \\\\end{split}\\\\end{split}\\\\] Our complete system of ordinary differential equations is (1) for , along with (2) and (3) . We can now starting implementing this system in code. We must combine and into a single vector of length . The two obvious choices are and . Mathematically, it does not matter, but the choice affects how efficiently odeint can solve the system. The reason is in how the order affects the pattern of the nonzero elements of the Jacobian matrix. When the variables are ordered as , the pattern of nonzero elements of the Jacobian matrix is \\\\[\\\\begin{split}\\\\begin{smallmatrix} * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & * & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & * & * & 0 & 0 & 0 & 0 & 0 & 0 & * \\\\\\\\ * & 0 & 0 & 0 & 0 & 0 & 0 & * & * & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & 0 & * & * & * \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & 0 & 0 & 0 & ) & * & * \\\\\\\\ \\\\end{smallmatrix}\\\\end{split}\\\\] The Jacobian pattern with variables interleaved as is \\\\[\\\\begin{split}\\\\begin{smallmatrix} * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * \\\\\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * \\\\\\\\ \\\\end{smallmatrix}\\\\end{split}\\\\] In both cases, there are just five nontrivial diagonals, but when the variables are interleaved, the bandwidth is much smaller. That is, the main diagonal and the two diagonals immediately above and the two immediately below the main diagonal are the nonzero diagonals. This is important, because the inputs and of odeint are the upper and lower bandwidths of the Jacobian matrix. When the variables are interleaved, and are 2. When the variables are stacked with following , the upper and lower bandwidths are . With that decision made, we can write the function that implements the system of differential equations. First, we define the functions for the source and reaction terms of the system: Next, we define the function that computes the right-hand side of the system of differential equations: We wont implement a function to compute the Jacobian, but we will tell odeint that the Jacobian matrix is banded. This allows the underlying solver (LSODA) to avoid computing values that it knows are zero. For a large system, this improves the performance significantly, as demonstrated in the following ipython session. First, we define the required inputs: Time the computation without taking advantage of the banded structure of the Jacobian matrix: Now set and , so odeint knows that the Jacobian matrix is banded: That is quite a bit faster! Lets ensure that they have computed the same result: References  WPR https://en.wikipedia.org/wiki/Rombergs_method MOL https://en.wikipedia.org/wiki/Method_of_lines \"\\n',\n",
       " '\"scipy_integration_scipy_integrate Integration (scipy.integrate) integrate.html  Romberg Integration  Rombergs method [WPR] is another method for numerically evaluating an integral. See the help function for romberg for further details. \"\\n',\n",
       " '\"scipy_interpolation_scipy_interpolate Interpolation (scipy.interpolate) interpolate.html  1-D interpolation ( interp1d )  The interp1d class in scipy.interpolate is a convenient method to create a function based on fixed data points, which can be evaluated anywhere within the domain defined by the given data using linear interpolation. An instance of this class is created by passing the 1-D vectors comprising the data. The instance of this class defines a __call__ method and can therefore by treated like a function which interpolates between known data values to obtain unknown values (it also has a docstring for help). Behavior at the boundary can be specified at instantiation time. The following example demonstrates its use, for linear and cubic spline interpolation: Another set of interpolations in interp1d is nearest , previous , and next , where they return the nearest, previous, or next point along the x-axis. Nearest and next can be thought of as a special case of a causal interpolating filter. The following example demonstrates their use, using the same data as in the previous example: \"\\n',\n",
       " '\"scipy_interpolation_scipy_interpolate Interpolation (scipy.interpolate) interpolate.html  Multivariate data interpolation ( griddata )  Suppose you have multidimensional data, for instance, for an underlying function f(x, y) you only know the values at points (x[i], y[i]) that do not form a regular grid. Suppose we want to interpolate the 2-D function on a grid in [0, 1]x[0, 1] but we only know its values at 1000 data points: This can be done with griddata  below, we try out all of the interpolation methods: One can see that the exact result is reproduced by all of the methods to some degree, but for this smooth function the piecewise cubic interpolant gives the best results: \"\\n',\n",
       " '\"scipy_interpolation_scipy_interpolate Interpolation (scipy.interpolate) interpolate.html  Spline interpolation  Spline interpolation in 1-D: Procedural (interpolate.splXXX)  Spline interpolation requires two essential steps: (1) a spline representation of the curve is computed, and (2) the spline is evaluated at the desired points. In order to find the spline representation, there are two different ways to represent a curve and obtain (smoothing) spline coefficients: directly and parametrically. The direct method finds the spline representation of a curve in a 2-D plane using the function splrep . The first two arguments are the only ones required, and these provide the and components of the curve. The normal output is a 3-tuple, , containing the knot-points, , the coefficients and the order of the spline. The default spline order is cubic, but this can be changed with the input keyword, k. For curves in N-D space the function splprep allows defining the curve parametrically. For this function only 1 input argument is required. This input is a list of -arrays representing the curve in N-D space. The length of each array is the number of curve points, and each array provides one component of the N-D data point. The parameter variable is given with the keyword argument, u, , which defaults to an equally-spaced monotonic sequence between and . The default output consists of two objects: a 3-tuple, , containing the spline representation and the parameter variable The keyword argument, s , is used to specify the amount of smoothing to perform during the spline fit. The default value of is where is the number of data-points being fit. Therefore, if no smoothing is desired a value of should be passed to the routines. Once the spline representation of the data has been determined, functions are available for evaluating the spline ( splev ) and its derivatives ( splev , spalde ) at any point and the integral of the spline between any two points ( splint ). In addition, for cubic splines ( ) with 8 or more knots, the roots of the spline can be estimated ( sproot ). These functions are demonstrated in the example that follows. Cubic-spline Derivative of spline Integral of spline Roots of spline Notice that sproot failed to find an obvious solution at the edge of the approximation interval, . If we define the spline on a slightly larger interval, we recover both roots and : Parametric spline Spline interpolation in 1-d: Object-oriented ( UnivariateSpline )  The spline-fitting capabilities described above are also available via an objected-oriented interface. The 1-D splines are objects of the UnivariateSpline class, and are created with the and components of the curve provided as arguments to the constructor. The class defines __call__ , allowing the object to be called with the x-axis values, at which the spline should be evaluated, returning the interpolated y-values. This is shown in the example below for the subclass InterpolatedUnivariateSpline . The integral , derivatives , and roots methods are also available on UnivariateSpline objects, allowing definite integrals, derivatives, and roots to be computed for the spline. The UnivariateSpline class can also be used to smooth data by providing a non-zero value of the smoothing parameter s , with the same meaning as the s keyword of the splrep function described above. This results in a spline that has fewer knots than the number of data points, and hence is no longer strictly an interpolating spline, but rather a smoothing spline. If this is not desired, the InterpolatedUnivariateSpline class is available. It is a subclass of UnivariateSpline that always passes through all points (equivalent to forcing the smoothing parameter to 0). This class is demonstrated in the example below. The LSQUnivariateSpline class is the other subclass of UnivariateSpline . It allows the user to specify the number and location of internal knots explicitly with the parameter t . This allows for the creation of customized splines with non-linear spacing, to interpolate in some domains and smooth in others, or change the character of the spline. InterpolatedUnivariateSpline LSQUnivarateSpline with non-uniform knots 2-D spline representation: Procedural ( bisplrep )  For (smooth) spline-fitting to a 2-D surface, the function bisplrep is available. This function takes as required inputs the 1-D arrays x , y , and z , which represent points on the surface The default output is a list whose entries represent respectively, the components of the knot positions, the coefficients of the spline, and the order of the spline in each coordinate. It is convenient to hold this list in a single object, tck, so that it can be passed easily to the function bisplev . The keyword, s , can be used to change the amount of smoothing performed on the data while determining the appropriate spline. The default value is , where is the number of data points in the x, y, and z vectors. As a result, if no smoothing is desired, then should be passed to bisplrep . To evaluate the 2-D spline and its partial derivatives (up to the order of the spline), the function bisplev is required. This function takes as the first two arguments two 1-D arrays whose cross-product specifies the domain over which to evaluate the spline. The third argument is the tck list returned from bisplrep . If desired, the fourth and fifth arguments provide the orders of the partial derivative in the and direction, respectively. It is important to note that 2-D interpolation should not be used to find the spline representation of images. The algorithm used is not amenable to large numbers of input points. The signal-processing toolbox contains more appropriate algorithms for finding the spline representation of an image. The 2-D interpolation commands are intended for use when interpolating a 2-D function as shown in the example that follows. This example uses the mgrid command in NumPy which is useful for defining a mesh-grid in many dimensions. (See also the ogrid command if the full-mesh is not needed). The number of output arguments and the number of dimensions of each argument is determined by the number of indexing objects passed in mgrid . Define function over a sparse 20x20 grid Interpolate function over a new 70x70 grid 2-D spline representation: Object-oriented ( BivariateSpline )  The BivariateSpline class is the 2-D analog of the UnivariateSpline class. It and its subclasses implement the FITPACK functions described above in an object-oriented fashion, allowing objects to be instantiated that can be called to compute the spline value by passing in the two coordinates as the two arguments. \"\\n',\n",
       " '\"scipy_introduction Introduction general.html  Finding Documentation  SciPy and NumPy have documentation versions in both HTML and PDF format available at https://docs.scipy.org/ , that cover nearly all available functionality. However, this documentation is still work-in-progress and some parts may be incomplete or sparse. As we are a volunteer organization and depend on the community for growth, your participation - everything from providing feedback to improving the documentation and code - is welcome and actively encouraged. Pythons documentation strings are used in SciPy for on-line documentation. There are two methods for reading them and getting help. One is Pythons command help in the pydoc module. Entering this command with no arguments (i.e. ) launches an interactive help session that allows searching through the keywords and modules available to all of Python. Secondly, running the command help(obj) with an object as the argument displays that objects calling signature, and documentation string. The pydoc method of is sophisticated but uses a pager to display the text. Sometimes this can interfere with the terminal within which you are running the interactive session. A numpy/scipy-specific help system is also available under the command . The signature and documentation string for the object passed to the command are printed to standard output (or to a writeable object passed as the third argument). The second keyword argument of defines the maximum width of the line for printing. If a module is passed as the argument to then a list of the functions and classes defined in that module is printed. For example: Another useful command is , which can be used to look at the namespace of a module or package. \"\\n',\n",
       " '\"scipy_introduction Introduction general.html  SciPy Organization  SciPy is organized into subpackages covering different scientific computing domains. These are summarized in the following table: Subpackage Description cluster Clustering algorithms constants Physical and mathematical constants fftpack Fast Fourier Transform routines integrate Integration and ordinary differential equation solvers interpolate Interpolation and smoothing splines io Input and Output linalg Linear algebra ndimage N-dimensional image processing odr Orthogonal distance regression optimize Optimization and root-finding routines signal Signal processing sparse Sparse matrices and associated routines spatial Spatial data structures and algorithms special Special functions stats Statistical distributions and functions SciPy sub-packages need to be imported separately, for example: Because of their ubiquitousness, some of the functions in these subpackages are also made available in the scipy namespace to ease their use in interactive sessions and programs. In addition, many basic array functions from numpy are also available at the top-level of the scipy package. Before looking at the sub-packages individually, we will first look at some of these common functions. \"\\n',\n",
       " '\"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Basic routines  Finding the inverse  The inverse of a matrix is the matrix , such that , where is the identity matrix consisting of ones down the main diagonal. Usually, is denoted . In SciPy, the matrix inverse of the NumPy array, A, is obtained using linalg.inv , or using if is a Matrix. For example, let \\\\[\\\\begin{split}\\\\mathbf{A}  \\\\left[\\\\begin{array}{ccc} 1 & 3 & 5\\\\\\\\ 2 & 5 & 1\\\\\\\\ 2 & 3 & 8\\\\end{array}\\\\right],\\\\end{split}\\\\] then \\\\[\\\\begin{split}\\\\mathbf{A^{-1}}  \\\\frac{1}{25} \\\\left[\\\\begin{array}{ccc} -37 & 9 & 22 \\\\\\\\ 14 & 2 & -9 \\\\\\\\ 4 & -3 & 1 \\\\end{array}\\\\right]  % \\\\left[\\\\begin{array}{ccc} -1.48 & 0.36 & 0.88 \\\\\\\\ 0.56 & 0.08 & -0.36 \\\\\\\\ 0.16 & -0.12 & 0.04 \\\\end{array}\\\\right].\\\\end{split}\\\\] The following example demonstrates this computation in SciPy Solving a linear system  Solving linear systems of equations is straightforward using the scipy command linalg.solve . This command expects an input matrix and a right-hand side vector. The solution vector is then computed. An option for entering a symmetric matrix is offered, which can speed up the processing when applicable. As an example, suppose it is desired to solve the following simultaneous equations: \\\\begin{eqnarray*} x + 3y + 5z &  & 10 \\\\\\\\ 2x + 5y + z &  & 8 \\\\\\\\ 2x + 3y + 8z &  & 3 \\\\end{eqnarray*} We could find the solution vector using a matrix inverse: \\\\[\\\\begin{split}\\\\left[\\\\begin{array}{c} x\\\\\\\\ y\\\\\\\\ z\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{ccc} 1 & 3 & 5\\\\\\\\ 2 & 5 & 1\\\\\\\\ 2 & 3 & 8\\\\end{array}\\\\right]^{-1}\\\\left[\\\\begin{array}{c} 10\\\\\\\\ 8\\\\\\\\ 3\\\\end{array}\\\\right]\\\\frac{1}{25}\\\\left[\\\\begin{array}{c} -232\\\\\\\\ 129\\\\\\\\ 19\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c} -9.28\\\\\\\\ 5.16\\\\\\\\ 0.76\\\\end{array}\\\\right].\\\\end{split}\\\\] However, it is better to use the linalg.solve command, which can be faster and more numerically stable. In this case, it, however, gives the same answer as shown in the following example: Finding the determinant  The determinant of a square matrix is often denoted and is a quantity often used in linear algebra. Suppose are the elements of the matrix and let be the determinant of the matrix left by removing the row and column from . Then, for any row \\\\[\\\\left|\\\\mathbf{A}\\\\right|\\\\sum_{j}\\\\left(-1\\\\right)^{i+j}a_{ij}M_{ij}.\\\\] This is a recursive way to define the determinant, where the base case is defined by accepting that the determinant of a matrix is the only matrix element. In SciPy the determinant can be calculated with linalg.det . For example, the determinant of \\\\[\\\\begin{split}\\\\mathbf{A}\\\\left[\\\\begin{array}{ccc} 1 & 3 & 5\\\\\\\\ 2 & 5 & 1\\\\\\\\ 2 & 3 & 8\\\\end{array}\\\\right]\\\\end{split}\\\\] is \\\\begin{eqnarray*} \\\\left|\\\\mathbf{A}\\\\right| &  & 1\\\\left|\\\\begin{array}{cc} 5 & 1\\\\\\\\ 3 & 8\\\\end{array}\\\\right|-3\\\\left|\\\\begin{array}{cc} 2 & 1\\\\\\\\ 2 & 8\\\\end{array}\\\\right|+5\\\\left|\\\\begin{array}{cc} 2 & 5\\\\\\\\ 2 & 3\\\\end{array}\\\\right|\\\\\\\\ &  & 1\\\\left(5\\\\cdot8-3\\\\cdot1\\\\right)-3\\\\left(2\\\\cdot8-2\\\\cdot1\\\\right)+5\\\\left(2\\\\cdot3-2\\\\cdot5\\\\right)-25.\\\\end{eqnarray*}. In SciPy, this is computed as shown in this example: Computing norms  Matrix and vector norms can also be computed with SciPy. A wide range of norm definitions are available using different parameters to the order argument of linalg.norm . This function takes a rank-1 (vectors) or a rank-2 (matrices) array and an optional order argument (default is 2). Based on these inputs, a vector or matrix norm of the requested order is computed. For vector x , the order parameter can be any real number including or . The computed norm is \\\\[\\\\begin{split}\\\\left\\\\Vert \\\\mathbf{x}\\\\right\\\\Vert \\\\left\\\\{ \\\\begin{array}{cc} \\\\max\\\\left|x_{i}\\\\right| & \\\\textrm{ord}\\\\textrm{inf}\\\\\\\\ \\\\min\\\\left|x_{i}\\\\right| & \\\\textrm{ord}-\\\\textrm{inf}\\\\\\\\ \\\\left(\\\\sum_{i}\\\\left|x_{i}\\\\right|^{\\\\textrm{ord}}\\\\right)^{1/\\\\textrm{ord}} & \\\\left|\\\\textrm{ord}\\\\right|<\\\\infty.\\\\end{array}\\\\right.\\\\end{split}\\\\] For matrix , the only valid values for norm are inf, and fro (or f) Thus, \\\\[\\\\begin{split}\\\\left\\\\Vert \\\\mathbf{A}\\\\right\\\\Vert \\\\left\\\\{ \\\\begin{array}{cc} \\\\max_{i}\\\\sum_{j}\\\\left|a_{ij}\\\\right| & \\\\textrm{ord}\\\\textrm{inf}\\\\\\\\ \\\\min_{i}\\\\sum_{j}\\\\left|a_{ij}\\\\right| & \\\\textrm{ord}-\\\\textrm{inf}\\\\\\\\ \\\\max_{j}\\\\sum_{i}\\\\left|a_{ij}\\\\right| & \\\\textrm{ord}1\\\\\\\\ \\\\min_{j}\\\\sum_{i}\\\\left|a_{ij}\\\\right| & \\\\textrm{ord}-1\\\\\\\\ \\\\max\\\\sigma_{i} & \\\\textrm{ord}2\\\\\\\\ \\\\min\\\\sigma_{i} & \\\\textrm{ord}-2\\\\\\\\ \\\\sqrt{\\\\textrm{trace}\\\\left(\\\\mathbf{A}^{H}\\\\mathbf{A}\\\\right)} & \\\\textrm{ord}\\\\textrm{\\'fro\\'}\\\\end{array}\\\\right.\\\\end{split}\\\\] where are the singular values of . Examples: Solving linear least-squares problems and pseudo-inverses  Linear least-squares problems occur in many branches of applied mathematics. In this problem, a set of linear scaling coefficients is sought that allows a model to fit the data. In particular, it is assumed that data is related to data through a set of coefficients and model functions via the model \\\\[y_{i}\\\\sum_{j}c_{j}f_{j}\\\\left(\\\\mathbf{x}_{i}\\\\right)+\\\\epsilon_{i},\\\\] where represents uncertainty in the data. The strategy of least squares is to pick the coefficients to minimize \\\\[J\\\\left(\\\\mathbf{c}\\\\right)\\\\sum_{i}\\\\left|y_{i}-\\\\sum_{j}c_{j}f_{j}\\\\left(x_{i}\\\\right)\\\\right|^{2}.\\\\] Theoretically, a global minimum will occur when \\\\[\\\\frac{\\\\partial J}{\\\\partial c_{n}^{*}}0\\\\sum_{i}\\\\left(y_{i}-\\\\sum_{j}c_{j}f_{j}\\\\left(x_{i}\\\\right)\\\\right)\\\\left(-f_{n}^{*}\\\\left(x_{i}\\\\right)\\\\right)\\\\] or \\\\begin{eqnarray*} \\\\sum_{j}c_{j}\\\\sum_{i}f_{j}\\\\left(x_{i}\\\\right)f_{n}^{*}\\\\left(x_{i}\\\\right) &  & \\\\sum_{i}y_{i}f_{n}^{*}\\\\left(x_{i}\\\\right)\\\\\\\\ \\\\mathbf{A}^{H}\\\\mathbf{Ac} &  & \\\\mathbf{A}^{H}\\\\mathbf{y}\\\\end{eqnarray*}, where \\\\[\\\\left\\\\{ \\\\mathbf{A}\\\\right\\\\} _{ij}f_{j}\\\\left(x_{i}\\\\right).\\\\] When is invertible, then \\\\[\\\\mathbf{c}\\\\left(\\\\mathbf{A}^{H}\\\\mathbf{A}\\\\right)^{-1}\\\\mathbf{A}^{H}\\\\mathbf{y}\\\\mathbf{A}^{\\\\dagger}\\\\mathbf{y},\\\\] where is called the pseudo-inverse of Notice that using this definition of the model can be written \\\\[\\\\mathbf{y}\\\\mathbf{Ac}+\\\\boldsymbol{\\\\epsilon}.\\\\] The command linalg.lstsq will solve the linear least-squares problem for given and . In addition, linalg.pinv or linalg.pinv2 (uses a different method based on singular value decomposition) will find given The following example and figure demonstrate the use of linalg.lstsq and linalg.pinv for solving a data-fitting problem. The data shown below were generated using the model: \\\\[y_{i}c_{1}e^{-x_{i}}+c_{2}x_{i},\\\\] where for , , and Noise is added to and the coefficients and are estimated using linear least squares. Generalized inverse  The generalized inverse is calculated using the command linalg.pinv or linalg.pinv2 . These two commands differ in how they compute the generalized inverse. The first uses the linalg.lstsq algorithm, while the second uses singular value decomposition. Let be an matrix, then if , the generalized inverse is \\\\[\\\\mathbf{A}^{\\\\dagger}\\\\left(\\\\mathbf{A}^{H}\\\\mathbf{A}\\\\right)^{-1}\\\\mathbf{A}^{H},\\\\] while if matrix, the generalized inverse is \\\\[\\\\mathbf{A}^{\\\\#}\\\\mathbf{A}^{H}\\\\left(\\\\mathbf{A}\\\\mathbf{A}^{H}\\\\right)^{-1}.\\\\] In the case that , then \\\\[\\\\mathbf{A}^{\\\\dagger}\\\\mathbf{A}^{\\\\#}\\\\mathbf{A}^{-1},\\\\] as long as is invertible. \"\\n',\n",
       " '\"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Decompositions  In many applications, it is useful to decompose a matrix using other representations. There are several decompositions supported by SciPy. Eigenvalues and eigenvectors  The eigenvalue-eigenvector problem is one of the most commonly employed linear algebra operations. In one popular form, the eigenvalue-eigenvector problem is to find for some square matrix scalars and corresponding vectors , such that \\\\[\\\\mathbf{Av}\\\\lambda\\\\mathbf{v}.\\\\] For an matrix, there are (not necessarily distinct) eigenvalues  roots of the (characteristic) polynomial \\\\[\\\\left|\\\\mathbf{A}-\\\\lambda\\\\mathbf{I}\\\\right|0.\\\\] The eigenvectors, , are also sometimes called right eigenvectors to distinguish them from another set of left eigenvectors that satisfy \\\\[\\\\mathbf{v}_{L}^{H}\\\\mathbf{A}\\\\lambda\\\\mathbf{v}_{L}^{H}\\\\] or \\\\[\\\\mathbf{A}^{H}\\\\mathbf{v}_{L}\\\\lambda^{*}\\\\mathbf{v}_{L}.\\\\] With its default optional arguments, the command linalg.eig returns and However, it can also return and just by itself ( linalg.eigvals returns just as well). In addition, linalg.eig can also solve the more general eigenvalue problem \\\\begin{eqnarray*} \\\\mathbf{Av} &  & \\\\lambda\\\\mathbf{Bv}\\\\\\\\ \\\\mathbf{A}^{H}\\\\mathbf{v}_{L} &  & \\\\lambda^{*}\\\\mathbf{B}^{H}\\\\mathbf{v}_{L}\\\\end{eqnarray*} for square matrices and The standard eigenvalue problem is an example of the general eigenvalue problem for When a generalized eigenvalue problem can be solved, it provides a decomposition of as \\\\[\\\\mathbf{A}\\\\mathbf{BV}\\\\boldsymbol{\\\\Lambda}\\\\mathbf{V}^{-1},\\\\] where is the collection of eigenvectors into columns and is a diagonal matrix of eigenvalues. By definition, eigenvectors are only defined up to a constant scale factor. In SciPy, the scaling factor for the eigenvectors is chosen so that As an example, consider finding the eigenvalues and eigenvectors of the matrix \\\\[\\\\begin{split}\\\\mathbf{A}\\\\left[\\\\begin{array}{ccc} 1 & 5 & 2\\\\\\\\ 2 & 4 & 1\\\\\\\\ 3 & 6 & 2\\\\end{array}\\\\right].\\\\end{split}\\\\] The characteristic polynomial is \\\\begin{eqnarray*} \\\\left|\\\\mathbf{A}-\\\\lambda\\\\mathbf{I}\\\\right| &  & \\\\left(1-\\\\lambda\\\\right)\\\\left[\\\\left(4-\\\\lambda\\\\right)\\\\left(2-\\\\lambda\\\\right)-6\\\\right]-\\\\\\\\ & & 5\\\\left[2\\\\left(2-\\\\lambda\\\\right)-3\\\\right]+2\\\\left[12-3\\\\left(4-\\\\lambda\\\\right)\\\\right]\\\\\\\\ &  & -\\\\lambda^{3}+7\\\\lambda^{2}+8\\\\lambda-3.\\\\end{eqnarray*} The roots of this polynomial are the eigenvalues of : \\\\begin{eqnarray*} \\\\lambda_{1} &  & 7.9579\\\\\\\\ \\\\lambda_{2} &  & -1.2577\\\\\\\\ \\\\lambda_{3} &  & 0.2997.\\\\end{eqnarray*} The eigenvectors corresponding to each eigenvalue can be found using the original equation. The eigenvectors associated with these eigenvalues can then be found. Singular value decomposition  Singular value decomposition (SVD) can be thought of as an extension of the eigenvalue problem to matrices that are not square. Let be an matrix with and arbitrary. The matrices and are square hermitian matrices 1 of size and , respectively. It is known that the eigenvalues of square hermitian matrices are real and non-negative. In addition, there are at most identical non-zero eigenvalues of and Define these positive eigenvalues as The square-root of these are called singular values of The eigenvectors of are collected by columns into an unitary 2 matrix , while the eigenvectors of are collected by columns in the unitary matrix , the singular values are collected in an zero matrix with main diagonal entries set to the singular values. Then \\\\[\\\\mathbf{AU}\\\\boldsymbol{\\\\Sigma}\\\\mathbf{V}^{H}\\\\] is the singular value decomposition of Every matrix has a singular value decomposition. Sometimes, the singular values are called the spectrum of The command linalg.svd will return , , and as an array of the singular values. To obtain the matrix , use linalg.diagsvd . The following example illustrates the use of linalg.svd : 1 A hermitian matrix satisfies 2 A unitary matrix satisfies so that LU decomposition  The LU decomposition finds a representation for the matrix as \\\\[\\\\mathbf{A}\\\\mathbf{P}\\\\,\\\\mathbf{L}\\\\,\\\\mathbf{U},\\\\] where is an permutation matrix (a permutation of the rows of the identity matrix), is in lower triangular or trapezoidal matrix ( ) with unit-diagonal, and is an upper triangular or trapezoidal matrix. The SciPy command for this decomposition is linalg.lu . Such a decomposition is often useful for solving many simultaneous equations where the left-hand side does not change but the right-hand side does. For example, suppose we are going to solve \\\\[\\\\mathbf{A}\\\\mathbf{x}_{i}\\\\mathbf{b}_{i}\\\\] for many different . The LU decomposition allows this to be written as \\\\[\\\\mathbf{PLUx}_{i}\\\\mathbf{b}_{i}.\\\\] Because is lower-triangular, the equation can be solved for and, finally, very rapidly using forward- and back-substitution. An initial time spent factoring allows for very rapid solution of similar systems of equations in the future. If the intent for performing LU decomposition is for solving linear systems, then the command linalg.lu_factor should be used followed by repeated applications of the command linalg.lu_solve to solve the system for each new right-hand side. Cholesky decomposition  Cholesky decomposition is a special case of LU decomposition applicable to Hermitian positive definite matrices. When and for all , then decompositions of can be found so that \\\\begin{eqnarray*} \\\\mathbf{A} &  & \\\\mathbf{U}^{H}\\\\mathbf{U}\\\\\\\\ \\\\mathbf{A} &  & \\\\mathbf{L}\\\\mathbf{L}^{H}\\\\end{eqnarray*}, where is lower triangular and is upper triangular. Notice that The command linalg.cholesky computes the Cholesky factorization. For using the Cholesky factorization to solve systems of equations, there are also linalg.cho_factor and linalg.cho_solve routines that work similarly to their LU decomposition counterparts. QR decomposition  The QR decomposition (sometimes called a polar decomposition) works for any array and finds an unitary matrix and an upper-trapezoidal matrix , such that \\\\[\\\\mathbf{AQR}.\\\\] Notice that if the SVD of is known, then the QR decomposition can be found. \\\\[\\\\mathbf{A}\\\\mathbf{U}\\\\boldsymbol{\\\\Sigma}\\\\mathbf{V}^{H}\\\\mathbf{QR}\\\\] implies that and Note, however, that in SciPy independent algorithms are used to find QR and SVD decompositions. The command for QR decomposition is linalg.qr . Schur decomposition  For a square matrix, , the Schur decomposition finds (not necessarily unique) matrices and , such that \\\\[\\\\mathbf{A}\\\\mathbf{ZT}\\\\mathbf{Z}^{H},\\\\] where is a unitary matrix and is either upper triangular or quasi upper triangular, depending on whether or not a real Schur form or complex Schur form is requested. For a real Schur form both and are real-valued when is real-valued. When is a real-valued matrix, the real Schur form is only quasi upper triangular because blocks extrude from the main diagonal corresponding to any complex-valued eigenvalues. The command linalg.schur finds the Schur decomposition, while the command linalg.rsf2csf converts and from a real Schur form to a complex Schur form. The Schur form is especially useful in calculating functions of matrices. The following example illustrates the Schur decomposition: Interpolative decomposition  scipy.linalg.interpolative contains routines for computing the interpolative decomposition (ID) of a matrix. For a matrix of rank this is a factorization \\\\[A \\\\Pi  \\\\begin{bmatrix} A \\\\Pi_{1} & A \\\\Pi_{2} \\\\end{bmatrix}  A \\\\Pi_{1} \\\\begin{bmatrix} I & T \\\\end{bmatrix},\\\\] where is a permutation matrix with , i.e., . This can equivalently be written as , where and are the skeleton and interpolation matrices , respectively. See also scipy.linalg.interpolative  for more information. \"\\n',\n",
       " '\"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Matrix functions  Consider the function with Taylor series expansion \\\\[f\\\\left(x\\\\right)\\\\sum_{k0}^{\\\\infty}\\\\frac{f^{\\\\left(k\\\\right)}\\\\left(0\\\\right)}{k!}x^{k}.\\\\] A matrix function can be defined using this Taylor series for the square matrix as \\\\[f\\\\left(\\\\mathbf{A}\\\\right)\\\\sum_{k0}^{\\\\infty}\\\\frac{f^{\\\\left(k\\\\right)}\\\\left(0\\\\right)}{k!}\\\\mathbf{A}^{k}.\\\\] While this serves as a useful representation of a matrix function, it is rarely the best way to calculate a matrix function. Exponential and logarithm functions  The matrix exponential is one of the more common matrix functions. The preferred method for implementing the matrix exponential is to use scaling and a Pad approximation for . This algorithm is implemented as linalg.expm . The inverse of the matrix exponential is the matrix logarithm defined as the inverse of the matrix exponential: \\\\[\\\\mathbf{A}\\\\equiv\\\\exp\\\\left(\\\\log\\\\left(\\\\mathbf{A}\\\\right)\\\\right).\\\\] The matrix logarithm can be obtained with linalg.logm . Trigonometric functions  The trigonometric functions, , , and , are implemented for matrices in linalg.sinm , linalg.cosm , and linalg.tanm , respectively. The matrix sine and cosine can be defined using Eulers identity as \\\\begin{eqnarray*} \\\\sin\\\\left(\\\\mathbf{A}\\\\right) &  & \\\\frac{e^{j\\\\mathbf{A}}-e^{-j\\\\mathbf{A}}}{2j}\\\\\\\\ \\\\cos\\\\left(\\\\mathbf{A}\\\\right) &  & \\\\frac{e^{j\\\\mathbf{A}}+e^{-j\\\\mathbf{A}}}{2}.\\\\end{eqnarray*} The tangent is \\\\[\\\\tan\\\\left(x\\\\right)\\\\frac{\\\\sin\\\\left(x\\\\right)}{\\\\cos\\\\left(x\\\\right)}\\\\left[\\\\cos\\\\left(x\\\\right)\\\\right]^{-1}\\\\sin\\\\left(x\\\\right)\\\\] and so the matrix tangent is defined as \\\\[\\\\left[\\\\cos\\\\left(\\\\mathbf{A}\\\\right)\\\\right]^{-1}\\\\sin\\\\left(\\\\mathbf{A}\\\\right).\\\\] Hyperbolic trigonometric functions  The hyperbolic trigonometric functions, , , and , can also be defined for matrices using the familiar definitions: \\\\begin{eqnarray*} \\\\sinh\\\\left(\\\\mathbf{A}\\\\right) &  & \\\\frac{e^{\\\\mathbf{A}}-e^{-\\\\mathbf{A}}}{2}\\\\\\\\ \\\\cosh\\\\left(\\\\mathbf{A}\\\\right) &  & \\\\frac{e^{\\\\mathbf{A}}+e^{-\\\\mathbf{A}}}{2}\\\\\\\\ \\\\tanh\\\\left(\\\\mathbf{A}\\\\right) &  & \\\\left[\\\\cosh\\\\left(\\\\mathbf{A}\\\\right)\\\\right]^{-1}\\\\sinh\\\\left(\\\\mathbf{A}\\\\right).\\\\end{eqnarray*} These matrix functions can be found using linalg.sinhm , linalg.coshm , and linalg.tanhm . Arbitrary function  Finally, any arbitrary function that takes one complex number and returns a complex number can be called as a matrix function using the command linalg.funm . This command takes the matrix and an arbitrary Python function. It then implements an algorithm from Golub and Van Loans book Matrix Computations to compute the function applied to the matrix using a Schur decomposition. Note that the function needs to accept complex numbers as input in order to work with this algorithm. For example, the following code computes the zeroth-order Bessel function applied to a matrix. Note how, by virtue of how matrix analytic functions are defined, the Bessel function has acted on the matrix eigenvalues. \"\\n',\n",
       " '\"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  numpy.matrix vs 2-D numpy.ndarray  The classes that represent matrices, and basic operations, such as matrix multiplications and transpose are a part of . For convenience, we summarize the differences between numpy.matrix and numpy.ndarray here. is matrix class that has a more convenient interface than for matrix operations. This class supports, for example, MATLAB-like creation syntax via the semicolon, has matrix multiplication as default for the operator, and contains and members that serve as shortcuts for inverse and transpose: Despite its convenience, the use of the class is discouraged, since it adds nothing that cannot be accomplished with 2-D objects, and may lead to a confusion of which class is being used. For example, the above code can be rewritten as: operations can be applied equally to or to 2D objects. \"\\n',\n",
       " '\"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  scipy.linalg vs numpy.linalg  scipy.linalg contains all the functions in numpy.linalg . plus some other more advanced ones not contained in . Another advantage of using over is that it is always compiled with BLAS/LAPACK support, while for numpy this is optional. Therefore, the scipy version might be faster depending on how numpy was installed. Therefore, unless you dont want to add as a dependency to your program, use instead of . \"\\n',\n",
       " '\"scipy_linear_algebra_scipy_linalg Linear Algebra (scipy.linalg) linalg.html  Special matrices  SciPy and NumPy provide several functions for creating special matrices that are frequently used in engineering and science. Type Function Description block diagonal scipy.linalg.block_diag Create a block diagonal matrix from the provided arrays. circulant scipy.linalg.circulant Create a circulant matrix. companion scipy.linalg.companion Create a companion matrix. convolution scipy.linalg.convolution_matrix Create a convolution matrix. Discrete Fourier scipy.linalg.dft Create a discrete Fourier transform matrix. Fiedler scipy.linalg.fiedler Create a symmetric Fiedler matrix. Fiedler Companion scipy.linalg.fiedler_companion Create a Fiedler companion matrix. Hadamard scipy.linalg.hadamard Create an Hadamard matrix. Hankel scipy.linalg.hankel Create a Hankel matrix. Helmert scipy.linalg.helmert Create a Helmert matrix. Hilbert scipy.linalg.hilbert Create a Hilbert matrix. Inverse Hilbert scipy.linalg.invhilbert Create the inverse of a Hilbert matrix. Leslie scipy.linalg.leslie Create a Leslie matrix. Pascal scipy.linalg.pascal Create a Pascal matrix. Inverse Pascal scipy.linalg.invpascal Create the inverse of a Pascal matrix. Toeplitz scipy.linalg.toeplitz Create a Toeplitz matrix. Van der Monde numpy.vander Create a Van der Monde matrix. For examples of the use of these functions, see their respective docstrings. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Distance transforms  Distance transforms are used to calculate the minimum distance from each element of an object to the background. The following functions implement distance transforms for three different distance metrics: Euclidean, city block, and chessboard distances. The function distance_transform_cdt uses a chamfer type algorithm to calculate the distance transform of the input, by replacing each object element (defined by values larger than zero) with the shortest distance to the background (all non-object elements). The structure determines the type of chamfering that is done. If the structure is equal to cityblock, a structure is generated using generate_binary_structure with a squared distance equal to 1. If the structure is equal to chessboard, a structure is generated using generate_binary_structure with a squared distance equal to the rank of the array. These choices correspond to the common interpretations of the city block and the chessboard distance metrics in two dimensions. In addition to the distance transform, the feature transform can be calculated. In this case, the index of the closest background element is returned along the first axis of the result. The return_distances , and return_indices flags can be used to indicate if the distance transform, the feature transform, or both must be returned. The distances and indices arguments can be used to give optional output arrays that must be of the correct size and type (both ). The basics of the algorithm used to implement this function are described in 2 . The function distance_transform_edt calculates the exact Euclidean distance transform of the input, by replacing each object element (defined by values larger than zero) with the shortest Euclidean distance to the background (all non-object elements). In addition to the distance transform, the feature transform can be calculated. In this case, the index of the closest background element is returned along the first axis of the result. The return_distances and return_indices flags can be used to indicate if the distance transform, the feature transform, or both must be returned. Optionally, the sampling along each axis can be given by the sampling parameter, which should be a sequence of length equal to the input rank, or a single number in which the sampling is assumed to be equal along all axes. The distances and indices arguments can be used to give optional output arrays that must be of the correct size and type ( and ).The algorithm used to implement this function is described in 3 . The function distance_transform_bf uses a brute-force algorithm to calculate the distance transform of the input, by replacing each object element (defined by values larger than zero) with the shortest distance to the background (all non-object elements). The metric must be one of euclidean, cityblock, or chessboard. In addition to the distance transform, the feature transform can be calculated. In this case, the index of the closest background element is returned along the first axis of the result. The return_distances and return_indices flags can be used to indicate if the distance transform, the feature transform, or both must be returned. Optionally, the sampling along each axis can be given by the sampling parameter, which should be a sequence of length equal to the input rank, or a single number in which the sampling is assumed to be equal along all axes. This parameter is only used in the case of the Euclidean distance transform. The distances and indices arguments can be used to give optional output arrays that must be of the correct size and type ( and ). Note This function uses a slow brute-force algorithm, the function distance_transform_cdt can be used to more efficiently calculate city block and chessboard distance transforms. The function distance_transform_edt can be used to more efficiently calculate the exact Euclidean distance transform. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Extending scipy.ndimage in C  A few functions in scipy.ndimage take a callback argument. This can be either a python function or a scipy.LowLevelCallable containing a pointer to a C function. Using a C function will generally be more efficient, since it avoids the overhead of calling a python function on many elements of an array. To use a C function, you must write a C extension that contains the callback function and a Python function that returns a scipy.LowLevelCallable containing a pointer to the callback. An example of a function that supports callbacks is geometric_transform , which accepts a callback function that defines a mapping from all output coordinates to corresponding coordinates in the input array. Consider the following python example, which uses geometric_transform to implement a shift function. We can also implement the callback function with the following C code: More information on writing Python extension modules can be found here . If the C code is in the file , then it can be compiled with the following , and now running the script produces the same result as the original python script. In the C version, is the callback function and the parameters and play the same role as they do in the python version, while and provide the equivalents of and . The variable is passed through instead of . Finally, the C callback function returns an integer status, which is one upon success and zero otherwise. The function wraps the callback function in a PyCapsule . The main steps are: Initialize a PyCapsule . The first argument is a pointer to the callback function. The second argument is the function signature, which must match exactly the one expected by ndimage . Above, we used scipy.LowLevelCallable to specify that we generated with ctypes . A different approach would be to supply the data in the capsule context, that can be set by PyCapsule_SetContext and omit specifying in scipy.LowLevelCallable . However, in this approach we would need to deal with allocation/freeing of the data  freeing the data after the capsule has been destroyed can be done by specifying a non-NULL callback function in the third argument of PyCapsule_New . C callback functions for ndimage all follow this scheme. The next section lists the ndimage functions that accept a C callback function and gives the prototype of the function. See also The functions that support low-level callback arguments are: generic_filter , generic_filter1d , geometric_transform Below, we show alternative ways to write the code, using Numba , Cython , ctypes , or cffi instead of writing wrapper code in C. Numba Numba provides a way to write low-level functions easily in Python. We can write the above using Numba as: Cython Functionally the same code as above can be written in Cython with somewhat less boilerplate as follows: cffi With cffi , you can interface with a C function residing in a shared library (DLL). First, we need to write the shared library, which we do in C  this example is for Linux/OSX: The Python code calling the library is: You can find more information in the cffi documentation. ctypes With ctypes , the C code and the compilation of the so/DLL is as for cffi above. The Python code is different: You can find more information in the ctypes documentation. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Filter functions  The functions described in this section all perform some type of spatial filtering of the input array: the elements in the output are some function of the values in the neighborhood of the corresponding input element. We refer to this neighborhood of elements as the filter kernel, which is often rectangular in shape but may also have an arbitrary footprint. Many of the functions described below allow you to define the footprint of the kernel by passing a mask through the footprint parameter. For example, a cross-shaped kernel can be defined as follows: Usually, the origin of the kernel is at the center calculated by dividing the dimensions of the kernel shape by two. For instance, the origin of a 1-D kernel of length three is at the second element. Take, for example, the correlation of a 1-D array with a filter of length 3 consisting of ones: Sometimes, it is convenient to choose a different origin for the kernel. For this reason, most functions support the origin parameter, which gives the origin of the filter relative to its center. For example: The effect is a shift of the result towards the left. This feature will not be needed very often, but it may be useful, especially for filters that have an even size. A good example is the calculation of backward and forward differences: We could also have calculated the forward difference as follows: However, using the origin parameter instead of a larger kernel is more efficient. For multidimensional kernels, origin can be a number, in which case the origin is assumed to be equal along all axes, or a sequence giving the origin along each axis. Since the output elements are a function of elements in the neighborhood of the input elements, the borders of the array need to be dealt with appropriately by providing the values outside the borders. This is done by assuming that the arrays are extended beyond their boundaries according to certain boundary conditions. In the functions described below, the boundary conditions can be selected using the mode parameter, which must be a string with the name of the boundary condition. The following boundary conditions are currently supported: nearest use the value at the boundary [1 2 3]->[1 1 2 3 3] wrap periodically replicate the array [1 2 3]->[3 1 2 3 1] reflect reflect the array at the boundary [1 2 3]->[1 1 2 3 3] constant use a constant value, default is 0.0 [1 2 3]->[0 1 2 3 0] The constant mode is special since it needs an additional parameter to specify the constant value that should be used. Note The easiest way to implement such boundary conditions would be to copy the data to a larger array and extend the data at the borders according to the boundary conditions. For large arrays and large filter kernels, this would be very memory consuming, and the functions described below, therefore, use a different approach that does not require allocating large temporary buffers. Correlation and convolution  The correlate1d function calculates a 1-D correlation along the given axis. The lines of the array along the given axis are correlated with the given weights . The weights parameter must be a 1-D sequence of numbers. The function correlate implements multidimensional correlation of the input array with a given kernel. The convolve1d function calculates a 1-D convolution along the given axis. The lines of the array along the given axis are convoluted with the given weights . The weights parameter must be a 1-D sequence of numbers. The function convolve implements multidimensional convolution of the input array with a given kernel. Note A convolution is essentially a correlation after mirroring the kernel. As a result, the origin parameter behaves differently than in the case of a correlation: the results is shifted in the opposite direction. Smoothing filters  The gaussian_filter1d function implements a 1-D Gaussian filter. The standard deviation of the Gaussian filter is passed through the parameter sigma . Setting order  0 corresponds to convolution with a Gaussian kernel. An order of 1, 2, or 3 corresponds to convolution with the first, second, or third derivatives of a Gaussian. Higher-order derivatives are not implemented. The gaussian_filter function implements a multidimensional Gaussian filter. The standard deviations of the Gaussian filter along each axis are passed through the parameter sigma as a sequence or numbers. If sigma is not a sequence but a single number, the standard deviation of the filter is equal along all directions. The order of the filter can be specified separately for each axis. An order of 0 corresponds to convolution with a Gaussian kernel. An order of 1, 2, or 3 corresponds to convolution with the first, second, or third derivatives of a Gaussian. Higher-order derivatives are not implemented. The order parameter must be a number, to specify the same order for all axes, or a sequence of numbers to specify a different order for each axis. Note The multidimensional filter is implemented as a sequence of 1-D Gaussian filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a lower precision, the results may be imprecise because intermediate results may be stored with insufficient precision. This can be prevented by specifying a more precise output type. The uniform_filter1d function calculates a 1-D uniform filter of the given size along the given axis. The uniform_filter implements a multidimensional uniform filter. The sizes of the uniform filter are given for each axis as a sequence of integers by the size parameter. If size is not a sequence, but a single number, the sizes along all axes are assumed to be equal. Note The multidimensional filter is implemented as a sequence of 1-D uniform filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a lower precision, the results may be imprecise because intermediate results may be stored with insufficient precision. This can be prevented by specifying a more precise output type. Filters based on order statistics  The minimum_filter1d function calculates a 1-D minimum filter of the given size along the given axis. The maximum_filter1d function calculates a 1-D maximum filter of the given size along the given axis. The minimum_filter function calculates a multidimensional minimum filter. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The maximum_filter function calculates a multidimensional maximum filter. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The rank_filter function calculates a multidimensional rank filter. The rank may be less then zero, i.e., rank  -1 indicates the largest element. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The percentile_filter function calculates a multidimensional percentile filter. The percentile may be less then zero, i.e., percentile  -20 equals percentile  80. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. The median_filter function calculates a multidimensional median filter. Either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint if provided, must be an array that defines the shape of the kernel by its non-zero elements. Derivatives  Derivative filters can be constructed in several ways. The function gaussian_filter1d , described in Smoothing filters , can be used to calculate derivatives along a given axis using the order parameter. Other derivative filters are the Prewitt and Sobel filters: The prewitt function calculates a derivative along the given axis. The sobel function calculates a derivative along the given axis. The Laplace filter is calculated by the sum of the second derivatives along all axes. Thus, different Laplace filters can be constructed using different second-derivative functions. Therefore, we provide a general function that takes a function argument to calculate the second derivative along a given direction. The function generic_laplace calculates a Laplace filter using the function passed through to calculate second derivatives. The function should have the following signature It should calculate the second derivative along the dimension axis . If output is not , it should use that for the output and return , otherwise it should return the result. mode , cval have the usual meaning. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and a dictionary of named arguments that are passed to at each call. For example To demonstrate the use of the extra_arguments argument, we could do or The following two functions are implemented using generic_laplace by providing appropriate functions for the second-derivative function: The function laplace calculates the Laplace using discrete differentiation for the second derivative (i.e., convolution with ). The function gaussian_laplace calculates the Laplace filter using gaussian_filter to calculate the second derivatives. The standard deviations of the Gaussian filter along each axis are passed through the parameter sigma as a sequence or numbers. If sigma is not a sequence but a single number, the standard deviation of the filter is equal along all directions. The gradient magnitude is defined as the square root of the sum of the squares of the gradients in all directions. Similar to the generic Laplace function, there is a generic_gradient_magnitude function that calculates the gradient magnitude of an array. The function generic_gradient_magnitude calculates a gradient magnitude using the function passed through to calculate first derivatives. The function should have the following signature It should calculate the derivative along the dimension axis . If output is not , it should use that for the output and return , otherwise it should return the result. mode , cval have the usual meaning. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and a dictionary of named arguments that are passed to derivative at each call. For example, the sobel function fits the required signature See the documentation of generic_laplace for examples of using the extra_arguments and extra_keywords arguments. The sobel and prewitt functions fit the required signature and can, therefore, be used directly with generic_gradient_magnitude . The function gaussian_gradient_magnitude calculates the gradient magnitude using gaussian_filter to calculate the first derivatives. The standard deviations of the Gaussian filter along each axis are passed through the parameter sigma as a sequence or numbers. If sigma is not a sequence but a single number, the standard deviation of the filter is equal along all directions. Generic filter functions  To implement filter functions, generic functions can be used that accept a callable object that implements the filtering operation. The iteration over the input and output arrays is handled by these generic functions, along with such details as the implementation of the boundary conditions. Only a callable object implementing a callback function that does the actual filtering work must be provided. The callback function can also be written in C and passed using a PyCapsule (see Extending scipy.ndimage in C for more information). The generic_filter1d function implements a generic 1-D filter function, where the actual filtering operation must be supplied as a python function (or other callable object). The generic_filter1d function iterates over the lines of an array and calls at each line. The arguments that are passed to are 1-D arrays of the type. The first contains the values of the current line. It is extended at the beginning and the end, according to the filter_size and origin arguments. The second array should be modified in-place to provide the output values of the line. For example, consider a correlation along one dimension: The same operation can be implemented using generic_filter1d , as follows: Here, the origin of the kernel was (by default) assumed to be in the middle of the filter of length 3. Therefore, each input line had been extended by one value at the beginning and at the end, before the function was called. Optionally, extra arguments can be defined and passed to the filter function. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and/or a dictionary of named arguments that are passed to derivative at each call. For example, we can pass the parameters of our filter as an argument or The generic_filter function implements a generic filter function, where the actual filtering operation must be supplied as a python function (or other callable object). The generic_filter function iterates over the array and calls at each element. The argument of is a 1-D array of the type that contains the values around the current element that are within the footprint of the filter. The function should return a single value that can be converted to a double precision number. For example, consider a correlation: The same operation can be implemented using generic_filter , as follows: Here, a kernel footprint was specified that contains only two elements. Therefore, the filter function receives a buffer of length equal to two, which was multiplied with the proper weights and the result summed. When calling generic_filter , either the sizes of a rectangular kernel or the footprint of the kernel must be provided. The size parameter, if provided, must be a sequence of sizes or a single number, in which case the size of the filter is assumed to be equal along each axis. The footprint , if provided, must be an array that defines the shape of the kernel by its non-zero elements. Optionally, extra arguments can be defined and passed to the filter function. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and/or a dictionary of named arguments that are passed to derivative at each call. For example, we can pass the parameters of our filter as an argument or These functions iterate over the lines or elements starting at the last axis, i.e., the last index changes the fastest. This order of iteration is guaranteed for the case that it is important to adapt the filter depending on spatial location. Here is an example of using a class that implements the filter and keeps track of the current coordinates while iterating. It performs the same filter operation as described above for generic_filter , but additionally prints the current coordinates: For the generic_filter1d function, the same approach works, except that this function does not iterate over the axis that is being filtered. The example for generic_filter1d then becomes this: Fourier domain filters  The functions described in this section perform filtering operations in the Fourier domain. Thus, the input array of such a function should be compatible with an inverse Fourier transform function, such as the functions from the numpy.fft module. We, therefore, have to deal with arrays that may be the result of a real or a complex Fourier transform. In the case of a real Fourier transform, only half of the of the symmetric complex transform is stored. Additionally, it needs to be known what the length of the axis was that was transformed by the real fft. The functions described here provide a parameter n that, in the case of a real transform, must be equal to the length of the real transform axis before transformation. If this parameter is less than zero, it is assumed that the input array was the result of a complex Fourier transform. The parameter axis can be used to indicate along which axis the real transform was executed. The fourier_shift function multiplies the input array with the multidimensional Fourier transform of a shift operation for the given shift. The shift parameter is a sequence of shifts for each dimension or a single value for all dimensions. The fourier_gaussian function multiplies the input array with the multidimensional Fourier transform of a Gaussian filter with given standard deviations sigma . The sigma parameter is a sequence of values for each dimension or a single value for all dimensions. The fourier_uniform function multiplies the input array with the multidimensional Fourier transform of a uniform filter with given sizes size . The size parameter is a sequence of values for each dimension or a single value for all dimensions. The fourier_ellipsoid function multiplies the input array with the multidimensional Fourier transform of an elliptically-shaped filter with given sizes size . The size parameter is a sequence of values for each dimension or a single value for all dimensions. This function is only implemented for dimensions 1, 2, and 3. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Interpolation functions  This section describes various interpolation functions that are based on B-spline theory. A good introduction to B-splines can be found in 1 . Spline pre-filters  Interpolation using splines of an order larger than 1 requires a pre-filtering step. The interpolation functions described in section Interpolation functions apply pre-filtering by calling spline_filter , but they can be instructed not to do this by setting the prefilter keyword equal to False. This is useful if more than one interpolation operation is done on the same array. In this case, it is more efficient to do the pre-filtering only once and use a pre-filtered array as the input of the interpolation functions. The following two functions implement the pre-filtering: The spline_filter1d function calculates a 1-D spline filter along the given axis. An output array can optionally be provided. The order of the spline must be larger than 1 and less than 6. The spline_filter function calculates a multidimensional spline filter. Note The multidimensional filter is implemented as a sequence of 1-D spline filters. The intermediate arrays are stored in the same data type as the output. Therefore, if an output with a limited precision is requested, the results may be imprecise because intermediate results may be stored with insufficient precision. This can be prevented by specifying a output type of high precision. Interpolation functions  The following functions all employ spline interpolation to effect some type of geometric transformation of the input array. This requires a mapping of the output coordinates to the input coordinates, and therefore, the possibility arises that input values outside the boundaries may be needed. This problem is solved in the same way as described in Filter functions for the multidimensional filter functions. Therefore, these functions all support a mode parameter that determines how the boundaries are handled, and a cval parameter that gives a constant value in case that the constant mode is used. The geometric_transform function applies an arbitrary geometric transform to the input. The given mapping function is called at each point in the output to find the corresponding coordinates in the input. mapping must be a callable object that accepts a tuple of length equal to the output array rank and returns the corresponding input coordinates as a tuple of length equal to the input array rank. The output shape and output type can optionally be provided. If not given, they are equal to the input shape and type. For example: Optionally, extra arguments can be defined and passed to the filter function. The extra_arguments and extra_keywords arguments can be used to pass a tuple of extra arguments and/or a dictionary of named arguments that are passed to derivative at each call. For example, we can pass the shifts in our example as arguments or Note The mapping function can also be written in C and passed using a scipy.LowLevelCallable . See Extending scipy.ndimage in C for more information. The function map_coordinates applies an arbitrary coordinate transformation using the given array of coordinates. The shape of the output is derived from that of the coordinate array by dropping the first axis. The parameter coordinates is used to find for each point in the output the corresponding coordinates in the input. The values of coordinates along the first axis are the coordinates in the input array at which the output value is found. (See also the numarray coordinates function.) Since the coordinates may be non- integer coordinates, the value of the input at these coordinates is determined by spline interpolation of the requested order. Here is an example that interpolates a 2D array at and : The affine_transform function applies an affine transformation to the input array. The given transformation matrix and offset are used to find for each point in the output the corresponding coordinates in the input. The value of the input at the calculated coordinates is determined by spline interpolation of the requested order. The transformation matrix must be 2-D or can also be given as a 1-D sequence or array. In the latter case, it is assumed that the matrix is diagonal. A more efficient interpolation algorithm is then applied that exploits the separability of the problem. The output shape and output type can optionally be provided. If not given, they are equal to the input shape and type. The shift function returns a shifted version of the input, using spline interpolation of the requested order . The zoom function returns a rescaled version of the input, using spline interpolation of the requested order . The rotate function returns the input array rotated in the plane defined by the two axes given by the parameter axes , using spline interpolation of the requested order . The angle must be given in degrees. If reshape is true, then the size of the output array is adapted to contain the rotated input. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Introduction  Image processing and analysis are generally seen as operations on 2-D arrays of values. There are, however, a number of fields where images of higher dimensionality must be analyzed. Good examples of these are medical imaging and biological imaging. numpy is suited very well for this type of applications due to its inherent multidimensional nature. The scipy.ndimage packages provides a number of general image processing and analysis functions that are designed to operate with arrays of arbitrary dimensionality. The packages currently includes: functions for linear and non-linear filtering, binary morphology, B-spline interpolation, and object measurements. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Morphology  Binary morphology  The generate_binary_structure functions generates a binary structuring element for use in binary morphology operations. The rank of the structure must be provided. The size of the structure that is returned is equal to three in each direction. The value of each element is equal to one if the square of the Euclidean distance from the element to the center is less than or equal to connectivity . For instance, 2-D 4-connected and 8-connected structures are generated as follows: Most binary morphology functions can be expressed in terms of the basic operations erosion and dilation. The binary_erosion function implements binary erosion of arrays of arbitrary rank with the given structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The border_value parameter gives the value of the array outside boundaries. The erosion is repeated iterations times. If iterations is less than one, the erosion is repeated until the result does not change anymore. If a mask array is given, only those elements with a true value at the corresponding mask element are modified at each iteration. The binary_dilation function implements binary dilation of arrays of arbitrary rank with the given structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The border_value parameter gives the value of the array outside boundaries. The dilation is repeated iterations times. If iterations is less than one, the dilation is repeated until the result does not change anymore. If a mask array is given, only those elements with a true value at the corresponding mask element are modified at each iteration. Here is an example of using binary_dilation to find all elements that touch the border, by repeatedly dilating an empty array from the border using the data array as the mask: The binary_erosion and binary_dilation functions both have an iterations parameter, which allows the erosion or dilation to be repeated a number of times. Repeating an erosion or a dilation with a given structure n times is equivalent to an erosion or a dilation with a structure that is n-1 times dilated with itself. A function is provided that allows the calculation of a structure that is dilated a number of times with itself: The iterate_structure function returns a structure by dilation of the input structure iteration - 1 times with itself. For instance: Other morphology operations can be defined in terms of erosion and dilation. The following functions provide a few of these operations for convenience: The binary_opening function implements binary opening of arrays of arbitrary rank with the given structuring element. Binary opening is equivalent to a binary erosion followed by a binary dilation with the same structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The iterations parameter gives the number of erosions that is performed followed by the same number of dilations. The binary_closing function implements binary closing of arrays of arbitrary rank with the given structuring element. Binary closing is equivalent to a binary dilation followed by a binary erosion with the same structuring element. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The iterations parameter gives the number of dilations that is performed followed by the same number of erosions. The binary_fill_holes function is used to close holes in objects in a binary image, where the structure defines the connectivity of the holes. The origin parameter controls the placement of the structuring element, as described in Filter functions . If no structuring element is provided, an element with connectivity equal to one is generated using generate_binary_structure . The binary_hit_or_miss function implements a binary hit-or-miss transform of arrays of arbitrary rank with the given structuring elements. The hit-or-miss transform is calculated by erosion of the input with the first structure, erosion of the logical not of the input with the second structure, followed by the logical and of these two erosions. The origin parameters control the placement of the structuring elements, as described in Filter functions . If origin2 equals , it is set equal to the origin1 parameter. If the first structuring element is not provided, a structuring element with connectivity equal to one is generated using generate_binary_structure . If structure2 is not provided, it is set equal to the logical not of structure1 . Grey-scale morphology  Grey-scale morphology operations are the equivalents of binary morphology operations that operate on arrays with arbitrary values. Below, we describe the grey-scale equivalents of erosion, dilation, opening and closing. These operations are implemented in a similar fashion as the filters described in Filter functions , and we refer to this section for the description of filter kernels and footprints, and the handling of array borders. The grey-scale morphology operations optionally take a structure parameter that gives the values of the structuring element. If this parameter is not given, the structuring element is assumed to be flat with a value equal to zero. The shape of the structure can optionally be defined by the footprint parameter. If this parameter is not given, the structure is assumed to be rectangular, with sizes equal to the dimensions of the structure array, or by the size parameter if structure is not given. The size parameter is only used if both structure and footprint are not given, in which case the structuring element is assumed to be rectangular and flat with the dimensions given by size . The size parameter, if provided, must be a sequence of sizes or a single number in which case the size of the filter is assumed to be equal along each axis. The footprint parameter, if provided, must be an array that defines the shape of the kernel by its non-zero elements. Similarly to binary erosion and dilation, there are operations for grey-scale erosion and dilation: The grey_erosion function calculates a multidimensional grey-scale erosion. The grey_dilation function calculates a multidimensional grey-scale dilation. Grey-scale opening and closing operations can be defined similarly to their binary counterparts: The grey_opening function implements grey-scale opening of arrays of arbitrary rank. Grey-scale opening is equivalent to a grey-scale erosion followed by a grey-scale dilation. The grey_closing function implements grey-scale closing of arrays of arbitrary rank. Grey-scale opening is equivalent to a grey-scale dilation followed by a grey-scale erosion. The morphological_gradient function implements a grey-scale morphological gradient of arrays of arbitrary rank. The grey-scale morphological gradient is equal to the difference of a grey-scale dilation and a grey-scale erosion. The morphological_laplace function implements a grey-scale morphological laplace of arrays of arbitrary rank. The grey-scale morphological laplace is equal to the sum of a grey-scale dilation and a grey-scale erosion minus twice the input. The white_tophat function implements a white top-hat filter of arrays of arbitrary rank. The white top-hat is equal to the difference of the input and a grey-scale opening. The black_tophat function implements a black top-hat filter of arrays of arbitrary rank. The black top-hat is equal to the difference of a grey-scale closing and the input. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Object measurements  Given an array of labeled objects, the properties of the individual objects can be measured. The find_objects function can be used to generate a list of slices that for each object, give the smallest sub-array that fully contains the object: The find_objects function finds all objects in a labeled array and returns a list of slices that correspond to the smallest regions in the array that contains the object. For instance: The function find_objects returns slices for all objects, unless the max_label parameter is larger then zero, in which case only the first max_label objects are returned. If an index is missing in the label array, is return instead of a slice. For example: The list of slices generated by find_objects is useful to find the position and dimensions of the objects in the array, but can also be used to perform measurements on the individual objects. Say, we want to find the sum of the intensities of an object in image: Then we can calculate the sum of the elements in the second object: That is, however, not particularly efficient and may also be more complicated for other types of measurements. Therefore, a few measurements functions are defined that accept the array of object labels and the index of the object to be measured. For instance, calculating the sum of the intensities can be done by: For large arrays and small objects, it is more efficient to call the measurement functions after slicing the array: Alternatively, we can do the measurements for a number of labels with a single function call, returning a list of results. For instance, to measure the sum of the values of the background and the second object in our example, we give a list of labels: The measurement functions described below all support the index parameter to indicate which object(s) should be measured. The default value of index is . This indicates that all elements where the label is larger than zero should be treated as a single object and measured. Thus, in this case the labels array is treated as a mask defined by the elements that are larger than zero. If index is a number or a sequence of numbers it gives the labels of the objects that are measured. If index is a sequence, a list of the results is returned. Functions that return more than one result return their result as a tuple if index is a single number, or as a tuple of lists if index is a sequence. The sum function calculates the sum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The mean function calculates the mean of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The variance function calculates the variance of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The standard_deviation function calculates the standard deviation of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The minimum function calculates the minimum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The maximum function calculates the maximum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The minimum_position function calculates the position of the minimum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The maximum_position function calculates the position of the maximum of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The extrema function calculates the minimum, the maximum, and their positions, of the elements of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The result is a tuple giving the minimum, the maximum, the position of the minimum, and the position of the maximum. The result is the same as a tuple formed by the results of the functions minimum , maximum , minimum_position , and maximum_position that are described above. The center_of_mass function calculates the center of mass of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. The histogram function calculates a histogram of the object with label(s) given by index , using the labels array for the object labels. If index is , all elements with a non-zero label value are treated as a single object. If label is , all elements of input are used in the calculation. Histograms are defined by their minimum ( min ), maximum ( max ), and the number of bins ( bins ). They are returned as 1-D arrays of type . \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Properties shared by all functions  All functions share some common properties. Notably, all functions allow the specification of an output array with the output argument. With this argument, you can specify an array that will be changed in-place with the result with the operation. In this case, the result is not returned. Usually, using the output argument is more efficient, since an existing array is used to store the result. The type of arrays returned is dependent on the type of operation, but it is, in most cases, equal to the type of the input. If, however, the output argument is used, the type of the result is equal to the type of the specified output argument. If no output argument is given, it is still possible to specify what the result of the output should be. This is done by simply assigning the desired numpy type object to the output argument. For example: \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  References  1 M. Unser, Splines: A Perfect Fit for Signal and Image Processing, IEEE Signal Processing Magazine, vol. 16, no. 6, pp. 22-38, November 1999. 2 G. Borgefors, Distance transformations in arbitrary dimensions., Computer Vision, Graphics, and Image Processing, 27:321-345, 1984. 3 C. R. Maurer, Jr., R. Qi, and V. Raghavan, A linear time algorithm for computing exact euclidean distance transforms of binary images in arbitrary dimensions. IEEE Trans. PAMI 25, 265-270, 2003. 4 P. Felkel, R. Wegenkittl, and M. Bruckschwaiger, Implementation and Complexity of the Watershed-from-Markers Algorithm Computed as a Minimal Cost Forest., Eurographics 2001, pp. C:26-35. \"\\n',\n",
       " '\"scipy_multidimensional_image_processing_scipy_ndimage Multidimensional image processing (scipy.ndimage) ndimage.html  Segmentation and labeling  Segmentation is the process of separating objects of interest from the background. The most simple approach is, probably, intensity thresholding, which is easily done with numpy functions: The result is a binary image, in which the individual objects still need to be identified and labeled. The function label generates an array where each object is assigned a unique number: The label function generates an array where the objects in the input are labeled with an integer index. It returns a tuple consisting of the array of object labels and the number of objects found, unless the output parameter is given, in which case only the number of objects is returned. The connectivity of the objects is defined by a structuring element. For instance, in 2D using a 4-connected structuring element gives: These two objects are not connected because there is no way in which we can place the structuring element, such that it overlaps with both objects. However, an 8-connected structuring element results in only a single object: If no structuring element is provided, one is generated by calling generate_binary_structure (see Binary morphology ) using a connectivity of one (which in 2D is the 4-connected structure of the first example). The input can be of any type, any value not equal to zero is taken to be part of an object. This is useful if you need to re-label an array of object indices, for instance, after removing unwanted objects. Just apply the label function again to the index array. For instance: Note The structuring element used by label is assumed to be symmetric. There is a large number of other approaches for segmentation, for instance, from an estimation of the borders of the objects that can be obtained by derivative filters. One such approach is watershed segmentation. The function watershed_ift generates an array where each object is assigned a unique label, from an array that localizes the object borders, generated, for instance, by a gradient magnitude filter. It uses an array containing initial markers for the objects: The watershed_ift function applies a watershed from markers algorithm, using an Iterative Forest Transform, as described in 4 . The inputs of this function are the array to which the transform is applied, and an array of markers that designate the objects by a unique label, where any non-zero value is a marker. For instance: Here, two markers were used to designate an object ( marker  2) and the background ( marker  1). The order in which these are processed is arbitrary: moving the marker for the background to the lower-right corner of the array yields a different result: The result is that the object ( marker  2) is smaller because the second marker was processed earlier. This may not be the desired effect if the first marker was supposed to designate a background object. Therefore, watershed_ift treats markers with a negative value explicitly as background markers and processes them after the normal markers. For instance, replacing the first marker by a negative marker gives a result similar to the first example: The connectivity of the objects is defined by a structuring element. If no structuring element is provided, one is generated by calling generate_binary_structure (see Binary morphology ) using a connectivity of one (which in 2D is a 4-connected structure.) For example, using an 8-connected structure with the last example yields a different object: Note The implementation of watershed_ift limits the data types of the input to and . \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Constrained minimization of multivariate scalar functions ( minimize )  The minimize function provides algorithms for constrained minimization, namely , and . They require the constraints to be defined using slightly different structures. The method requires the constraints to be defined as a sequence of objects LinearConstraint and NonlinearConstraint . Methods and , on the other hand, require constraints to be defined as a sequence of dictionaries, with keys , and . As an example let us consider the constrained minimization of the Rosenbrock function: \\\\begin{eqnarray*} \\\\min_{x_0, x_1} & ~~100\\\\left(x_{1}-x_{0}^{2}\\\\right)^{2}+\\\\left(1-x_{0}\\\\right)^{2} &\\\\\\\\ \\\\text{subject to: } & x_0 + 2 x_1 \\\\leq 1 & \\\\\\\\ & x_0^2 + x_1 \\\\leq 1 & \\\\\\\\ & x_0^2 - x_1 \\\\leq 1 & \\\\\\\\ & 2 x_0 + x_1  1 & \\\\\\\\ & 0 \\\\leq x_0 \\\\leq 1 & \\\\\\\\ & -0.5 \\\\leq x_1 \\\\leq 2.0. & \\\\end{eqnarray*} This optimization problem has the unique solution , for which only the first and fourth constraints are active. Trust-Region Constrained Algorithm ( )  The trust-region constrained method deals with constrained minimization problems of the form: \\\\begin{eqnarray*} \\\\min_x & f(x) & \\\\\\\\ \\\\text{subject to: } & ~~~ c^l \\\\leq c(x) \\\\leq c^u, &\\\\\\\\ & x^l \\\\leq x \\\\leq x^u. & \\\\end{eqnarray*} When the method reads the -th constraint as an equality constraint and deals with it accordingly. Besides that, one-sided constraint can be specified by setting the upper or lower bound to with the appropriate sign. The implementation is based on [EQSQP] for equality-constraint problems and on [TRIP] for problems with inequality constraints. Both are trust-region type algorithms suitable for large-scale problems. Defining Bounds Constraints:  The bound constraints and are defined using a Bounds object. Defining Linear Constraints:  The constraints and can be written in the linear constraint standard format: \\\\begin{equation*} \\\\begin{bmatrix}-\\\\infty \\\\\\\\1\\\\end{bmatrix} \\\\leq \\\\begin{bmatrix} 1& 2 \\\\\\\\ 2& 1\\\\end{bmatrix} \\\\begin{bmatrix} x_0 \\\\\\\\x_1\\\\end{bmatrix} \\\\leq \\\\begin{bmatrix} 1 \\\\\\\\ 1\\\\end{bmatrix},\\\\end{equation*} and defined using a LinearConstraint object. Defining Nonlinear Constraints:  The nonlinear constraint: \\\\begin{equation*} c(x)  \\\\begin{bmatrix} x_0^2 + x_1 \\\\\\\\ x_0^2 - x_1\\\\end{bmatrix} \\\\leq \\\\begin{bmatrix} 1 \\\\\\\\ 1\\\\end{bmatrix}, \\\\end{equation*} with Jacobian matrix: \\\\begin{equation*} J(x)  \\\\begin{bmatrix} 2x_0 & 1 \\\\\\\\ 2x_0 & -1\\\\end{bmatrix},\\\\end{equation*} and linear combination of the Hessians: \\\\begin{equation*} H(x, v)  \\\\sum_{i0}^1 v_i \\\\nabla^2 c_i(x)  v_0\\\\begin{bmatrix} 2 & 0 \\\\\\\\ 0 & 0\\\\end{bmatrix} + v_1\\\\begin{bmatrix} 2 & 0 \\\\\\\\ 0 & 0\\\\end{bmatrix}, \\\\end{equation*} is defined using a NonlinearConstraint object. Alternatively, it is also possible to define the Hessian as a sparse matrix, or as a LinearOperator object. When the evaluation of the Hessian is difficult to implement or computationally infeasible, one may use HessianUpdateStrategy . Currently available strategies are BFGS and SR1 . Alternatively, the Hessian may be approximated using finite differences. The Jacobian of the constraints can be approximated by finite differences as well. In this case, however, the Hessian cannot be computed with finite differences and needs to be provided by the user or defined using HessianUpdateStrategy . Solving the Optimization Problem:  The optimization problem is solved using: When needed, the objective function Hessian can be defined using a LinearOperator object, or a Hessian-vector product through the parameter . Alternatively, the first and second derivatives of the objective function can be approximated. For instance, the Hessian can be approximated with SR1 quasi-Newton approximation and the gradient with finite differences. TRIP Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999. An interior point algorithm for large-scale nonlinear programming. SIAM Journal on Optimization 9.4: 877-900. EQSQP Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the implementation of an algorithm for large-scale equality constrained optimization. SIAM Journal on Optimization 8.3: 682-706. Sequential Least SQuares Programming (SLSQP) Algorithm ( )  The SLSQP method deals with constrained minimization problems of the form: \\\\begin{eqnarray*} \\\\min_x & f(x) \\\\\\\\ \\\\text{subject to: } & c_j(x)  0 , &j \\\\in \\\\mathcal{E}\\\\\\\\ & c_j(x) \\\\geq 0 , &j \\\\in \\\\mathcal{I}\\\\\\\\ & \\\\text{lb}_i \\\\leq x_i \\\\leq \\\\text{ub}_i , &i  1,...,N. \\\\end{eqnarray*} Where or are sets of indices containing equality and inequality constraints. Both linear and nonlinear constraints are defined as dictionaries with keys , and . And the optimization problem is solved with: Most of the options available for the method are not available for . \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Custom minimizers  Sometimes, it may be useful to use a custom method as a (multivariate or univariate) minimizer, for example, when using some library wrappers of minimize (e.g., basinhopping ). We can achieve that by, instead of passing a method name, passing a callable (either a function or an object implementing a __call__ method) as the method parameter. Let us consider an (admittedly rather virtual) need to use a trivial custom multivariate minimization method that will just search the neighborhood in each dimension independently with a fixed step size: This will work just as well in case of univariate optimization: \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Global optimization  Global optimization aims to find the global minimum of a function within given bounds, in the presence of potentially many local minima. Typically, global minimizers efficiently search the parameter space, while using a local minimizer (e.g., minimize ) under the hood. SciPy contains a number of good global optimizers. Here, well use those on the same objective function, namely the (aptly named) function: This function looks like an egg carton: We now use the global optimizers to obtain the minimum and the function value at the minimum. Well store the results in a dictionary so we can compare different optimization results later. All optimizers return an , which in addition to the solution contains information on the number of function evaluations, whether the optimization was successful, and more. For brevity, we wont show the full output of the other optimizers: shgo has a second method, which returns all local minima rather than only what it thinks is the global minimum: Well now plot all found minima on a heatmap of the function: \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Least-squares minimization ( least_squares )  SciPy is capable of solving robustified bound-constrained nonlinear least-squares problems: \\\\begin{align} &\\\\min_\\\\mathbf{x} \\\\frac{1}{2} \\\\sum_{i  1}^m \\\\rho\\\\left(f_i(\\\\mathbf{x})^2\\\\right) \\\\\\\\ &\\\\text{subject to }\\\\mathbf{lb} \\\\leq \\\\mathbf{x} \\\\leq \\\\mathbf{ub} \\\\end{align} Here are smooth functions from to , we refer to them as residuals. The purpose of a scalar-valued function is to reduce the influence of outlier residuals and contribute to robustness of the solution, we refer to it as a loss function. A linear loss function gives a standard least-squares problem. Additionally, constraints in a form of lower and upper bounds on some of are allowed. All methods specific to least-squares minimization utilize a matrix of partial derivatives called Jacobian and defined as . It is highly recommended to compute this matrix analytically and pass it to least_squares , otherwise, it will be estimated by finite differences, which takes a lot of additional time and can be very inaccurate in hard cases. Function least_squares can be used for fitting a function to empirical data . To do this, one should simply precompute residuals as , where are weights assigned to each observation. Example of solving a fitting problem  Here we consider an enzymatic reaction 1 . There are 11 residuals defined as \\\\[f_i(x)  \\\\frac{x_0 (u_i^2 + u_i x_1)}{u_i^2 + u_i x_2 + x_3} - y_i, \\\\quad i  0, \\\\ldots, 10,\\\\] where are measurement values and are values of the independent variable. The unknown vector of parameters is . As was said previously, it is recommended to compute Jacobian matrix in a closed form: \\\\begin{align} &J_{i0}  \\\\frac{\\\\partial f_i}{\\\\partial x_0}  \\\\frac{u_i^2 + u_i x_1}{u_i^2 + u_i x_2 + x_3} \\\\\\\\ &J_{i1}  \\\\frac{\\\\partial f_i}{\\\\partial x_1}  \\\\frac{u_i x_0}{u_i^2 + u_i x_2 + x_3} \\\\\\\\ &J_{i2}  \\\\frac{\\\\partial f_i}{\\\\partial x_2}  -\\\\frac{x_0 (u_i^2 + u_i x_1) u_i}{(u_i^2 + u_i x_2 + x_3)^2} \\\\\\\\ &J_{i3}  \\\\frac{\\\\partial f_i}{\\\\partial x_3}  -\\\\frac{x_0 (u_i^2 + u_i x_1)}{(u_i^2 + u_i x_2 + x_3)^2} \\\\end{align} We are going to use the hard starting point defined in 2 . To find a physically meaningful solution, avoid potential division by zero and assure convergence to the global minimum we impose constraints . The code below implements least-squares estimation of and finally plots the original data and the fitted model function: 1 J. Kowalik and J. F. Morrison, Analysis of kinetic data for allosteric enzyme reactions as a nonlinear regression problem, Math. Biosci., vol. 2, pp. 57-66, 1968. 2 Averick et al., The MINPACK-2 Test Problem Collection. Further examples  Three interactive examples below illustrate usage of least_squares in greater detail. Large-scale bundle adjustment in scipy demonstrates large-scale capabilities of least_squares and how to efficiently compute finite difference approximation of sparse Jacobian. Robust nonlinear regression in scipy shows how to handle outliers with a robust loss function in a nonlinear regression. Solving a discrete boundary-value problem in scipy examines how to solve a large system of equations and use bounds to achieve desired properties of the solution. For the details about mathematical algorithms behind the implementation refer to documentation of least_squares . \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Linear programming ( linprog )  The function linprog can minimize a linear objective function subject to linear equality and inequality constraints. This kind of problem is well known as linear programming. Linear programming solves problems of the following form: \\\\[\\\\begin{split}\\\\min_x \\\\ & c^T x \\\\\\\\ \\\\mbox{such that} \\\\ & A_{ub} x \\\\leq b_{ub},\\\\\\\\ & A_{eq} x  b_{eq},\\\\\\\\ & l \\\\leq x \\\\leq u ,\\\\end{split}\\\\] where is a vector of decision variables; , , , , and are vectors; and and are matrices. In this tutorial, we will try to solve a typical linear programming problem using linprog . Linear programming example  Consider the following simple linear programming problem: \\\\[\\\\begin{split}\\\\max_{x_1, x_2, x_3, x_4} \\\\ & 29x_1 + 45x_2 \\\\\\\\ \\\\mbox{such that} \\\\ & x_1 -x_2 -3x_3 \\\\leq 5\\\\\\\\ & 2x_1 -3x_2 -7x_3 + 3x_4 \\\\geq 10\\\\\\\\ & 2x_1 + 8x_2 + x_3  60\\\\\\\\ & 4x_1 + 4x_2 + x_4  60\\\\\\\\ & 0 \\\\leq x_0\\\\\\\\ & 0 \\\\leq x_1 \\\\leq 5\\\\\\\\ & x_2 \\\\leq 0.5\\\\\\\\ & -3 \\\\leq x_3\\\\\\\\\\\\end{split}\\\\] We need some mathematical manipulations to convert the target problem to the form accepted by linprog . First of all, lets consider the objective function. We want to maximize the objective function, but linprog can only accept a minimization problem. This is easily remedied by converting the maximize to minimizing . Also, are not shown in the objective function. That means the weights corresponding with are zero. So, the objective function can be converted to: \\\\[\\\\min_{x_1, x_2, x_3, x_4} \\\\ -29x_1 -45x_2 + 0x_3 + 0x_4\\\\] If we define the vector of decision variables , the objective weights vector of linprog in this problem should be \\\\[c  [-29, -45, 0, 0]^T\\\\] Next, lets consider the two inequality constraints. The first one is a less than inequality, so it is already in the form accepted by linprog . The second one is a greater than inequality, so we need to multiply both sides by to convert it to a less than inequality. Explicitly showing zero coefficients, we have: \\\\[\\\\begin{split}x_1 -x_2 -3x_3 + 0x_4 &\\\\leq 5\\\\\\\\ -2x_1 + 3x_2 + 7x_3 - 3x_4 &\\\\leq -10\\\\\\\\\\\\end{split}\\\\] These equations can be converted to matrix form: \\\\[\\\\begin{split}A_{ub} x \\\\leq b_{ub}\\\\\\\\\\\\end{split}\\\\] where \\\\begin{equation*} A_{ub}  \\\\begin{bmatrix} 1 & -1 & -3 & 0 \\\\\\\\ -2 & 3 & 7 & -3 \\\\end{bmatrix} \\\\end{equation*} \\\\begin{equation*} b_{ub}  \\\\begin{bmatrix} 5 \\\\\\\\ -10 \\\\end{bmatrix} \\\\end{equation*} Next, lets consider the two equality constraints. Showing zero weights explicitly, these are: \\\\[\\\\begin{split}2x_1 + 8x_2 + 1x_3 + 0x_4 & 60\\\\\\\\ 4x_1 + 4x_2 + 0x_3 + 1x_4 & 60\\\\\\\\\\\\end{split}\\\\] These equations can be converted to matrix form: \\\\[\\\\begin{split}A_{eq} x  b_{eq}\\\\\\\\\\\\end{split}\\\\] where \\\\begin{equation*} A_{eq}  \\\\begin{bmatrix} 2 & 8 & 1 & 0 \\\\\\\\ 4 & 4 & 0 & 1 \\\\end{bmatrix} \\\\end{equation*} \\\\begin{equation*} b_{eq}  \\\\begin{bmatrix} 60 \\\\\\\\ 60 \\\\end{bmatrix} \\\\end{equation*} Lastly, lets consider the separate inequality constraints on individual decision variables, which are known as box constraints or simple bounds. These constraints can be applied using the bounds argument of linprog . As noted in the linprog documentation, the default value of bounds is , meaning that the lower bound on each decision variable is 0, and the upper bound on each decision variable is infinity: all the decision variables are non-negative. Our bounds are different, so we will need to specify the lower and upper bound on each decision variable as a tuple and group these tuples into a list. Finally, we can solve the transformed problem using linprog . The result states that our problem is infeasible, meaning that there is no solution vector that satisfies all the constraints. That doesnt necessarily mean we did anything wrong; some problems truly are infeasible. Suppose, however, that we were to decide that our bound constraint on was too tight and that it could be loosened to . After adjusting our code to reflect the change and executing it again: The result shows the optimization was successful. We can check the objective value ( ) is same as : We can also check that all constraints are satisfied within reasonable tolerances: If we need greater accuracy, typically at the expense of speed, we can solve using the method: References Some further reading and related software, such as Newton-Krylov [KK] , PETSc [PP] , and PyAMG [AMG] : KK D.A. Knoll and D.E. Keyes, Jacobian-free Newton-Krylov methods, J. Comp. Phys. 193, 357 (2004). doi:10.1016/j.jcp.2003.08.010 PP PETSc https://www.mcs.anl.gov/petsc/ and its Python bindings https://bitbucket.org/petsc/petsc4py/ AMG PyAMG (algebraic multigrid preconditioners/solvers) https://github.com/pyamg/pyamg/issues \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Root finding  Scalar functions  If one has a single-variable equation, there are multiple different root finding algorithms that can be tried. Most of these algorithms require the endpoints of an interval in which a root is expected (because the function changes signs). In general, brentq is the best choice, but the other methods may be useful in certain circumstances or for academic purposes. When a bracket is not available, but one or more derivatives are available, then newton (or , ) may be applicable. This is especially the case if the function is defined on a subset of the complex plane, and the bracketing methods cannot be used. Fixed-point solving  A problem closely related to finding the zeros of a function is the problem of finding a fixed point of a function. A fixed point of a function is the point at which evaluation of the function returns the point: Clearly, the fixed point of is the root of Equivalently, the root of is the fixed point of The routine fixed_point provides a simple iterative method using Aitkens sequence acceleration to estimate the fixed point of given a starting point. Sets of equations  Finding a root of a set of non-linear equations can be achieved using the root function. Several methods are available, amongst which (the default) and , which, respectively, use the hybrid method of Powell and the Levenberg-Marquardt method from MINPACK. The following example considers the single-variable transcendental equation \\\\[x+2\\\\cos\\\\left(x\\\\right)0,\\\\] a root of which can be found as follows: Consider now a set of non-linear equations \\\\begin{eqnarray*} x_{0}\\\\cos\\\\left(x_{1}\\\\right) &  & 4,\\\\\\\\ x_{0}x_{1}-x_{1} &  & 5. \\\\end{eqnarray*} We define the objective function so that it also returns the Jacobian and indicate this by setting the parameter to . Also, the Levenberg-Marquardt solver is used here. Root finding for large problems  Methods and in root cannot deal with a very large number of variables ( N ), as they need to calculate and invert a dense N x N Jacobian matrix on every Newton step. This becomes rather inefficient when N grows. Consider, for instance, the following problem: we need to solve the following integrodifferential equation on the square : \\\\[(\\\\partial_x^2 + \\\\partial_y^2) P + 5 \\\\left(\\\\int_0^1\\\\int_0^1\\\\cosh(P)\\\\,dx\\\\,dy\\\\right)^2  0\\\\] with the boundary condition on the upper edge and elsewhere on the boundary of the square. This can be done by approximating the continuous function P by its values on a grid, , with a small grid spacing h . The derivatives and integrals can then be approximated; for instance . The problem is then equivalent to finding the root of some function , where is a vector of length . Now, because can be large, methods or in root will take a long time to solve this problem. The solution can, however, be found using one of the large-scale solvers, for example , , or . These use what is known as the inexact Newton method, which instead of computing the Jacobian matrix exactly, forms an approximation for it. The problem we have can now be solved as follows: Still too slow? Preconditioning.  When looking for the zero of the functions , i  1, 2, , N , the solver spends most of the time inverting the Jacobian matrix, \\\\[J_{ij}  \\\\frac{\\\\partial f_i}{\\\\partial x_j} .\\\\] If you have an approximation for the inverse matrix , you can use it for preconditioning the linear-inversion problem. The idea is that instead of solving one solves : since matrix is closer to the identity matrix than is, the equation should be easier for the Krylov method to deal with. The matrix M can be passed to root with method as an option . It can be a (sparse) matrix or a scipy.sparse.linalg.LinearOperator instance. For the problem in the previous section, we note that the function to solve consists of two parts: the first one is the application of the Laplace operator, , and the second is the integral. We can actually easily compute the Jacobian corresponding to the Laplace operator part: we know that in 1-D \\\\[\\\\begin{split}\\\\partial_x^2 \\\\approx \\\\frac{1}{h_x^2} \\\\begin{pmatrix} -2 & 1 & 0 & 0 \\\\cdots \\\\\\\\ 1 & -2 & 1 & 0 \\\\cdots \\\\\\\\ 0 & 1 & -2 & 1 \\\\cdots \\\\\\\\ \\\\ldots \\\\end{pmatrix}  h_x^{-2} L\\\\end{split}\\\\] so that the whole 2-D operator is represented by \\\\[J_1  \\\\partial_x^2 + \\\\partial_y^2 \\\\simeq h_x^{-2} L \\\\otimes I + h_y^{-2} I \\\\otimes L\\\\] The matrix of the Jacobian corresponding to the integral is more difficult to calculate, and since all of it entries are nonzero, it will be difficult to invert. on the other hand is a relatively simple matrix, and can be inverted by scipy.sparse.linalg.splu (or the inverse can be approximated by scipy.sparse.linalg.spilu ). So we are content to take and hope for the best. In the example below, we use the preconditioner . Resulting run, first without preconditioning: and then with preconditioning: Using a preconditioner reduced the number of evaluations of the function by a factor of 4 . For problems where the residual is expensive to compute, good preconditioning can be crucial  it can even decide whether the problem is solvable in practice or not. Preconditioning is an art, science, and industry. Here, we were lucky in making a simple choice that worked reasonably well, but there is a lot more depth to this topic than is shown here. \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Unconstrained minimization of multivariate scalar functions ( minimize )  The minimize function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in scipy.optimize . To demonstrate the minimization function, consider the problem of minimizing the Rosenbrock function of variables: \\\\[f\\\\left(\\\\mathbf{x}\\\\right)\\\\sum_{i1}^{N-1}100\\\\left(x_{i+1}-x_{i}^{2}\\\\right)^{2}+\\\\left(1-x_{i}\\\\right)^{2}.\\\\] The minimum value of this function is 0 which is achieved when Note that the Rosenbrock function and its derivatives are included in scipy.optimize . The implementations shown in the following sections provide examples of how to define an objective function as well as its jacobian and hessian functions. Nelder-Mead Simplex algorithm ( )  In the example below, the minimize routine is used with the Nelder-Mead simplex algorithm (selected through the parameter): The simplex algorithm is probably the simplest way to minimize a fairly well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum. Another optimization algorithm that needs only function calls to find the minimum is Powell s method available by setting in minimize . Broyden-Fletcher-Goldfarb-Shanno algorithm ( )  In order to converge more quickly to the solution, this routine uses the gradient of the objective function. If the gradient is not given by the user, then it is estimated using first-differences. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) method typically requires fewer function calls than the simplex algorithm even when the gradient must be estimated. To demonstrate this algorithm, the Rosenbrock function is again used. The gradient of the Rosenbrock function is the vector: \\\\begin{eqnarray*} \\\\frac{\\\\partial f}{\\\\partial x_{j}} &  & \\\\sum_{i1}^{N}200\\\\left(x_{i}-x_{i-1}^{2}\\\\right)\\\\left(\\\\delta_{i,j}-2x_{i-1}\\\\delta_{i-1,j}\\\\right)-2\\\\left(1-x_{i-1}\\\\right)\\\\delta_{i-1,j}.\\\\\\\\ &  & 200\\\\left(x_{j}-x_{j-1}^{2}\\\\right)-400x_{j}\\\\left(x_{j+1}-x_{j}^{2}\\\\right)-2\\\\left(1-x_{j}\\\\right).\\\\end{eqnarray*} This expression is valid for the interior derivatives. Special cases are \\\\begin{eqnarray*} \\\\frac{\\\\partial f}{\\\\partial x_{0}} &  & -400x_{0}\\\\left(x_{1}-x_{0}^{2}\\\\right)-2\\\\left(1-x_{0}\\\\right),\\\\\\\\ \\\\frac{\\\\partial f}{\\\\partial x_{N-1}} &  & 200\\\\left(x_{N-1}-x_{N-2}^{2}\\\\right).\\\\end{eqnarray*} A Python function which computes this gradient is constructed by the code-segment: This gradient information is specified in the minimize function through the parameter as illustrated below. Newton-Conjugate-Gradient algorithm ( )  Newton-Conjugate Gradient algorithm is a modified Newtons method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian [NW] . Newtons method is based on fitting the function locally to a quadratic form: \\\\[f\\\\left(\\\\mathbf{x}\\\\right)\\\\approx f\\\\left(\\\\mathbf{x}_{0}\\\\right)+\\\\nabla f\\\\left(\\\\mathbf{x}_{0}\\\\right)\\\\cdot\\\\left(\\\\mathbf{x}-\\\\mathbf{x}_{0}\\\\right)+\\\\frac{1}{2}\\\\left(\\\\mathbf{x}-\\\\mathbf{x}_{0}\\\\right)^{T}\\\\mathbf{H}\\\\left(\\\\mathbf{x}_{0}\\\\right)\\\\left(\\\\mathbf{x}-\\\\mathbf{x}_{0}\\\\right).\\\\] where is a matrix of second-derivatives (the Hessian). If the Hessian is positive definite then the local minimum of this function can be found by setting the gradient of the quadratic form to zero, resulting in \\\\[\\\\mathbf{x}_{\\\\textrm{opt}}\\\\mathbf{x}_{0}-\\\\mathbf{H}^{-1}\\\\nabla f.\\\\] The inverse of the Hessian is evaluated using the conjugate-gradient method. An example of employing this method to minimizing the Rosenbrock function is given below. To take full advantage of the Newton-CG method, a function which computes the Hessian must be provided. The Hessian matrix itself does not need to be constructed, only a vector which is the product of the Hessian with an arbitrary vector needs to be available to the minimization routine. As a result, the user can provide either a function to compute the Hessian matrix, or a function to compute the product of the Hessian with an arbitrary vector. Full Hessian example:  The Hessian of the Rosenbrock function is \\\\begin{eqnarray*} H_{ij}\\\\frac{\\\\partial^{2}f}{\\\\partial x_{i}\\\\partial x_{j}} &  & 200\\\\left(\\\\delta_{i,j}-2x_{i-1}\\\\delta_{i-1,j}\\\\right)-400x_{i}\\\\left(\\\\delta_{i+1,j}-2x_{i}\\\\delta_{i,j}\\\\right)-400\\\\delta_{i,j}\\\\left(x_{i+1}-x_{i}^{2}\\\\right)+2\\\\delta_{i,j},\\\\\\\\ &  & \\\\left(202+1200x_{i}^{2}-400x_{i+1}\\\\right)\\\\delta_{i,j}-400x_{i}\\\\delta_{i+1,j}-400x_{i-1}\\\\delta_{i-1,j},\\\\end{eqnarray*} if with defining the matrix. Other non-zero entries of the matrix are \\\\begin{eqnarray*} \\\\frac{\\\\partial^{2}f}{\\\\partial x_{0}^{2}} &  & 1200x_{0}^{2}-400x_{1}+2,\\\\\\\\ \\\\frac{\\\\partial^{2}f}{\\\\partial x_{0}\\\\partial x_{1}}\\\\frac{\\\\partial^{2}f}{\\\\partial x_{1}\\\\partial x_{0}} &  & -400x_{0},\\\\\\\\ \\\\frac{\\\\partial^{2}f}{\\\\partial x_{N-1}\\\\partial x_{N-2}}\\\\frac{\\\\partial^{2}f}{\\\\partial x_{N-2}\\\\partial x_{N-1}} &  & -400x_{N-2},\\\\\\\\ \\\\frac{\\\\partial^{2}f}{\\\\partial x_{N-1}^{2}} &  & 200.\\\\end{eqnarray*} For example, the Hessian when is \\\\[\\\\begin{split}\\\\mathbf{H}\\\\begin{bmatrix} 1200x_{0}^{2}-400x_{1}+2 & -400x_{0} & 0 & 0 & 0\\\\\\\\ -400x_{0} & 202+1200x_{1}^{2}-400x_{2} & -400x_{1} & 0 & 0\\\\\\\\ 0 & -400x_{1} & 202+1200x_{2}^{2}-400x_{3} & -400x_{2} & 0\\\\\\\\ 0 & & -400x_{2} & 202+1200x_{3}^{2}-400x_{4} & -400x_{3}\\\\\\\\ 0 & 0 & 0 & -400x_{3} & 200\\\\end{bmatrix}.\\\\end{split}\\\\] The code which computes this Hessian along with the code to minimize the function using Newton-CG method is shown in the following example: Hessian product example:  For larger minimization problems, storing the entire Hessian matrix can consume considerable time and memory. The Newton-CG algorithm only needs the product of the Hessian times an arbitrary vector. As a result, the user can supply code to compute this product rather than the full Hessian by giving a function which take the minimization vector as the first argument and the arbitrary vector as the second argument (along with extra arguments passed to the function to be minimized). If possible, using Newton-CG with the Hessian product option is probably the fastest way to minimize the function. In this case, the product of the Rosenbrock Hessian with an arbitrary vector is not difficult to compute. If is the arbitrary vector, then has elements: \\\\[\\\\begin{split}\\\\mathbf{H}\\\\left(\\\\mathbf{x}\\\\right)\\\\mathbf{p}\\\\begin{bmatrix} \\\\left(1200x_{0}^{2}-400x_{1}+2\\\\right)p_{0}-400x_{0}p_{1}\\\\\\\\ \\\\vdots\\\\\\\\ -400x_{i-1}p_{i-1}+\\\\left(202+1200x_{i}^{2}-400x_{i+1}\\\\right)p_{i}-400x_{i}p_{i+1}\\\\\\\\ \\\\vdots\\\\\\\\ -400x_{N-2}p_{N-2}+200p_{N-1}\\\\end{bmatrix}.\\\\end{split}\\\\] Code which makes use of this Hessian product to minimize the Rosenbrock function using minimize follows: According to [NW] p. 170 the algorithm can be inefficient when the Hessian is ill-conditioned because of the poor quality search directions provided by the method in those situations. The method , according to the authors, deals more effectively with this problematic situation and will be described next. Trust-Region Newton-Conjugate-Gradient Algorithm ( )  The method is a line search method: it finds a direction of search minimizing a quadratic approximation of the function and then uses a line search algorithm to find the (nearly) optimal step size in that direction. An alternative approach is to, first, fix the step size limit and then find the optimal step inside the given trust-radius by solving the following quadratic subproblem: \\\\begin{eqnarray*} \\\\min_{\\\\mathbf{p}} f\\\\left(\\\\mathbf{x}_{k}\\\\right)+\\\\nabla f\\\\left(\\\\mathbf{x}_{k}\\\\right)\\\\cdot\\\\mathbf{p}+\\\\frac{1}{2}\\\\mathbf{p}^{T}\\\\mathbf{H}\\\\left(\\\\mathbf{x}_{k}\\\\right)\\\\mathbf{p};&\\\\\\\\ \\\\text{subject to: } \\\\|\\\\mathbf{p}\\\\|\\\\le \\\\Delta.& \\\\end{eqnarray*} The solution is then updated and the trust-radius is adjusted according to the degree of agreement of the quadratic model with the real function. This family of methods is known as trust-region methods. The algorithm is a trust-region method that uses a conjugate gradient algorithm to solve the trust-region subproblem [NW] . Full Hessian example:  Hessian product example:  Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm ( )  Similar to the method, the method is a method suitable for large-scale problems as it uses the hessian only as linear operator by means of matrix-vector products. It solves the quadratic subproblem more accurately than the method. \\\\begin{eqnarray*} \\\\min_{\\\\mathbf{p}} f\\\\left(\\\\mathbf{x}_{k}\\\\right)+\\\\nabla f\\\\left(\\\\mathbf{x}_{k}\\\\right)\\\\cdot\\\\mathbf{p}+\\\\frac{1}{2}\\\\mathbf{p}^{T}\\\\mathbf{H}\\\\left(\\\\mathbf{x}_{k}\\\\right)\\\\mathbf{p};&\\\\\\\\ \\\\text{subject to: } \\\\|\\\\mathbf{p}\\\\|\\\\le \\\\Delta.& \\\\end{eqnarray*} This method wraps the [TRLIB] implementation of the [GLTR] method solving exactly a trust-region subproblem restricted to a truncated Krylov subspace. For indefinite problems it is usually better to use this method as it reduces the number of nonlinear iterations at the expense of few more matrix-vector products per subproblem solve in comparison to the method. Full Hessian example:  Hessian product example:  TRLIB F. Lenders, C. Kirches, A. Potschka: trlib: A vector-free implementation of the GLTR method for iterative solution of the trust region problem, https://arxiv.org/abs/1611.04718 GLTR N. Gould, S. Lucidi, M. Roma, P. Toint: Solving the Trust-Region Subproblem using the Lanczos Method, SIAM J. Optim., 9(2), 504525, (1999). https://doi.org/10.1137/S1052623497322735 Trust-Region Nearly Exact Algorithm ( )  All methods , and are suitable for dealing with large-scale problems (problems with thousands of variables). That is because the conjugate gradient algorithm approximately solve the trust-region subproblem (or invert the Hessian) by iterations without the explicit Hessian factorization. Since only the product of the Hessian with an arbitrary vector is needed, the algorithm is specially suited for dealing with sparse Hessians, allowing low storage requirements and significant time savings for those sparse problems. For medium-size problems, for which the storage and factorization cost of the Hessian are not critical, it is possible to obtain a solution within fewer iteration by solving the trust-region subproblems almost exactly. To achieve that, a certain nonlinear equations is solved iteratively for each quadratic subproblem [CGT] . This solution requires usually 3 or 4 Cholesky factorizations of the Hessian matrix. As the result, the method converges in fewer number of iterations and takes fewer evaluations of the objective function than the other implemented trust-region methods. The Hessian product option is not supported by this algorithm. An example using the Rosenbrock function follows: NW ( 1 , 2 , 3 ) J. Nocedal, S.J. Wright Numerical optimization. 2nd edition. Springer Science (2006). CGT Conn, A. R., Gould, N. I., & Toint, P. L. Trust region methods. Siam. (2000). pp. 169-200. \"\\n',\n",
       " '\"scipy_optimization_scipy_optimize Optimization (scipy.optimize) optimize.html  Univariate function minimizers ( minimize_scalar )  Often only the minimum of an univariate function (i.e., a function that takes a scalar as input) is needed. In these circumstances, other optimization techniques have been developed that can work faster. These are accessible from the minimize_scalar function, which proposes several algorithms. Unconstrained minimization ( )  There are, actually, two methods that can be used to minimize an univariate function: brent and golden , but golden is included only for academic purposes and should rarely be used. These can be respectively selected through the method parameter in minimize_scalar . The brent method uses Brents algorithm for locating a minimum. Optimally, a bracket (the bracket parameter) should be given which contains the minimum desired. A bracket is a triple such that and . If this is not given, then alternatively two starting points can be chosen and a bracket will be found from these points using a simple marching algorithm. If these two starting points are not provided, 0 and 1 will be used (this may not be the right choice for your function and result in an unexpected minimum being returned). Here is an example: Bounded minimization ( )  Very often, there are constraints that can be placed on the solution space before minimization occurs. The bounded method in minimize_scalar is an example of a constrained minimization procedure that provides a rudimentary interval constraint for scalar functions. The interval constraint allows the minimization to occur only between two fixed endpoints, specified using the mandatory bounds parameter. For example, to find the minimum of near , minimize_scalar can be called using the interval as a constraint. The result is : \"\\n',\n",
       " '\"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  B-splines  A B-spline is an approximation of a continuous function over a finite- domain in terms of B-spline coefficients and knot points. If the knot- points are equally spaced with spacing , then the B-spline approximation to a 1-D function is the finite-basis expansion. \\\\[y\\\\left(x\\\\right)\\\\approx\\\\sum_{j}c_{j}\\\\beta^{o}\\\\left(\\\\frac{x}{\\\\Delta x}-j\\\\right).\\\\] In two dimensions with knot-spacing and , the function representation is \\\\[z\\\\left(x,y\\\\right)\\\\approx\\\\sum_{j}\\\\sum_{k}c_{jk}\\\\beta^{o}\\\\left(\\\\frac{x}{\\\\Delta x}-j\\\\right)\\\\beta^{o}\\\\left(\\\\frac{y}{\\\\Delta y}-k\\\\right).\\\\] In these expressions, is the space-limited B-spline basis function of order . The requirement of equally-spaced knot-points and equally-spaced data points, allows the development of fast (inverse-filtering) algorithms for determining the coefficients, , from sample-values, . Unlike the general spline interpolation algorithms, these algorithms can quickly find the spline coefficients for large images. The advantage of representing a set of samples via B-spline basis functions is that continuous-domain operators (derivatives, re- sampling, integral, etc.), which assume that the data samples are drawn from an underlying continuous function, can be computed with relative ease from the spline coefficients. For example, the second derivative of a spline is \\\\[y{}^{\\\\prime\\\\prime}\\\\left(x\\\\right)\\\\frac{1}{\\\\Delta x^{2}}\\\\sum_{j}c_{j}\\\\beta^{o\\\\prime\\\\prime}\\\\left(\\\\frac{x}{\\\\Delta x}-j\\\\right).\\\\] Using the property of B-splines that \\\\[\\\\frac{d^{2}\\\\beta^{o}\\\\left(w\\\\right)}{dw^{2}}\\\\beta^{o-2}\\\\left(w+1\\\\right)-2\\\\beta^{o-2}\\\\left(w\\\\right)+\\\\beta^{o-2}\\\\left(w-1\\\\right),\\\\] it can be seen that \\\\[y^{\\\\prime\\\\prime}\\\\left(x\\\\right)\\\\frac{1}{\\\\Delta x^{2}}\\\\sum_{j}c_{j}\\\\left[\\\\beta^{o-2}\\\\left(\\\\frac{x}{\\\\Delta x}-j+1\\\\right)-2\\\\beta^{o-2}\\\\left(\\\\frac{x}{\\\\Delta x}-j\\\\right)+\\\\beta^{o-2}\\\\left(\\\\frac{x}{\\\\Delta x}-j-1\\\\right)\\\\right].\\\\] If , then at the sample points: \\\\begin{eqnarray*} \\\\Delta x^{2}\\\\left.y^{\\\\prime}\\\\left(x\\\\right)\\\\right|_{xn\\\\Delta x} &  & \\\\sum_{j}c_{j}\\\\delta_{n-j+1}-2c_{j}\\\\delta_{n-j}+c_{j}\\\\delta_{n-j-1},\\\\\\\\ &  & c_{n+1}-2c_{n}+c_{n-1}.\\\\end{eqnarray*} Thus, the second-derivative signal can be easily calculated from the spline fit. If desired, smoothing splines can be found to make the second derivative less sensitive to random errors. The savvy reader will have already noticed that the data samples are related to the knot coefficients via a convolution operator, so that simple convolution with the sampled B-spline function recovers the original data from the spline coefficients. The output of convolutions can change depending on how the boundaries are handled (this becomes increasingly more important as the number of dimensions in the dataset increases). The algorithms relating to B-splines in the signal-processing subpackage assume mirror-symmetric boundary conditions. Thus, spline coefficients are computed based on that assumption, and data-samples can be recovered exactly from the spline coefficients by assuming them to be mirror-symmetric also. Currently the package provides functions for determining second- and third- order cubic spline coefficients from equally-spaced samples in one and two dimensions ( qspline1d , qspline2d , cspline1d , cspline2d ). The package also supplies a function ( bspline ) for evaluating the B-spline basis function, for arbitrary order and For large , the B-spline basis function can be approximated well by a zero-mean Gaussian function with standard-deviation equal to : \\\\[\\\\beta^{o}\\\\left(x\\\\right)\\\\approx\\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma_{o}^{2}}}\\\\exp\\\\left(-\\\\frac{x^{2}}{2\\\\sigma_{o}}\\\\right).\\\\] A function to compute this Gaussian for arbitrary and is also available ( gauss_spline ). The following code and figure use spline-filtering to compute an edge-image (the second derivative of a smoothed spline) of a raccoons face, which is an array returned by the command scipy.misc.face . The command sepfir2d was used to apply a separable 2-D FIR filter with mirror-symmetric boundary conditions to the spline coefficients. This function is ideally-suited for reconstructing samples from spline coefficients and is faster than convolve2d , which convolves arbitrary 2-D filters and allows for choosing mirror-symmetric boundary conditions. Alternatively, we could have done: \"\\n',\n",
       " '\"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  Detrend  SciPy provides the function detrend to remove a constant or linear trend in a data series in order to see effect of higher order. The example below removes the constant and linear trend of a second-order polynomial time series and plots the remaining signal components. References Some further reading and related software: 1 N.R. Lomb Least-squares frequency analysis of unequally spaced data, Astrophysics and Space Science, vol 39, pp. 447-462, 1976 2 J.D. Scargle Studies in astronomical time series analysis. II - Statistical aspects of spectral analysis of unevenly spaced data, The Astrophysical Journal, vol 263, pp. 835-853, 1982 3 R.H.D. Townsend, Fast calculation of the Lomb-Scargle periodogram using graphics processing units., The Astrophysical Journal Supplement Series, vol 191, pp. 247-253, 2010 \"\\n',\n",
       " '\"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  Filtering  Filtering is a generic name for any system that modifies an input signal in some way. In SciPy, a signal can be thought of as a NumPy array. There are different kinds of filters for different kinds of operations. There are two broad kinds of filtering operations: linear and non-linear. Linear filters can always be reduced to multiplication of the flattened NumPy array by an appropriate matrix resulting in another flattened NumPy array. Of course, this is not usually the best way to compute the filter, as the matrices and vectors involved may be huge. For example, filtering a image with this method would require multiplication of a matrix with a vector. Just trying to store the matrix using a standard NumPy array would require elements. At 4 bytes per element this would require of memory. In most applications, most of the elements of this matrix are zero and a different method for computing the output of the filter is employed. Convolution/Correlation  Many linear filters also have the property of shift-invariance. This means that the filtering operation is the same at different locations in the signal and it implies that the filtering matrix can be constructed from knowledge of one row (or column) of the matrix alone. In this case, the matrix multiplication can be accomplished using Fourier transforms. Let define a 1-D signal indexed by the integer Full convolution of two 1-D signals can be expressed as \\\\[y\\\\left[n\\\\right]\\\\sum_{k-\\\\infty}^{\\\\infty}x\\\\left[k\\\\right]h\\\\left[n-k\\\\right].\\\\] This equation can only be implemented directly if we limit the sequences to finite-support sequences that can be stored in a computer, choose to be the starting point of both sequences, let be that value for which for all and be that value for which for all , then the discrete convolution expression is \\\\[y\\\\left[n\\\\right]\\\\sum_{k\\\\max\\\\left(n-M,0\\\\right)}^{\\\\min\\\\left(n,K\\\\right)}x\\\\left[k\\\\right]h\\\\left[n-k\\\\right].\\\\] For convenience, assume Then, more explicitly, the output of this operation is \\\\begin{eqnarray*} y\\\\left[0\\\\right] &  & x\\\\left[0\\\\right]h\\\\left[0\\\\right]\\\\\\\\ y\\\\left[1\\\\right] &  & x\\\\left[0\\\\right]h\\\\left[1\\\\right]+x\\\\left[1\\\\right]h\\\\left[0\\\\right]\\\\\\\\ y\\\\left[2\\\\right] &  & x\\\\left[0\\\\right]h\\\\left[2\\\\right]+x\\\\left[1\\\\right]h\\\\left[1\\\\right]+x\\\\left[2\\\\right]h\\\\left[0\\\\right]\\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots\\\\\\\\ y\\\\left[M\\\\right] &  & x\\\\left[0\\\\right]h\\\\left[M\\\\right]+x\\\\left[1\\\\right]h\\\\left[M-1\\\\right]+\\\\cdots+x\\\\left[M\\\\right]h\\\\left[0\\\\right]\\\\\\\\ y\\\\left[M+1\\\\right] &  & x\\\\left[1\\\\right]h\\\\left[M\\\\right]+x\\\\left[2\\\\right]h\\\\left[M-1\\\\right]+\\\\cdots+x\\\\left[M+1\\\\right]h\\\\left[0\\\\right]\\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots\\\\\\\\ y\\\\left[K\\\\right] &  & x\\\\left[K-M\\\\right]h\\\\left[M\\\\right]+\\\\cdots+x\\\\left[K\\\\right]h\\\\left[0\\\\right]\\\\\\\\ y\\\\left[K+1\\\\right] &  & x\\\\left[K+1-M\\\\right]h\\\\left[M\\\\right]+\\\\cdots+x\\\\left[K\\\\right]h\\\\left[1\\\\right]\\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots\\\\\\\\ y\\\\left[K+M-1\\\\right] &  & x\\\\left[K-1\\\\right]h\\\\left[M\\\\right]+x\\\\left[K\\\\right]h\\\\left[M-1\\\\right]\\\\\\\\ y\\\\left[K+M\\\\right] &  & x\\\\left[K\\\\right]h\\\\left[M\\\\right].\\\\end{eqnarray*} Thus, the full discrete convolution of two finite sequences of lengths and , respectively, results in a finite sequence of length 1-D convolution is implemented in SciPy with the function convolve . This function takes as inputs the signals , and two optional flags mode and method, and returns the signal The first optional flag, mode, allows for the specification of which part of the output signal to return. The default value of full returns the entire signal. If the flag has a value of same, then only the middle values are returned, starting at , so that the output has the same length as the first input. If the flag has a value of valid, then only the middle output values are returned, where depends on all of the values of the smallest input from to In other words, only the values to inclusive are returned. The second optional flag, method, determines how the convolution is computed, either through the Fourier transform approach with fftconvolve or through the direct method. By default, it selects the expected faster method. The Fourier transform method has order , while the direct method has order . Depending on the big O constant and the value of , one of these two methods may be faster. The default value, auto, performs a rough calculation and chooses the expected faster method, while the values direct and fft force computation with the other two methods. The code below shows a simple example for convolution of 2 sequences: This same function convolve can actually take N-D arrays as inputs and will return the N-D convolution of the two arrays, as is shown in the code example below. The same input flags are available for that case as well. Correlation is very similar to convolution except that the minus sign becomes a plus sign. Thus, \\\\[w\\\\left[n\\\\right]\\\\sum_{k-\\\\infty}^{\\\\infty}y\\\\left[k\\\\right]x\\\\left[n+k\\\\right],\\\\] is the (cross) correlation of the signals and For finite-length signals with outside of the range and outside of the range the summation can simplify to \\\\[w\\\\left[n\\\\right]\\\\sum_{k\\\\max\\\\left(0,-n\\\\right)}^{\\\\min\\\\left(K,M-n\\\\right)}y\\\\left[k\\\\right]x\\\\left[n+k\\\\right].\\\\] Assuming again that , this is \\\\begin{eqnarray*} w\\\\left[-K\\\\right] &  & y\\\\left[K\\\\right]x\\\\left[0\\\\right]\\\\\\\\ w\\\\left[-K+1\\\\right] &  & y\\\\left[K-1\\\\right]x\\\\left[0\\\\right]+y\\\\left[K\\\\right]x\\\\left[1\\\\right]\\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots\\\\\\\\ w\\\\left[M-K\\\\right] &  & y\\\\left[K-M\\\\right]x\\\\left[0\\\\right]+y\\\\left[K-M+1\\\\right]x\\\\left[1\\\\right]+\\\\cdots+y\\\\left[K\\\\right]x\\\\left[M\\\\right]\\\\\\\\ w\\\\left[M-K+1\\\\right] &  & y\\\\left[K-M-1\\\\right]x\\\\left[0\\\\right]+\\\\cdots+y\\\\left[K-1\\\\right]x\\\\left[M\\\\right]\\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots\\\\\\\\ w\\\\left[-1\\\\right] &  & y\\\\left[1\\\\right]x\\\\left[0\\\\right]+y\\\\left[2\\\\right]x\\\\left[1\\\\right]+\\\\cdots+y\\\\left[M+1\\\\right]x\\\\left[M\\\\right]\\\\\\\\ w\\\\left[0\\\\right] &  & y\\\\left[0\\\\right]x\\\\left[0\\\\right]+y\\\\left[1\\\\right]x\\\\left[1\\\\right]+\\\\cdots+y\\\\left[M\\\\right]x\\\\left[M\\\\right]\\\\\\\\ w\\\\left[1\\\\right] &  & y\\\\left[0\\\\right]x\\\\left[1\\\\right]+y\\\\left[1\\\\right]x\\\\left[2\\\\right]+\\\\cdots+y\\\\left[M-1\\\\right]x\\\\left[M\\\\right]\\\\\\\\ w\\\\left[2\\\\right] &  & y\\\\left[0\\\\right]x\\\\left[2\\\\right]+y\\\\left[1\\\\right]x\\\\left[3\\\\right]+\\\\cdots+y\\\\left[M-2\\\\right]x\\\\left[M\\\\right]\\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots\\\\\\\\ w\\\\left[M-1\\\\right] &  & y\\\\left[0\\\\right]x\\\\left[M-1\\\\right]+y\\\\left[1\\\\right]x\\\\left[M\\\\right]\\\\\\\\ w\\\\left[M\\\\right] &  & y\\\\left[0\\\\right]x\\\\left[M\\\\right].\\\\end{eqnarray*} The SciPy function correlate implements this operation. Equivalent flags are available for this operation to return the full length sequence (full) or a sequence with the same size as the largest sequence starting at (same) or a sequence where the values depend on all the values of the smallest sequence (valid). This final option returns the values to inclusive. The function correlate can also take arbitrary N-D arrays as input and return the N-D convolution of the two arrays on output. When correlate and/or convolve can be used to construct arbitrary image filters to perform actions such as blurring, enhancing, and edge-detection for an image. Calculating the convolution in the time domain as above is mainly used for filtering when one of the signals is much smaller than the other ( ), otherwise linear filtering is more efficiently calculated in the frequency domain provided by the function fftconvolve . By default, convolve estimates the fastest method using choose_conv_method . If the filter function can be factored according to \\\\[h[n, m]  h_1[n] h_2[m],\\\\] convolution can be calculated by means of the function sepfir2d . As an example, we consider a Gaussian filter gaussian \\\\[h[n, m] \\\\propto e^{-x^2-y^2}  e^{-x^2} e^{-y^2},\\\\] which is often used for blurring. Difference-equation filtering  A general class of linear 1-D filters (that includes convolution filters) are filters described by the difference equation \\\\[\\\\sum_{k0}^{N}a_{k}y\\\\left[n-k\\\\right]\\\\sum_{k0}^{M}b_{k}x\\\\left[n-k\\\\right],\\\\] where is the input sequence and is the output sequence. If we assume initial rest so that for , then this kind of filter can be implemented using convolution. However, the convolution filter sequence could be infinite if for In addition, this general class of linear filter allows initial conditions to be placed on for resulting in a filter that cannot be expressed using convolution. The difference equation filter can be thought of as finding recursively in terms of its previous values \\\\[a_{0}y\\\\left[n\\\\right]-a_{1}y\\\\left[n-1\\\\right]-\\\\cdots-a_{N}y\\\\left[n-N\\\\right]+\\\\cdots+b_{0}x\\\\left[n\\\\right]+\\\\cdots+b_{M}x\\\\left[n-M\\\\right].\\\\] Often, is chosen for normalization. The implementation in SciPy of this general difference equation filter is a little more complicated than would be implied by the previous equation. It is implemented so that only one signal needs to be delayed. The actual implementation equations are (assuming ): \\\\begin{eqnarray*} y\\\\left[n\\\\right] &  & b_{0}x\\\\left[n\\\\right]+z_{0}\\\\left[n-1\\\\right]\\\\\\\\ z_{0}\\\\left[n\\\\right] &  & b_{1}x\\\\left[n\\\\right]+z_{1}\\\\left[n-1\\\\right]-a_{1}y\\\\left[n\\\\right]\\\\\\\\ z_{1}\\\\left[n\\\\right] &  & b_{2}x\\\\left[n\\\\right]+z_{2}\\\\left[n-1\\\\right]-a_{2}y\\\\left[n\\\\right]\\\\\\\\ \\\\vdots & \\\\vdots & \\\\vdots\\\\\\\\ z_{K-2}\\\\left[n\\\\right] &  & b_{K-1}x\\\\left[n\\\\right]+z_{K-1}\\\\left[n-1\\\\right]-a_{K-1}y\\\\left[n\\\\right]\\\\\\\\ z_{K-1}\\\\left[n\\\\right] &  & b_{K}x\\\\left[n\\\\right]-a_{K}y\\\\left[n\\\\right],\\\\end{eqnarray*} where Note that if and if In this way, the output at time depends only on the input at time and the value of at the previous time. This can always be calculated as long as the values are computed and stored at each time step. The difference-equation filter is called using the command lfilter in SciPy. This command takes as inputs the vector the vector, a signal and returns the vector (the same length as ) computed using the equation given above. If is N-D, then the filter is computed along the axis provided. If desired, initial conditions providing the values of to can be provided or else it will be assumed that they are all zero. If initial conditions are provided, then the final conditions on the intermediate variables are also returned. These could be used, for example, to restart the calculation in the same state. Sometimes, it is more convenient to express the initial conditions in terms of the signals and In other words, perhaps you have the values of to and the values of to and would like to determine what values of should be delivered as initial conditions to the difference-equation filter. It is not difficult to show that, for \\\\[z_{m}\\\\left[n\\\\right]\\\\sum_{p0}^{K-m-1}\\\\left(b_{m+p+1}x\\\\left[n-p\\\\right]-a_{m+p+1}y\\\\left[n-p\\\\right]\\\\right).\\\\] Using this formula, we can find the initial-condition vector to given initial conditions on (and ). The command lfiltic performs this function. As an example, consider the following system: \\\\[y[n]  \\\\frac{1}{2} x[n] + \\\\frac{1}{4} x[n-1] + \\\\frac{1}{3} y[n-1]\\\\] The code calculates the signal for a given signal ; first for initial conditions (default case), then for by means of lfiltic . Note that the output signal has the same length as the length as the input signal . Analysis of Linear Systems  Linear system described a linear-difference equation can be fully described by the coefficient vectors and as was done above; an alternative representation is to provide a factor , zeros and poles , respectively, to describe the system by means of its transfer function , according to \\\\[H(z)  k \\\\frac{ (z-z_1)(z-z_2)...(z-z_{N_z})}{ (z-p_1)(z-p_2)...(z-p_{N_p})}.\\\\] This alternative representation can be obtained with the scipy function tf2zpk ; the inverse is provided by zpk2tf . For the above example we have i.e., the system has a zero at and a pole at . The scipy function freqz allows calculation of the frequency response of a system described by the coefficients and . See the help of the freqz function for a comprehensive example. Filter Design  Time-discrete filters can be classified into finite response (FIR) filters and infinite response (IIR) filters. FIR filters can provide a linear phase response, whereas IIR filters cannot. SciPy provides functions for designing both types of filters. FIR Filter  The function firwin designs filters according to the window method. Depending on the provided arguments, the function returns different filter types (e.g., low-pass, band-pass). The example below designs a low-pass and a band-stop filter, respectively. Note that firwin uses, per default, a normalized frequency defined such that the value corresponds to the Nyquist frequency, whereas the function freqz is defined such that the value corresponds to the Nyquist frequency. The function firwin2 allows design of almost arbitrary frequency responses by specifying an array of corner frequencies and corresponding gains, respectively. The example below designs a filter with such an arbitrary amplitude response. Note the linear scaling of the y-axis and the different definition of the Nyquist frequency in firwin2 and freqz (as explained above). IIR Filter  SciPy provides two functions to directly design IIR iirdesign and iirfilter , where the filter type (e.g., elliptic) is passed as an argument and several more filter design functions for specific filter types, e.g., ellip . The example below designs an elliptic low-pass filter with defined pass-band and stop-band ripple, respectively. Note the much lower filter order (order 4) compared with the FIR filters from the examples above in order to reach the same stop-band attenuation of dB. Filter Coefficients  Filter coefficients can be stored in several different formats: ba or tf  transfer function coefficients zpk  zeros, poles, and overall gain ss  state-space system representation sos  transfer function coefficients of second-order sections Functions, such as tf2zpk and zpk2ss , can convert between them. Transfer function representation  The or format is a 2-tuple representing a transfer function, where b is a length array of coefficients of the M -order numerator polynomial, and a is a length array of coefficients of the N -order denominator, as positive, descending powers of the transfer function variable. So the tuple of and can represent an analog filter of the form: \\\\[H(s)  \\\\frac {b_0 s^M + b_1 s^{(M-1)} + \\\\cdots + b_M} {a_0 s^N + a_1 s^{(N-1)} + \\\\cdots + a_N}  \\\\frac {\\\\sum_{i0}^M b_i s^{(M-i)}} {\\\\sum_{i0}^N a_i s^{(N-i)}}\\\\] or a discrete-time filter of the form: \\\\[H(z)  \\\\frac {b_0 z^M + b_1 z^{(M-1)} + \\\\cdots + b_M} {a_0 z^N + a_1 z^{(N-1)} + \\\\cdots + a_N}  \\\\frac {\\\\sum_{i0}^M b_i z^{(M-i)}} {\\\\sum_{i0}^N a_i z^{(N-i)}}.\\\\] This positive powers form is found more commonly in controls engineering. If M and N are equal (which is true for all filters generated by the bilinear transform), then this happens to be equivalent to the negative powers discrete-time form preferred in DSP: \\\\[H(z)  \\\\frac {b_0 + b_1 z^{-1} + \\\\cdots + b_M z^{-M}} {a_0 + a_1 z^{-1} + \\\\cdots + a_N z^{-N}}  \\\\frac {\\\\sum_{i0}^M b_i z^{-i}} {\\\\sum_{i0}^N a_i z^{-i}}.\\\\] Although this is true for common filters, remember that this is not true in the general case. If M and N are not equal, the discrete-time transfer function coefficients must first be converted to the positive powers form before finding the poles and zeros. This representation suffers from numerical error at higher orders, so other formats are preferred when possible. Zeros and poles representation  The format is a 3-tuple , where z is an M -length array of the complex zeros of the transfer function , p is an N -length array of the complex poles of the transfer function , and k is a scalar gain. These represent the digital transfer function: \\\\[H(z)  k \\\\cdot \\\\frac {(z - z_0) (z - z_1) \\\\cdots (z - z_{(M-1)})} {(z - p_0) (z - p_1) \\\\cdots (z - p_{(N-1)})}  k \\\\frac {\\\\prod_{i0}^{M-1} (z - z_i)} {\\\\prod_{i0}^{N-1} (z - p_i)}\\\\] or the analog transfer function: \\\\[H(s)  k \\\\cdot \\\\frac {(s - z_0) (s - z_1) \\\\cdots (s - z_{(M-1)})} {(s - p_0) (s - p_1) \\\\cdots (s - p_{(N-1)})}  k \\\\frac {\\\\prod_{i0}^{M-1} (s - z_i)} {\\\\prod_{i0}^{N-1} (s - p_i)}.\\\\] Although the sets of roots are stored as ordered NumPy arrays, their ordering does not matter: is the same filter as . State-space system representation  The format is a 4-tuple of arrays representing the state-space of an N -order digital/discrete-time system of the form: \\\\[\\\\begin{split}\\\\mathbf{x}[k+1]  A \\\\mathbf{x}[k] + B \\\\mathbf{u}[k]\\\\\\\\ \\\\mathbf{y}[k]  C \\\\mathbf{x}[k] + D \\\\mathbf{u}[k]\\\\end{split}\\\\] or a continuous/analog system of the form: \\\\[\\\\begin{split}\\\\dot{\\\\mathbf{x}}(t)  A \\\\mathbf{x}(t) + B \\\\mathbf{u}(t)\\\\\\\\ \\\\mathbf{y}(t)  C \\\\mathbf{x}(t) + D \\\\mathbf{u}(t),\\\\end{split}\\\\] with P inputs, Q outputs and N state variables, where: x is the state vector y is the output vector of length Q u is the input vector of length P A is the state matrix, with shape B is the input matrix with shape C is the output matrix with shape D is the feedthrough or feedforward matrix with shape . (In cases where the system does not have a direct feedthrough, all values in D are zero.) State-space is the most general representation and the only one that allows for multiple-input, multiple-output (MIMO) systems. There are multiple state-space representations for a given transfer function. Specifically, the controllable canonical form and observable canonical form have the same coefficients as the representation, and, therefore, suffer from the same numerical errors. Second-order sections representation  The format is a single 2-D array of shape , representing a sequence of second-order transfer functions which, when cascaded in series, realize a higher-order filter with minimal numerical error. Each row corresponds to a second-order representation, with the first three columns providing the numerator coefficients and the last three providing the denominator coefficients: \\\\[[b_0, b_1, b_2, a_0, a_1, a_2]\\\\] The coefficients are typically normalized, such that is always 1. The section order is usually not important with floating-point computation; the filter output will be the same, regardless of the order. Filter transformations  The IIR filter design functions first generate a prototype analog low-pass filter with a normalized cutoff frequency of 1 rad/sec. This is then transformed into other frequencies and band types using the following substitutions: Type Transformation lp2lp lp2hp lp2bp lp2bs Here, is the new cutoff or center frequency, and is the bandwidth. These preserve symmetry on a logarithmic frequency axis. To convert the transformed analog filter into a digital filter, the bilinear transform is used, which makes the following substitution: \\\\[s \\\\rightarrow \\\\frac{2}{T} \\\\frac{z - 1}{z + 1},\\\\] where T is the sampling time (the inverse of the sampling frequency). Other filters  The signal processing package provides many more filters as well. Median Filter  A median filter is commonly applied when noise is markedly non-Gaussian or when it is desired to preserve edges. The median filter works by sorting all of the array pixel values in a rectangular region surrounding the point of interest. The sample median of this list of neighborhood pixel values is used as the value for the output array. The sample median is the middle-array value in a sorted list of neighborhood values. If there are an even number of elements in the neighborhood, then the average of the middle two values is used as the median. A general purpose median filter that works on N-D arrays is medfilt . A specialized version that works only for 2-D arrays is available as medfilt2d . Order Filter  A median filter is a specific example of a more general class of filters called order filters. To compute the output at a particular pixel, all order filters use the array values in a region surrounding that pixel. These array values are sorted and then one of them is selected as the output value. For the median filter, the sample median of the list of array values is used as the output. A general-order filter allows the user to select which of the sorted values will be used as the output. So, for example, one could choose to pick the maximum in the list or the minimum. The order filter takes an additional argument besides the input array and the region mask that specifies which of the elements in the sorted list of neighbor array values should be used as the output. The command to perform an order filter is order_filter . Wiener filter  The Wiener filter is a simple deblurring filter for denoising images. This is not the Wiener filter commonly described in image-reconstruction problems but, instead, it is a simple, local-mean filter. Let be the input signal, then the output is \\\\[\\\\begin{split}y\\\\left\\\\{ \\\\begin{array}{cc} \\\\frac{\\\\sigma^{2}}{\\\\sigma_{x}^{2}}m_{x}+\\\\left(1-\\\\frac{\\\\sigma^{2}}{\\\\sigma_{x}^{2}}\\\\right)x & \\\\sigma_{x}^{2}\\\\geq\\\\sigma^{2},\\\\\\\\ m_{x} & \\\\sigma_{x}^{2}<\\\\sigma^{2},\\\\end{array}\\\\right.\\\\end{split}\\\\] where is the local estimate of the mean and is the local estimate of the variance. The window for these estimates is an optional input parameter (default is ). The parameter is a threshold noise parameter. If is not given, then it is estimated as the average of the local variances. Hilbert filter  The Hilbert transform constructs the complex-valued analytic signal from a real signal. For example, if , then would return (except near the edges) In the frequency domain, the hilbert transform performs \\\\[YX\\\\cdot H,\\\\] where is for positive frequencies, for negative frequencies, and for zero-frequencies. Analog Filter Design  The functions iirdesign , iirfilter , and the filter design functions for specific filter types (e.g., ellip ) all have a flag analog , which allows the design of analog filters as well. The example below designs an analog (IIR) filter, obtains via tf2zpk the poles and zeros and plots them in the complex s-plane. The zeros at and can be clearly seen in the amplitude response. \"\\n',\n",
       " '\"scipy_signal_processing_scipy_signal Signal Processing (scipy.signal) signal.html  Spectral Analysis  Periodogram Measurements  The scipy function periodogram provides a method to estimate the spectral density using the periodogram method. The example below calculates the periodogram of a sine signal in white Gaussian noise. Spectral Analysis using Welchs Method  An improved method, especially with respect to noise immunity, is Welchs method, which is implemented by the scipy function welch . The example below estimates the spectrum using Welchs method and uses the same parameters as the example above. Note the much smoother noise floor of the spectrogram. Lomb-Scargle Periodograms ( lombscargle )  Least-squares spectral analysis (LSSA) 1 2 is a method of estimating a frequency spectrum, based on a least-squares fit of sinusoids to data samples, similar to Fourier analysis. Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in long-gapped records; LSSA mitigates such problems. The Lomb-Scargle method performs spectral analysis on unevenly-sampled data and is known to be a powerful way to find, and test the significance of, weak periodic signals. For a time series comprising measurements sampled at times , where , assumed to have been scaled and shifted, such that its mean is zero and its variance is unity, the normalized Lomb-Scargle periodogram at frequency is \\\\[P_{n}(f) \\\\frac{1}{2}\\\\left\\\\{\\\\frac{\\\\left[\\\\sum_{j}^{N_{t}}X_{j}\\\\cos\\\\omega(t_{j}-\\\\tau)\\\\right]^{2}}{\\\\sum_{j}^{N_{t}}\\\\cos^{2}\\\\omega(t_{j}-\\\\tau)}+\\\\frac{\\\\left[\\\\sum_{j}^{N_{t}}X_{j}\\\\sin\\\\omega(t_{j}-\\\\tau)\\\\right]^{2}}{\\\\sum_{j}^{N_{t}}\\\\sin^{2}\\\\omega(t_{j}-\\\\tau)}\\\\right\\\\}.\\\\] Here, is the angular frequency. The frequency-dependent time offset is given by \\\\[\\\\tan 2\\\\omega\\\\tau  \\\\frac{\\\\sum_{j}^{N_{t}}\\\\sin 2\\\\omega t_{j}}{\\\\sum_{j}^{N_{t}}\\\\cos 2\\\\omega t_{j}}.\\\\] The lombscargle function calculates the periodogram using a slightly modified algorithm due to Townsend 3 , which allows the periodogram to be calculated using only a single pass through the input arrays for each frequency. The equation is refactored as: \\\\[P_{n}(f)  \\\\frac{1}{2}\\\\left[\\\\frac{(c_{\\\\tau}XC + s_{\\\\tau}XS)^{2}}{c_{\\\\tau}^{2}CC + 2c_{\\\\tau}s_{\\\\tau}CS + s_{\\\\tau}^{2}SS} + \\\\frac{(c_{\\\\tau}XS - s_{\\\\tau}XC)^{2}}{c_{\\\\tau}^{2}SS - 2c_{\\\\tau}s_{\\\\tau}CS + s_{\\\\tau}^{2}CC}\\\\right]\\\\] and \\\\[\\\\tan 2\\\\omega\\\\tau  \\\\frac{2CS}{CC-SS}.\\\\] Here, \\\\[c_{\\\\tau}  \\\\cos\\\\omega\\\\tau,\\\\qquad s_{\\\\tau}  \\\\sin\\\\omega\\\\tau,\\\\] while the sums are \\\\[\\\\begin{split}XC & \\\\sum_{j}^{N_{t}} X_{j}\\\\cos\\\\omega t_{j}\\\\\\\\ XS & \\\\sum_{j}^{N_{t}} X_{j}\\\\sin\\\\omega t_{j}\\\\\\\\ CC & \\\\sum_{j}^{N_{t}} \\\\cos^{2}\\\\omega t_{j}\\\\\\\\ SS & \\\\sum_{j}^{N_{t}} \\\\sin^{2}\\\\omega t_{j}\\\\\\\\ CS & \\\\sum_{j}^{N_{t}} \\\\cos\\\\omega t_{j}\\\\sin\\\\omega t_{j}.\\\\end{split}\\\\] This requires trigonometric function evaluations giving a factor of speed increase over the straightforward implementation. \"\\n',\n",
       " '\"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Basic functionality  ARPACK can solve either standard eigenvalue problems of the form \\\\[A \\\\mathbf{x}  \\\\lambda \\\\mathbf{x}\\\\] or general eigenvalue problems of the form \\\\[A \\\\mathbf{x}  \\\\lambda M \\\\mathbf{x}.\\\\] The power of ARPACK is that it can compute only a specified subset of eigenvalue/eigenvector pairs. This is accomplished through the keyword . The following values of are available: : Eigenvalues with largest magnitude ( , ), that is, largest eigenvalues in the euclidean norm of complex numbers. : Eigenvalues with smallest magnitude ( , ), that is, smallest eigenvalues in the euclidean norm of complex numbers. : Eigenvalues with largest real part ( ). : Eigenvalues with smallest real part ( ). : Eigenvalues with largest imaginary part ( ). : Eigenvalues with smallest imaginary part ( ). : Eigenvalues with largest algebraic value ( ), that is, largest eigenvalues inclusive of any negative sign. : Eigenvalues with smallest algebraic value ( ), that is, smallest eigenvalues inclusive of any negative sign. : Eigenvalues from both ends of the spectrum ( ). Note that ARPACK is generally better at finding extremal eigenvalues, that is, eigenvalues with large magnitudes. In particular, using may lead to slow execution time and/or anomalous results. A better approach is to use shift-invert mode . \"\\n',\n",
       " '\"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Examples  Imagine youd like to find the smallest and largest eigenvalues and the corresponding eigenvectors for a large matrix. ARPACK can handle many forms of input: dense matrices ,such as numpy.ndarray instances, sparse matrices, such as scipy.sparse.csr_matrix , or a general linear operator derived from scipy.sparse.linalg.LinearOperator . For this example, for simplicity, well construct a symmetric, positive-definite matrix. We now have a symmetric matrix , with which to test the routines. First, compute a standard eigenvalue decomposition using : As the dimension of grows, this routine becomes very slow. Especially, if only a few eigenvectors and eigenvalues are needed, can be a better option. First lets compute the largest eigenvalues ( ) of and compare them to the known results: The results are as expected. ARPACK recovers the desired eigenvalues and they match the previously known results. Furthermore, the eigenvectors are orthogonal, as wed expect. Now, lets attempt to solve for the eigenvalues with smallest magnitude: Oops. We see that, as mentioned above, is not quite as adept at finding small eigenvalues. There are a few ways this problem can be addressed. We could increase the tolerance ( ) to lead to faster convergence: This works, but we lose the precision in the results. Another option is to increase the maximum number of iterations ( ) from 1000 to 5000: We get the results wed hoped for, but the computation time is much longer. Fortunately, contains a mode that allows a quick determination of non-external eigenvalues: shift-invert mode . As mentioned above, this mode involves transforming the eigenvalue problem to an equivalent problem with different eigenvalues. In this case, we hope to find eigenvalues near zero, so well choose . The transformed eigenvalues will then satisfy , so our small eigenvalues become large eigenvalues . We get the results we were hoping for, with much less computational time. Note that the transformation from takes place entirely in the background. The user need not worry about the details. The shift-invert mode provides more than just a fast way to obtain a few small eigenvalues. Say, you desire to find internal eigenvalues and eigenvectors, e.g., those nearest to . Simply set and ARPACK will take care of the rest: The eigenvalues come out in a different order, but theyre all there. Note that the shift-invert mode requires the internal solution of a matrix inverse. This is taken care of automatically by and eigs , but the operation can also be specified by the user. See the docstring of scipy.sparse.linalg.eigsh and scipy.sparse.linalg.eigs for details. \"\\n',\n",
       " '\"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Introduction  ARPACK 1 is a Fortran package which provides routines for quickly finding a few eigenvalues/eigenvectors of large sparse matrices. In order to find these solutions, it requires only left-multiplication by the matrix in question. This operation is performed through a reverse-communication interface. The result of this structure is that ARPACK is able to find eigenvalues and eigenvectors of any linear function mapping a vector to a vector. All of the functionality provided in ARPACK is contained within the two high-level interfaces scipy.sparse.linalg.eigs and scipy.sparse.linalg.eigsh . eigs provides interfaces for finding the eigenvalues/vectors of real or complex nonsymmetric square matrices, while eigsh provides interfaces for real-symmetric or complex-hermitian matrices. \"\\n',\n",
       " '\"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  References  1 http://www.caam.rice.edu/software/ARPACK/ \"\\n',\n",
       " '\"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Shift-invert mode  Shift-invert mode relies on the following observation. For the generalized eigenvalue problem \\\\[A \\\\mathbf{x}  \\\\lambda M \\\\mathbf{x},\\\\] it can be shown that \\\\[(A - \\\\sigma M)^{-1} M \\\\mathbf{x}  \\\\nu \\\\mathbf{x},\\\\] where \\\\[\\\\nu  \\\\frac{1}{\\\\lambda - \\\\sigma}.\\\\] \"\\n',\n",
       " '\"scipy_sparse_eigenvalue_problems_with_arpack Sparse eigenvalue problems with ARPACK arpack.html  Use of LinearOperator  We consider now the case where youd like to avoid creating a dense matrix and use scipy.sparse.linalg.LinearOperator instead. Our first linear operator applies element-wise multiplication between the input vector and a vector provided by the user to the operator itself. This operator mimics a diagonal matrix with the elements of along the main diagonal and it has the main benefit that the forward and adjoint operations are simple element-wise multiplications other than matrix-vector multiplications. For a diagonal matrix, we expect the eigenvalues to be equal to the elements along the main diagonal, in this case . The eigenvalues and eigenvectors obtained with are compared to those obtained by using when applied to the dense matrix: In this case, we have created a quick and easy operator. The external library PyLops provides similar capabilities in the Diagonal operator, as well as several other operators. Finally, we consider a linear operator that mimics the application of a first-derivative stencil. In this case, the operator is equivalent to a real nonsymmetric matrix. Once again, we compare the estimated eigenvalues and eigenvectors with those from a dense matrix that applies the same first derivative to an input signal: Note that the eigenvalues of this operator are all imaginary. Moreover, the keyword of scipy.sparse.linalg.eigs produces the eigenvalues with largest absolute imaginary part (both positive and negative). Again, a more advanced implementation of the first-derivative operator is available in the PyLops library under the name of FirstDerivative operator. \"\\n',\n",
       " '\"scipy_spatial_data_structures_and_algorithms_scipy_spatial Spatial data structures and algorithms (scipy.spatial) spatial.html  Convex hulls  A convex hull is the smallest convex object containing all points in a given point set. These can be computed via the Qhull wrappers in scipy.spatial as follows: The convex hull is represented as a set of N 1-D simplices, which in 2-D means line segments. The storage scheme is exactly the same as for the simplices in the Delaunay triangulation discussed above. We can illustrate the above result: The same can be achieved with scipy.spatial.convex_hull_plot_2d . \"\\n',\n",
       " '\"scipy_spatial_data_structures_and_algorithms_scipy_spatial Spatial data structures and algorithms (scipy.spatial) spatial.html  Delaunay triangulations  The Delaunay triangulation is a subdivision of a set of points into a non-overlapping set of triangles, such that no point is inside the circumcircle of any triangle. In practice, such triangulations tend to avoid triangles with small angles. Delaunay triangulation can be computed using scipy.spatial as follows: We can visualize it: And add some further decorations: The structure of the triangulation is encoded in the following way: the attribute contains the indices of the points in the array that make up the triangle. For instance: Moreover, neighboring triangles can also be found: What this tells us is that this triangle has triangle #0 as a neighbor, but no other neighbors. Moreover, it tells us that neighbor 0 is opposite the vertex 1 of the triangle: Indeed, from the figure, we see that this is the case. Qhull can also perform tessellations to simplices for higher-dimensional point sets (for instance, subdivision into tetrahedra in 3-D). Coplanar points  It is important to note that not all points necessarily appear as vertices of the triangulation, due to numerical precision issues in forming the triangulation. Consider the above with a duplicated point: Observe that point #4, which is a duplicate, does not occur as a vertex of the triangulation. That this happened is recorded: This means that point 4 resides near triangle 0 and vertex 3, but is not included in the triangulation. Note that such degeneracies can occur not only because of duplicated points, but also for more complicated geometrical reasons, even in point sets that at first sight seem well-behaved. However, Qhull has the QJ option, which instructs it to perturb the input data randomly until degeneracies are resolved: Two new triangles appeared. However, we see that they are degenerate and have zero area. \"\\n',\n",
       " '\"scipy_spatial_data_structures_and_algorithms_scipy_spatial Spatial data structures and algorithms (scipy.spatial) spatial.html  Voronoi diagrams  A Voronoi diagram is a subdivision of the space into the nearest neighborhoods of a given set of points. There are two ways to approach this object using scipy.spatial . First, one can use the KDTree to answer the question which of the points is closest to this one, and define the regions that way: So the point belongs to region . In color: This does not, however, give the Voronoi diagram as a geometrical object. The representation in terms of lines and points can be again obtained via the Qhull wrappers in scipy.spatial : The Voronoi vertices denote the set of points forming the polygonal edges of the Voronoi regions. In this case, there are 9 different regions: Negative value again indicates a point at infinity. Indeed, only one of the regions, , is bounded. Note here that due to similar numerical precision issues as in Delaunay triangulation above, there may be fewer Voronoi regions than input points. The ridges (lines in 2-D) separating the regions are described as a similar collection of simplices as the convex hull pieces: These numbers present the indices of the Voronoi vertices making up the line segments. is again a point at infinity  only 4 of the 12 lines are a bounded line segment, while others extend to infinity. The Voronoi ridges are perpendicular to the lines drawn between the input points. To which two points each ridge corresponds is also recorded: This information, taken together, is enough to construct the full diagram. We can plot it as follows. First, the points and the Voronoi vertices: Plotting the finite line segments goes as for the convex hull, but now we have to guard for the infinite edges: The ridges extending to infinity require a bit more care: This plot can also be created using scipy.spatial.voronoi_plot_2d . Voronoi diagrams can be used to create interesting generative art. Try playing with the settings of this function to create your own! \"\\n',\n",
       " '\"scipy_special_functions_scipy_special Special functions (scipy.special) special.html  Bessel functions of real order( jv , jn_zeros )  Bessel functions are a family of solutions to Bessels differential equation with real or complex order alpha: \\\\[x^2 \\\\frac{d^2 y}{dx^2} + x \\\\frac{dy}{dx} + (x^2 - \\\\alpha^2)y  0\\\\] Among other uses, these functions arise in wave propagation problems, such as the vibrational modes of a thin drum head. Here is an example of a circular drum head anchored at the edge: \"\\n',\n",
       " '\"scipy_special_functions_scipy_special Special functions (scipy.special) special.html  Cython Bindings for Special Functions ( scipy.special.cython_special )  SciPy also offers Cython bindings for scalar, typed versions of many of the functions in special. The following Cython code gives a simple example of how to use these functions: (See the Cython documentation for help with compiling Cython.) In the example the function works essentially like its ufunc counterpart gamma , though it takes C types as arguments instead of NumPy arrays. Note, in particular, that the function is overloaded to support real and complex arguments; the correct variant is selected at compile time. The function works slightly differently from sici ; for the ufunc we could write , whereas in the Cython version multiple return values are passed as pointers. It might help to think of this as analogous to calling a ufunc with an output array: . There are two potential advantages to using the Cython bindings: they avoid Python function overhead they do not require the Python Global Interpreter Lock (GIL) The following sections discuss how to use these advantages to potentially speed up your code, though, of course, one should always profile the code first to make sure putting in the extra effort will be worth it. Avoiding Python Function Overhead  For the ufuncs in special, Python function overhead is avoided by vectorizing, that is, by passing an array to the function. Typically, this approach works quite well, but sometimes it is more convenient to call a special function on scalar inputs inside a loop, for example, when implementing your own ufunc. In this case, the Python function overhead can become significant. Consider the following example: On one computer took about 131 microseconds to run and took about 18.2 microseconds to run. Obviously this example is contrived: one could just call and get results just as fast as in . The point is that if Python function overhead becomes significant in your code, then the Cython bindings might be useful. Releasing the GIL  One often needs to evaluate a special function at many points, and typically the evaluations are trivially parallelizable. Since the Cython bindings do not require the GIL, it is easy to run them in parallel using Cythons function. For example, suppose that we wanted to compute the fundamental solution to the Helmholtz equation: \\\\[\\\\Delta_x G(x, y) + k^2G(x, y)  \\\\delta(x - y),\\\\] where is the wavenumber and is the Dirac delta function. It is known that in two dimensions the unique (radiating) solution is \\\\[G(x, y)  \\\\frac{i}{4}H_0^{(1)}(k|x - y|),\\\\] where is the Hankel function of the first kind, i.e., the function hankel1 . The following example shows how we could compute this function in parallel: (For help with compiling parallel code in Cython see here .) If the above Cython code is in a file , then we can write an informal benchmark which compares the parallel and serial versions of the function: On one quad-core computer the serial method took 1.29 seconds and the parallel method took 0.29 seconds. \"\\n',\n",
       " '\"scipy_special_functions_scipy_special Special functions (scipy.special) special.html  Functions not in scipy.special  Some functions are not included in special because they are straightforward to implement with existing functions in NumPy and SciPy. To prevent reinventing the wheel, this section provides implementations of several such functions, which hopefully illustrate how to handle similar functions. In all examples NumPy is imported as and special is imported as . The binary entropy function : A rectangular step function on [0, 1]: Translating and scaling can be used to get an arbitrary step function. The ramp function : \"\\n',\n",
       " '\"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Analysing one sample  First, we create some random variables. We set a seed so that in each run we get identical results to look at. As an example we take a sample from the Student t distribution: Here, we set the required shape parameter of the t distribution, which in statistics corresponds to the degrees of freedom, to 10. Using size1000 means that our sample consists of 1000 independently drawn (pseudo) random numbers. Since we did not specify the keyword arguments loc and scale , those are set to their default values zero and one. Descriptive statistics  x is a numpy array, and we have direct access to all array methods, e.g., How do the sample properties compare to their theoretical counterparts? Note: stats.describe uses the unbiased estimator for the variance, while np.var is the biased estimator. For our sample the sample statistics differ a by a small amount from their theoretical counterparts. T-test and KS-test  We can use the t-test to test whether the mean of our sample differs in a statistically significant way from the theoretical expectation. The pvalue is 0.7, this means that with an alpha error of, for example, 10%, we cannot reject the hypothesis that the sample mean is equal to zero, the expectation of the standard t-distribution. As an exercise, we can calculate our ttest also directly without using the provided function, which should give us the same answer, and so it does: The Kolmogorov-Smirnov test can be used to test the hypothesis that the sample comes from the standard t-distribution Again, the p-value is high enough that we cannot reject the hypothesis that the random sample really is distributed according to the t-distribution. In real applications, we dont know what the underlying distribution is. If we perform the Kolmogorov-Smirnov test of our sample against the standard normal distribution, then we also cannot reject the hypothesis that our sample was generated by the normal distribution given that, in this example, the p-value is almost 40%. However, the standard normal distribution has a variance of 1, while our sample has a variance of 1.29. If we standardize our sample and test it against the normal distribution, then the p-value is again large enough that we cannot reject the hypothesis that the sample came form the normal distribution. Note: The Kolmogorov-Smirnov test assumes that we test against a distribution with given parameters, since, in the last case, we estimated mean and variance, this assumption is violated and the distribution of the test statistic, on which the p-value is based, is not correct. Tails of the distribution  Finally, we can check the upper tail of the distribution. We can use the percent point function ppf, which is the inverse of the cdf function, to obtain the critical values, or, more directly, we can use the inverse of the survival function In all three cases, our sample has more weight in the top tail than the underlying distribution. We can briefly check a larger sample to see if we get a closer match. In this case, the empirical frequency is quite close to the theoretical probability, but if we repeat this several times, the fluctuations are still pretty large. We can also compare it with the tail of the normal distribution, which has less weight in the tails: The chisquare test can be used to test whether for a finite number of bins, the observed frequencies differ significantly from the probabilities of the hypothesized distribution. We see that the standard normal distribution is clearly rejected, while the standard t-distribution cannot be rejected. Since the variance of our sample differs from both standard distributions, we can again redo the test taking the estimate for scale and location into account. The fit method of the distributions can be used to estimate the parameters of the distribution, and the test is repeated using probabilities of the estimated distribution. Taking account of the estimated parameters, we can still reject the hypothesis that our sample came from a normal distribution (at the 5% level), but again, with a p-value of 0.95, we cannot reject the t-distribution. Special tests for normal distributions  Since the normal distribution is the most common distribution in statistics, there are several additional functions available to test whether a sample could have been drawn from a normal distribution. First, we can test if skew and kurtosis of our sample differ significantly from those of a normal distribution: These two tests are combined in the normality test In all three tests, the p-values are very low and we can reject the hypothesis that the our sample has skew and kurtosis of the normal distribution. Since skew and kurtosis of our sample are based on central moments, we get exactly the same results if we test the standardized sample: Because normality is rejected so strongly, we can check whether the normaltest gives reasonable results for other cases: When testing for normality of a small sample of t-distributed observations and a large sample of normal-distributed observations, then in neither case can we reject the null hypothesis that the sample comes from a normal distribution. In the first case, this is because the test is not powerful enough to distinguish a t and a normally distributed random variable in a small sample. \"\\n',\n",
       " '\"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Building specific distributions  The next examples shows how to build your own distributions. Further examples show the usage of the distributions and some statistical tests. Making a continuous distribution, i.e., subclassing  Making continuous distributions is fairly simple. Interestingly, the is now computed automatically: Be aware of the performance issues mentioned in Performance issues and cautionary remarks . The computation of unspecified common methods can become very slow, since only general methods are called, which, by their very nature, cannot use any specific information about the distribution. Thus, as a cautionary example: But this is not correct: the integral over this pdf should be 1. Lets make the integration interval smaller: This looks better. However, the problem originated from the fact that the pdf is not specified in the class definition of the deterministic distribution. Subclassing  In the following, we use stats.rv_discrete to generate a discrete distribution that has the probabilities of the truncated normal for the intervals centered around the integers. General info From the docstring of rv_discrete, , You can construct an arbitrary discrete rv where P{Xxk}  pk by passing to the rv_discrete initialization method (through the values keyword) a tuple of sequences (xk, pk) which describes only those values of X (xk) that occur with nonzero probability (pk). Next to this, there are some further requirements for this approach to work: The keyword name is required. The support points of the distribution xk have to be integers. The number of significant digits (decimals) needs to be specified. In fact, if the last two requirements are not satisfied, an exception may be raised or the resulting numbers may be incorrect. An example Lets do the work. First: And, finally, we can subclass : Now that we have defined the distribution, we have access to all common methods of discrete distributions. Testing the implementation Lets generate a random sample and compare observed frequencies with the probabilities. Next, we can test whether our sample was generated by our norm-discrete distribution. This also verifies whether the random numbers were generated correctly. The chisquare test requires that there are a minimum number of observations in each bin. We combine the tail bins into larger bins so that they contain enough observations. The pvalue in this case is high, so we can be quite confident that our random sample was actually generated by the distribution. \"\\n',\n",
       " '\"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Comparing two samples  In the following, we are given two samples, which can come either from the same or from different distribution, and we want to test whether these samples have the same statistical properties. Comparing means  Test with sample with identical means: Test with sample with different means: Kolmogorov-Smirnov test for two samples ks_2samp  For the example, where both samples are drawn from the same distribution, we cannot reject the null hypothesis, since the pvalue is high In the second example, with different location, i.e., means, we can reject the null hypothesis, since the pvalue is below 1% \"\\n',\n",
       " '\"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Introduction  In this tutorial, we discuss many, but certainly not all, features of . The intention here is to provide a user with a working knowledge of this package. We refer to the reference manual for further details. Note: This documentation is work in progress. Discrete Statistical Distributions Continuous Statistical Distributions \"\\n',\n",
       " '\"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Kernel density estimation  A common task in statistics is to estimate the probability density function (PDF) of a random variable from a set of data samples. This task is called density estimation. The most well-known tool to do this is the histogram. A histogram is a useful tool for visualization (mainly because everyone understands it), but doesnt use the available data very efficiently. Kernel density estimation (KDE) is a more efficient tool for the same task. The gaussian_kde estimator can be used to estimate the PDF of univariate as well as multivariate data. It works best if the data is unimodal. Univariate estimation  We start with a minimal amount of data in order to see how gaussian_kde works and what the different options for bandwidth selection do. The data sampled from the PDF are shown as blue dashes at the bottom of the figure (this is called a rug plot): We see that there is very little difference between Scotts Rule and Silvermans Rule, and that the bandwidth selection with a limited amount of data is probably a bit too wide. We can define our own bandwidth function to get a less smoothed-out result. We see that if we set bandwidth to be very narrow, the obtained estimate for the probability density function (PDF) is simply the sum of Gaussians around each data point. We now take a more realistic example and look at the difference between the two available bandwidth selection rules. Those rules are known to work well for (close to) normal distributions, but even for unimodal distributions that are quite strongly non-normal they work reasonably well. As a non-normal distribution we take a Students T distribution with 5 degrees of freedom. We now take a look at a bimodal distribution with one wider and one narrower Gaussian feature. We expect that this will be a more difficult density to approximate, due to the different bandwidths required to accurately resolve each feature. As expected, the KDE is not as close to the true PDF as we would like due to the different characteristic size of the two features of the bimodal distribution. By halving the default bandwidth ( ), we can do somewhat better, while using a factor 5 smaller bandwidth than the default doesnt smooth enough. What we really need, though, in this case, is a non-uniform (adaptive) bandwidth. Multivariate estimation  With gaussian_kde we can perform multivariate, as well as univariate estimation. We demonstrate the bivariate case. First, we generate some random data with a model in which the two variates are correlated. Then we apply the KDE to the data: Finally, we plot the estimated bivariate distribution as a colormap and plot the individual data points on top. Multiscale Graph Correlation (MGC)  With multiscale_graphcorr , we can test for independence on high dimensional and nonlinear data. Before we start, lets import some useful packages: Lets use a custom plotting function to plot the data relationship: Lets look at some linear data first: The simulation relationship can be plotted below: Now, we can see the test statistic, p-value, and MGC map visualized below. The optimal scale is shown on the map as a red x: It is clear from here, that MGC is able to determine a relationship between the input data matrices because the p-value is very low and the MGC test statistic is relatively high. The MGC-map indicates a strongly linear relationship . Intuitively, this is because having more neighbors will help in identifying a linear relationship between and . The optimal scale in this case is equivalent to the global scale , marked by a red spot on the map. The same can be done for nonlinear data sets. The following and arrays are derived from a nonlinear simulation: The simulation relationship can be plotted below: Now, we can see the test statistic, p-value, and MGC map visualized below. The optimal scale is shown on the map as a red x: It is clear from here, that MGC is able to determine a relationship again because the p-value is very low and the MGC test statistic is relatively high. The MGC-map indicates a strongly nonlinear relationship . The optimal scale in this case is equivalent to the local scale , marked by a red spot on the map. \"\\n',\n",
       " '\"scipy_statistics_scipy_stats Statistics (scipy.stats) stats.html  Random variables  There are two general distribution classes that have been implemented for encapsulating continuous random variables and discrete random variables . Over 80 continuous random variables (RVs) and 10 discrete random variables have been implemented using these classes. Besides this, new routines and distributions can be easily added by the end user. (If you create one, please contribute it.) All of the statistics functions are located in the sub-package scipy.stats and a fairly complete listing of these functions can be obtained using . The list of the random variables available can also be obtained from the docstring for the stats sub-package. In the discussion below, we mostly focus on continuous RVs. Nearly everything also applies to discrete variables, but we point out some differences here: Specific points for discrete distributions . In the code samples below, we assume that the scipy.stats package is imported as and in some cases we assume that individual objects are imported as Getting help  First of all, all distributions are accompanied with help functions. To obtain just some basic information, we print the relevant docstring: . To find the support, i.e., upper and lower bounds of the distribution, call: We can list all methods and properties of the distribution with . As it turns out, some of the methods are private, although they are not named as such (their names do not start with a leading underscore), for example , are only available for internal calculation (those methods will give warnings when one tries to use them, and will be removed at some point). To obtain the real main methods, we list the methods of the frozen distribution. (We explain the meaning of a frozen distribution below). Finally, we can obtain the list of available distribution through introspection: Common methods  The main public methods for continuous RVs are: rvs: Random Variates pdf: Probability Density Function cdf: Cumulative Distribution Function sf: Survival Function (1-CDF) ppf: Percent Point Function (Inverse of CDF) isf: Inverse Survival Function (Inverse of SF) stats: Return mean, variance, (Fishers) skew, or (Fishers) kurtosis moment: non-central moments of the distribution Lets take a normal RV as an example. To compute the at a number of points, we can pass a list or a numpy array. Thus, the basic methods, such as pdf , cdf , and so on, are vectorized. Other generally useful methods are supported too: To find the median of a distribution, we can use the percent point function , which is the inverse of the : To generate a sequence of random variates, use the keyword argument: Note that drawing random numbers relies on generators from numpy.random package. In the example above, the specific stream of random numbers is not reproducible across runs. To achieve reproducibility, you can explicitly seed a global variable Relying on a global state is not recommended, though. A better way is to use the random_state parameter, which accepts an instance of numpy.random.RandomState class, or an integer, which is then used to seed an internal object: Dont think that generates 5 variates: Here, with no keyword is being interpreted as the first possible keyword argument, , which is the first of a pair of keyword arguments taken by all continuous distributions. This brings us to the topic of the next subsection. Shifting and scaling  All continuous distributions take and as keyword parameters to adjust the location and scale of the distribution, e.g., for the standard normal distribution, the location is the mean and the scale is the standard deviation. In many cases, the standardized distribution for a random variable is obtained through the transformation . The default values are and . Smart use of and can help modify the standard distributions in many ways. To illustrate the scaling further, the of an exponentially distributed RV with mean is given by \\\\[F(x)  1 - \\\\exp(-\\\\lambda x)\\\\] By applying the scaling rule above, it can be seen that by taking we get the proper scale. Note Distributions that take shape parameters may require more than simple application of and/or to achieve the desired form. For example, the distribution of 2-D vector lengths given a constant vector of length perturbed by independent N(0, ) deviations in each component is rice( , scale ). The first argument is a shape parameter that needs to be scaled along with . The uniform distribution is also interesting: Finally, recall from the previous paragraph that we are left with the problem of the meaning of . As it turns out, calling a distribution like this, the first argument, i.e., the 5, gets passed to set the parameter. Lets see: Thus, to explain the output of the example of the last section: generates a single normally distributed random variate with mean , because of the default . We recommend that you set and parameters explicitly, by passing the values as keywords rather than as arguments. Repetition can be minimized when calling more than one method of a given RV by using the technique of Freezing a Distribution , as explained below. Shape parameters  While a general continuous random variable can be shifted and scaled with the and parameters, some distributions require additional shape parameters. For instance, the gamma distribution with density \\\\[\\\\gamma(x, a)  \\\\frac{\\\\lambda (\\\\lambda x)^{a-1}}{\\\\Gamma(a)} e^{-\\\\lambda x}\\\\;,\\\\] requires the shape parameter . Observe that setting can be obtained by setting the keyword to . Lets check the number and name of the shape parameters of the gamma distribution. (We know from the above that this should be 1.) Now, we set the value of the shape variable to 1 to obtain the exponential distribution, so that we compare easily whether we get the results we expect. Notice that we can also specify shape parameters as keywords: Freezing a distribution  Passing the and keywords time and again can become quite bothersome. The concept of freezing a RV is used to solve such problems. By using we no longer have to include the scale or the shape parameters anymore. Thus, distributions can be used in one of two ways, either by passing all distribution parameters to each method call (such as we did earlier) or by freezing the parameters for the instance of the distribution. Let us check this: This is, indeed, what we should get. Broadcasting  The basic methods , and so on, satisfy the usual numpy broadcasting rules. For example, we can calculate the critical values for the upper tail of the t distribution for different probabilities and degrees of freedom. Here, the first row contains the critical values for 10 degrees of freedom and the second row for 11 degrees of freedom (d.o.f.). Thus, the broadcasting rules give the same result of calling twice: If the array with probabilities, i.e., and the array of degrees of freedom i.e., , have the same array shape, then element-wise matching is used. As an example, we can obtain the 10% tail for 10 d.o.f., the 5% tail for 11 d.o.f. and the 1% tail for 12 d.o.f. by calling Specific points for discrete distributions  Discrete distributions have mostly the same basic methods as the continuous distributions. However is replaced by the probability mass function , no estimation methods, such as fit, are available, and is not a valid keyword parameter. The location parameter, keyword , can still be used to shift the distribution. The computation of the cdf requires some extra attention. In the case of continuous distribution, the cumulative distribution function is, in most standard cases, strictly monotonic increasing in the bounds (a,b) and has, therefore, a unique inverse. The cdf of a discrete distribution, however, is a step function, hence the inverse cdf, i.e., the percent point function, requires a different definition: For further info, see the docs here . We can look at the hypergeometric distribution as an example If we use the cdf at some integer points and then evaluate the ppf at those cdf values, we get the initial integers back, for example If we use values that are not at the kinks of the cdf step function, we get the next higher integer back: Fitting distributions  The main additional methods of the not frozen distribution are related to the estimation of distribution parameters: fit: maximum likelihood estimation of distribution parameters, including location and scale fit_loc_scale: estimation of location and scale when shape parameters are given nnlf: negative log likelihood function expect: calculate the expectation of a function against the pdf or pmf Performance issues and cautionary remarks  The performance of the individual methods, in terms of speed, varies widely by distribution and method. The results of a method are obtained in one of two ways: either by explicit calculation, or by a generic algorithm that is independent of the specific distribution. Explicit calculation, on the one hand, requires that the method is directly specified for the given distribution, either through analytic formulas or through special functions in or for . These are usually relatively fast calculations. The generic methods, on the other hand, are used if the distribution does not specify any explicit calculation. To define a distribution, only one of pdf or cdf is necessary; all other methods can be derived using numeric integration and root finding. However, these indirect methods can be very slow. As an example, creates random variables in a very indirect way and takes about 19 seconds for 100 random variables on my computer, while one million random variables from the standard normal or from the t distribution take just above one second. Remaining issues  The distributions in have recently been corrected and improved and gained a considerable test suite; however, a few issues remain: The distributions have been tested over some range of parameters; however, in some corner ranges, a few incorrect results may remain. The maximum likelihood estimation in fit does not work with default starting parameters for all distributions and the user needs to supply good starting parameters. Also, for some distribution using a maximum likelihood estimator might inherently not be the best choice. \"']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in matched_clean[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineList = []\n",
    "with open('model_data/data/all_labels.csv') as f:\n",
    "    for line in f:\n",
    "        lineList.append(line.strip().split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caret</td>\n",
       "      <td>R</td>\n",
       "      <td>caret_11_subsampling_for_class_imbalances</td>\n",
       "      <td>11 Subsampling For Class Imbalances</td>\n",
       "      <td>subsampling-for-class-imbalances.html</td>\n",
       "      <td>11.1 Subsampling Techniques To</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caret</td>\n",
       "      <td>R</td>\n",
       "      <td>caret_11_subsampling_for_class_imbalances</td>\n",
       "      <td>11 Subsampling For Class Imbalances</td>\n",
       "      <td>subsampling-for-class-imbalances.html</td>\n",
       "      <td>11.2 Subsampling During Resampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caret</td>\n",
       "      <td>R</td>\n",
       "      <td>caret_11_subsampling_for_class_imbalances</td>\n",
       "      <td>11 Subsampling For Class Imbalances</td>\n",
       "      <td>subsampling-for-class-imbalances.html</td>\n",
       "      <td>11.3 Complications The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>caret</td>\n",
       "      <td>R</td>\n",
       "      <td>caret_11_subsampling_for_class_imbalances</td>\n",
       "      <td>11 Subsampling For Class Imbalances</td>\n",
       "      <td>subsampling-for-class-imbalances.html</td>\n",
       "      <td>11.4 Using Custom Subsampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>caret</td>\n",
       "      <td>R</td>\n",
       "      <td>caret_12_using_recipes_with_train</td>\n",
       "      <td>12 Using Recipes with train</td>\n",
       "      <td>using-recipes-with-train.html</td>\n",
       "      <td>12.1 Why Should</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>scipy</td>\n",
       "      <td>Python</td>\n",
       "      <td>scipy_statistics_scipy_stats</td>\n",
       "      <td>Statistics (scipy.stats)</td>\n",
       "      <td>stats.html</td>\n",
       "      <td>Building specific distributions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>scipy</td>\n",
       "      <td>Python</td>\n",
       "      <td>scipy_statistics_scipy_stats</td>\n",
       "      <td>Statistics (scipy.stats)</td>\n",
       "      <td>stats.html</td>\n",
       "      <td>Comparing two samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>scipy</td>\n",
       "      <td>Python</td>\n",
       "      <td>scipy_statistics_scipy_stats</td>\n",
       "      <td>Statistics (scipy.stats)</td>\n",
       "      <td>stats.html</td>\n",
       "      <td>Introduction  In this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>scipy</td>\n",
       "      <td>Python</td>\n",
       "      <td>scipy_statistics_scipy_stats</td>\n",
       "      <td>Statistics (scipy.stats)</td>\n",
       "      <td>stats.html</td>\n",
       "      <td>Kernel density estimation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>scipy</td>\n",
       "      <td>Python</td>\n",
       "      <td>scipy_statistics_scipy_stats</td>\n",
       "      <td>Statistics (scipy.stats)</td>\n",
       "      <td>stats.html</td>\n",
       "      <td>Random variables  There</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1                                          2  \\\n",
       "0    caret       R  caret_11_subsampling_for_class_imbalances   \n",
       "1    caret       R  caret_11_subsampling_for_class_imbalances   \n",
       "2    caret       R  caret_11_subsampling_for_class_imbalances   \n",
       "3    caret       R  caret_11_subsampling_for_class_imbalances   \n",
       "4    caret       R          caret_12_using_recipes_with_train   \n",
       "..     ...     ...                                        ...   \n",
       "470  scipy  Python               scipy_statistics_scipy_stats   \n",
       "471  scipy  Python               scipy_statistics_scipy_stats   \n",
       "472  scipy  Python               scipy_statistics_scipy_stats   \n",
       "473  scipy  Python               scipy_statistics_scipy_stats   \n",
       "474  scipy  Python               scipy_statistics_scipy_stats   \n",
       "\n",
       "                                       3  \\\n",
       "0    11 Subsampling For Class Imbalances   \n",
       "1    11 Subsampling For Class Imbalances   \n",
       "2    11 Subsampling For Class Imbalances   \n",
       "3    11 Subsampling For Class Imbalances   \n",
       "4            12 Using Recipes with train   \n",
       "..                                   ...   \n",
       "470             Statistics (scipy.stats)   \n",
       "471             Statistics (scipy.stats)   \n",
       "472             Statistics (scipy.stats)   \n",
       "473             Statistics (scipy.stats)   \n",
       "474             Statistics (scipy.stats)   \n",
       "\n",
       "                                         4                                   5  \n",
       "0    subsampling-for-class-imbalances.html      11.1 Subsampling Techniques To  \n",
       "1    subsampling-for-class-imbalances.html  11.2 Subsampling During Resampling  \n",
       "2    subsampling-for-class-imbalances.html              11.3 Complications The  \n",
       "3    subsampling-for-class-imbalances.html       11.4 Using Custom Subsampling  \n",
       "4            using-recipes-with-train.html                     12.1 Why Should  \n",
       "..                                     ...                                 ...  \n",
       "470                             stats.html     Building specific distributions  \n",
       "471                             stats.html               Comparing two samples  \n",
       "472                             stats.html               Introduction  In this  \n",
       "473                             stats.html           Kernel density estimation  \n",
       "474                             stats.html             Random variables  There  \n",
       "\n",
       "[475 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lineList[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3",
   "language": "python",
   "name": ".venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
