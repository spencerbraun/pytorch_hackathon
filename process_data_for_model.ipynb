{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import signal\n",
    "from math import ceil\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.random import choice\n",
    "from torchtext.data import Field, TabularDataset\n",
    "\n",
    "import glob\n",
    "from io import StringIO\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model_data/data'):\n",
    "    os.mkdir('model_data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'model_data/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_glob = sorted(glob.glob('sklearn/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for file in sk_glob:\n",
    "    with open(file) as f:\n",
    "        samples.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{base_path}sklearn.csv', 'w') as f:\n",
    "    doc_text = \"text\\n\" + \"\\n\".join(['\"' + sample.replace('\\n', ' ') + '\"' for sample in samples])\n",
    "    f.write(doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "caret_glob = sorted(glob.glob('caret/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "caret_samples = []\n",
    "for file in caret_glob:\n",
    "    with open(file) as f:\n",
    "        caret_samples.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caret_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{base_path}caret.csv', 'w') as f:\n",
    "    doc_text = \"text\\n\" + \"\\n\".join(['\"' + sample.replace('\\n', ' ') + '\"' for sample in samples])\n",
    "    f.write(doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_glob = sorted(glob.glob('numpy/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_samples = []\n",
    "for file in numpy_glob:\n",
    "    with open(file) as f:\n",
    "        numpy_samples.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numpy_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{base_path}numpy.csv', 'w') as f:\n",
    "    doc_text = \"text\\n\" + \"\\n\".join(['\"' + sample.replace('\\n', ' ') + '\"' for sample in samples])\n",
    "    f.write(doc_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_glob = sorted(glob.glob('scipy/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_samples = []\n",
    "for file in scipy_glob:\n",
    "    with open(file) as f:\n",
    "        scipy_samples.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scipy_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{base_path}scipy.csv', 'w') as f:\n",
    "    doc_text = \"text\\n\" + \"\\n\".join(['\"' + sample.replace('\\n', ' ') + '\"' for sample in samples])\n",
    "    f.write(doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caret.csv',\n",
       " 'numpy.csv',\n",
       " 'sklearn.csv',\n",
       " 'all_data.csv',\n",
       " 'scipy.csv',\n",
       " 'all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.750086.csv']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "text\n",
      "text\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "filenames = [\n",
    "    'caret.csv',\n",
    "    'numpy.csv',\n",
    "    'sklearn.csv',\n",
    "    'scipy.csv'\n",
    "]\n",
    "with open(base_path + 'all_data.csv', 'w') as outfile:\n",
    "    outfile.write(\"text\\n\")\n",
    "    for fname in filenames:\n",
    "        with open(base_path + fname) as infile:\n",
    "            for line in infile:\n",
    "                if line.strip() == 'text':\n",
    "                    print(\"text\")\n",
    "                    continue\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output vs Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lines = []\n",
    "with open('model_data/data/all_data.csv') as f:\n",
    "    for line in f:\n",
    "        input_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lines = []\n",
    "with open('model_data/data/all_data_model.dbow_numnoisewords.2_vecdim.100_batchsize.32_lr.0.001000_epoch.97_loss.0.750086.csv') as f:\n",
    "    for line in f:\n",
    "        output_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14,d15,d16,d17,d18,d19,d20,d21,d22,d23,d24,d25,d26,d27,d28,d29,d30,d31,d32,d33,d34,d35,d36,d37,d38,d39,d40,d41,d42,d43,d44,d45,d46,d47,d48,d49,d50,d51,d52,d53,d54,d55,d56,d57,d58,d59,d60,d61,d62,d63,d64,d65,d66,d67,d68,d69,d70,d71,d72,d73,d74,d75,d76,d77,d78,d79,d80,d81,d82,d83,d84,d85,d86,d87,d88,d89,d90,d91,d92,d93,d94,d95,d96,d97,d98,d99\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sklearn_2_1_gaussian_mixture_models',\n",
       " '2.1.',\n",
       " 'Gaussian',\n",
       " 'mixture',\n",
       " 'models',\n",
       " 'modules/mixture.html',\n",
       " '',\n",
       " '2.1.2.',\n",
       " 'Variational',\n",
       " 'Bayesian',\n",
       " 'Gaussian',\n",
       " 'Mixture',\n",
       " '',\n",
       " 'The',\n",
       " 'BayesianGaussianMixture']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lines[100].strip('\"').split(\" \")[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_index = []\n",
    "for sample in samples:\n",
    "\n",
    "    sample_split = sample.split(\"\\n\")\n",
    "    page_id = sample_split[0]\n",
    "    page_name = sample_split[1]\n",
    "    link = sample_split[2]\n",
    "    text = sample_split[3]\n",
    "    try:\n",
    "        field_name = re.search(\"^[0-9.]*[ ][A-Z][-a-zA-Z]*(?:\\s+[A-Z][-a-zA-Z]*)*\", text.strip()).group(0)\n",
    "    except AttributeError:\n",
    "        field_name = \" \".join(text.strip().split(\" \")[0:10])\n",
    "    sklearn_index.append((page_id, page_name, link, field_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sklearn_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "caret_index = []\n",
    "for caret_sample in caret_samples:\n",
    "    sample_split = caret_sample.split(\"\\n\")\n",
    "    page_id = sample_split[0]\n",
    "    page_name = sample_split[1]\n",
    "    link = sample_split[2]\n",
    "    text = sample_split[3]\n",
    "\n",
    "    field_name = re.search(\"^[0-9.]*[ ][A-Z][-a-zA-Z]*(?:\\s+[A-Z][-a-zA-Z]*)*\", text.strip()).group(0)\n",
    "    caret_index.append((page_id, page_name, link, field_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('caret_11_subsampling_for_class_imbalances',\n",
       " '11 Subsampling For Class Imbalances',\n",
       " 'subsampling-for-class-imbalances.html',\n",
       " '11.1 Subsampling')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.0.9 Distance Weighted Discrimination'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"^[0-9.]*[ ][A-Z][-a-zA-Z]*(?:\\s+[A-Z][-a-zA-Z]*)*\", text.strip()).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = []\n",
    "for line_idx in range(len(output_lines)):\n",
    "    matched.append((input_lines[line_idx], output_lines[line_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_clean = [x for x in matched if not re.match(\".*[A-Z].*\", x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_clean_vectors = [x[1].split(\",\") for x in matched_clean[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_matrix = np.array(matched_clean_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481, 100)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "distances = distance.cdist([vec_matrix[5]], vec_matrix, \"cosine\")[0]\n",
    "# min_index = np.argmin(distances)\n",
    "ind = np.argpartition(a, 5)[:5]\n",
    "sorted_ind = ind[np.argsort(distances[ind])]\n",
    "min_distances = distances[sorted_ind]\n",
    "max_similarity = [1 - x for x in min_distances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6670743594734331,\n",
       " 0.6652293382107732,\n",
       " 0.6196681421232006,\n",
       " 0.5960319745742784,\n",
       " 0.5836655087197274]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3",
   "language": "python",
   "name": ".venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
